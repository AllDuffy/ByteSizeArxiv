{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import urllib.request as libreq\n",
    "import feedparser\n",
    "import pdfminer.layout\n",
    "import pdfminer.high_level\n",
    "from io import StringIO\n",
    "from pdfminer.layout import LAParams\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "import re\n",
    "import heapq\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=pdfminer.high_level.extract_text('C:\\\\Users\\\\Al\\\\Documents\\\\ByteSizeArxiv\\\\library/9905014v1.pdf', codec='utf-8', laparams=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9\\n9\\n9\\n1\\n \\ny\\na\\nM\\n1\\n2\\n \\n \\n]\\n\\n \\n\\nG\\nL\\n.\\ns\\nc\\n[\\n \\n \\n1\\nv\\n4\\n1\\n0\\n5\\n0\\n9\\n9\\n/\\ns\\nc\\n:\\nv\\ni\\nX\\nr\\na\\n\\nHierarchical Reinforcement Learning with the MAXQ Value\\nFunction Decomposition\\n\\nThomas G. Dietterich\\nDepartment of Computer Science\\nOregon State University\\nCorvallis, OR 97331\\ntgd@cs.orst.edu\\n\\nFebruary 1, 2008\\n\\nAbstract\\n\\nThis paper presents a new approach to hierarchical reinforcement learning based on decomposing\\nthe target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing\\nthe value function of the target MDP into an additive combination of the value functions of the\\nsmaller MDPs. The decomposition, known as the MAXQ decomposition, has both a procedural\\nsemantics—as a subroutine hierarchy—and a declarative semantics—as a representation of the\\nvalue function of a hierarchical policy. MAXQ uniﬁes and extends previous work on hierar-\\nchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. It is based on the\\nassumption that the programmer can identify useful subgoals and deﬁne subtasks that achieve\\nthese subgoals. By deﬁning such subgoals, the programmer constrains the set of policies that\\nneed to be considered during reinforcement learning. The MAXQ value function decomposition\\ncan represent the value function of any policy that is consistent with the given hierarchy. The\\ndecomposition also creates opportunities to exploit state abstractions, so that individual MDPs\\nwithin the hierarchy can ignore large parts of the state space. This is important for the practical\\napplication of the method. This paper deﬁnes the MAXQ hierarchy, proves formal results on its\\nrepresentational power, and establishes ﬁve conditions for the safe use of state abstractions. The\\npaper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges\\nwih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even\\nin the presence of the ﬁve kinds of state abstraction. The paper evaluates the MAXQ represen-\\ntation and MAXQ-Q through a series of experiments in three domains and shows experimentally\\nthat MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster\\nthan ﬂat Q learning. The fact that MAXQ learns a representation of the value function has an\\nimportant beneﬁt: it makes it possible to compute and execute an improved, non-hierarchical\\npolicy via a procedure similar to the policy improvement step of policy iteration. The paper\\ndemonstrates the eﬀectiveness of this non-hierarchical execution experimentally. Finally, the\\npaper concludes with a comparison to related work and a discussion of the design tradeoﬀs in\\nhierarchical reinforcement learning.\\n\\n1\\n\\n\\x0c1 Introduction\\n\\nA central goal of artiﬁcial intelligence is to develop techniques for constructing robust, autonomous\\nagents that are able to achieve good performance in complex, real-world environments. One fruitful\\nline of research views agents from an “economic” perspective (Boutilier, Shoham, & Wellman, 1997):\\nAn agent interacts with an environment and receives real-valued rewards and penalties. The agent’s\\ngoal is to maximize the total reward it receives. The economic view makes it easy to formalize\\ntraditional goals of achievement (“land this airplane”). But it also makes it easy to formulate\\ngoals of prevention (“don’t crash into any other airplanes”) and goals of maintenance (“keep the\\nair-traﬃc control system working as long as possible”). Goals of achievement can be represented\\nby giving a positive reward for achieving the goal. Goals of prevention can be represented by\\ngiving a negative reward when bad events occur, and goals of maintenance can be represented by\\ngiving a positive reward for each time step that the desireable state is maintained. Furthermore,\\nthe economic formalism makes it possible to incorporate uncertainty—we can require the agent to\\nmaximize the expected value of the total reward in the face of random events in the world.\\n\\nThis brief review shows that the economic approach is very expressive—a diﬃcult research chal-\\nlenge, however, is to develop eﬃcient and scalable methods for reasoning, planning, and learning\\nwithin the economic AI framework. The area of Stochastic Planning studies methods for ﬁnding\\noptimal or near-optimal plans to maximize expected total reward in the case where the agent has\\ncomplete knowledge of the probabilistic behavior of the environment and the reward function. The\\nbasic methods for this case were developed in the 1950s in the ﬁeld of “Dynamic Programming.”\\nUnfortunately, these methods require time polynomial in the number of states in the state space,\\nwhich makes them prohibitively expensive for most AI problems. Hence, recent research has fo-\\ncused on methods that can exploit structure within the planning problem to work more eﬃciently\\n(Boutilier, Dean, & Hanks, 1999).\\n\\nThe area of Reinforcement Learning (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998) stud-\\nies methods for learning optimal or near-optimal plans by interacting directly with the external\\nenvironment (as opposed to analyzing a user-provided model of the environment). Again, the basic\\nmethods in reinforcement learning are based on dynamic programming algorithms. However, rein-\\nforcement learning methods oﬀer two important advantages over classical dynamic programming.\\nFirst, the methods are online. This permits them to focus their attention on the parts of the state\\nspace that are important and ignore the rest of the space. Second, the methods can employ function\\napproximation algorithms (e.g., neural networks) to represent their knowledge. This allows them\\nto generalize across the state space so that the learning time scales much better.\\n\\nDespite the recent advances in both probabilistic planning and reinforcement learning, there\\nare still many shortcomings. The biggest of these is the lack of a fully satisfactory method for\\nincorporating hierarchies into these algorithms. Research in classical planning has shown that\\nhierarchical methods such as hierarchical task networks (Currie & Tate, 1991), macro actions (Fikes,\\nHart, & Nilsson, 1972; Korf, 1985), and state abstraction methods (Sacerdoti, 1974; Knoblock,\\n1990) can provide exponential reductions in the computational cost of ﬁnding good plans. However,\\nall of the basic algorithms for probabilistic planning and reinforcement learning are “ﬂat” methods—\\nthey treat the state space as one huge ﬂat search space. This means that the paths from the start\\nstate to the goal state are very long, and the length of these paths determines the cost of learning\\nand planning, because information about future rewards must be propagated backward along these\\npaths.\\n\\nMany researchers (Singh, 1992a; Lin, 1993; Kaelbling, 1993; Dayan & Hinton, 1993; Hauskrecht,\\nMeuleau, Boutilier, Kaelbling, & Dean, 1998; Parr & Russell, 1998; Sutton, Precup, & Singh, 1998)\\nhave experimented with diﬀerent methods of hierarchical reinforcement learning and hierarchical\\n\\n2\\n\\n\\x0cprobabilistic planning. This research has explored many diﬀerent points in the design space of\\nhierarchical methods, but several of these systems were designed for speciﬁc situations. We lack\\ncrisp deﬁnitions of the main approaches and a clear understanding of the relative merits of the\\ndiﬀerent methods.\\n\\nThis paper formalizes and clariﬁes one approach and attempts to understand how it compares\\nwith the other techniques. The approach, called the MAXQ method, provides a hierarchical decom-\\nposition of the given reinforcement learning problem into a set of subproblems. It simultaneously\\nprovides a decomposition of the value function for the given problem into a set of value functions\\nfor the subproblems. Hence, it has both a declarative semantics (as a value function decomposition)\\nand a procedural semantics (as a subroutine hierarchy).\\n\\nA review of previous research shows that there are several important design decisions that must\\nbe made when constructing a hierarchical reinforcement learning system. As a way of providing\\nan overview of the results in this paper, let us review these issues and see how the MAXQ method\\napproaches each of them.\\n\\nThe ﬁrst issue is how subtasks should be speciﬁed. Hierarchical reinforcement learning involves\\nbreaking the target Markov decision problem into a hierarchy of subproblems or subtasks. There\\nare three general approaches to deﬁning these subtasks. One approach is to deﬁne each subtask\\nin terms of a ﬁxed policy that is provided by the programmer. The “option” method of Sutton,\\nPrecup, and Singh (1998) takes this approach. The second approach is to deﬁne each subtask in\\nterms of a non-deterministic ﬁnite-state controller. The Hierarchy of Abstract Machines (HAM)\\nmethod of Parr and Russell (1998) takes this approach. This method permits the programmer\\nto provide a “partial policy” that constrains the set of permitted actions at each point, but does\\nnot specify a complete policy for each subtask. The third approach is to deﬁne each subtask in\\nterms of a termination predicate and a local reward function. These deﬁne what it means for the\\nsubtask to be completed and what the ﬁnal reward should be for completing the subtask. The\\nMAXQ method described in this paper follows this approach, building upon previous work by\\nSingh (1992a), Kaelbling (1993), Dayan and Hinton (1993), and Dean and Lin (1995).\\n\\nAn advantage of the “option” and partial policy approaches is that the subtask can be deﬁned in\\nterms of an amount of eﬀort or a course of action rather than in terms of achieving a particular goal\\ncondition. However, the “option” approach (at least in the simple form described here), requires\\nthe programmer to provide complete policies for the subtasks, which can be a diﬃcult programming\\ntask in real-world problems. On the other hand, the termination predicate method requires the\\nprogrammer to guess the relative desirability of the diﬀerent states in which the subtask might\\nterminate. This can also be diﬃcult, although Dean and Lin show how these guesses can be revised\\nautomatically by the learning algorithm.\\n\\nA potential drawback of all hierarchical methods is that the learned policy may be suboptimal.\\nThe programmer-provided hierarchy constrains the set of possible policies that can be considered. If\\nthese constraints are poorly chosen, the resulting policy will be suboptimal. Nonetheless, the learn-\\ning algorithms that have been developed for the “option” and partial policy approaches guarantee\\nthat the learned policy will be the best possible policy consistent with these constraints.\\n\\nThe termination predicate method suﬀers from an additional source of suboptimality. The\\nlearning algorithm described in this paper converges to a form of local optimality that we call\\nrecursive optimality. This means that the policy of each subtask is locally optimal given the policies\\nof its children. But there might exist better hierarchical policies where the policy for a subtask\\nmust be locally suboptimal so that the overall policy is optimal. This problem can be avoided by\\ncareful deﬁnition of termination predicates and local reward functions, but this is an added burden\\non the programmer. (It is interesting to note that this problem of recursive optimality has not been\\nnoticed previously. This is because previous work focused on subtasks with a single terminal state,\\n\\n3\\n\\n\\x0cand in such cases, the problem does not arise.)\\n\\nThe second design issue is whether to employ state abstractions within subtasks. A subtask\\nemploys state abstraction if it ignores some aspects of the state of the environment. For example,\\nin many robot navigation problems, choices about what route to take to reach a goal location are\\nindependent of what the robot is currently carrying. With few exceptions, state abstraction has\\nnot been explored previously. We will see that the MAXQ method creates many opportunities\\nto exploit state abstraction, and that these abstractions can have a huge impact in accelerating\\nlearning. We will also see that there is an important design tradeoﬀ: the successful use of state\\nabstraction requires that subtasks be deﬁned in terms of termination predicates rather than using\\nthe option or partial policy methods. This is why the MAXQ method must employ termination\\npredicates, despite the problems that this can create.\\n\\nThe third design issue concerns the non-hierarchical “execution” of a learned hierarchical pol-\\nicy. Kaelbling (1993) was the ﬁrst to point out that a value function learned from a hierarchical\\npolicy could be evaluated incrementally to yield a potentially much better non-hierarchical policy.\\nDietterich (1998) and Sutton, Singh, Precup, and Ravindran (1999) generalized this to show how\\narbitrary subroutines could be executed non-hierarchically to yield improved policies. However, in\\norder to support this non-hierarchical execution, extra learning is required. Ordinarily, in hierar-\\nchical reinforcement learning, the only states where learning is required at the higher levels of the\\nhierarchy are states where one or more of the subroutines could terminate (plus all possible initial\\nstates). But to support non-hierarchical execution, learning is required in all states (and at all levels\\nof the hierarchy). In general, this requires additional exploration as well as additional computation\\nand memory. As a consequence of the hierarchical decomposition of the value function, the MAXQ\\nmethod is able to support either form of execution, and we will see that there are many problems\\nwhere the improvement from non-hierarchical execution is worth the added cost.\\n\\nThe fourth and ﬁnal issue is what form of learning algorithm to employ. An important advantage\\nof reinforcement learning algorithms is that they typically operate online. However, ﬁnding online\\nalgorithms that work for general hierarchical reinforcement learning has been diﬃcult, particularly\\nwithin the termination predicate family of methods. Singh’s method relied on each subtask having\\na unique terminal state; Kaelbling employed a mix of online and batch algorithms to train her\\nhierarchy; and work within the “options” framework usually assumes that the policies for the\\nsubproblems are given and do not need to be learned at all. The best previous online algorithms\\nare the HAMQ Q learning algorithm of Parr and Russell (for the partial policy method) and the\\nFeudal Q algorithm of Dayan and Hinton. Unfortunately, the HAMQ method requires “ﬂattening”\\nthe hierarchy, and this has several undesirable consequences. The Feudal Q algorithm is tailored\\nto a speciﬁc kind of problem, and it does not converge to any well-deﬁned optimal policy.\\n\\nIn this paper, we present a general algorithm, called MAXQ-Q, for fully-online learning of a\\nhierarchical value function. We show experimentally and theoretically that the algorithm converges\\nto a recursively optimal policy. We also show that it is substantially faster than “ﬂat” (i.e., non-\\nhierarchical) Q learning when state abstractions are employed. Without state abstractions, it gives\\nperformance similar to (or even worse than) the HAMQ algorithm.\\n\\nThe remainder of this paper is organized as follows. After introducing our notation in Section\\n2, we deﬁne the MAXQ value function decomposition in Section 3 and illustrate it with a sim-\\nple example Markov decision problem. Section 4 presents an analytically tractable version of the\\nMAXQ-Q learning algorithm called the MAXQ-0 algorithm and proves its convergence to a recur-\\nsively optimal policy. It then shows how to extend MAXQ-0 to produce the MAXQ-Q algorithm,\\nand shows how to extend the theorem similarly. Section 5 takes up the issue of state abstraction\\nand formalizes a series of ﬁve conditions under which state abstractions can be safely incorporated\\ninto the MAXQ representation. State abstraction can give rise to a hierarchical credit assignment\\n\\n4\\n\\n\\x0cproblem, and the paper brieﬂy discusses one solution to this problem. Finally, Section 7 presents\\nexperiments with three example domains. These experiments give some idea of the generality of\\nthe MAXQ representation. They also provide results on the relative importance of temporal and\\nstate abstractions and on the importance of non-hierarchical execution. The paper concludes with\\nfurther discussion of the design issues that were brieﬂy described above, and in particular, it tackles\\nthe question of the tradeoﬀ between the method of deﬁning subtasks (via termination predicates)\\nand the ability to exploit state abstractions.\\n\\nSome readers may be disappointed that MAXQ provides no way of learning the structure of\\nthe hierarchy. Our philosophy in developing MAXQ (which we share with other reinforcement\\nlearning researchers, notably Parr and Russell) has been to draw inspiration from the development\\nof Belief Networks (Pearl, 1988). Belief networks were ﬁrst introduced as a formalism in which\\nthe knowledge engineer would describe the structure of the networks and domain experts would\\nprovide the necessary probability estimates. Subsequently, methods were developed for learning\\nthe probability values directly from observational data. Most recently, several methods have been\\ndeveloped for learning the structure of the belief networks from data, so that the dependence on\\nthe knowledge engineer is reduced.\\n\\nIn this paper, we will likewise require that the programmer provide the structure of the hierarchy.\\nThe programmer will also need to make several important design decisions. We will see below that\\na MAXQ representation is very much like a computer program, and we will rely on the programmer\\nto design each of the modules and indicate the permissible ways in which the modules can invoke\\neach other. Our learning algorithms will ﬁll in “implementations” of each module in such a way\\nthat the overall program will work well. We believe that this approach will provide a practical tool\\nfor solving large real-world MDPs. We also believe that it will help us understand the structure of\\nhierarchical learning algorithms. It is our hope that subsequent research will be able to automate\\nmost of the work that we are currently requiring the programmer to do.\\n\\n2 Formal Deﬁnitions\\n\\n2.1 Markov Decision Problems and Semi-Markov Decision Problems\\n\\nWe employ the standard deﬁnitions for Markov Decision Problems and Semi-Markov Decision\\nProblems.\\n\\nIn this paper, we restrict our attention to situations in which an agent is interacting with a fully-\\nobservable stochastic environment. This situation can be modeled as a Markov Decision Problem\\n(MDP) hS, A, P, R, P0i deﬁned as follows:\\n\\n• S: this is the set of states of the environment. At each point in time, the agent can observe\\n\\nthe complete state of the environment.\\n\\n• A: this is a ﬁnite set of actions. Technically, the set of available actions depends on the\\n\\ncurrent state s, but we will suppress this dependence in our notation.\\n\\n• P : When an action a ∈ A is performed, the environment makes a probabilistic transition from\\nits current state s to a resulting state s′ according to the probability distribution P (s′|s, a).\\n\\n• R: Similarly, when action a is performed and the environment makes its transition from s\\nto s′, the agent receives a real-valued (possibly stochastic) reward R(s′|s, a). To simplify the\\nnotation, it is customary to treat this reward as being given at the time that action a is\\ninitiated, even though it may in general depend on s′ as well as on s and a.\\n\\n5\\n\\n\\x0c• P0: This is the starting state distribution. When the MDP is initialized, it is in state s with\\n\\nprobability P0(s).\\n\\nA policy, π, is a mapping from states to actions that tells what action a = π(s) to perform when\\nthe environment is in state s.\\n\\nWe will consider two settings: Episodic and Inﬁnite-Horizon.\\nIn the episodic setting, all rewards are ﬁnite and there is at least one zero-cost absorbing\\nterminal state. An absorbing terminal state is a state in which all actions lead back to the same\\nstate with probability 1 and zero reward. We will only consider problems where all deterministic\\npolicies are “proper”—that is, all deterministic policies have a non-zero probability of reaching a\\nterminal state when started in an arbitrary state. In this setting, the goal of the agent is to ﬁnd\\na policy that maximizes the expected cumulative reward. In the special case where all rewards\\nare non-positive, these problems are referred to as stochastic shortest path problems, because the\\nrewards can be viewed as costs (i.e., lengths), and the policy attempts to move the agent along the\\npath of minimum expected cost.\\n\\nIn the inﬁnite horizon setting, all rewards are also ﬁnite. In addition, there is a discount factor\\nγ, and the agent’s goal is to ﬁnd a policy that minimizes the inﬁnite discounted sum of future\\nrewards.\\n\\nThe value function V π for policy π is a function that tells, for each state s, what the expected\\ncumulative reward will be of executing that policy. Let rt be a random variable that tells the reward\\nthat the agent receives at time step t while following policy π. We can deﬁne the value function in\\nthe episodic setting as\\n\\nV π(s) = E {rt + rt+1 + rt+2 + · · · |st = t, π} .\\n\\nIn the discounted setting, the value function is\\n\\nV π(s) = E\\n\\nrt + γrt+1 + γ2rt+2 + · · ·\\n\\nst = t, π\\n\\n.\\n\\nWe can see that this equation reduces to the previous one when γ = 1. However, in the inﬁnite\\nhorizon case, this inﬁnite sum will not converge unless γ < 1.\\n\\nThe value function satisﬁes the Bellman equation for a ﬁxed policy:\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\no\\n\\nV π(s) =\\n\\n′\\n\\nP (s\\n\\n|s, π(s))\\n\\nR(s\\n\\n′\\n\\n|s, π(s)) + γV π(s\\n\\n′\\n\\n)\\n\\n.\\n\\n(cid:2)\\nThe quantity on the right-hand side is called the backed-up value of performing action a in state s.\\nFor each possible successor state s′, it computes the reward that would be received and the value\\nof the resulting state and then weights those according to the probability of ending up in s′.\\n\\nThe optimal value function V ∗ is the value function that simultaneously maximizes the expected\\ncumulative reward in all states s ∈ S. Bellman (1957) proved that it is the unique solution to what\\nis now known as the Bellman equation:\\n\\n(cid:3)\\n\\n∗\\n\\nV\\n\\n(s) = max\\n\\nP (s\\n\\n|s, a)\\n\\nR(s\\n\\n|s, a) + γV\\n\\n′\\n\\n′\\n\\n∗\\n\\n′\\n\\n(s\\n\\n)\\n\\n.\\n\\n(1)\\n\\na\\n\\nXs′\\n\\n(cid:2)\\n\\n(cid:3)\\n\\nThere may be many optimal policies that achieve this value. Any policy that chooses a in s to\\nachieve the maximum on the right-hand side of this equation is an optimal policy. We will denote\\nan optimal policy by π∗. Note that all optimal policies are “greedy” with respect to the backed-up\\nvalue of the available actions.\\n\\nClosely related to the value function is the so-called action-value function, or Q function\\n(Watkins, 1989). This function, Qπ(s, a), gives the expected cumulative reward of performing\\n\\nn\\n\\nXs′\\n\\n6\\n\\n\\x0cXs′\\n\\nXs′\\n\\n(cid:2)\\n\\n(cid:20)\\n\\naction a in state s and then following policy π thereafter. The Q function also satisﬁes a Bellman\\nequation:\\n\\nQπ(s, a) =\\n\\n′\\n\\nP (s\\n\\n|s, a)\\n\\nR(s\\n\\n′\\n\\n|s, a) + γQπ(s\\n\\n′\\n\\n′\\n\\n, π(s\\n\\n))\\n\\n.\\n\\nThe optimal action-value function is written Q∗(s, a), and it satisﬁes the equation\\n\\n(cid:3)\\n\\n∗\\n\\nQ\\n\\n(s, a) =\\n\\n′\\n\\n′\\n\\nP (s\\n\\n|s, a)\\n\\nR(s\\n\\n|s, a) + γ max\\n\\n∗\\n\\n′\\n\\n′\\n, a\\n\\n(s\\n\\na′ Q\\n\\n.\\n\\n)\\n(cid:21)\\n\\n(2)\\n\\nNote that any policy that is greedy with respect to Q∗ is an optimal policy. There may be many\\nsuch optimal policies—they diﬀer only in how they break ties between actions with identical Q∗\\nvalues.\\n\\nAn action order, denoted ω, is a total order over the actions within an MDP. That is, ω is an\\nanti-symmetric, transitive relation such that ω(a1, a2) is true iﬀ a1 is preferred to a2. An ordered\\ngreedy policy, πω is a greedy policy that breaks ties using ω. For example, suppose that the two best\\nactions at state s are a1 and a2, that Q(s, a1) = Q(s, a2), and that ω(a1, a2). Then the ordered\\ngreedy policy πω will choose a1: πω(s) = a1. Note that although there may be many optimal\\npolicies for a given MDP, the ordered greedy policy, π∗\\n\\nω, is unique.\\n\\nA discrete-time semi-Markov Decision Process (SMDP) is a generalization of the Markov Deci-\\nsion Process in which the actions can take a variable amount of time to complete. In particular, let\\nthe random variable N denote the number of time steps that action a takes when it is executed in\\nstate s. We can extend the state transition probability function to be the joint distribution of the\\nresult states s′ and the number of time steps N when action a is performed in state s: P (s′, N |s, a).\\nSimilarly, the reward function can be changed to be R(s′, N |s, a).1\\n\\nIt is straightforward to modify the Bellman equation to deﬁne the value function for a ﬁxed\\n\\npolicy π as\\n\\nV π(s) =\\n\\n′\\n\\nP (s\\n\\n, N |s, π(s))\\n\\nR(s\\n\\n′\\n\\n, N |s, π(s)) + γN V π(s\\n\\n′\\n\\n)\\n\\n.\\n\\nXs′,N\\n\\nh\\nThe only change is that the expected value on the right-hand side is taken with respect to both s′\\nand N , and γ is raised to the power N to reﬂect the variable amount of time that may elapse while\\nexecuting action a.\\n\\ni\\n\\nNote that because expectation is a linear operator, we can write each of these Bellman equations\\nas the sum of the expected reward for performing action a and the expected value of the resulting\\nstate s. For example, we can rewrite the equation above as\\n\\nV π(s) = R(s, π(s)) +\\n\\n′\\n\\nP (s\\n\\n, N |s, π(s))γN V π(s\\n\\n′\\n\\n).\\n\\n(3)\\n\\nwhere R(s, π(s)) is the expected reward of performing action π(s) in state s, where the expectation\\nis taken with respect to s′ and N .\\n\\nNote that for the episodic case, there is no diﬀerence between a MDP and a Semi-Markov\\n\\nDecision Process.\\n\\n1This formalization is slightly diﬀerent than the standard formulation of SMDPs, which separates P (s′|s, a) and\\nF (t|s, a), where F is the cumulative distribution function for the probability that a will terminate in t time units,\\nwhere t is real-valued rather than integer-valued. In our case, it is important to consider the joint distribution of s′\\nand N , but we do not need to consider actions with arbitrary real-valued durations.\\n\\nXs′,N\\n\\n7\\n\\n\\x0c2.2 Reinforcement Learning Algorithms\\n\\nA reinforcement learning algorithm is an algorithm that is given access to an unknown MDP via\\nthe following reinforcement learning protocol. At each time step t, the algorithm is told the current\\nstate s of the MDP and the set of actions A(s) ⊆ A that are executable in that state. The algorithm\\nchooses an action a ∈ A(s), and the MDP executes this action (which causes it to move to state\\ns’) and returns a real-valued reward r. If s is an absorbing terminal state, the set of actions A(s)\\ncontains only the special action reset, which causes the MDP to move to one of its initial states,\\ndrawn according to P0.\\n\\nThe learning algorithm is evaluated based on its observed cumulative reward. The cumulative\\nreward of a good learning algorithm should converge to the cumulative reward of the optimal policy\\nfor the MDP.\\n\\nIn this paper, we will make use of two well-known learning algorithms: Q learning (Watkins,\\n1989; Watkins & Dayan, 1992) and SARSA(0) (Rummery & Niranjan, 1994). Both of these\\nalgorithms maintain a tabular representation of the action-value function Q(s, a). Every entry of\\nthe table is initialized arbitrarily.\\n\\nIn Q learning, after the algorithm has observed s, chosen a, received r, and observed s′, it\\n\\nperforms the following update:\\n\\nQt(s, a) := (1 − αt)Qt−1(s, a) + αt[r + γ max\\n\\na′ Qt−1(s\\n\\n′\\n\\n′\\n, a\\n\\n)],\\n\\nwhere αt is a learning rate parameter.\\n\\nJaakkola, Jordan and Singh (1994) and Bertsekas and Tsitsiklis (1996) prove that if the agent\\n\\nfollows an “exploration policy” that tries every action in every state inﬁnitely often and if\\n\\nlim\\nT →∞\\n\\nT\\n\\nt=1\\nX\\n\\nαt = ∞ and\\n\\nlim\\nT→∞\\n\\nT\\n\\nXt=1\\n\\nα2\\n\\nt < ∞\\n\\n(4)\\n\\nthen Qt converges to the optimal action-value function Q∗ with probability 1. Their proof holds in\\nboth settings discussed in this paper (episodic and inﬁnite-horizon).\\n\\nThe SARSA(0) algorithm is very similar. After observing s, choosing a, observing r, observing\\n\\ns′, and choosing a′, the algorithm performs the following update:\\n\\nQt(s, a) := (1 − αt)Qt−1(s, a) + αt(s, a)[r + γQt−1(s\\n\\n′\\n\\n′\\n, a\\n\\n)],\\n\\nwhere αt is a learning rate parameter. The key diﬀerence is that the Q value of the chosen action\\na′, Q(s′, a′), appears on the right-hand side in the place where Q learning uses the Q value of the\\nbest action. Singh, Jaakkola, Littman, and Szepesv´ari (1998) provide two important convergence\\nresults: First, if a ﬁxed policy π is employed to choose actions, SARSA(0) will converge to the value\\nfunction of that policy provided αt decreases according to Equation (4). Second, if a so-called GLIE\\npolicy is employed to choose actions, SARSA(0) will converge to the value function of the optimal\\npolicy, provided again that αt decreases according to Equation (4). A GLIE policy is deﬁned as\\nfollows:\\n\\nDeﬁnition 1 A GLIE (greedy in the limit with inﬁnite exploration) policy is any policy satisfying\\n\\n1. Each action is executed inﬁnitely often in every state that is visited inﬁnitely often.\\n\\n2. In the limit, the policy is greedy with respect to the Q-value function with probability 1.\\n\\n8\\n\\n\\x0c4\\n\\n3\\n\\n2\\n\\n1\\n\\n0\\n\\nR\\n\\nY\\n\\n0\\n\\nG\\n\\nB\\n\\n3\\n\\n1\\n\\n2\\n\\n4\\n\\nFigure 1: The Taxi Domain\\n\\n3 The MAXQ Value Function Decomposition\\n\\nAt the center of the MAXQ method for hierarchical reinforcement learning is the MAXQ value\\nfunction decomposition. MAXQ describes how to decompose the overall value function for a policy\\ninto a collection of value functions for individual subtasks (and subsubtasks, recursively).\\n\\n3.1 A Motivating Example\\n\\nTo make the discussion concrete, let us consider the following simple example. Figure 1 shows\\na 5-by-5 grid world inhabited by a taxi agent. There are four specially-designated locations in\\nIn\\nthis world, marked as R(ed), B(lue), G(reen), and Y(ellow). The taxi problem is episodic.\\neach episode, the taxi starts in a randomly-chosen square. There is a passenger at one of the\\nfour locations (chosen randomly), and that passenger wishes to be transported to one of the four\\nlocations (also chosen randomly). The taxi must go to the passenger’s location (the “source”), pick\\nup the passenger, go to the destination location (the “destination”), and put down the passenger\\nthere. (To keep things uniform, the taxi must pick up and drop oﬀ the passenger even if he/she\\nis already located at the destination!) The episode ends when the passenger is deposited at the\\ndestination location.\\n\\nThere are six primitive actions in this domain: (a) four navigation actions that move the taxi\\none square North, South, East, or West, (b) a Pickup action, and (c) a Putdown action. Each action\\nis deterministic. There is a reward of −1 for each action and an additional reward of +20 for\\nsuccessfully delivering the passenger. There is a reward of −10 if the taxi attempts to execute the\\nPutdown or Pickup actions illegally. If a navigation action would cause the taxi to hit a wall, the\\naction is a no-op, and there is only the usual reward of −1.\\n\\nWe seek a policy that maximizes the total reward per episode. There are 500 possible states:\\n25 squares, 5 locations for the passenger (counting the four starting locations and the taxi), and 4\\ndestinations.\\n\\nThis task has a simple hierarchical structure in which there are two main sub-tasks: Get\\nthe passenger and Deliver the passenger. Each of these subtasks in turn involves the subtask\\nof navigating to one of the four locations and then performing a Pickup or Putdown action.\\n\\nThis task illustrates the need to support temporal abstraction, state abstraction, and subtask\\nsharing. The temporal abstraction is obvious—for example, the process of navigating to the passen-\\nger’s location and picking up the passenger is a temporally extended action that can take diﬀerent\\nnumbers of steps to complete depending on the distance to the target. The top level policy (get\\npassenger; deliver passenger) can be expressed very simply if these temporal abstractions can be\\n\\n9\\n\\n\\x0cemployed.\\n\\nThe need for state abstraction is perhaps less obvious. Consider the subtask of getting the\\npassenger. While this subtask is being solved, the destination of the passenger is completely\\nirrelevant—it cannot aﬀect any of the nagivation or pickup decisions. Perhaps more importantly,\\nwhen navigating to a target location (either the source or destination location of the passenger),\\nonly the identity of the target location is important. The fact that in some cases the taxi is carrying\\nthe passenger and in other cases it is not is irrelevant.\\nFinally, support for subtask sharing is critical.\\n\\nIf the system could learn how to solve the\\nnavigation subtask once, then the solution could be shared by both of the “Get the passenger”\\nand “Deliver the passenger” subtasks. We will show below that the MAXQ method provides a\\nvalue function representation and learning algorithm that supports temporal abstraction, state\\nabstraction, and subtask sharing.\\n\\nTo construct a MAXQ decomposition for the taxi problem, we must identify a set of individual\\nsubtasks that we believe will be important for solving the overall task. In this case, let us deﬁne\\nthe following four tasks:\\n\\n• Navigate(t). In this subtask, the goal is to move the taxi from its current location to one of\\n\\nthe four target locations, which will be indicated by the formal parameter t.\\n\\n• Get. In this subtask, the goal is to move the taxi from its current location to the passenger’s\\n\\ncurrent location and pick up the passenger.\\n\\n• Put. The goal of this subtask is to move the taxi from the current location to the passenger’s\\n\\ndestination location and drop oﬀ the passenger.\\n\\n• Root. This is the whole taxi task.\\n\\nEach of these subtasks is deﬁned by a subgoal, and each subtask terminates when the subgoal\\n\\nis achieved.\\n\\nAfter deﬁning these subtasks, we must indicate for each subtask which other subtasks or prim-\\nitive actions it should employ to reach its goal. For example, the Navigate(t) subtask should use\\nthe four primitive actions North, South, East, and West. The Get subtask should use the Navigate\\nsubtask and the Pickup primitive action, and so on.\\n\\nAll of this information can be summarized by a directed acyclic graph called the task graph,\\nwhich is shown in Figure 2. In this graph, each node corresponds to a subtask or a primitive action,\\nand each edge corresponds to a potential way in which one subtask can “call” one of its child tasks.\\nThe notation f ormal/actual (e.g., t/source) tells how a formal parameter is to be bound to an\\nactual parameter.\\n\\nNow suppose that for each of these subtasks, we write a policy (e.g., as a computer program)\\nto achieve the subtask. We will refer to the policy for a subtask as a “subroutine”, and we can\\nview the parent subroutine as invoking the child subroutine via ordinary subroutine-call-and-return\\nsemantics. If we have a policy for each subtask, then this gives us an overall policy for the Taxi\\nMDP. The Root subtask executes its policy by calling subroutines that are policies for the Get and\\nPut subtasks. The Get policy calls subroutines for the Pickup primitive action and the Navigate(t)\\nsubtask. And so on. We will call this collection of policies a hierarchical policy. In a hierarchical\\npolicy, each subroutine executes until it enters a terminal state for its subtask.\\n\\n3.2 Deﬁnitions\\n\\nLet us formalize the discussion so far.\\n\\n10\\n\\n\\x0cPickup\\n\\nNavigate(t)\\n\\nPutdown\\n\\nRoot\\n\\nGet\\n\\nPut\\n\\nt/source\\n\\nt/destination\\n\\nNorth\\n\\nSouth\\n\\nEast\\n\\nWest\\n\\nFigure 2: A task graph for the Taxi problem.\\n\\nThe MAXQ decomposition takes a given MDP M and decomposes it into a set of subtasks\\n{M0, M1, . . . , Mn} with the convention that M0 is the root subtask (i.e., solving M0 solves the\\nentire original MDP M ).\\n\\nDeﬁnition 2 An unparameterized subtask is a three-tuple, hTi, Ai, ˜Rii, deﬁned as follows:\\n\\n1. Ti(si) is a termination predicate that partitions S into a set of active states, Si and a set of\\nterminal states, Ti. The policy for subtask Mi can only be executed if the current state s is\\nin Si.\\n\\n2. Ai is a set of actions that can be performed to achieve subtask Mi. These actions can either\\nbe primitive actions from A, the set of primitive actions for the MDP, or they can be other\\nsubtasks, which we will denote by their indexes i. We will refer to these actions as the\\n“children” of subtask i.\\nIf a child subtask Mj has formal parameters, then it can occur\\nmultiple times in Ai, and each such occurrence must specify the actual values that will be\\nbound to the formal parameters. The set of actions Ai may diﬀer from one state to another, so\\ntechnically, Ai is a function of s. However, we will suppress this dependence in our notation.\\n3. ˜Ri(s′|s, a) is the pseudo-reward function, which speciﬁes a pseudo-reward for each transition\\nfrom a state s ∈ Si to a terminal state s′ ∈ Ti. This pseudo-reward tells how desirable each\\nof the terminal states is for this subtask. It is typically employed to give goal terminal states\\na pseudo-reward of 0 and any non-goal terminal states a negative reward.\\n\\nEach primitive action a from M is a primitive subtask in the MAXQ decomposition such that\\na is always executable, it always terminates immediately after execution, and its pseudo-reward\\nfunction is uniformly zero.\\n\\nIf a subtask has formal parameters, then each possible binding of actual values to the formal\\nparameters speciﬁes a distinct subtask. We can think of the values of the formal parameters as being\\npart of the “name” of the subtask. In practice, of course, we implement a parameterized subtask\\nby parameterizing the various components of the task. If b speciﬁes the actual parameter values\\nfor task Mi, then we can deﬁne a parameterized termination predicate Ti(s, b) and a parameterized\\npseudo-reward function ˜Ri(s′|s, a, b). To simplify notation in the rest of the paper, we will usually\\nomit these parameter bindings from our notation.\\n\\n11\\n\\n\\x0cTable 1: Pseudo-Code for Execution of a Hierarchical Policy\\n\\nst is the state of the world at time t\\n\\n1\\n2 Kt is the state of the execution stack at time t\\n\\nwhile top(Kt) is not a primitive action\\n\\nLet (i, fi) := top(Kt), where\\n\\ni is the name of the “current” subroutine, and\\nfi gives the parameter bindings for i\\n\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n\\nLet (a, fa) := πi(s, fi), where\\n\\npush (a, fa) onto the stack Kt\\n\\na is the action and fa gives the parameter bindings chosen by policy πi\\n\\n10 Let (a, nil) := pop(Kt) be the primitive action on the top of the stack.\\n11 Execute primitive action a, and update st+1 to be\\nthe resulting state of the environment.\\n12\\n\\n13 while top(Kt) speciﬁes a terminated subtask do\\n14\\n\\npop(Kt)\\n\\n15 Kt+1 := Kt is the resulting execution stack.\\n\\nDeﬁnition 3 A hierarchical policy, π, is a set containing a policy for each of the subtasks in the\\nproblem: π = {π0, . . . , πn}.\\n\\nEach subtask policy πi takes a state and returns the name of a primitive action to execute or\\nthe name of a subroutine (and bindings for its formal parameters) to invoke. In the terminology of\\nSutton, Precup, and Singh (1998), a subtask policy is a deterministic “option”, and its probability\\nof terminating in state s (which they denote by β(s)) is 0 if s ∈ Si, and 1 if s ∈ Ti.\\n\\nIn a parameterized task, the policy must be parameterized as well so that π takes a state and\\nthe bindings of formal parameters and returns a chosen action and the bindings (if any) of its\\nformal parameters.\\n\\nTable 1 gives a pseudo-code description of the procedure for executing a hierarchical policy.\\nThe hierarchical policy is executed using a stack discipline, as in ordinary programming languages.\\nLet Kt denote the contents of the pushdown stack at time t. When a subroutine is invoked, its\\nname and actual parameters are pushed onto the stack. When a subroutine terminates, its name\\nand actual parameters are popped oﬀ the stack. It is sometimes useful to think of the contents\\nof the stack as being an additional part of the state space for the problem. Hence, a hierarchical\\npolicy implicitly deﬁnes a mapping from the current state st and current stack contents Kt to a\\nprimitive action a. This action is executed, and this yields a resulting state st+1 and a resulting\\nstack contents Kt+1. Because of the added state information in the stack, the hierarchical policy\\nis non-Markovian with respect to the original MDP.\\n\\nBecause a hierarchical policy maps from states s and stack contents K to actions, the value\\nfunction for a hierarchical policy must in general also assign values to all combinations of states s\\nand stack contents K.\\n\\nDeﬁnition 4 A hierarchical value function, denoted V π(hs, Ki), gives the expected cumulative\\nreward of following the hierarchical policy π starting in state s with stack contents K.\\n\\nIn this paper, we will primarily be interested only in the “top level” value of the hierarchical\\npolicy—that is, the value when the stack K is empty: V π(hs, nili). This is the value of executing\\nthe hierarchical policy beginning in state s and starting at the top level of the hierarchy.\\n\\n12\\n\\n\\x0cDeﬁnition 5 The projected value function, denoted V π(s), is the value of executing hierarchical\\npolicy π starting in state s and starting at the root of the task hierarchy.\\n\\n3.3 Decomposition of the Projected Value Function\\n\\nNow that we have deﬁned a hierarchical policy and its projected value function, we can show how\\nthat value function can be decomposed hierarchically. The decomposition is based on the following\\ntheorem:\\n\\nTheorem 1 Given a task graph over tasks M0, . . . , Mn and a hierarchical policy π, each subtask Mi\\ndeﬁnes a semi-Markov decision process with states Si, actions Ai, probability transition function\\ni (s′, N |s, a), and expected reward function R(s, a) = V π(a, s), where V π(a, s) is the projected\\nP π\\nvalue function for child task Ma in state s. If a is a primitive action, V π(a, s) is deﬁned as the\\nexpected immediate reward of executing a in s: V π(a, s) =\\n\\ns′ P (s′|s, a)R(s′|s, a).\\n\\nProof: Consider all of the subroutines that are descendants of task Mi in the task graph. Be-\\ncause all of these subroutines are executing ﬁxed policies (speciﬁed by hierarchical policy π), the\\ni (s′, N |s, a) is a well deﬁned, stationary distribution for each child\\nprobability transition function P π\\nsubroutine a. The set of states Si and the set of actions Ai are obvious. The interesting part of\\nthis theorem is the fact that the expected reward function R(s, a) of the SMDP is the projected\\nvalue function of the child task Ma.\\n\\nP\\n\\nTo see this, let us write out the value of V π(i, s):\\n\\nV π(i, s) = E{rt + γrt+1 + γ2rt+2 + · · · |st = s, π}\\n\\nThis sum continues until the subroutine for task Mi enters a state in Ti.\\n\\nNow let us suppose that the ﬁrst action chosen by πi is a subroutine a. This subroutine is in-\\ni (s′, N |s, a).\\n\\nvoked, and it executes for a number of steps N and terminates in state s′ according to P π\\nWe can rewrite Equation (5) as\\n\\nV π(i, s) = E\\n\\nγurt+u +\\n\\nγurt+u\\n\\nst = s, π\\n\\nN −1\\n\\n(\\n\\nu=0\\nX\\n\\n∞\\n\\nXu=N\\n\\n)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nThe ﬁrst summation on the right-hand side of Equation (6) is the discounted sum of rewards for\\nexecuting subroutine a starting in state s until it terminates, in other words, it is V π(a, s), the\\nprojected value function for the child task Ma. The second term on the right-hand side of the\\nequation is the value of s′ for the current task i, V π(i, s′), discounted by γN , where s′ is the current\\nstate when subroutine a terminates. We can write this in the form of a Bellman equation:\\n\\n(5)\\n\\n(6)\\n\\n(7)\\n\\nV π(i, s) = V π(πi(s), s) +\\n\\n′\\n\\nP π\\n\\ni (s\\n\\n, N |s, πi(s))γN V π(i, s\\n\\n′\\n\\n)\\n\\nXs′,N\\n\\nThis has the same form as Equation (3), which is the Bellman equation for an SMDP, where the\\nﬁrst term is the expected reward R(s, π(s)). Q.E.D.\\n\\nTo obtain a hierarchical decomposition of the projected value function, let us switch to the\\naction-value (or Q) representation. First, we need to extend the Q notation to handle the task\\nhierarchy. Let Qπ(i, s, a) be the expected cumulative reward for subtask Mi of performing action a\\nin state s and then following hierarchical policy π until subtask Mi terminates. With this notation,\\nwe can re-state Equation (7) as follows:\\n\\nQπ(i, s, a) = V π(a, s) +\\n\\n′\\n\\nP π\\n\\ni (s\\n\\n, N |s, a)γN Qπ(i, s\\n\\n′\\n\\n′\\n\\n, π(s\\n\\n)),\\n\\n(8)\\n\\nXs′,N\\n\\n13\\n\\n\\x0cThe right-most term in this equation is the expected discounted reward of completing task Mi after\\nexecuting action a in state s. This term only depends on i, s, and a, because the summation\\nmarginalizes away the dependence on s′ and N . Let us deﬁne C π(i, s, a) to be equal to this term:\\n\\nDeﬁnition 6 The completion function, C π(i, s, a), is the expected discounted cumulative reward\\nof completing subtask Mi after invoking the subroutine for subtask Ma in state s. The reward is\\ndiscounted back to the point in time where a begins execution.\\n\\nC π(i, s, a) =\\n\\n′\\n\\nP π\\n\\ni (s\\n\\n, N |s, a)γN Qπ(i, s\\n\\n′\\n\\n′\\n\\n, π(s\\n\\n))\\n\\nXs′,N\\n\\nWith this deﬁnition, we can express the Q function recursively as\\n\\nQπ(i, s, a) = V π(a, s) + C π(i, s, a).\\n\\nFinally, we can re-express the deﬁnition for V π(i, s) as\\n\\nV π(i, s) =\\n\\nQπ(i, s, πi(s))\\n\\ns′ P (s′|s, i)R(s′|s, i)\\n\\n(\\n\\nif i is composite\\nif i is primitive\\n\\n(9)\\n\\n(10)\\n\\n(11)\\n\\nP\\n\\nWe will refer to equations (9), (10), and (11) as the decomposition equations for the MAXQ\\nhierarchy under a ﬁxed hierarchical policy π. These equations recursively decompose the projected\\nvalue function for the root, V π(0, s) into the projected value functions for the individual subtasks,\\nM1, . . . , Mn and the individual completion functions C π(j, s, a) for j = 1, . . . , n. The fundamental\\nquantities that must be stored to represent the value function decomposition are just the C values\\nfor all non-primitive subtasks and the V values for all primitive actions.\\n\\nTo make it easier for programmers to design and debug MAXQ decompositions, we have de-\\nveloped a graphical representation that we call the MAXQ graph. A MAXQ graph for the Taxi\\ndomain is shown in Figure 3. The graph contains two kinds of nodes, Max nodes and Q nodes. The\\nMax nodes correspond to the subtasks in the task decomposition—there is one Max node for each\\nprimitive action and one Max node for each subtask (including the Root) task. Each primitive Max\\nnode i stores the value of V π(i, s). The Q nodes correspond to the actions that are available for\\neach subtask. Each Q node for parent task i, state s and subtask a stores the value of C π(i, s, a).\\nIn addition to storing information, the Max nodes and Q nodes can be viewed as performing\\nparts of the computation described by the decomposition equations. Speciﬁcally, each Max node\\ni can be viewed as computing the projected value function V π(i, s) for its subtask. For primitive\\nMax nodes, this information is stored in the node. For composite Max nodes, this information is\\nobtained by “asking” the Q node corresponding to πi(s). Each Q node with parent task i and child\\ntask a can be viewed as computing the value of Qπ(i, s, a). It does this by “asking” its child task\\na for its projected value function V π(a, s) and then adding its completion function C π(i, s, a).\\n\\nAs an example, consider the situation shown in Figure 1, which we will denote by s1. Suppose\\nthat the passenger is at R and wishes to go to B. Let the hierarchical policy we are evaluating be an\\noptimal policy denoted by π (we will omit the superscript * to reduce the clutter of the notation).\\nThe value of this state under π is 10, because it will cost 1 unit to move the taxi to R, 1 unit to\\npickup the passenger, 7 units to move the taxi to B, and 1 unit to putdown the passenger, for a\\ntotal of 10 units (a reward of −10). When the passenger is delivered, the agent gets a reward of\\n+20, so the net value is +10.\\n\\nFigure 4 shows how the MAXQ hierarchy computes this value. To compute the value V π(Root, s1),\\n\\nMaxRoot consults its policy and ﬁnds that πRoot(s1) is Get. Hence, it “asks” the Q node, QGet\\n\\n14\\n\\n\\x0cMaxRoot\\n\\nQGet\\n\\nQPut\\n\\nMaxGet\\n\\nMaxPut\\n\\nQPickup\\n\\nQNavigateForGet\\n\\nQNavigateForPut\\n\\nQPutdown\\n\\nt/source\\n\\nt/destination\\n\\nPickup\\n\\nPutdown\\n\\nMaxNavigate(t)\\n\\nQNorth(t)\\n\\nQEast(t)\\n\\nQSouth(t)\\n\\nQWest(t)\\n\\nNorth\\n\\nEast\\n\\nSouth\\n\\nWest\\n\\nFigure 3: A MAXQ graph for the Taxi Domain\\n\\nto compute Qπ(Root, s1, Get). The completion cost for the Root task after performing a Get,\\nC π(Root, s1, Get), is 12, because it will cost 8 units to deliver the customer (for a net reward of\\n20 − 8 = 12) after completing the Get subtask. However, this is just the reward after completing\\nthe Get, so it must ask MaxGet to estimate the expected reward of performing the Get itself.\\n\\nThe policy for MaxGet dictates that in s1, the Navigate subroutine should be invoked with\\nt bound to R, so MaxGet consults the Q node, QNavigateForGet to compute the expected re-\\nward. QNavigateForGet knows that after completing the Navigate(R) task, one more action (the\\nPickup) will be required to complete the Get, so C π(MaxGet, s1, Navigate(R)) = −1. It then asks\\nMaxNavigate(R) to compute the expected reward of performing a Navigate to location R.\\n\\nThe policy for MaxNavigate chooses the North action, so MaxNavigate asks QNorth to compute\\nthe value. QNorth looks up its completion cost, and ﬁnds that C π(Navigate, s1, North) is 0 (i.e.,\\nthe Navigate task will be completed after performing the North action). It consults MaxNorth to\\ndetermine the expected cost of performing the North action itself. Because MaxNorth is a primitive\\naction, it looks up its expected reward, which is −1.\\n\\nNow this series of recursive computations can conclude as follows:\\n\\n• Qπ(Navigate(R), s1, North) = −1 + 0\\n\\n15\\n\\n\\x0c10\\n\\nMaxRoot\\n\\nQPut\\n\\nMaxPut\\n\\n10\\n\\n12\\n\\nQGet\\n\\n-2\\n\\nMaxGet\\n\\n-2\\n\\n-1\\n\\n-1\\n\\n-1\\n\\nQPickup\\n\\nQNavigateForGet\\n\\nQNavigateForPut\\n\\nQPutdown\\n\\nPickup\\n\\nPutdown\\n\\n-1\\n\\nMaxNavigate(t)\\n\\n0\\n\\nQNorth(t)\\n\\nQEast(t)\\n\\nQSouth(t)\\n\\nQWest(t)\\n\\nNorth\\n\\nEast\\n\\nSouth\\n\\nWest\\n\\nFigure 4: Computing the value of a state using the MAXQ hierarchy. The C value of each Q node\\nis shown to the left of the node. All other numbers show the values being returned up the graph.\\n\\n• V π(Navigate(R), s1) = −1\\n\\n• Qπ(Get, s1, Navigate(R)) = −1 + −1\\n\\n(−1 to perform the Navigate plus −1 to complete the Get.\\n\\n• V π(Get, s1) = −2\\n\\n• Qπ(Root, s1, Get) = −2 + 12\\n\\n(−2 to perform the Get plus 12 to complete the Root task and collect the ﬁnal reward).\\n\\nThe end result of all of this is that the value of V π(Root, s1) is decomposed into a sum of C\\n\\nterms plus the expected reward of the chosen primitive action:\\n\\nV π(Root, s1) = V π(North, s1) + C π(Navigate(R), s1, North) +\\nC π(Get, s1, Navigate(R)) + C π(Root, s1, Get)\\n\\n= −1 + 0 + −1 + 12\\n\\n= 10\\n\\n16\\n\\n\\x0cV π(0, s)\\n\\nXXXXXXX\\n\\n(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)\\nPPPPP\\n\\nV π(a1, s)\\n\\n(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)\\n\\n.\\n\\n.\\n\\n.\\nV π(am−1, s)\\n(cid:26)\\nZ\\n\\n(cid:26)\\n\\n(cid:26)(cid:26)\\n\\nZ\\n\\nZZ\\n\\nV π(am, s) C π(am−1, s, am)\\n\\nC π(a1, s, a2)\\n\\nC π(0, s, a1)\\n\\nr1\\n\\nr2\\n\\nr3\\n\\nr4\\n\\nr5\\n\\nr8\\n\\nr9\\n\\nr10\\n\\nr11\\n\\nr12\\n\\nr13\\n\\nr14\\n\\n.\\n\\n.\\n\\n.\\n\\nFigure 5: The MAXQ decomposition; r1, . . . , r14 denote the sequence of rewards received from\\nprimitive actions at times 1, . . . , 14.\\n\\nIn general, the MAXQ value function decomposition has the form\\n\\nV π(0, s) = V π(am, s) + C π(am−1, s, am) + . . . + C π(a1, s, a2) + C π(0, s, a1),\\n\\n(12)\\n\\nwhere a0, a1, . . . , am is the “path” of Max nodes chosen by the hierarchical policy going from the\\nRoot down to a primitive leaf node.\\n\\nWe can summarize the presentation of this section by the following theorem:\\n\\nTheorem 2 Let π = {πi; i = 0, . . . , n} be a hierarchical policy deﬁned for a given MAXQ graph\\nwith subtasks M0, . . . , Mn, and let i = 0 be the root node of the graph. Then there exist values\\nfor C π(i, s, a) (for internal Max nodes) and V π(i, s) (for primitive, leaf Max nodes) such that\\nV π(0, s) (as computed by the decomposition equations (9), (10), and (11)) is the expected discounted\\ncumulative reward of following policy π starting in state s.\\n\\nProof: The proof is by induction on the number of levels in the task graph. At each level i,\\nwe compute values for C π(i, s, π(s)) (or V π(i, s), if i is primitive) according to the decomposition\\nequations. We can apply the decomposition equations again to compute Qπ(i, s, π(s)) and apply\\nEquation (8) and Theorem 1 to conclude that Qπ(i, s, π(s)) gives the value function for level i.\\nWhen i = 0, we obtain the value function for the entire hierarchical policy. Q. E. D.\\n\\nIt is important to note that this representation theorem does not mention the pseudo-reward\\nfunction, because the pseudo-reward is used only during learning. This theorem captures the\\nrepresentational power of the MAXQ decomposition, but it does not address the question of whether\\nthere is a learning algorithm that can ﬁnd a given policy. That is the subject of the next section.\\n\\n4 A Learning Algorithm for the MAXQ Decomposition\\n\\nIn order to develop a learning algorithm for the MAXQ decomposition, we must consider exactly\\nwhat we are hoping to achieve. Of course, for any MDP M , we would like to ﬁnd an optimal policy\\nπ∗. However, in the MAXQ method (and in hierarchical reinforcement learning in general), the\\nprogrammer imposes a hierarchy on the problem. This hierarchy constrains the space of possible\\npolicies so that it may not be possible to represent the optimal policy or its value function.\\n\\n17\\n\\n\\x0cIn the MAXQ method, the constraints take two forms. First, within a subtask, only some of the\\npossible primitive actions may be permitted. For example, in the taxi task, during a Navigate(t),\\nonly the North, South, East, and West actions are available—the Pickup and Putdown actions are\\nnot allowed. Second, consider a Max node Mj with child nodes {Mj1, . . . , Mjk}. The policy learned\\nfor Mj must involve executing the learned policies of these child nodes. When the policy for child\\nnode Mji is executed, it will run until it enters a state in Tji. Hence, any policy learned for Mj\\nmust pass through some subset of these terminal state sets {Tj1, . . . , Tjk}.\\n\\nThe HAM method shares these same two constraints and in addition, it imposes a partial policy\\non each node, so that the policy for any subtask Mi must be a deterministic reﬁnement of the given\\nnon-deterministic initial policy for node i.\\n\\nIn the “option” approach, the policy is even further constrained. In this approach, there are\\nonly two non-primitive levels in the hierarchy, and the subtasks at the lower level are given complete\\npolicies by the programmer. Hence, any learned policy must be constructed by “concatenating”\\nthe given lower level policies in some order.\\n\\nThe purpose of imposing these constraints on the policy is to incorporate prior knowledge and\\nthereby reduce the size of the space that must be searched to ﬁnd a good policy. However, these\\nconstraints may make it impossible to learn the optimal policy.\\n\\nIf we can’t learn the optimal policy, the next best target would be to learn the best policy that\\n\\nis consistent with (i.e., can be represented by) the given hierarchy.\\n\\nDeﬁnition 7 A hierarchically optimal policy for MDP M is a policy that achieves the highest\\ncumulative reward among all policies consistent with the given hierarchy.\\n\\nParr (1998b) proves that his HAMQ learning algorithm converges with probability 1 to a hier-\\narchically optimal policy. Similarly, given a ﬁxed set of options, Sutton, Precup, and Singh (1998)\\nprove that their SMDP learning algorithm converges to a hierarchically optimal value function.\\n(Incidentally, they also show that if the primitive actions are also made available as “trivial” op-\\ntions, then their SMDP method converges to the optimal policy. However, in this case, it is hard\\nto say anything formal about how the options speed the learning process. They may in fact hinder\\nit (Hauskrecht et al., 1998).)\\n\\nWith the MAXQ method, we will seek an even weaker form of optimality: recursive optimality.\\n\\nDeﬁnition 8 A recursively optimal policy for MDP M with MAXQ decomposition {M0, . . . , Mk}\\nis a hierarchical policy π = {π0, . . . , πk} such that for each subtask Mi, the corresponding policy πi\\nis optimal for the SMDP deﬁned by the set of states Si, the set of actions Ai, the state transition\\nprobability function P π(s′, N |s, a), and the reward function given by the sum of the original reward\\nfunction R(s′|s, a) and the pseudo-reward function ˜Ri(s′).\\n\\nNote that in this deﬁnition, the state transition probability distribution is deﬁned by the locally\\noptimal policies {πj} of all subtasks that are descendants of Mi in the MAXQ graph. Hence,\\nrecursive optimality is a kind of local optimality in which the policy at each node is optimal given\\nthe policies of its children.\\n\\nThe reason to seek recursive optimality rather than hierarchical optimality is that recursive\\noptimality makes it possible to solve each subtask without reference to the context in which it is\\nexecuted. This context-free property makes it easier to share and re-use subtasks. It will also turn\\nout to be essential for the successful use of state abstraction.\\n\\nBefore we proceed to describe our learning algorithm for recursive optimality, let us see how\\n\\nrecursive optimality diﬀers from hierarchical optimality.\\n\\n18\\n\\n\\x0cMaxRoot\\n\\nQExit\\n\\nQGotoGoal\\n\\nMaxExit\\n\\nMaxGotoGoal\\n\\nQExitNorth\\n\\nQExitSouth\\n\\nQExitEast\\n\\nQNorthG\\n\\nQSouthG\\n\\nQEastG\\n\\nG\\n\\n*\\n\\n*\\n\\nFigure 6: A simple MDP (left) and its associated MAXQ graph (right). The policy shown in the\\nleft diagram is recursively optimal but not hierarchically optimal. The shaded cells indicate points\\nwhere the locally-optimal policy is not globally optimal.\\n\\nNorth\\n\\nSouth\\n\\nEast\\n\\nIt is easy to construct examples of policies that are recursively optimal but not hierarchically\\noptimal. Consider the simple maze problem and its associated MAXQ graph shown in Figures 6.\\nSuppose a robot starts somewhere in the left room, and it must reach the goal G in the right room.\\nThe robot has three actions, North, South, and East, and these actions are deterministic. The robot\\nreceives a reward of −1 for each move. Let us deﬁne two subtasks:\\n\\n• Exit. This task terminates when the robot exits the left room. We can set the pseudo-reward\\n\\nfunction ˜R to be 0 for the two terminal states (i.e., the two states indicated by *’s).\\n\\n• GotoGoal. This task terminates when the robot reaches the goal G.\\n\\nThe arrows in Figure 6 show the locally optimal policy within each room. The arrows on the\\nleft seek to exit the left room by the shortest path, because this is what we speciﬁed when we set\\nthe pseudo-reward function to 0. The arrows on the right follow the shortest path to the goal,\\nwhich is ﬁne. However, the resulting policy is neither hierarchically optimal nor optimal.\\n\\nThere exists a hierarchical policy that would always exit the left room by the upper door. The\\nMAXQ value function decomposition can represent the value function of this policy, but such a\\npolicy would not be locally optimal (because, for example, the states in the “shaded” region would\\nIf we consider for a moment, we can see a way to\\nnot follow the shortest path to a doorway).\\nﬁx this problem. The value of the upper starred state under the optimal hierarchical policy is\\n−2 and the value of the lower starred state is −6. Hence, if we set ˜R to have these values, then\\nthe recursively-optimal policy would be hierarchically optimal (and globally optimal).\\nIn other\\nwords, if the programmer can guess the right values for the terminal states of a subtask, then the\\nrecursively optimal policy will be hierarchically optimal (provided that all primitive actions are\\navailable within the subtask).\\n\\nThis basic idea was ﬁrst pointed out by Dean and Lin (1995). They describe an algorithm that\\nmakes initial guesses for the values of these starred states and then updates those guesses based\\non the computed values of the starred states under the resulting recursively-optimal policy. They\\nproved that this will converge to a hierarchically optimal policy. The drawback of their method is\\n\\n19\\n\\n\\x0cthat it requires repeated solution of the resulting hierarchical learning problem, and this does not\\nalways yield a speedup over just solving the original, ﬂat problem.\\n\\nParr (1998a) proposed an interesting approach that constructs a set of diﬀerent ˜R functions and\\ncomputes the recursively optimal policy under each of them for each subtask. His method chooses\\nthe ˜R functions in such a way that the hierarchically optimal policy can be approximated to any\\ndesired degree. Unfortunately, the method is quite ineﬃcient, because it relies on solving a series\\nof linear programming problems each of which requires time polynomial in several parameters,\\nincluding the number of states |Si| within the subtask.\\n\\nThis discussion suggests that while, in principle, it is possible to learn good values for the pseudo-\\nreward function, in practice, we must rely on the programmer to specify a single pseudo-reward\\nfunction, ˜R. If the programmer wishes to consider a small number of alternative pseudo-reward\\nfunctions, they can be handled by deﬁning a small number of subtasks that are identical except\\nfor their ˜R functions, and permitting the learning algorithm to choose the one that gives the best\\nrecursively-optimal policy.\\n\\nIn practice, we have employed the following simpliﬁed approach to deﬁning ˜R. For each subtask\\nMi, we deﬁne two predicates: the termination predicate, Ti, and a goal predicate Gi. The goal\\npredicate deﬁnes a subset of the terminated states that are “goal states”, and these have a pseudo-\\nreward of 0. All other terminal states have a ﬁxed constant pseudo-reward (e.g., −100) that is set\\nso that it is always better to terminate in a goal state than in a non-goal state. For the problems\\non which we have tested the MAXQ method, this worked very well.\\n\\nIn our experiments with MAXQ, we have found that it is easy to make mistakes in deﬁning\\nTi and Gi. If the goal is not deﬁned carefully, it is easy to create a set of subtasks that lead to\\ninﬁnite looping. For example, consider again the problem in Figure 6. Suppose we permit a fourth\\naction, West in the MDP and let us deﬁne the termination and goal predicates for the right hand\\nroom to be satisﬁed iﬀ either the robot reaches the goal or it exits the room. This is a very natural\\ndeﬁnition, since it is quite similar to the deﬁnition for the left-hand room. However, the resulting\\nlocally-optimal policy for this room will attempt to move to the nearest of these three locations:\\nthe goal, the upper door, or the lower door. We can easily see that for all but a few states near the\\ngoal, the only policies that can be constructed by MaxRoot will loop forever, ﬁrst trying to leave\\nthe left room by entering the right room, and then trying to leave the right room by entering the\\nleft room. This problem is easily ﬁxed by deﬁning the goal predicate Gi for the right room to be\\ntrue if and only if the robot reaches the goal G. But avoiding such “undesired termination” bugs\\ncan be hard in more complex domains.\\n\\nNow that we have an understanding of recursively optimal policies, we present two learning\\nalgorithms. The ﬁrst one, called MAXQ-0, applies only in the case when the pseudo-reward function\\n˜R is always zero. We will ﬁrst prove its convergence properties and then show how it can be extended\\nto give the second algorithm, MAXQ-Q, which works with general pseudo-reward functions.\\n\\nTable 2 gives pseudo-code for MAXQ-0. MAXQ-0 is a recursive function that executes the\\ncurrent exploration policy starting at Max node i in state s. It performs actions until it reaches\\na terminal state, at which point it returns a count of the total number of primitive actions that\\nhave been executed. To execute an action, MAXQ-0 calls itself recursively. When the recursive call\\nreturns, it updates the value of the completion function for node i. It uses the count of the number\\nof primitive actions to appropriately discount the value of the resulting state s′. At leaf nodes,\\nMAXQ-0 updates the estimated one-step expected reward, V (i, s). The value αt(i) is a “learning\\nrate” parameter that should be gradually decreased to zero in the limit.\\n\\nThere are two things that must be speciﬁed in order to make this algorithm description complete.\\nFirst, we must specify how to compute Vt(i, s′) in line 12, since it is not stored in the Max node.\\nIt is computed by the following modiﬁed versions of the decomposition equations:\\n\\n20\\n\\n\\x0cTable 2: The MAXQ-0 learning algorithm.\\n\\n1\\n\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n\\nfunction MAXQ-0(MaxNode i, State s)\\n\\nif i is a primitive MaxNode\\n\\nexecute i, receive r, and observe result state s′\\nVt+1(i, s) := (1 − αt(i)) · Vt(i, s) + αt(i) · rt\\nreturn 1\\n\\nelse\\n\\nlet count = 0\\nwhile Ti(s) is false do\\n\\nchoose an action a according to the current exploration policy πx(i, s)\\nlet N = MAXQ-0(a, s)\\nobserve result state s′\\nCt+1(i, s, a) := (1 − αt(i)) · Ct(i, s, a) + αt(i) · γN Vt(i, s′\\ncount := count + N\\ns := s′\\nend\\nreturn count\\n\\n)\\n\\nend MAXQ-0\\n\\nTable 3: Pseudo-code for Greedy Execution of the MAXQ Graph\\n\\nfunction EvaluateMaxNode(i, s)\\n\\nif i is a primitive Max node\\nreturn hVt(i, s), ii\\n\\nelse\\n\\nfor each j ∈ Ai,\\n\\n1\\n\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n\\nlet hV(j, s), aji = EvaluateMaxNode(j, s)\\n\\nlet jhg = argmaxj Vt(j, s) + Ct(i, s, j)\\nreturn hVt(jhg, s), ajhg i\\n\\nend // EvaluateMaxNode\\n\\nVt(i, s) =\\n\\nmaxa Qt(i, s, a)\\nVt(i, s)\\n\\nif i is composite\\nif i is primitive\\n\\n(\\n\\nQt(i, s, a) = Vt(a, s) + Ct(i, s, a).\\n\\n(13)\\n\\n(14)\\n\\nThese equations reﬂect two important changes compared with Equations (10) and (11). First,\\nin the ﬁrst equation, Vt(i, s) is deﬁned in terms of the Q value of the best action a, rather than of\\nthe action chosen by a ﬁxed hierarchical policy. Second, there are no π superscripts, because the\\ncurrent value function, Vt(i, s) is not based on a ﬁxed hierarchical policy π.\\n\\nTo compute Vt(i, s) using these equations, we must perform a complete search of all paths\\nthrough the MAXQ graph starting at node i and ending at the leaf nodes. Table 3 gives pseudo-\\ncode for a recursive function, EvaluateMaxNode, that implements a depth-ﬁrst search. In addition\\nto returning Vt(i, s), EvaluateMaxNode also returns the action at the leaf node that achieves this\\nvalue. This information is not needed for MAXQ-0, but it will be useful later when we consider\\nnon-hierarchical execution of the learned recursively-optimal policy.\\n\\nThe second thing that must be speciﬁed to complete our deﬁnition of MAXQ-0 is the exploration\\n\\npolicy, πx. We require that πx be an ordered GLIE policy.\\n\\n21\\n\\n\\x0cDeﬁnition 9 An ordered GLIE policy is a GLIE policy (Greedy in the Limit of Inﬁnite Explo-\\nration) that converges in the limit to an ordered greedy policy, which is a greedy policy that imposes\\nan arbitrary ﬁxed order ω on the available actions and breaks ties in favor of the action a that ap-\\npears earliest in that order.\\n\\nWe need this property in order to ensure that MAXQ-0 converges to a uniquely-deﬁned recur-\\nsively optimal policy. A fundamental problem with recursive optimality is that in general, each Max\\nnode i will have a choice of many diﬀerent locally optimal policies given the policies adopted by its\\ndescendant nodes. These diﬀerent locally optimal policies will all achieve the same locally optimal\\nvalue function, but they can give rise to diﬀerent probability transition functions P (s′, N |s, i). The\\nresult will be that the Semi-Markov Decision Problem deﬁned at the next level above node i in\\nthe MAXQ graph will diﬀer depending on which of these various locally optimal policies is chosen\\nby node i. However, if we establish a ﬁxed ordering over the Max nodes in the MAXQ graph\\n(e.g., a left-to-right depth-ﬁrst numbering), and break ties in favor of the lowest-numbered action,\\nthen this deﬁnes a unique policy at each Max node. And consequently, by induction, it deﬁnes a\\nunique policy for the entire MAXQ graph. Let us call this policy π∗\\nr . We will use the r subscript\\nto denote recursively optimal quantities under an ordered greedy policy. Hence, the corresponding\\nvalue function is V ∗\\nr denote the corresponding completion function and action-value\\nfunction. We now prove that the MAXQ-0 algorithm converges to π∗\\nr .\\n\\nr , and C ∗\\n\\nr and Q∗\\n\\nTheorem 3 Let M = hS, A, P, R, P0i be either an episodic MDP for which all deterministic policies\\nare proper or a discounted inﬁnite horizon MDP with discount factor γ. Let H be a MAXQ graph\\ndeﬁned over subtasks {M0, . . . , Mk} such that the pseudo-reward function ˜Ri(s′|s, a) is zero for all\\ni, s, a, and s′. Let αt(i) > 0 be a sequence of constants for each Max node i such that\\n\\nlim\\nT →∞\\n\\nT\\n\\nt=1\\nX\\n\\nlim\\nT →∞\\n\\nT\\n\\nt=1\\nX\\n\\nαt(i) = ∞ and\\n\\nα2\\n\\nt (i) < ∞\\n\\n(15)\\n\\nLet πx(i, s) be an ordered GLIE policy at each node i and state s and assume that |Vt(i, s)| and\\n|Ct(i, s, a)| are bounded for all t, i, s, and a. Then with probability 1, algorithm MAXQ-0 converges\\nto π∗\\n\\nr , the unique recursively optimal policy for M consistent with H and πx.\\n\\nProof: The proof follows an argument similar to those introduced to prove the convergence of Q\\nlearning and SARSA(0) (Bertsekas & Tsitsiklis, 1996; Jaakkola et al., 1994). We will employ the\\nfollowing result from stochastic approximation theory:\\n\\nLemma 1 (Proposition 4.5 from Bertsekas and Tsitsiklis, 1996) Consider the iteration\\n\\nrt+1(x) := (1 − αt(x))rt(x) + αt(x)((U rt)(x) + wt(x) + ut(x)).\\n\\nLet Ft = {r0(x), . . . , rt(x), w0(x), . . . , wt−1(x), α0(x), . . . , αt(x), ∀x} be the entire history of the it-\\neration.\\nIf\\n\\n(a) The αt(i) ≥ 0 satisfy conditions (15)\\n\\n(b) For every i and t the noise terms wt(i) satisfy E[wt(i)|Ft] = 0\\n\\n(c) Given any norm ||·|| on Rn, there exist constants A and B such that E[w2\\n\\nt (i)|Ft] ≤ A+B||rt||2.\\n\\n22\\n\\n\\x0c(d) There exists a vector r∗, a positive vector ξ, and a scalar β ∈ [0, 1), such that for all t,\\n\\n||U rt − r\\n\\n||ξ ≤ β||rt − r\\n\\n||ξ\\n\\n∗\\n\\n∗\\n\\n(e) There exists a nonnegative random sequence θt that converges to zero with probability 1 and\\n\\nis such that for all t\\n\\n|ut(x)| ≤ θt(||rt||ξ + 1)\\n\\nthen rt converges to r∗ with probability 1. The notation || · ||ξ denotes a weighted maximum norm\\n\\n||A||ξ = max\\n\\nx\\n\\n|A(x)|\\nξ(x)\\n\\n.\\n\\nThe structure of the proof of Theorem 3 will be inductive, starting at the leaves of the MAXQ\\ngraph and working toward the root. We will employ a diﬀerent time clock at each node i to count\\nthe number of update steps performed by MAXQ-0 at that node. The variable t will always refer\\nto the time clock of the current node i.\\n\\nTo prove the base case for any primitive Max node, we note that line 4 of MAXQ-0 is just the\\nstandard stochastic approximation algorithm for computing the expected reward for performing\\naction a in state s, and therefore it converges under the conditions given above.\\n\\nTo prove the recursive case, consider any composite Max node i with child node j. Let\\nPt(s′, N |s, j) be the transition probability distribution for performing child action j in state s\\nat time t (i.e., while following the exploration policy in all descendent nodes of node j). By the\\ninductive assumption, MAXQ-0 applied to j will converge to the (unique) recursively optimal value\\nfunction V ∗\\nr (j, s) with probability 1. Furthermore, because MAXQ-0 is following an ordered GLIE\\npolicy for j and its descendants, Pt(s′, N |s, j) will converge to P ∗\\nr (s′, N |s, j), the unique transition\\nprobability function for executing child j under the locally optimal policy π∗\\nr . What remains to be\\nshown is that the update assignment for C (line 12 of the MAXQ-0 algorithm) will converge to the\\noptimal C ∗\\n\\nr function with probability 1.\\n\\nTo prove this, we will apply Lemma 1. We will identify the x in the lemma with a state-action\\npair (s, a). The vector rt will be the completion-cost table Ct(i, s, a) for all s, a and ﬁxed i after\\nt update steps. The vector r∗ will be the optimal completion-cost C ∗\\nr (i, s, a) (again, for ﬁxed i).\\nDeﬁne the mapping U to be\\n\\n(U C)(i, s, a) =\\n\\n′\\n\\n∗\\nr (s\\n\\nP\\n\\n, N |s, a)γN (max\\n\\n[C(i, s\\n\\n′\\n\\n′\\n, a\\n\\n) + V\\n\\n′\\n∗\\nr (a\\n\\n′\\n\\n, s\\n\\n)])\\n\\na′\\n\\nXs′\\n\\nThis is a C update under the MDP Mi assuming that all descendant value functions, V ∗\\ntransition probabilities, P ∗\\n\\nr (s′, N |s, a), have converged.\\n\\nr (a, s), and\\n\\nTo apply the lemma, we must ﬁrst express the C update formula in the form of the update rule\\nin the lemma. Let s be the state that results from performing a in state s. Line 12 can be written\\n\\nCt+1(i, s, a)\\n\\n:= (1 − αt(i)) · Ct(i, s, a) + αt(i) · γN (max\\na′\\n\\n′\\n[Ct(i, s, a\\n\\n′\\n) + Vt(a\\n\\n, s)])\\n\\n:= (1 − αt(i)) · Ct(i, s, a) + αt(i) · [(U Ct)(i, s, a) + wt(i, s, a) + ut(i, s, a)]\\n\\nwhere\\n\\nwt(i, s, a) = γN\\n\\n′\\n[Ct(i, s, a\\n\\n′\\n) + Vt(a\\n\\n, s)]\\n\\n−\\n\\nmax\\na′\\n\\n(cid:18)\\n\\n(cid:19)\\n\\n23\\n\\n\\x0cut(i, s, a) =\\n\\nPt(s\\n\\n, N |s, a)γN\\n\\n[Ct(i, s\\n\\n′\\n\\n′\\n, a\\n\\n′\\n) + Vt(a\\n\\n, s\\n\\n)]\\n\\n−\\n\\nPt(s\\n\\n, N |s, a)γN\\n\\n[Ct(i, s\\n\\n′\\n\\n′\\n, a\\n\\n′\\n) + Vt(a\\n\\n, s\\n\\n)]\\n\\n′\\n\\n′\\n\\nXs′,N\\n\\nXs′,N\\n\\nXs′,N\\n\\nmax\\na′\\n\\nmax\\na′\\n\\n(cid:18)\\n\\n(cid:18)\\n\\nmax\\na′\\n\\n(cid:18)\\n\\n′\\n\\n∗\\nr (s\\n\\nP\\n\\n, N |s, a)γN\\n\\n[Ct(i, s\\n\\n′\\n\\n′\\n, a\\n\\n) + V\\n\\n′\\n∗\\nr (a\\n\\n′\\n\\n, s\\n\\n)]\\n\\n′\\n\\n′\\n\\n(cid:19)\\n\\n(cid:19)\\n\\n(cid:19)\\n\\nHere wt(i, s, a) is the diﬀerence between doing an update at node i using the single sample point s\\ndrawn according to Pt(s′, N |s, a) and doing an update using the full distribution Pt(s′, N |s, a). The\\nvalue of ut(i, s, a) captures the diﬀerence between doing an update using the current probability\\ntransitions Pt(s′, N |s, a) and current value functions of the children Vt(a′, s′) and doing an up-\\ndate using the optimal probability transitions P ∗\\nr (s′, N |s, a) and the optimal values of the children\\nr (a′, s′).\\nV ∗\\n\\nWe now verify the conditions of Lemma 1.\\nCondition (a) is assumed in the conditions of the theorem with αt(s, a) = αt(i).\\nCondition (b) is satisﬁed because s is sampled from Pt(s′, N |s, a), so the expected value of the\\n\\ndiﬀerence is zero.\\n\\nCondition (c) follows directly from the assumption that the |Ct(i, s, a)| and |Vt(i, s)| are bounded.\\nCondition (d) is the condition that U is a weighted max norm pseudo-contraction. We can\\nderive this by starting with the weighted max norm for Q learning. It is well known that Q is\\na weighted max norm pseudo-contraction (Bertsekas & Tsitsiklis, 1996) in both the episodic case\\nwhere all deterministic policies are proper (and the discount factor γ = 1) and in the inﬁnite horizon\\ndiscounted case (with γ < 1). That is, there exists a positive vector ξ and a scalar β ∈ [0, 1), such\\nthat for all t,\\n\\n||T Qt − Q\\n\\n||ξ ≤ β||Qt − Q\\n\\n||ξ,\\n\\n∗\\n\\n∗\\n\\n(16)\\n\\nwhere T is the operator\\n\\n(T Q)(s, a) =\\n\\n′\\n\\nP (s\\n\\n, N |s, a)γN [R(s\\n\\n′\\n\\n|s, a) + max\\n\\na′ Q(s\\n\\n′\\n\\n′\\n, a\\n\\n)].\\n\\nXs′,N\\n\\nNow we will show how to derive the contraction for the C update operator U . Our plan is to show\\nﬁrst how to express the U operator for learning C in terms of the T operator for updating Q values.\\nThen we will replace T Q in the contraction equation for Q learning with U C, and show that U is\\na weighted max-norm contraction under the same weights ξ and the same β.\\n\\nRecall from Eqn. (10) that Q(i, s, a) = C(i, s, a)+V (a, s). Furthermore, the U operator performs\\nits updates using the optimal value functions of the child nodes, so we can write this as Qt(i, s, a) =\\nCt(i, s, a) + V ∗(a, s). Now once the children of node i have converged, the Q-function version of\\nthe Bellman equation for MDP Mi can be written as\\n\\nQ(i, s, a) =\\n\\n′\\n\\n∗\\nr (s\\n\\nP\\n\\n, N |s, a)γN [V\\n\\n∗\\nr (a, s) + max\\n\\na′ Q(i, s\\n\\n′\\n\\n′\\n, a\\n\\n)].\\n\\nAs we have noted before, V ∗\\nfor node i, the T operator can be rewritten as\\n\\nr (a, s) plays the role of the immediate reward function for Mi. Therefore,\\n\\n(T Q)(i, s, a) =\\n\\n′\\n\\n∗\\nr (s\\n\\nP\\n\\n|s, a)γN [V\\n\\n∗\\nr (a, s) + max\\n\\na′ Q(i, s\\n\\n′\\n\\n′\\n, a\\n\\n)].\\n\\nXs′,N\\n\\nXs′,N\\n\\n24\\n\\n\\x0cNow we replace Q(i, s, a) by C(i, s, a) + V ∗\\n\\nr (a, s), and obtain\\n\\n(T Q)(i, s, a) =\\n\\n′\\n\\n∗\\nr (s\\n\\nP\\n\\n, N |s, a)γN (V\\n\\n∗\\nr (a, s) + max\\n\\n[C(i, s\\n\\n′\\n\\n′\\n, a\\n\\n) + V\\n\\n′\\n∗\\nr (a\\n\\n′\\n\\n, s\\n\\n)]).\\n\\nXs′,N\\n\\nNote that V ∗\\nobtain\\n\\nr (a, s) does not depend on s′ or N , so we can move it outside the expectation and\\n\\n(T Q)(i, s, a) = V\\n\\n∗\\nr (a, s) +\\n\\n′\\n\\n∗\\nr (s\\n\\nP\\n\\n|s, a)γN (max\\n\\n[C(i, s\\n\\n′\\n\\n′\\n, a\\n\\n) + V\\n\\n′\\n∗\\nr (a\\n\\n′\\n\\n, s\\n\\n)])\\n\\nXs′,N\\n∗\\nr (a, s) + (U C)(i, s, a)\\n\\n= V\\n\\nAbusing notation slightly, we will express this in vector form as T Q(i) = V ∗\\nwe can write Qt(i, s, a) = Ct(i, s, a) + V ∗\\n\\nr (a, s) in vector form as Qt(i) = Ct(i) + V ∗\\nr .\\n\\nr + U C(i). Similarly,\\n\\nNow we can substitute these two formulas into the max norm pseudo-contraction formula for\\n\\nT , Eqn. (16) to obtain\\n\\na′\\n\\na′\\n\\n||V\\n\\n∗\\nr + U Ct(i) − (C\\n\\n∗\\nr (i) + V\\n\\n∗\\nr )||ξ ≤ β||V\\n\\n∗\\nr + Ct(i) − (V\\n\\n∗\\nr + C\\n\\n∗\\nr (i))||ξ.\\n\\nThe V ∗ terms cancel on both sides of the equation, and we get\\n\\n||U Ct(i) − C\\n\\n∗\\nr (i)||ξ ≤ β||Ct(i) − C\\n\\n∗\\nr (i)||ξ.\\n\\nFinally, it is easy verify (e), the most important condition. By assumption, the ordered GLIE\\npolicies in the child nodes converge with probability 1 to locally optimal policies for the children.\\nTherefore Pt(s′, N |s, a) converges to P ∗\\nr (s′, N |s, a) for all s′, N, s, and a with probability 1 and\\nVt(a, s) converges with probability 1 to V ∗\\nr (a, s) for all child actions a. Therefore, |ut| converges\\nto zero with probability 1. We can trivially construct a sequence θt = |ut| that bounds this\\nconvergence, so\\n\\n|ut(s, a)| ≤ θt ≤ θt(||Ct(s, a)||ξ + 1).\\n\\nWe have veriﬁed all of the conditions of Lemma 1, so we can conclude that Ct(i) converges to\\nC ∗\\nr (i) with probability 1. By induction, we can conclude that this holds for all nodes in the MAXQ\\nincluding the root node, so the value function represented by the MAXQ graph converges to the\\nunique value function of the recursively optimal policy π∗\\n\\nr . Q.E.D.\\n\\nAlgorithm MAXQ-0 can be extended to accelerate learning in the higher nodes of the graph by\\na technique that we call “all states updating”. When an action a is chosen for Max node i in state s,\\nthe execution of a will move the environment through a sequence of states s = s1, . . . , sN , sN +1 = s′.\\nIf a was indeed the best abstract action to choose in s1, then it should also be the best action to\\nchoose (at node i) in states s2 through sN . Hence, we can execute a version of line 12 in MAXQ-0\\nfor each of these intermediate states as shown in this replacement pseudo-code:\\n\\n12a\\n12b\\n12c\\n\\nfor j from 1 to N do\\n\\nCt+1(i, sj, a) := (1 − αt(i)) · Ct(i, sj, a) + αt(i) · γ(N +1−j)maxa′ Qt(i, s′, a′)\\nend // for\\n\\nIn our implementation, as each composite action is executed by MAXQ-0, it constructs a linked\\nlist of the sequence of primitive states that were visited. This list is returned when the composite\\naction terminates. The parent Max node can then process each state in this list as shown above.\\nThe parent Max node appends the state lists that it receives from its children and passes them to\\nits parent when it terminates. All experiments in this paper employ all states updating.\\n\\n25\\n\\n\\x0cKaelbling (1993) introduced a related, but more powerful, method for accelerating hierarchical\\nreinforcement learning that she calls “all goals updating.” This method is suitable for a MAXQ\\nhierarchy containing only a root task and one level of composite tasks. To understand all goals\\nupdating, suppose that for each primitive action, there are several composite tasks that could have\\ninvoked that primitive action. In all goals updating, whenever a primitive action is executed, the\\nequivalent of line 12 of MAXQ-0 is applied in every composite task that could have invoked that\\nprimitive action. Sutton, Precup, and Singh (1998) prove that each of the composite tasks will\\nconverge to the optimal Q values under all goals updating.\\n\\nAll goals updating would work in the MAXQ hierarchy for composite tasks all of whose children\\nare primitive actions. However, as we have seen, at higher levels in the hierarchy, node i needs to\\nobtain samples of result states drawn according to P ∗(s′, N |s, a) for composite tasks a. All goals\\nupdating cannot provide these samples, so it cannot be applied at these higher levels.\\n\\nNow that we have shown the convergence of MAXQ-0, let us design a learning algorithm for\\narbitrary pseudo-reward functions, ˜Ri(s). We could just add the pseudo-reward into MAXQ-0,\\nbut this has the eﬀect of changing the MDP M to have a diﬀerent reward function. The pseudo-\\nrewards “contaminate” the values of all of the completion functions computed in the hierarchy.\\nThe resulting learned policy will not be recursively optimal for the original MDP.\\n\\nThis problem can be solved by learning two completion functions. The ﬁrst one, C(i, s, a) is the\\ncompletion function that we have been discussing so far in this paper. It computes the expected\\nreward for completing task Mi after performing action a in state s and then following the learned\\npolicy for Mi. It is computed without any reference to ˜Ri. This completion function will be used\\nby parent tasks to compute V (i, s), the expected reward for performing action i starting in state s.\\nThe second completion function ˜C(i, s, a) is a completion function that we will use only “inside”\\nnode i in order to discover the locally optimal policy for task Mi. This function will incorporate\\nrewards both from the “real” reward function, R(s′|s, a) and from the pseudo-reward function\\n˜Ri(s).\\n\\nWe will employ two diﬀerent update rules to learn these two completion functions. The ˜C\\nfunction will be learned using an update rule similar to the Q learning rule in line 12 of MAXQ-0.\\nBut the C function will be learned using an update rule similar to SARSA(0)—its purpose is to\\nlearn the value function for the policy that is discovered by optimizing ˜C. Pseudo-code for the\\nresulting algorithm, MAXQ-Q is shown in Table 4.\\n\\nThe key step is at lines 16 and 17. In line 16, MAXQ-Q ﬁrst updates ˜C using the value of the\\ngreedy action, a∗, in the resulting state. This update includes the pseudo-reward ˜Ri. Then in line\\n17, MAXQ-Q updates C using this same greedy action a∗, even if this would not be the greedy\\naction according to the “uncontaminated” value function. This update, of course, does not include\\nthe pseudo-reward function.\\n\\nIt is important to note that whereever Vt(a, s) appears in this pseudo-code, it refers to the\\n“uncontaminated” value function of state s when executing the Max node a. This is computed\\nrecursively in exactly the same way as in MAXQ-0.\\n\\nFinally, note that the pseudo-code also incorporates all-states updating, so each call to MAXQ-\\nQ returns a list of all of the states that were visited during its execution, and the updates of lines\\n16 and 17 are performed for each of those states. The list of states is ordered most-recent-ﬁrst, so\\nthe states are updated starting with the last state visited and working backward to the starting\\nstate, which helps speed up the algorithm.\\n\\nWhen MAXQ-Q has converged, the resulting recursively optimal policy is computed at each\\nnode by choosing the action a that maximizes ˜Q(i, s, a) = ˜C(i, s, a)+V (a, s) (breaking ties according\\nto the ﬁxed ordering established by the ordered GLIE policy). It is for this reason that we gave the\\nname “Max nodes” to the nodes that represent subtasks (and learned policies) within the MAXQ\\n\\n26\\n\\n\\x0cTable 4: The MAXQ-Q learning algorithm.\\n\\nfunction MAXQ-Q(MaxNode i, State s)\\n\\nlet seq = () be the sequence of states visited while executing i\\nif i is a primitive MaxNode\\n\\nexecute i, receive r, and observe result state s′\\nVt+1(i, s) := (1 − αt(i)) · Vt(i, s) + αt(i) · rt\\npush s into the beginning of seq\\n\\nelse\\n\\nlet count = 0\\nwhile Ti(s) is false do\\n\\nchoose an action a according to the current exploration policy πx(i, s)\\nlet childSeq = MAXQ-Q(a, s), where childSeq is the sequence of states visited\\n\\nwhile executing action a.\\n\\nobserve result state s′\\nlet a∗ = argmaxa′ [ ˜Ct(i, s′, a′) + Vt(a′, s′)]\\nlet N = length(childSeq)\\nfor each s in childSeq do\\n\\n˜Ct+1(i, s, a) := (1 − αt(i)) · ˜Ct(i, s, a) + αt(i) · γN [ ˜Ri(s′\\nCt+1(i, s, a) := (1 − αt(i)) · Ct(i, s, a) + αt(i) · γN [Ct(i, s′, a∗) + Vt(a∗, s′)]\\nN := N − 1\\nend // for\\n\\n) + ˜Ct(i, s′, a∗\\n\\n) + Vt(a∗, s)]\\n\\nappend childSeq onto the front of seq\\ns := s′\\nend // while\\n\\nend // else\\n\\nreturn seq\\nend MAXQ-0\\n\\n1\\n\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n\\ngraph. Each Q node j with parent node i stores both ˜C(i, s, j) and C(i, s, j), and it computes both\\n˜Q(i, s, j) and Q(i, s, j) by invoking its child Max node j. Each Max node i takes the maximum of\\nthese Q values and computes either V (i, s) or computes the best action, a∗ using ˜Q.\\n\\nCorollary 1 Under the same conditions as Theorem 3, MAXQ-Q converges the unique recursively\\noptimal policy for MDP M deﬁned by MAXQ graph H, pseudo-reward functions ˜R, and ordered\\nGLIE exploration policy πx.\\n\\nProof: The argument is identical to, but more tedious than, the proof of Theorem 3. The proof\\nof convergence of the ˜C values is identical to the original proof for the C values, but it relies on\\nproving convergence of the “new” C values as well, which follows from the same weighted max\\nnorm pseudo-contraction argument. Q.E.D.\\n\\n5 State Abstraction\\n\\nThere are many reasons to introduce hierarchical reinforcement learning, but perhaps the most\\nimportant reason is to create opportunities for state abstraction. When we introduced the simple\\ntaxi problem in Figure 1, we pointed out that within each subtask, we can ignore certain aspects of\\nthe state space. For example, while performing a MaxNavigate(t), the taxi should make the same\\nnavigation decisions regardless of whether the passenger is in the taxi. The purpose of this section\\nis to formalize the conditions under which it is safe to introduce such state abstractions and to show\\n\\n27\\n\\n\\x0chow the convergence proofs for MAXQ-Q can be extended to prove convergence in the presence of\\nstate abstraction. Speciﬁcally, we will identify ﬁve conditions that permit the “safe” introduction\\nof state abstractions.\\n\\nThroughout this section, we will use the taxi problem as a running example, and we will see\\nhow each of the ﬁve conditions will permit us to reduce the number of distinct values that must\\nbe stored in order to represent the MAXQ value function decomposition. To establish a starting\\npoint, let us compute the number of values that must be stored for the taxi problem without any\\nstate abstraction.\\n\\nThe MAXQ representation must have tables for each of the C functions at the internal nodes\\nand the V functions at the leaves. First, at the six leaf nodes, to store V (i, s), we must store\\n500 values at each node (because there are 500 states; 25 locations, 4 possible destinations for the\\npassenger, and 5 possible current locations for the passenger (the four special locations and inside\\nthe taxi itself)). Second, at the root node, there are two children, which requires 2 × 500 = 1000\\nvalues. Third, at the MaxGet and MaxPut nodes, we have 2 actions each, so each one requires\\n1000 values, for a total of 2000. Finally, at MaxNavigate(t), we have four actions, but now we\\nmust also consider the target parameter t, which can take four possible values. Hence, there are\\neﬀectively 2000 combinations of states and t values for each action, or 8000 total values that must\\nbe represented. In total, therefore, the MAXQ representation requires 14,000 separate quantities\\nto represent the value function.\\n\\nTo place this number in perspective, consider that a ﬂat Q learning representation must store a\\nseparate value for each of the six primitive actions in each of the 500 possible states, for a total of\\n3,000 values. Hence, we can see that without state abstraction, the MAXQ representation requires\\nmore than four times the memory of a ﬂat Q table!\\n\\n5.1 Five Conditions that Permit State Abstraction\\n\\nWe now introduce ﬁve conditions that permit the introduction of state abstractions. For each\\ncondition, we give a deﬁnition and then prove a lemma which states that if the condition is satisﬁed,\\nthen the value function for some corresponding class of policies can be represented abstractly (i.e.,\\nby abstract versions of the V and C functions). For each condition, we then provide some rules for\\nidentifying when that condition can be satisﬁed and give examples from the taxi domain.\\n\\nWe begin by introducing some deﬁnitions and notation.\\n\\nDeﬁnition 10 Let M be a MDP and H be a MAXQ graph deﬁned over M . Suppose that each state\\ns can be written as a vector of values of a set of state variables. At each Max node i, suppose the\\nstate variables are partitioned into two sets Xi and Yi, and let χi be a function that projects a state\\ns onto only the values of the variables in Xi. Then H combined with χi is called a state-abstracted\\nMAXQ graph.\\n\\nIn cases where the state variables can be partitioned, we will often write s = (x, y) to mean\\nthat a state s is represented by a vector of values for the state variables in X and a vector of values\\nfor the state variables in Y . Similarly, we will sometimes write P (x′, y′, N |x, y, a), V (a, x, y), and\\n˜Ra(x′, y′) in place of P (s′, N |s, a), V (a, s), and ˜Ra(s′), respectively.\\n\\nDeﬁnition 11 An abstract hierarchical policy for MDP M with state-abstracted MAXQ graph H\\nand associated abstraction functions χi, is a hierarchical policy in which each policy πi (correspond-\\ning to subtask Mi) satisﬁes the condition that for any two states s1 and s2 such that χi(s1) = χi(s2),\\nπi(s1) = πi(s2). (When πi is a stationary stochastic policy, this is interpreted to mean that the prob-\\nability distributions for choosing actions are the same in both states.)\\n\\n28\\n\\n\\x0cIn order for MAXQ-Q to converge in the presence of state abstractions, we will require that\\nat all times t its (instantaneous) exploration policy is an abstract hierarchical policy. One way to\\nachieve this is to construct the exploration policy so that it only uses information from the relevant\\nstate variables in deciding what action to perform. Boltzmann exploration based on the (state-\\nabstracted) Q values, ǫ-greedy exploration, and counter-based exploration based on abstracted\\nstates are all abstract exploration policies. Counter-based exploration based on the full state space\\nis not an abstract exploration policy.\\n\\nNow that we have introduced our notation, let us describe and analyze the ﬁve abstraction\\nconditions. We have identiﬁed three diﬀerent kinds of conditions under which abstractions can be\\nintroduced. The ﬁrst kind involves eliminating irrelevant variables within a subtask of the MAXQ\\ngraph. Under this form of abstraction, nodes toward the leaves of the MAXQ graph tend to have\\nvery few relevant variables, and nodes higher in the graph have more relevant variables. Hence,\\nthis kind of abstraction is most useful at the lower levels of the MAXQ graph.\\n\\nThe second kind of abstraction arises from “funnel” actions. These are macro actions that move\\nthe environment from some large number of initial states to a small number of resulting states.\\nThe completion cost of such subtasks can be represented using a number of values proportional to\\nthe number of resulting states. Funnel actions tend to appear higher in the MAXQ graph, so this\\nform of abstraction is most useful near the root of the graph.\\n\\nThe third kind of abstraction arises from the structure of the MAXQ graph itself. It exploits\\nthe fact that large parts of the state space for a subtask may not be reachable because of the\\ntermination conditions of its ancestors in the MAXQ graph.\\n\\nWe begin by describing two abstraction conditions of the ﬁrst type. Then we will present two\\n\\nconditions of the second type. And ﬁnally, we describe one condition of the third type.\\n\\n5.1.1 Condition 1: Max Node Irrelevance\\n\\nThe ﬁrst condition arises when a set of state variables is irrelevant to a Max node.\\n\\nDeﬁnition 12 Let Mi be a Max node in a MAXQ graph H for MDP M . A set of state variables\\nY is irrelevant to node i if the state variables of M can be partitioned into two sets X and Y such\\nthat for any stationary abstract hierarchical policy π executed by the descendants of i, the following\\ntwo properties hold:\\n\\n• the state transition probability distribution P π(s′, N |s, a) at node i can be factored into the\\n\\nproduct of two distributions:\\n\\n′\\n\\nP π(x\\n\\n, y\\n\\n′\\n\\n, N |x, y, a) = P π(y\\n\\n′\\n\\n′\\n|y, a) · P π(x\\n\\n, N |x, a),\\n\\n(17)\\n\\nwhere y and y′ give values for the variables in Y , and x and x′ give values for the variables\\nin X.\\n\\n• for any pair of states s1 = (x, y1) and s2 = (x, y2) such that χ(s1) = χ(s2) = x, and any child\\n\\naction a, V π(a, s1) = V π(a, s2) and ˜Ri(s1) = ˜Ri(s2).\\n\\nLemma 2 Let M be an MDP with full-state MAXQ graph H, and suppose that state variables Yi\\nare irrelevant for Max node i. Let χi(s) = x be the associated abstraction function that maps s onto\\nthe remaining relevant variables Xi. Let π be any abstract hierarchical policy. Then the action-\\nvalue function Qπ at node i can be represented compactly, with only one value of the completion\\n\\n29\\n\\n\\x0cfunction C π(i, s, j) for each equivalence class of states s that share the same values on the relevant\\nvariables.\\n\\nSpeciﬁcally Qπ(i, s, j) can be computed as follows:\\n\\nQπ(i, s, j) = V π(j, χi(s)) + C π(i, χi(s), j)\\n\\nwhere\\n\\nC π(i, x, j) =\\n\\n′\\n\\nP π(x\\n\\n, N |x, j) · γN [V π(π(x\\n\\n′\\n\\n′\\n), x\\n\\n′\\n) + ˜Ri(x\\n\\n′\\n) + C π(i, x\\n\\n′\\n\\n, π(x\\n\\n))],\\n\\nXx′,N\\n\\nwhere V π(j′, x′) = V π(j′, x′, y0), ˜Ri(x′) = ˜Ri(x′, y0), and π(x) = π(x, y0) for some arbitrary value\\ny0 for the irrelevant state variables Yi.\\n\\nProof: Deﬁne a new MDP χi(Mi) at node i as follows:\\n\\n• States: X = {x | χi(s) = x, for some s ∈ S}.\\n\\n• Actions: A.\\n\\n• Transition probabilities: P π(x′, N |x, a)\\n\\n• Reward function: V π(a, x) + ˜Ri(x′)\\n\\nBecause π is an abstract policy, its decisions are the same for all states s such that χi(s) = x for\\nsome x. Therefore, it is also a well-deﬁned policy over χi(Mi). The action-value function for π over\\nχi(Mi) is the unique solution to the following Bellman equation:\\n\\nQπ(i, x, j) = V π(j, x) +\\n\\n′\\nP π(x\\n\\n, N |x, j) · γN [ ˜Ri(x\\n\\n′\\n\\n′\\n) + Qπ(i, x\\n\\n′\\n, π(x\\n\\n))]\\n\\n(18)\\n\\nCompare this to the Bellman equation over Mi:\\n\\nQπ(i, s, j) = V π(j, s) +\\n\\n′\\n\\nP π(s\\n\\n, N |s, j) · γN [ ˜Ri(s\\n\\n) + Qπ(i, s\\n\\n′\\n\\n′\\n\\n′\\n\\n, π(s\\n\\n))]\\n\\n(19)\\n\\nand note that V π(j, s) = V π(j, χ(s)) = V π(j, x) and ˜Ri(s′) = ˜Ri(χ(s′)) = ˜Ri(x′). Furthermore, we\\nknow that the distribution P π can be factored into separate distributions for Yi and Xi. Hence, we\\ncan rewrite (19) as\\n\\nQπ(i, s, j) = V π(j, x) +\\n\\n′\\n\\nP (y\\n\\n|y, j)\\n\\n′\\nP π(x\\n\\n, N |x, j) · γN [ ˜Ri(x\\n\\n) + Qπ(i, s\\n\\n′\\n\\n′\\n\\n′\\n\\n, π(s\\n\\n))]\\n\\nXy′\\n\\nXx′,N\\n\\nThe right-most sum does not depend on y or y′, so the sum over y′ evaluates to 1, and can be\\neliminated to give\\n\\nQπ(i, s, j) = V π(j, x) +\\n\\nP π(x\\n\\n′\\n\\n′\\n, N |x, j) · γN [ ˜Ri(x\\n\\n) + Qπ(i, s\\n\\n′\\n\\n′\\n\\n, π(s\\n\\n))].\\n\\n(20)\\n\\nXx′,N\\n\\nXs′,N\\n\\nXx′,N\\n\\nFinally, note that equations (18) and (20) are identical except for the expressions for the Q\\n\\nvalues. Since the solution to the Bellman equation is unique, we must conclude that\\n\\nQπ(i, s, j) = Qπ(i, χ(s), j).\\n\\n30\\n\\n\\x0cWe can rewrite the right-hand side to obtain\\n\\nQπ(i, s, j) = V π(j, χ(s)) + C π(i, χ(s), j),\\n\\nC π(i, x, j) =\\n\\n′\\nP (x\\n\\n′\\n, N |x, j) · γN [V π(π(x\\n\\n′\\n), x\\n\\n′\\n) + ˜Ri(x\\n\\n′\\n) + C π(i, x\\n\\n′\\n\\n, π(x\\n\\n))].\\n\\nwhere\\n\\nQ.E.D.\\n\\nXx′,N\\n\\nOf course we are primarily interested in being able to discover and represent the optimal policy\\nat each node i. The following corollary shows that the optimal policy is an abstract policy, and\\nhence, that it can be represented abstractly.\\n\\nCorollary 2 Consider the same conditions as Lemma 2, but with the change that the abstract\\nhierarchical policy π is executed only by the descendants of node i, but not by node i. Let ρ be an\\nordering over actions. Then the optimal ordered policy π∗\\nρ at node i is an abstract policy, and its\\naction-value function can be represented abstracted.\\n\\nProof: Deﬁne the policy ω∗\\nρ to be the optimal ordered policy over the abstract MDP χ(M ), and\\nlet Q∗(i, x, j) be the corresponding optimal action-value function. Then by the same argument given\\nabove, Q∗ is also a solution to the optimal Bellman equation for the original MDP. This means\\nthat the policy π∗\\nρ deﬁned by π∗\\nρ(s) = ω∗(χ(s)) is an optimal ordered policy, and by construction,\\nit is an abstract policy. Q.E.D.\\n\\nAs stated, this condition appears quite diﬃcult to satisfy, since it requires that the state transi-\\ntion probability distribution factor into X and Y components for all possible abstract hierarchical\\npolicies. However, in practice, this condition is often satisﬁed.\\n\\nFor example, let us consider the Navigate(t) subtask. The source and destination of the passenger\\nare irrelevant to the achievement of this subtask. Any policy that successfully completes this subtask\\nwill have the same value function regardless of the source and destination locations of the passenger.\\n(Any policy that does not complete the subtask will have the same value function also, but all states\\nwill have a value of −∞.) By abstracting away the passenger source and destination, we obtain a\\nhuge savings in space. Instead of requiring 8000 values to represent the C functions for this task,\\nwe require only 400 values (4 actions, 25 locations, 4 possible values for t).\\n\\nOne rule for noticing cases where this abstraction condition holds is to examine the subgraph\\nrooted at the given Max node i. If a set of state variables is irrelevant to the leaf state transi-\\ntion probabilities and reward functions and also to all pseudo-reward functions and termination\\nconditions in the subgraph, then those variables satisfy the Max Node Irrelevance condition:\\n\\nLemma 3 Let M be an MDP with associated MAXQ graph H, and let i be a Max node in H. Let\\nXi and Yi be a partition of the state variables for M . A set of state variables Yi is irrelevant to\\nnode i if\\n\\n• For each primitive leaf node a that is a descendant of i, P (x′, y′|x, y, a) = P (y′|y, a)P (x′|x, a)\\n\\nand R(x′, y′|x, y, a) = R(x′|x, a),\\n\\n• For each internal node j that is equal to node i or is a descendent of i , ˜Rj(x′, y′) = ˜Rj(x′)\\n\\nand the termination predicate Tj(x′, y′) is true iﬀ Tj(x′).\\n\\n31\\n\\n\\x0cProof: We must show that any abstract hierarchical policy will give rise to an SMDP at node i\\nwhose transition probability distribution factors and whose reward function depends only on Xi.\\nBy deﬁnition, any abstract hierarchical policy will choose actions based only upon information in\\nXi. Because the primitive probability transition functions factor into an independent component\\nfor Xi and since the termination conditions at all nodes below i are based only on the variables\\nin Xi, the probability transition function Pi(x′, y′, N |x, y, a) must also factor into Pi(y′|y, a) and\\nPi(x′, N |x, a). Similarly, all of the reward functions V (j, x, y) must be equal to V (j, x), because all\\nrewards received within the subtree (either at the leaves or through pseudo-rewards) depend only\\non the variables in Xi. Therefore, the variables in Yi are irrelevant for Max node i. Q.E.D.\\n\\nIn the Taxi task, the primitive navigation actions, North, South, East, and West only depend on\\nthe location of the taxi and not on the location of the passenger. The pseudo-reward function and\\ntermination condition for the MaxNavigate(t) node only depend on the location of the taxi (and the\\nparameter t). Hence, this lemma applies, and the passenger source and destination are irrelevant\\nfor the MaxNavigate node.\\n\\n5.1.2 Condition 2: Leaf Irrelevance\\n\\nThe second abstraction condition describes situations under which we can apply state abstractions\\nto leaf nodes of the MAXQ graph. For leaf nodes, we can obtain a stronger result than Lemma 2\\nby using a slightly weaker deﬁnition of irrelevance.\\n\\nDeﬁnition 13 (Leaf Irrelevance) A set of state variables Y is irrelevant for a primitive action\\na of a MAXQ graph if for all states s the expected value of the reward function,\\n\\nV (a, s) =\\n\\nP (s\\n\\n|s, a)R(s\\n\\n|s, a)\\n\\n′\\n\\n′\\n\\nXs′\\n\\ndoes not depend on any of the values of the state variables in Y . In other words, for any pair of\\nstates s1 and s2 that diﬀer only in their values for the variables in Y ,\\n\\nP (s\\n\\n′\\n1|s1, a)R(s\\n\\n′\\n1|s1, a) =\\n\\nP (s\\n\\n′\\n2|s2, a)R(s\\n\\n′\\n2|s2, a).\\n\\nXs′\\nIf this condition is satisﬁed at leaf a, then the following lemma shows that we can represent its\\n\\nXs′\\n\\n1\\n\\n2\\n\\nvalue function V (a, s) compactly.\\n\\nLemma 4 Let M be an MDP with full-state MAXQ graph H, and suppose that state variables Y\\nare irrelevant for leaf node a. Let χ(s) = x be the associated abstraction function that maps s onto\\nthe remaining relevant variables X. Then we can represent V (a, s) for any state s by an abstracted\\nvalue function V (a, χ(s)) = V (a, x).\\n\\nProof: According to the deﬁnition of Leaf Irrelevance, any two states that diﬀer only on the\\nirrelevant state variables have the same value for V (a, s). Hence, we can represent this unique\\nvalue by V (a, x). Q.E.D.\\n\\nHere are two rules for ﬁnding cases where Leaf Irrelevance applies. The ﬁrst rule shows that if\\n\\nthe probability distribution factors, then we have Leaf Irrelevance.\\n\\nLemma 5 Suppose the probability transition function for primitive action a, P (s′|s, a), factors\\nas P (x′, y′|x, y, a) = P (y′|y, a)P (x′|x, a) and the reward function satisﬁes R(s′|s, a) = R(x′|x, a).\\nThen the variables in Y are irrelevant to the leaf node a.\\n\\n32\\n\\n\\x0cProof: Plug in to the deﬁnition of V (a, s) and simplify.\\n\\nV (a, s) =\\n\\nP (s\\n\\n|s, a)R(s\\n\\n|s, a)\\n\\n′\\n\\n′\\n\\n′\\n\\n=\\n\\n=\\n\\n=\\n\\nXs′\\n\\nXx′,y′\\n\\nXy′\\n\\nXx′\\n\\nP (y\\n\\n′\\n\\n′\\n|y, a)P (x\\n\\n|x, a)R(x\\n\\n|x, a)\\n\\n′\\n\\n′\\nP (x\\n\\n′\\n|x, a)R(x\\n\\n|x, a)\\n\\nP (y\\n\\n|y, a)\\n\\nXx′\\n|x, a)R(x\\n\\n′\\n\\n′\\nP (x\\n\\n|x, a)\\n\\nHence, the expected reward for the action a depends only on the variables in X and not on the\\nvariables in Y . Q.E.D.\\n\\nLemma 6 Let R(s′|s, a) = ra be the reward function for action a in MDP M , which is always\\nequal to a constant ra. Then the entire state s is irrelevant to the primitive action a.\\n\\nProof:\\n\\nV (a, s) =\\n\\nP (s\\n\\n|s, a)R(s\\n\\n|s, a)\\n\\n′\\n\\n′\\n\\n′\\n\\n=\\n\\nP (s\\n\\n|s, a)ra\\n\\nXs′\\n\\nXs′\\n= ra.\\n\\nThis does not depend on s, so the entire state is irrelevant to the primitive action a. Q.E.D.\\n\\nThis lemma is satisﬁed by the four leaf nodes North, South, East, and West in the taxi task,\\nbecause their one-step reward is a constant (−1). Hence, instead of requiring 2000 values to store\\nthe V functions, we only need 4 values—one for each action. Similarly, the expected rewards of the\\nPickup and Putdown actions each require only 2 values, depending on whether the corresponding\\nactions are legal or illegal. Hence, together, they require 4 values, instead of 1000 values.\\n\\n5.1.3 Condition 3: Result Distribution Irrelevance\\n\\nNow we consider a condition that results from “funnel” actions.\\n\\nDeﬁnition 14 (Result Distribution Irrelevance). A set of state variables Yj is irrelevant for\\nthe result distribution of action j if, for all abstract policies π executed by node j and its descendants\\nin the MAXQ hierarchy, the following holds:\\nfor all pairs of states s1 and s2 that diﬀer only in\\ntheir values for the state variables in Yj,\\n\\n′\\n\\nP π(s\\n\\n, N |s1, j) = P π(s\\n\\n′\\n\\n, N |s2, j)\\n\\nfor all s′ and N .\\n\\nLemma 7 Let M be an MDP with full-state MAXQ graph H, and suppose that the set of state\\nvariables Yj is irrelevant to the result distribution of action j, which is a child of Max node i. Let\\nχij be the associated abstraction function: χij(s) = x. Then we can deﬁne an abstract completion\\ncost function C π(i, χij(s), j) such that for all states s,\\n\\nC π(i, s, j) = C π(i, χij (s), j).\\n\\n33\\n\\n\\x0cProof: The completion function for ﬁxed policy π is deﬁned as follows:\\n\\nC π(i, s, j) =\\n\\n′\\n\\nP (s\\n\\n, N |s, j) · γN [ ˜Ri(s\\n\\n′\\n\\n) + Qπ(i, s\\n\\n)].\\n\\n′\\n\\n(21)\\n\\nXs′,N\\n\\nConsider any two states s1 and s2, such that χij(s1) = χij(s2) = x. Under Result Distribution\\nIrrelevance, their transition probability distributions are the same. Hence, the right-hand sides of\\n(21) have the same value, and we can conclude that\\n\\nC π(i, s1, j) = C π(i, s2, j).\\nTherefore, we can deﬁne an abstract completion function, C π(i, x, j) to represent this quantity.\\nQ.E.D.\\n\\nIt might appear that this condition would rarely be satisﬁed, and indeed, for inﬁnite horizon\\ndiscounted problems, this is true. Consider, for example, the Get subroutine under an optimal\\npolicy for the taxi task. No matter what location that taxi has in state s, the taxi will be at the\\npassenger’s starting location when the Get ﬁnishes executing (i.e., because the taxi will have just\\ncompleted picking up the passenger). Hence, the starting location is irrelevant to the resulting\\nlocation of the taxi. In the discounted cumulative reward setting, however, the number of steps\\nN required to complete the Get action will depend very much on the starting location of the taxi.\\nConsequently, P (s′, N |s, a) is not necessarily the same for any two states s with diﬀerent starting\\nlocations even though s′ is always the same.\\n\\nThe important lesson to draw from this is that discounting interferes with introducing state\\nabstractions based on “funnel” operators—the MAXQ framework is therefore less eﬀective when\\napplied in the discounted setting.\\n\\nHowever, if we restrict attention to the episodic, undiscounted setting, then the result dis-\\ntribution, P (s′|s, a), no longer depends on N , and the Result Distribution Irrelevance condition\\nis satisﬁed. Fortunately, the Taxi task is an undiscounted, ﬁnite-horizon task, so we can repre-\\nsent C(Root, s, Get) using 16 distinct values, because there are 16 equivalence classes of states (4\\nsource locations times 4 destination locations). This is much less than the 500 quantities in the\\nunabstracted representation.\\n\\n“Funnel” actions arise in many hierarchical reinforcement learning problems. For example,\\nabstract actions that move a robot to a doorway or that move a car onto the entrance ramp of a\\nfreeway have this property. The Result Distribution Irrelevance condition is applicable in all such\\nsituations as long as we are in the undiscounted setting.\\n\\n5.1.4 Condition 4: Termination\\n\\nThe fourth condition is closely related to the “funnel’ property.\\nIt applies when a subtask is\\nguaranteed to cause its parent task to terminate in a goal state. In a sense, the subtask is funneling\\nthe environment into the set of states described by the goal predicate of the parent task.\\n\\nLemma 8 (Termination). Let Mi be a task in a MAXQ graph such that for all states s where\\nthe goal predicate Gi(s) is true, the pseudo-reward function ˜Ri(s) = 0. Suppose there is a child task\\na and state s such that for all hierarchical policies π,\\n\\n′\\n\\n∀ s\\n\\n′\\n\\nP π\\n\\ni (s\\n\\n, N |s, a) > 0 ⇒ Gi(s\\n\\n).\\n\\n′\\n\\n(i.e., if s′ is a possible result state of applying a in s, then s′ is a goal terminal state for task i.)\\n\\nThen for any policy executed at node i, the completion cost C(i, s, a) is zero and does not need\\n\\nto be explicitly represented.\\n\\n34\\n\\n\\x0cProof: By the assumptions in the lemma, with probability 1 the completion cost is zero for any\\naction that results in a goal terminal state. Q.E.D.\\n\\nFor example, in the Taxi task, in all states where the taxi is holding the passenger, the Put\\nsubroutine will succeed and result in a goal terminal state for Root. This is because the termination\\npredicate for Put (i.e., that the passenger is at his or her destination location) implies the goal\\ncondition for Root (which is the same). This means that C(Root, s, Put) is uniformly zero, for all\\nstates s where Put is not terminated.\\n\\nIt is easy to detect cases where the Termination condition is satisﬁed. We only need to compare\\nthe termination predicate of a subtask with the goal predicate of the parent task. If the ﬁrst implies\\nthe second, then the termination condition is satisﬁed.\\n\\n5.1.5 Condition 5: Shielding\\n\\nThe shielding condition arises from the structure of the MAXQ graph.\\n\\nLemma 9 (Shielding). Let Mi be a task in a MAXQ graph and s be a state such that for all\\npaths from the root of the graph down to node Mi there exists a subtask j (possibly equal to i) whose\\ntermination predicate Tj(s) is true, then the Q nodes of Mi do not need to represent C values for\\nstate s.\\n\\nProof: Task i cannot be executed in state s, so no C values need to be estimated. Q.E.D.\\n\\nAs with the Termination condition, the Shielding condition can be veriﬁed by analyzing the\\n\\nstructure of the MAXQ graph and identifying nodes whose ancestor tasks are terminated.\\n\\nIn the Taxi task, a simple example of this arises in the Put task, which is terminated in all states\\nwhere the passenger is not in the taxi. This means that we do not need to represent C(Root, s, Put)\\nin these states. The result is that, when combined with the Termination condition above, we do\\nnot need to explcitly represent the completion function for Put at all!\\n\\n5.1.6 Dicussion\\n\\nBy applying these ﬁve abstraction conditions, we obtain the following “safe” state abstractions for\\nthe Taxi task:\\n\\n• North, South, East, and West. These terminal nodes require one quantity each, for a total of\\n\\nfour values. (Leaf Irrelevance).\\n\\n• Pickup and Putdown each require 2 values (legal and illegal states), for a total of four. (Leaf\\n\\nIrrelevance.)\\n\\n• QNorth(t), QSouth(t), QEast(t), and QWest(t) each require 100 values (four values for t and\\n\\n25 locations). (Max Node Irrelevance.)\\n\\n• QNavigateForGet requires 4 values (for the four possible source locations). (The passenger\\ndestination is Max Node Irrelevant for MaxGet, and the taxi starting location is Result Dis-\\ntribution Irrelevant for the Navigate action.)\\n\\n• QPickup requires 100 possible values, 4 possible source locations and 25 possible taxi locations.\\n\\n(Passenger destination is Max Node Irrelevant to MaxGet.)\\n\\n35\\n\\n\\x0c• QGet requires 16 possible values (4 source locations, 4 destination locations). (Result Distri-\\n\\nbution Irrelevance.)\\n\\n• QNavigateForPut requires only 4 values (for the four possible destination locations). (The\\npassenger source and destination are Max Node Irrelevant to MaxPut; the taxi location is\\nResult Distribution Irrelevant for the Navigate action.)\\n\\n• QPutdown requires 100 possible values (25 taxi locations, 4 possible destination locations).\\n\\n(Passenger source is Max Node Irrelevant for MaxPut.)\\n\\n• QPut requires 0 values. (Termination and Shielding.)\\n\\nThis gives a total of 632 distinct values, which is much less than the 3000 values required by\\nﬂat Q learning. Hence, we can see that by applying state abstractions, the MAXQ representation\\ncan give a much more compact representation of the value function. A key thing to note is that\\nthese state abstractions cannot be exploited with the ﬂat representation of the value function.\\n\\nWhat prior knowledge is required on the part of a programmer in order to introduce these state\\nabstractions? It suﬃces to know some general constraints on the one-step reward functions, the\\none-step transition probabilities, and termination predicates, goal predicates, and pseudo-reward\\nfunctions within the MAXQ graph. Speciﬁcally, the Max Node Irrelevance and Leaf Irrelevance\\nconditions require simple analysis of the one-step transition function and the reward and pseudo-\\nreward functions. Opportunities to apply the Result Distribution Irrelevance condition can be\\nfound by identifying “funnel” eﬀects that result from the deﬁnitions of the termination conditions\\nfor operators. Similarly, the Shielding and Termination conditions only require analysis of the\\ntermination predicates of the various subtasks. Hence, applying these ﬁve conditions to introduce\\nstate abstractions is a straightforward process, and once a model of the one-step transition and\\nreward functions has been learned, the abstraction conditions can be checked to see if they were\\nsatisﬁed.\\n\\n5.2 Convergence of MAXQ-Q with State Abstraction\\n\\nWe have shown that state abstractions can be safely introduced into the MAXQ value function\\ndecomposition under the ﬁve conditions described above. However, these conditions only guarantee\\nthat the value function of any ﬁxed abstract hierarchical policy can be represented—they do not\\nshow that the optimal policy can be represented, nor do they show that the MAXQ-Q learning\\nalgorithm will ﬁnd the optimal policy. The goal of this section is to prove these two results: (a) that\\nthe ordered recursively-optimal policy is an abstract policy and (b) that MAXQ-Q will converge\\nto this policy when applied to a MAXQ graph with safe state abstractions.\\n\\nLemma 10 Let M be an MDP with full-state MAXQ graph H and abstract-state MAXQ graph\\nχ(H) where the abstractions satisfy the ﬁve conditions given above. Let ρ be an ordering over all\\nactions in the MAXQ graph. Then the following statements are true:\\n\\n• The unique ordered recursively-optimal policy π∗\\n\\nr deﬁned by M , H, and ρ is an abstract policy\\n\\n(i.e., it depends only on the relevant state variables at each node),\\n\\n• The C and V functions in χ(H) can represent the projected value function of π∗\\nr .\\n\\n36\\n\\n\\x0cProof: The ﬁve abstraction lemmas tell us that if the ordered recursively-optimal policy is ab-\\nstract, then the C and V functions of χ(H) can represent its value function. Hence, the heart of\\nthis lemma is the ﬁrst claim. The last two forms of abstraction (Shielding and Termination) do not\\nplace any restrictions on abstract policies, so we ignore them in this proof.\\n\\nThe proof is by induction on the levels of the MAXQ graph, starting at the leaves. As a base\\ncase, let us consider a Max node i all of whose children are primitive actions. In this case, there are\\nno policies executed within the children of the Max node. Hence if variables Yi are irrelevant for\\nnode i, then we can apply our abstraction lemmas to represent the value function of any policy at\\nnode i—not just abstract policies. Consequently, the value function of any optimal policy for node\\ni can be represented, and it will have the property that Q∗(i, s1, a) = Q∗(i, s2, a) for any states s1\\nand s2 such that χi(s1) = χi(s2).\\n\\nNow let us impose the action ordering ρ to compute the optimal ordered policy. Consider two\\nactions a1 and a2 such that ρ(a1, a2) (i.e., ρ prefers a1), and suppose that there is a “tie” in the Q∗\\nfunction at state s1 such that the values\\n\\n∗\\n\\nQ\\n\\n(i, s1, a1) = Q\\n\\n(i, s1, a2)\\n\\n∗\\n\\nand they are the only two actions that maximize Q∗ in this state. Then the optimal ordered policy\\nmust choose a1. Now in all other states s2 such that χi(s1) = χi(s2), we know that the Q∗ values\\nwill be the same. Hence, the same tie will exist between a1 and a2, and hence, the optimal ordered\\npolicy must make the same choice in all such states. Hence, the optimal ordered policy for node i\\nis an abstract policy.\\n\\nNow let us turn to the recursive case at Max node i. Make the inductive assumption that the\\nordered recursively-optimal policy is abstract within all descendant nodes and consider the locally\\noptimal policy at node i. If Y is a set of state variables that are irrelevant to node i, Corollary 2\\ntells us that Q∗(i, s1, j) = Q∗(i, s2, j) for all states s1 and s2 such that χi(s1) = χi(s2). Similarly,\\nif Y is a set of variables irrelevant to the result distribution of a particular action j, then Lemma\\n7 tells us the same thing. Hence, by the same ordering argument given above, the ordered optimal\\npolicy at node i must be abstract. By induction, this proves the lemma. Q.E.D.\\n\\nWith this lemma, we have established that the combination of an MDP M , an abstract MAXQ\\ngraph H, and an action ordering deﬁnes a unique recursively-optimal ordered abstract policy. We\\nare now ready to prove that MAXQ-Q will converge to this policy.\\n\\nTheorem 4 Let M = hS, A, P, R, P0i be either an episodic MDP for which all deterministic policies\\nare proper or a discounted inﬁnite horizon MDP with discount factor γ < 1. Let H be an unab-\\nstracted MAXQ graph deﬁned over subtasks {M0, . . . , Mk} with pseudo-reward functions ˜Ri(s′|s, a).\\nLet χ(H) be a state-abstracted MAXQ graph deﬁned by applying state abstractions χi to each node i\\nof H under the ﬁve conditions given above. Let πx(i, χi(s)) be an abstract ordered GLIE exploration\\npolicy at each node i and state s whose decisions depend only on the “relevant” state variables at\\neach node i. Let π∗\\nr be the unique recursively-optimal hierarchical policy deﬁned by πx, M , and\\n˜R. Then with probability 1, algorithm MAXQ-Q applied to χ(H) converges to π∗\\nr provided that the\\nlearning rates αt(i) satisfy Equation (15) and |Vt(i, χi(s))| and |Ct(i, χi(s), a)| are bounded for all\\nt, i, χi(s), and a.\\n\\nProof: Rather than repeating the entire proof for MAXQ-Q, we will only describe what must\\nchange under state abstraction. The last two forms of state abstraction refer to states whose values\\ncan be inferred from the structure of the MAXQ graph, and therefore do not need to be represented\\n\\n37\\n\\n\\x0cat all. Since these values are not updated by MAXQ-Q, we can ignore them. We will now consider\\nthe ﬁrst three forms of state abstraction in turn.\\n\\nWe begin by considering primitive leaf nodes. Let a be a leaf node and let Y be a set of\\nstate variables that are Leaf Irrelevant for a. Let s1 = (x, y1) and s2 = (x, y2) be two states that\\ndiﬀer only in their values for Y . Under Leaf Irrelevance, the probability transitions P (s′\\n1|s1, a) and\\nP (s′\\n2|s2, a) need not be the same, but the expected reward of performing a in both states must be\\nthe same. When MAXQ-Q visits an abstract state x, it does not “know” the value of y, the part of\\nthe state that has been abstracted away. Nonetheless, it draws a sample according to P (s′|x, y, a),\\nreceives a reward R(s′|x, y, a), and updates its estimate of V (a, x) (line 5 of MAXQ-Q). Let Pt(y)\\nbe the probability that MAXQ-Q is visiting (x, y) given that the unabstracted part of the state is\\nx. Then Line 5 of MAXQ-Q is computing a stochastic approximation to\\n\\nWe can write this as\\n\\nPt(y)\\n\\n′\\n\\nPt(s\\n\\n, N |x, y, a)R(s\\n\\n|x, y, a).\\n\\nPt(y)Pt(s\\n\\n, N |x, y, a)R(s\\n\\n|x, y, a).\\n\\n′\\n\\n′\\n\\n′\\n\\nXs′,N,y\\n\\ny\\nX\\n\\nXs′,N\\n\\nAccording to Leaf Irrelevance, the inner sum has the same value for all states s such that χ(s) = x.\\nCall this value r0(x). This gives\\n\\nPt(y)r0(x),\\n\\ny\\nX\\n\\nwhich is equal to r0(x) for any distribution Pt(y). Hence, MAXQ-Q converges under Leaf Irrelevance\\nabstractions.\\n\\nNow let us turn to the two forms of abstraction that apply to internal nodes: Node Irrelevance\\nand Result Distribution Irrelevance. Consider the SMDP deﬁned at each node i of the abstracted\\nMAXQ graph at time t during MAXQ-Q. This would be an ordinary SMDP with transition prob-\\nability function Pt(x′, N |x, a) and reward function Vt(a, x) + ˜Ri(x′) except that when MAXQ-Q\\ndraws samples of state transitions, they are drawn according to the distribution Pt(s′, N |s, a) over\\nthe original state space. To prove the theorem, we must show that drawing (s′, N ) according to\\nthis second distribution is equivalent to drawing (x′, N ) according to the ﬁrst distribution.\\n\\nFor Max Node Irrelevance, we know that for all abstract policies applied to node i and its\\n\\ndescendants, the transition probability distribution factors as\\n\\n′\\n\\nP (s\\n\\n, N |s, a) = P (y\\n\\n′\\n\\n′\\n|y, a)P (x\\n\\n, N |x, a).\\n\\nBecause the exploration policy is an abstract policy, Pt(s′, N |s, a) factors in this way. This means\\nthat the Xi and Yi components of the state are independent of each other, and hence, sampling\\nfrom Pt(s′, N |s, a) gives samples for Pt(x′, N |x, a). Therefore, MAXQ-Q will converge under Max\\nNode Irrelevance abstractions.\\n\\nFinally, consider Result Distribution Irrelevance. Let j be a child of node i, and suppose Yj\\nis a set of state variables that are irrelevant to the result distribution of j. When the SMDP at\\nnode i wishes to draw a sample from Pt(x′, N |x, j), it does not “know” the current value of y, the\\nirrelevant part of the current state. However, this does not matter, because Result Distribution\\nIrrelevance means that for all possible values of y, Pt(x′, y′, N |x, y, j) is the same. Hence, MAXQ-Q\\nwill converge under Result Distribution Irrelevance abstractions.\\n\\nIn each of these three cases, MAXQ-Q will converge to a locally-optimal ordered policy at node\\ni in the MAXQ graph. By Lemma 10, this can be extended to produce a locally-optimal ordered\\npolicy for the unabstracted SMDP at node i. Hence, by induction, MAXQ-Q will converge to\\n\\n38\\n\\n\\x0cthe unique ordered recursively optimal policy π∗\\nexploration policy πx. Q.E.D.\\n\\nr deﬁned by MAXQ-Q H, MDP M , and ordered\\n\\n5.3 The Hierarchical Credit Assignment Problem\\n\\nThere are still some situations where we would like to introduce state abstractions but where the\\nﬁve properties described above do not permit them. Consider the following modiﬁcation of the\\ntaxi problem. Suppose that the taxi has a fuel tank and that each time the taxi moves one square,\\nit costs one unit of fuel. If the taxi runs out of fuel before delivering the passenger to his or her\\ndestination, it receives a reward of −20, and the trial ends. Fortunately, there is a ﬁlling station\\nwhere the taxi can execute a Fillup action to ﬁll the fuel tank.\\n\\nTo solve this modiﬁed problem using the MAXQ hierarchy, we can introduce another subtask,\\nRefuel, which has the goal of moving the taxi to the ﬁlling station and ﬁlling the tank. MaxRefuel\\nis a child of MaxRoot, and it invokes Navigate(t) (with t bound to the location of the ﬁlling station)\\nto move the taxi to the ﬁlling station.\\n\\nThe introduction of fuel and the possibility that we might run out of fuel means that we must\\ninclude the current amount of fuel as a feature in representing every C value (for internal nodes)\\nand V value (for leaf nodes). This is unfortunate, because our intuition tells us that the amount of\\nfuel should have no inﬂuence on our decisions inside the Navigate(t) subtask. The amount of fuel\\nshould be taken into account by the top-level Q nodes, which must decide whether to go refuel, go\\npick up the passenger, or go deliver the passenger.\\n\\nGiven this intuition, it is natural to try abstracting away the “amount of remaining fuel” within\\nthe Navigate(t) subtask. However, this doesn’t work, because when the taxi runs out of fuel and\\na −20 reward is given, the QNorth, QSouth, QEast, and QWest nodes cannot “explain” why this\\nreward was received—that is, they have no consistent way of setting their C tables to predict\\nwhen this negative reward will occur. Stated more formally, the diﬃculty is that the Max Node\\nIrrelevance condition is not satisﬁed because the one-step reward function R(s′|s, a) for these actions\\ndepends on the amount of fuel.\\n\\nWe call this the hierarchical credit assignment problem. The fundamental issue here is that in\\nthe MAXQ decomposition all information about rewards is stored in the leaf nodes of the hierarchy.\\nWe would like to separate out the basic rewards received for navigation (i.e., −1 for each action)\\nfrom the reward received for exhausting fuel (−20).\\nIf we make the reward at the leaves only\\ndepend on the location of the taxi, then the Max Node Irrelevance condition will be satisﬁed.\\n\\nOne way to do this is to have the programmer manually decompose the reward function and\\nindicate which nodes in the hierarchy will “receive” each reward. Let R(s′|s, a) =\\ni R(i, s′|s, a) be\\na decomposition of the reward function, such that R(i, s′|s, a) speciﬁes that part of the reward that\\nmust be handled by Max node i. In the modiﬁed taxi problem, for example, we can decompose the\\nreward so that the leaf nodes receive all of the original penalties, but the out-of-fuel rewards must\\nbe handled by MaxRoot. Lines 16 and 17 of the MAXQ-Q algorithm are easily modiﬁed to include\\nR(i, s′|s, a).\\n\\nP\\n\\nIn most domains, we believe it will be easy for the designer of the hierarchy to decompose the\\nreward function. It has been straightforward in all of the problems we have studied. However, an\\ninteresting problem for future research is to develop an algorithm that can solve the hierarchical\\ncredit assignment problem autonomously.\\n\\n39\\n\\n\\x0c6 Non-Hierarchical Execution of the MAXQ Hierarchy\\n\\nUp to this point in the paper, we have focused exclusively on representing and learning hierarchical\\npolicies. However, often the optimal policy for a MDP is not a strictly hierarchical policy. Kaelbling\\n(1993) ﬁrst introduced the idea of deriving a non-hierarchical policy from the value function of a\\nhierarchical policy. In this section, we exploit the MAXQ decomposition to generalize her ideas\\nand apply them recursively at all levels of the hierarchy.\\n\\nThe ﬁrst method is based on the dynamic programming algorithm known as policy iteration.\\nThe policy iteration algorithm starts with an initial policy π0. It then repeats the following two\\nsteps until the policy converges. In the policy evaluation step, it computes the value function V πk\\nof the current policy πk. Then, in the policy improvement step, it computes a new policy, πk+1\\naccording to the rule\\n\\nπk+1(s) := argmax\\n\\n′\\n\\nP (s\\n\\n|s, a)[R(s\\n\\n′\\n\\n|s, a) + γV πk(s\\n\\n)].\\n\\n′\\n\\n(22)\\n\\na\\n\\nXs′\\n\\nHoward (1960) proved that if πk is not an optimal policy, then πk+1 is guaranteed to be an im-\\nprovement. Note that in order to apply this method, we need to know the transition probability\\ndistribution P (s′|s, a) and the reward function R(s′|s, a).\\n\\nIf we know P (s′|s, a) and R(s′|s, a), we can use the MAXQ representation of the value function\\nto perform one step of policy iteration. We start with a hierarchical policy π and represent its value\\nfunction using the MAXQ hierarchy (e.g., π could have been learned via MAXQ-Q). Then, we can\\nperform one step of policy improvement by applying Equation (22) using V π(0, s′) (computed by\\nthe MAXQ hierarchy) to compute V π(s′).\\n\\ns′ P (s′|s, a)[R(s′|s, a) + γV π(0, s)], where V π(0, s) is the value\\nCorollary 3 Let πg(s) = argmaxa\\nfunction computed by the MAXQ hierarchy. Then, if π was not an optimal policy, πg is strictly\\nbetter for at least one state in S.\\n\\nP\\n\\nProof: This is a direct consequence of Howard’s policy improvement theorem. Q.E.D.\\n\\nUnfortunately, we can’t iterate this policy improvement process, because the new policy, πg is\\nvery unlikely to be a hierarchical policy (i.e., it is unlikely to be representable in terms of local\\npolicies for each node of the MAXQ graph). Nonetheless, one step of policy improvement can give\\nvery signiﬁcant improvements.\\n\\nThis approach to non-hierarchical execution ignores the internal structure of the MAXQ graph.\\nIn eﬀect, the MAXQ hierarchy is just viewed as a kind of function approximator for representing\\nV π—any other representation would give the same one-step improved policy πg.\\n\\nThe second approach to non-hierarchical execution borrows an idea from Q learning. One of\\nthe great beauties of the Q representation for value functions is that we can compute one step of\\npolicy improvement without knowing P (s′|s, a), simply by taking the new policy to be πg(s) :=\\nargmaxa Q(s, a). This gives us the same one-step greedy policy as we computed above using one-\\nstep lookahead. With the MAXQ decomposition, we can perform these policy improvement steps\\nat all levels of the hierarchy.\\n\\nWe have already deﬁned the function that we need.\\n\\nIn Table 3 we presented the function\\nEvaluateMaxNode, which, given the current state s, conducts a search along all paths from a\\ngiven Max node i to the leaves of the MAXQ graph and ﬁnds the path with the best value (i.e.,\\nwith the maximum sum of C values along the path, plus the V value at the leaf). In addition,\\nEvaluateMaxNode returns the primitive action a at the end of this best path. This action a would\\n\\n40\\n\\n\\x0cTable 5: The procedure for executing the one-step greedy policy.\\n\\nprocedure ExecuteHGPolicy(s)\\n\\nrepeat\\n\\nLet hV (0, s), ai := EvaluateMaxNode(0, s)\\nexecute primitive action a\\nLet s be the resulting state\\n\\nend // ExecuteHGPolicy\\n\\n1\\n2\\n3\\n4\\n\\nbe the ﬁrst primitive action to be executed if the learned hierarchical policy were executed starting\\nin the current state s. Our second method for non-hierarchical execution of the MAXQ graph is\\nto call EvaluateMaxNode in each state, and execute the primitive action a that is returned. The\\npseudo-code is shown in Table 5.\\n\\nWe will call the policy computed by ExecuteHGPolicy the hierarchical greedy policy, and denote\\nit πhg∗, where the superscript * indicates that we are computing the greedy action at each time\\nstep. The following theorem shows that this can give a better policy than the original, hierarchical\\npolicy.\\n\\nTheorem 5 Let G be a MAXQ graph representing the value function of hierarchical policy π (i.e.,\\nin terms of C π(i, s, j), computed for all i, s, and j). Let V hg(0, s) be the value computed by\\nExecuteHGPolicy, and let πhg∗ be the resulting policy. Deﬁne V hg∗ to be the value function of πhg∗.\\nThen for all states s, it is the case that\\n\\nV π(s) ≤ V hg(0, s) ≤ V hg∗\\n\\n(s).\\n\\n(23)\\n\\nProof:\\n(sketch) The left inequality in Equation (23) is satisﬁed by construction by line 7 of\\nEvaluateMaxNode. To see this, consider that the original hierarchical policy, π, can be viewed as\\nchoosing a “path” through the MAXQ graph running from the root to one of the leaf nodes, and\\nV π(0, s) is the sum of the C π values along this chosen path (plus the V π value at the leaf node). In\\ncontrast, EvaluateMaxNode performs a traversal of all paths through the MAXQ graph and ﬁnds\\nthe best path, that is, the path with the largest sum of C π (and leaf V π) values. Hence, V hg(0, s)\\nmust be at least as large as V π(0, s).\\n\\nTo establish the right inequality, note that by construction V hg(0, s) is the value function of a\\npolicy, call it πhg, that chooses one action greedily at each level of the MAXQ graph (recursively),\\nand then follows π thereafter. This is a consequence of the fact that line 7 of EvaluateMaxNode has\\nC π on its right-hand side, and C π represents the cost of “completing” each subroutine by following\\nπ, not by following some other, greedier, policy. (In Table 3, C π is written as Ct.) However, when\\nwe execute ExecuteHGPolicy (and hence, execute πhg∗), we have an opportunity to improve upon\\nπ and πhg at each time step. Hence, V hg(0, s) is an underestimate of the actual value of πhg∗.\\nQ.E.D.\\n\\nNote that this theorem only works in one direction. It says that if we can ﬁnd a state where\\nV hg(0, s) > V π(s), then the greedy policy, πhg∗, will be strictly better than π. However, it could\\nbe that π is not an optimal policy and yet the structure of the MAXQ graph prevents us from\\nconsidering an action (either primitive or composite) that would improve π. Hence, unlike the\\npolicy improvement theorem of Howard, we do not have a guarantee that if π is suboptimal, then\\nthe hierarchically greedy policy is a strict improvement.\\n\\n41\\n\\n\\x0cIn contrast, if we perform one-step policy improvement as discussed at the start of this section,\\nCorollary 3 guarantees that we will improve the policy. So we can see that in general, neither\\nof these two methods for non-hierarchical execution dominates the other. Nonetheless, the ﬁrst\\nmethod only operates at the level of individual primitive actions, so it is not able to produce very\\nlarge improvements in the policy. In contrast, the hierarchical greedy method can obtain very large\\nimprovements in the policy by changing which actions (i.e., subroutines) are chosen near the root\\nof the hierarchy. Hence, in general, hierarchical greedy execution is probably the better method.\\n(Of course, the value functions of both methods could be computed, and the one with the better\\nestimated value could be executed.)\\n\\nSutton, Singh, Precup and Ravindran (1999) have simultaneously developed a closely-related\\nmethod for non-hierarchical execution of macros. Their method is equivalent to ExecuteHGPolicy\\nfor the special case where the MAXQ hierarchy has only one level of subtasks. The interesting\\naspect of ExecuteHGPolicy is that it permits greedy improvements at all levels of the tree to\\ninﬂuence which action is chosen.\\n\\nSome care must be taken in applying Theorem 5 to a MAXQ hierarchy whose C values have\\nbeen learned via MAXQ-Q. Being an online algorithm, MAXQ-Q will not have correctly learned the\\nvalues of all states at all nodes of the MAXQ graph. For example, in the taxi problem, the value of\\nC(Put, s, QPutdown) will not have been learned very well except at the four special locations. This\\nis because the Put subtask cannot be executed until the passenger is in the taxi, and this usually\\nmeans that a Get has just been completed, so the taxi is at the passenger’s source location. During\\nexploration, both children of Put will be tried in such states. The PutDown will usually fail, whereas\\nthe Navigate will eventually succeed (perhaps after lengthy exploration) and take the taxi to the\\ndestination location. Now because of all states updating, the values for C(Put, s, Navigate(t)) will\\nhave been learned at all of the states, but the C values for the Putdown action will not. Hence, if\\nwe train the MAXQ representation using hierarchical execution (as in MAXQ-Q), and then switch\\nto hierarchically-greedy execution, the results will be quite bad. In particular, we need to introduce\\nhierarchically-greedy execution early enough so that the exploration policy is still actively exploring.\\n(In theory, a GLIE exploration policy never ceases to explore, but in practice, we want to ﬁnd a\\ngood policy quickly, not just asymptotically).\\n\\nOf course an alternative would be to use hierarchically-greedy execution from the very beginning\\nof learning. However, remember that the higher nodes in the MAXQ hierarchy need to obtain\\nsamples of P (s′, N |s, a) for each child action a.\\nIf the hierarchical greedy execution interrupts\\nchild a before it has reached a terminal state, then these samples cannot be obtained. Hence, it\\nis important to begin with purely hierarchical execution during training, and make a transition to\\ngreedy execution at some point.\\n\\nThe approach we have taken is to implement MAXQ-Q in such a way that we can specify a\\nnumber of primitive actions L that can be taken hierarchically before the hierarchical execution\\nis “interrupted” and control returns to the top level (where a new action can be chosen greedily).\\nWe start with L set very large, so that execution is completely hierarchical—when a child action\\nis invoked, we are committed to execute that action until it terminates. However, gradually, we\\nreduce L until it becomes 1, at which point we have hierarchical greedy execution. We time this\\nso that it reaches 1 at about the same time our Boltzmann exploration cools to a temperature of\\n0.1 (which is where exploration eﬀectively has halted). As the experimental results will show, this\\ngenerally gives excellent results with very little added exploration cost.\\n\\n42\\n\\n\\x0c7 Experimental Evaluation of the MAXQ Method\\n\\nWe have performed a series of experiments with the MAXQ method with three goals in mind: (a)\\nto understand the expressive power of the value function decomposition, (b) to characterize the\\nbehavior of the MAXQ-Q learning algorithm, and (c) to assess the relative importance of temporal\\nabstraction, state abstraction, and non-hierarchical execution. In this section, we describe these\\nexperiments and present the results.\\n\\n7.1 The Fickle Taxi Task\\n\\nOur ﬁrst experiments were performed on a modiﬁed version of the taxi task. This version incor-\\nporates two changes to the task described in Section 3.1. First, each of the four navigation actions\\nis noisy, so that with probability 0.8 it moves in the intended direction, but with probability 0.1\\nit instead moves to the right (of the intended direction) and with probability 0.1 it moves to the\\nleft. The second change is that after the taxi has picked up the passenger and moved one square\\naway from the passenger’s source location, the passenger changes his or her destination location\\nwith probability 0.3. The purpose of this change is to create a situation where the optimal policy\\nis not a hierarchical policy so that the eﬀectiveness of non-hierarchical execution can be measured.\\nWe compared four diﬀerent conﬁgurations of the learning algorithm: (a) ﬂat Q learning, (b)\\nMAXQ-Q learning without any form of state abstraction, (c) MAXQ-Q learning with state abstrac-\\ntion, and (d) MAXQ-Q learning with state abstraction and greedy execution. These conﬁgurations\\nare controlled by many parameters. These include the following: (a) the initial values of the Q\\nand C functions, (b) the learning rate (we employed a ﬁxed learning rate), (c) the cooling schedule\\nfor Boltzmann exploration (the GLIE policy that we employed), and (d) for non-hierarchical ex-\\necution, the schedule for decreasing L, the number of steps of consecutive hierarchical execution.\\nWe optimized these settings separately for each conﬁguration with the goal of matching or exceed-\\ning (with as few primitive actions as possible) the best policy that we could code by hand. For\\nBoltzmann exploration, we established an initial temperature and then a cooling rate. A separate\\ntemperature is maintained for each Max node in the MAXQ graph, and its temperature is reduced\\nby multiplying by the cooling rate each time that subtask terminates in a goal state.\\n\\nThe following parameters were chosen. For ﬂat Q learning: initial Q values of 0.123, learning\\nrate 0.25, and Boltzmann exploration with an initial temperature of 50 and a cooling rate of 0.9879.\\n(We use initial values that end in .123 as a “signature” to aid debugging.)\\n\\nFor MAXQ-Q learning without state abstraction, we used initial values of 0.123, a learning rate\\nof 0.50, and Boltzmann exploration with an initial temperature of 50 and cooling rates of .9996 at\\nMaxRoot and MaxPut, 0.9939 at MaxGet, and 0.9879 at MaxNavigate.\\n\\nFor MAXQ-Q learning with state abstraction, we used initial values of 0.123, a learning rate of\\n0.25, and Boltzmann exploration with an initial temperature of 50 and cooling rates of 0.9074 at\\nMaxRoot, 0.9526 at MaxPut, 0.9526 at MaxGet, and 0.9879 at MaxNavigate.\\n\\nFor MAXQ-Q learning with non-hierarchical execution, we used the same settings as with state\\nIn addition, we initialized L to 500 and decreased it by 10 with each trial until it\\n\\nabstraction.\\nreached 1. So after 50 trials, execution was completely greedy.\\n\\nFigure 7 shows the averaged results of 100 training trials. The ﬁrst thing to note is that all\\nforms of MAXQ learning have better initial performance than ﬂat Q learning. This is because of\\nthe constraints introduced by the MAXQ hierarchy. For example, while the agent is executing a\\nNavigate subtask, it will never attempt to pickup or putdown the passenger. Similarly, it will never\\nattempt to putdown the passenger until it has ﬁrst picked up the passenger (and vice versa).\\n\\nThe second thing to notice is that without state abstractions, MAXQ-Q learning actually takes\\n\\n43\\n\\n\\x0cd\\nr\\na\\nw\\ne\\nR\\n \\ne\\nv\\ni\\nt\\na\\nl\\nu\\nm\\nu\\nC\\n \\nn\\na\\ne\\n\\nM\\n\\n200\\n\\n0\\n\\n-200\\n\\n-400\\n\\n-600\\n\\n-800\\n\\n-1000\\n\\n0\\n\\nHierarchical Q Learning with State Abstraction and Greedy Execution\\nHierarchical Q Learning with State Abstraction\\nHierarchical Q Learning without State Abstraction\\nFlat Q Learning\\n\\n50000\\n\\n100000\\n\\n150000\\n\\n200000\\n\\n250000\\n\\nPrimitive Actions\\n\\nFigure 7: Comparison of performance of hierarchical Q learning with ﬂat Q learning, with and\\nwithout state abstractions, and with and without greedy evaluation.\\n\\nlonger to converge, so that the Flat Q curve crosses the MAXQ/no abstraction curve. This shows\\nthat without state abstraction, the cost of learning the huge number of parameters in the MAXQ\\nrepresentation is not really worth the beneﬁts.\\n\\nThe third thing to notice is that with state abstractions, MAXQ-Q converges very quickly to a\\nhierarchically optimal policy. This can be seen more clearly in Figure 8, which focuses on the range\\nof reward values in the neighborhood of the optimal policy. Here we can see that MAXQ with\\nabstractions attains the hierarchically optimal policy after approximately 40,000 steps, whereas\\nﬂat Q learning requires roughly twice as long to reach the same level. However, ﬂat Q learning, of\\ncourse, can continue onward and reach optimal performance, whereas with the MAXQ hierarchy,\\nthe best hierarchical policy is slow to respond to the “ﬁckle” behavior of the passenger when he/she\\nchanges the destination.\\n\\nThe last thing to notice is that with greedy execution, the MAXQ policy is also able to attain\\noptimal performance. But as the execution becomes “more greedy”, there is a drop in performance,\\nbecause MAXQ-Q must learn C values in new regions of the state space that were not visited by\\nthe recursively optimal policy. Despite this drop in performance, greedy MAXQ-Q recovers rapidly\\nand reaches hierarchically optimal performance faster than purely-hierarchical MAXQ-Q learning.\\nHence, there is no added cost—in terms of exploration—for introducing greedy execution.\\n\\nThis experiment presents evidence in favor of three claims: ﬁrst, that hierarchical reinforcement\\nlearning can be much faster than ﬂat Q learning; second, that state abstraction is required by\\nMAXQ for good performance; and third, that non-hierarchical execution can produce signiﬁcant\\nimprovements in performance with little or no added exploration cost.\\n\\n44\\n\\n\\x0cMAXQ Abstract+Greedy\\n\\nFlat Q\\n\\nOptimal Policy\\n\\nHier-Optimal Policy\\n\\nMAXQ Abstract\\n\\nMAXQ No Abstract\\n\\nd\\nr\\na\\nw\\ne\\nR\\n \\ne\\nv\\ni\\nt\\na\\nl\\nu\\nm\\nu\\nC\\n \\nn\\na\\ne\\n\\nM\\n\\n10\\n\\n5\\n\\n0\\n\\n-5\\n\\n-10\\n\\n-15\\n\\n0\\n\\n50000\\n\\n100000\\n\\n150000\\n\\n200000\\n\\n250000\\n\\n300000\\n\\nPrimitive Actions\\n\\nFigure 8: Close-up view of the previous ﬁgure. This ﬁgure also shows two horizontal lines indicating\\noptimal performance and hierarchically optimal performance in this domain. To make this ﬁgure\\nmore readable, we have applied a 100-step moving average to the data points.\\n\\n7.2 Kaelbling’s HDG Method\\n\\nThe second task that we will consider is a simple maze task introduced by Leslie Kaelbling\\n(1993) and shown in Figure 10. In each trial of this task, the agent starts in a randomly-chosen\\nstate and must move to a randomly-chosen goal state using the usual North, South, East, and West\\noperators (we employed deterministic operators). There is a small cost for each move, and the\\nagent must maximize the undiscounted sum of these costs.\\n\\nBecause the goal state can be in any of 100 diﬀerent locations, there are actually 100 diﬀerent\\nMDPs. Kaelbling’s HDG method starts by choosing an arbitrary set of landmark states and deﬁning\\na Voronoi partition of the state space based on the Manhattan distances to these landmarks (i.e.,\\ntwo states belong to the same Voronoi cell iﬀ they have the same nearest landmark). The method\\nthen deﬁnes one subtask for each landmark l. The subtask is to move from any state in the current\\nVoronoi cell or in any neighboring Voronoi cell to the landmark l. Optimal policies for these\\nsubtasks are then computed.\\n\\nOnce HDG has the policies for these subtasks, it can solve the abstract Markov Decision Problem\\nof moving from each landmark state to any other landmark state using the subtask solutions as\\nmacro actions (subroutines). So it computes a value function for this MDP.\\n\\nFinally, for each possible destination location g within a Voronoi cell for landmark l, the HDG\\n\\nmethod computes the optimal policy of getting from l to g.\\n\\nBy combining these subtasks, the HDG method can construct a good approximation to the\\noptimal policy as follows. In addition to the value functions discussed above, the agent maintains\\ntwo other functions: N L(s), the name of the landmark nearest to state s, and N (l), a list of the\\n\\n45\\n\\n\\x0c10\\n\\n9\\n\\n8\\n\\n7\\n\\n6\\n\\n5\\n\\n4\\n\\n3\\n\\n2\\n\\n1\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\nFigure 9: Kaelbling’s 10-by-10 navigation task. Each circled state is a landmark state, and the\\nheavy lines show the boundaries of the Voronoi cells. In each episode, a start state and a goal state\\nare chosen at random. In this ﬁgure, the start state is shown by the shaded hexagon, and the goal\\nstate is shown by the shaded square.\\n\\nlandmarks that are in the cells that are immediate neighbors of cell l. By combining these, the\\nagent can build a list for each state s of the current landmark and the landmarks of the neighboring\\ncells. For each such landmark, the agent computes the sum of three terms:\\n\\n(t1) the expected cost of reaching that landmark,\\n\\n(t2) the expected cost of moving from that landmark to the landmark in the goal cell, and\\n\\n(t3) the expected cost of moving from the goal-cell landmark to the goal state.\\n\\nNote that while terms (t1) and (t3) can be exact estimates, term (t2) is computed using the\\nlandmark subtasks as subroutines. This means that the corresponding path must pass through the\\nintermediate landmark states rather than going directly to the goal landmark. Hence, term (t2) is\\ntypically an overestimate of the required distance. (Also note that (t3) is the same for all choices\\nof the intermediate landmarks, so it does not need to be explicitly included in the computation.)\\n\\nGiven this information, the agent then chooses to move toward the best of the landmarks (unless\\nthe agent is already in the goal Voronoi cell, in which case the agent moves toward the goal state).\\nFor example, in Figure 9, term (t1) is the cost of reaching the landmark in row 7, column 4, which\\nis 4. Term (t2) is the cost of getting from row 7, column 4 to the landmark at row 1 column 4 (by\\ngoing from one landmark to another). In this case, the best landmark-to-landmark path is from\\nrow 7, column 1 to row 5 column 6, and then to row 1 column 4. Hence, term (t2) is 12. Term (t3)\\nis the cost of getting from row 1 column 4 to the goal, which is 2. The sum of these is 4 + 12 + 2\\n= 18. For comparison, the optimal path has length 10.\\n\\nIn Kaelbling’s experiments, she employed a variation of Q learning to learn terms (t1) and\\n(t3), and she computed (t2) at regular intervals via the Floyd-Warshall all-sources shortest paths\\nalgorithm.\\n\\n46\\n\\n\\x0cMaxRoot(g)\\n\\ngl/NL(g)\\n\\nQGotoGoalLmk(gl)\\n\\nQGotoGoal(g)\\n\\nMaxGotoGoalLmk(gl)\\n\\nQGotoLmk(l,gl)\\n\\nMaxGotoLmk(l)\\n\\nMaxGotoGoal(g)\\n\\nQNorthLmk(l)\\n\\nQSouthLmk(l)\\n\\nQEastLmk(l)\\n\\nQWestLmk(l)\\n\\nQNorthG(g)\\n\\nQSouthG(g)\\n\\nQEastG(g)\\n\\nQWestG(g)\\n\\nNorth\\n\\nSouth\\n\\nEast\\n\\nWest\\n\\nFigure 10: A MAXQ graph for the HDG navigation task.\\n\\nFigure 10 shows a MAXQ approach to solving this problem. The overall task Root, takes one\\n\\nargument g, which speciﬁes the goal cell. There are three subtasks:\\n\\n• GotoGoalLmk, go to the landmark nearest to the goal location. The termination for the\\npredicate is true if the agent reaches the landmark nearest to the goal. The goal predicate is\\nthe same as the termination predicate.\\n\\n• GotoLmk(l), go to landmark l. The termination predicate for this is true if either (a) the\\nagent reaches landmark l or (b) the agent is outside of the region deﬁned by the Voronoi cell\\nfor l and the neighboring Voronoi cells, N (l). The goal predicate for this subtask is true only\\nfor condition (a).\\n\\n• GotoGoal(g), go to the goal location g. The termination predicate for this subtask is true if\\neither the agent is in the goal location or the agent is outside of the Voronoi cell N L(g) that\\ncontains g. The goal predicate for this subtask is true if the agent is in the goal location.\\n\\nThe MAXQ decomposition is essentially the same as Kaelbling’s method, but somewhat redun-\\ndant. Consider a state where the agent is not inside the same Voronoi cell as the goal g. In such\\n\\n47\\n\\n\\x0cstates, HDG decomposes the value function into three terms (t1), (t2), and (t3). Similarly, MAXQ\\nalso decomposes it into these same three terms:\\n\\n• V (GotoLmk(l), s, a) the cost of getting to landmark l.\\n\\n(Actually the sum of V (a, s) and\\n\\nC(GotoLmk(l), s, a).)\\n\\nmark gl nearest the goal.\\n\\n• C(GotoGoalLmk(gl), s, M axGotoLmk(l)) the cost of getting from landmark l to the land-\\n\\n• C(Root, s, GotoGoalLmk(gl)) the cost of getting to the goal location after reaching gl.\\n\\nWhen the agent is inside the goal Voronoi cell, then again HDG and MAXQ store essentially\\nthe same information. HDG stores Q(GotoGoal(g), s, a), while MAXQ breaks this into two terms:\\nC(GotoGoal(g), s, a) and V (a, s) and then sums these two quantities to compute the Q value.\\n\\nNote that this MAXQ decomposition stores some information twice—speciﬁcally, the cost of\\ngetting from the goal landmark gl to the goal is stored both as C(Root, s, GotoGoalLmk(gl)) and\\nas C(GotoGoal(g), s, a) + V (a, s).\\n\\nLet us compare the amount of memory required by ﬂat Q learning, HDG, and MAXQ. There\\nare 100 locations, 4 possible actions, and 100 possible goal states, so ﬂat Q learning must store\\n40,000 values.\\n\\nTo compute quantity (t1), HDG must store 4 Q values (for the four actions) for each state s\\nwith respect to its own landmark and the landmarks in N (N L(s)). This gives a total of 2,028\\nvalues that must be stored.\\n\\nTo compute quantity (t2), HDG must store, for each landmark, information on the shortest\\npath to every other landmark. There are 12 landmarks. Consider the landmark at row 6, column\\n1.\\nIt has 5 neighboring landmarks which constitute the ﬁve macro actions that the agent can\\nperform to move to another landmark. The nearest landmark to the goal cell could be any of the\\nother 11 landmarks, so this gives a total of 55 Q values that must be stored. Similar computations\\nfor all 12 landmarks give a total of 506 values that must be stored.\\n\\nFinally, to compute quantity (t3), HDG must store information, for each square inside each\\nVoronoi cell, about how to get to each of the other squares inside the same Voronoi cell. This\\nrequires 3,536 values.\\n\\nHence, the grand total for HDG is 6,070, which is a huge savings over ﬂat Q learning.\\nNow let’s consider the MAXQ hierarchy with and without state abstractions.\\n\\n• V (a, s): This is the expected reward of each primitive action in each state. There are 100\\nstates and 4 primitive actions, so this requires 400 values. However, because the reward is\\nconstant (−1), we can apply Leaf Irrelevance to store only a single value.\\n\\n• C(GotoLmk(l), s, a), where a is one of the four primitive actions. This requires the same\\namount of space as (t1) in Kaelbling’s representation—indeed, combined with V (a, a), this\\nrepresents exactly the same information as (t1). It requires 2,028 values. No state abstractions\\ncan be applied.\\n\\n• C(GotoGoalLmk(gl), s, GotoLmk(l)): This is the cost of completing the GotoGoalLmk task after\\ngoing to landmark l. If the primitive actions are deterministic, then GotoLmk(l) will always\\nterminate at location l, and hence, we only need to store this for each pair of l and gl. This\\nis exactly the same as Kaelbling’s quantity (t2), which requires 506 values. However, if the\\nprimitive actions are stochastic—as they were in Kaelbling’s original paper—then we must\\nstore this value for each possible terminal state of each GotoLmk action. Each of these actions\\n\\n48\\n\\n\\x0cTable 6: Comparison of the number of values that must be stored to represent the value function\\nusing the HDG and MAXQ methods.\\n\\nHDG MAXQ\\nitem item\\n\\n(t1)\\n(t2)\\n(t3)\\n\\nV (a, s)\\nC(GotoLmk(l), s, a)\\nC(GotoGoalLmk, s, GotoLmk(l))\\nC(GotoGoal(g), s, a)\\nC(Root, s, GotoGoalLmk)\\nC(Root, s, GotoGoal)\\n\\nTotal Number of Values Required\\n\\nMAXQ\\n\\nHDG MAXQ MAXQ\\nno abs\\nvalues\\n400\\n0\\n2,028\\n2,028\\n6,600\\n506\\n3,536\\n3,536\\n100\\n0\\n96\\n0\\n12,760\\n6,070\\n\\nsafe abs unsafe abs\\n1\\n2,028\\n506\\n3,536\\n100\\n0\\n6,171\\n\\n1\\n2,028\\n6,600\\n3,536\\n100\\n96\\n12,361\\n\\ncould terminate at its target landmark l or in one of the states bordering the set of Voronoi\\ncells that are the neighbors of the cell for l. This requires 6,600 values. When Kaelbling\\nstores values only for (t2), she is eﬀectively making the assumption that GotoLmk(l) will\\nnever fail to reach landmark l. This is an approximation which we can introduce into the\\nMAXQ representation by our choice of state abstraction at this node.\\n\\n• C(GotoGoal, s, a): This is the cost of completing the GotoGoal task after making one of the\\nprimitive actions a. This is the same as quantity (t3) in the HDG representation, and it\\nrequires the same amoount of space: 3,536 values.\\n\\n• C(Root, s, GotoGoalLmk): This is the cost of reaching the goal once we have reached the\\nlandmark nearest the goal. MAXQ must represent this for all combinations of goal landmarks\\nand goals. This requires 100 values. Note that these values are the same as the values of\\nC(GotoGoal(g), s, a) + V (a, s) for each of the primitive actions. This means that the MAXQ\\nrepresentation stores this information twice, whereas the HDG representation only stores it\\nonce (as term (t3)).\\n\\n• C(Root, s, GotoGoal). This is the cost of completing the Root task after we have executed\\nthe GotoGoal task.\\nIf the primitive action are deterministic, this is always zero, because\\nGotoGoal will have reached the goal. Hence, we can apply the Termination condition and not\\nstore any values at all. However, if the primitive actions are stochastic, then we must store\\nthis value for each possible state that borders the Voronoi cell that contains the goal. This\\nrequires 96 diﬀerent values. Again, in Kaelbling’s HDG representation of the value function,\\nshe is ignoring the probability that GotoGoal will terminate in a non-goal state. Because\\nMAXQ is an exact representation of the value function, it does not ignore this possibility.\\nIf we (incorrectly) apply the Termination condition in this case, the MAXQ representation\\nbecomes a function approximation.\\n\\nIn the stochastic case, without state abstractions, the MAXQ representation requires 12,760\\nvalues. With safe state abstractions, it requires 12,361 values. With the approximations employed\\nby Kaelbling (or equivalently, if the primitive actions are deterministic), the MAXQ representation\\nwith state abstractions requires 6,171 values. These numbers are summarized in Table 6. We can\\nsee that, with the unsafe state abstractions, the MAXQ representation requires only slightly more\\nspace than the HDG representation (because of the redundancy in storing C(Root, s, GotoGoalLmk).\\n\\n49\\n\\n\\x0cThis example shows that for the HDG task, we can start with the fully-general formulation\\nprovided by MAXQ and impose assumptions to obtain a method that is similar to HDG. The\\nMAXQ formulation guarantees that the value function of the hierarchical policy will be represented\\nexactly. The assumptions will introduce approximations into the value function representation.\\nThis might be useful as a general design methodology for building application-speciﬁc hierarchical\\nrepresentations. Our long-term goal is to develop such methods so that each new application does\\nnot require inventing a new set of techniques. Instead, oﬀ-the-shelf tools (e.g., based on MAXQ)\\ncould be specialized by imposing assumptions and state abstractions to produce more eﬃcient\\nspecial-purpose systems.\\n\\nOne of the most important contributions of the HDG method was that it introduced a form of\\nnon-hierarchical execution. As soon as the agent crosses from one Voronoi cell into another, the\\ncurrent subtask is “interrupted”, and the agent recomputes the “current target landmark”. The\\neﬀect of this is that (until it reaches the goal Voronoi cell), the agent is always aiming for a landmark\\noutside of its current Voronoi cell. Hence, although the agent “aims for” a sequence of landmark\\nstates, it typically does not visit many of these states on its way to the goal. The states just provide\\na convenient set of intermediate targets. By taking these “shortcuts”, HDG compensates for the\\nfact that, in general, it has overestimated the cost of getting to the goal, because its computed\\nvalue function is based on a policy where the agent goes from one landmark to another.\\n\\nThe same eﬀect is obtained by hierarchical greedy execution of the MAXQ graph (which was\\ndirectly inspired by the HDG method). Note that by storing the N L (nearest landmark) function,\\nKaelbing’s HDG method can detect very eﬃciently when the current subtask should be interrupted.\\nThis technique only works for navigation problems in a space with a distance metric. In contrast,\\nExecuteHGPolicy performs a kind of “polling”, where it checks after each primitive action whether\\nit should interrupt the current subroutine and invoke a new one. An important goal for future\\nresearch on MAXQ is to ﬁnd a general purpose mechanism for avoiding unnecessary “polling”—\\nthat is, a mechanism that can discover eﬃciently-evaluable interrupt conditions.\\n\\nFigure 11 shows the results of our experiments with HDG using the MAXQ-Q learning al-\\ngorithm. We employed the following parameters:\\nfor Flat Q learning, initial values of 0.123, a\\nlearning rate of 1.0, initial temperature of 50, and cooling rate of .9074; for MAXQ-Q without\\ninitial values of −25.123, learning rate of 1.0, initial temperature of 50, and\\nstate abstractions:\\ncooling rates of .9074 for MaxRoot, .9999 for MaxGotoGoalLmk, .9074 for MaxGotoGoal, and .9526\\nfor MaxGotoLmk; for MAXQ-Q with state abstractions: initial values of −20.123, learning rate of\\n1.0, initial temperature of 50, and cooling rates of .9760 for MaxRoot, .9969 for MaxGotoGoal, .9984\\nfor MaxGotoGoalLmk, and .9969 for MaxGotoLmk. Hierarchical greedy execution was introduced\\nby starting with 3000 primitive actions per trial, and reducing this every trial by 2 actions, so that\\nafter 1500 trials, execution is completely greedy.\\n\\nThe ﬁgure conﬁrms the observations made in our experiments with the Fickle Taxi task. With-\\nout state abstractions, MAXQ-Q converges much more slowly than ﬂat Q learning. With state\\nabstractions, it converges roughly three times as fast. Figure 12 shows a close-up view of Figure 11\\nthat allows us to compare the diﬀerences in the ﬁnal levels of performance of the methods. Here,\\nwe can see that MAXQ-Q with no state abstractions was not able to reach the quality of our hand-\\ncoded hierarchical policy—presumably even more exploration would be required to achieve this,\\nwhereas with state abstractions, MAXQ-Q is able to do slightly better than our hand-coded policy.\\nWith hierarchical greedy execution, MAXQ-Q is able to reach the goal using one fewer action,\\non the average—so that it approaches the performance of the best hierarchical greedy policy (as\\ncomputed by value iteration). Notice however, that the best performance that can be obtained by\\nhierarchical greedy execution of the best recursively-optimal policy cannot match optimal perfor-\\nmance. Hence, Flat Q learning achieves a policy that reaches the goal state, on the average, with\\n\\n50\\n\\n\\x0cMAXQ\\n\\nFlat Q\\n\\nMAXQ No Abstractions\\n\\n0\\n\\n-20\\n\\n-40\\n\\n-60\\n\\n-80\\n\\n-100\\n\\n-120\\n\\n-140\\n\\nd\\nr\\na\\nw\\ne\\nR\\n \\ne\\nv\\ni\\nt\\na\\nl\\nu\\nm\\nu\\nC\\n \\nn\\na\\ne\\n\\nM\\n\\n0\\n\\n200000\\n\\n400000\\n\\n1e+06\\n\\n1.2e+06\\n\\n1.4e+06\\n\\n600000\\n800000\\nPrimitive Actions\\n\\nFigure 11: Comparison of Flat Q learning with MAXQ-Q learning with and without state abstrac-\\ntion. (Average of 100 runs.)\\n\\nabout one fewer primitive action. Finally notice that as in the taxi domain, there was no added\\nexploration cost for shifting to greedy execution.\\n\\n7.3 Parr and Russell: Hierarchies of Abstract Machines\\n\\nIn his (1998b) dissertation work, Ron Parr considered an approach to hierarchical reinforcement\\nlearning in which the programmer encodes prior knowledge in the form of a hierarchy of ﬁnite-state\\ncontrollers called a HAM (Hierarchy of Abstract Machines). The hierarchy is executed using a\\nprocedure-call-and-return discipline, and it provides a partial policy for the task. The policy is\\npartial because each machine can include non-deterministic, “choice” machine states, in which the\\nmachine lists several options for action but does not specify which one should be chosen. The\\nprogrammer puts “choice” states at any point where he/she does not know what action should\\nbe performed. Given this partial policy, Parr’s goal is to ﬁnd the best policy for making choices\\nin the choice states. In other words, his goal is to learn a hierarchical value function V (hs, mi),\\nwhere s is a state (of the external environment) and m contains all of the internal state of the\\nhierarchy (i.e., the contents of the procedure call stack and the values of the current machine states\\nfor all machines appearing in the stack). A key observation is that it is only necessary to learn this\\nvalue function at choice states hs, mi. Parr’s algorithm does not learn a decomposition of the value\\nfunction. Instead, it “ﬂattens” the hierarchy to create a new Markov decision problem over the\\nchoice states hs, mi. Hence, it is hierarchical primarily in the sense that the programmer structures\\nthe prior knowledge hierarchically. An advantage of this is that Parr’s method can ﬁnd the optimal\\nhierarchical policy subject to constraints provided by the programmer. A disadvantage is that the\\nmethod cannot be executed “non-hierarchically” to produce a better policy.\\n\\n51\\n\\n\\x0cMAXQ Greedy\\n\\nMAXQ\\n\\nOptimal Policy\\n\\nHierarchical Greedy Optimal Policy\\n\\nHierarchical Hand-coded Policy\\n\\nFlat Q\\n\\nMAXQ No Abstractions\\n\\nd\\nr\\na\\nw\\ne\\nR\\n \\ne\\nv\\ni\\nt\\na\\nl\\nu\\nm\\nu\\nC\\n \\nn\\na\\ne\\n\\nM\\n\\n-6\\n\\n-8\\n\\n-10\\n\\n-12\\n\\n-14\\n\\n0\\n\\n200000\\n\\n400000\\n\\n1e+06\\n\\n1.2e+06\\n\\n1.4e+06\\n\\n600000\\n800000\\nPrimitive Actions\\n\\nFigure 12: Expanded view comparing Flat Q learning with MAXQ-Q learning with and without\\nstate abstraction and with and without hierarchical greedy execution. (Average of 100 runs.)\\n\\nParr illustrated his work using the maze shown in Figure 13. This maze has a high-level structure\\n(i.e., as a series of hallways and intersections), and a low-level structure (a series of obstacles that\\nmust be avoided in order to move through the hallways and intersections). In each trial, the agent\\nstarts in the top left corner, and it must move to any state in the bottom right corner room. The\\nagent has the usual four primitive actions, North, South, East, and West. The actions are stochastic:\\nwith probability 0.8, they succeed, but with probability 0.1 the action will move to the “left” and\\nwith probability 0.1 the action will move to the “right” instead (e.g., a North action will move east\\nwith probability 0.1 and west with probability 0.1). If an action would collide with a wall or an\\nobstacle, it has no eﬀect.\\n\\nThe maze is structured as a series of “rooms”, each containing a 12-by-12 block of states (and\\nvarious obstacles). Some rooms are parts of “hallways”, because they contain walls on two opposite\\nsides, and they are open on the other two sides. Other rooms are “intersections”, where two or\\nmore hallways meet.\\n\\nTo test the representational power of the MAXQ hierarchy, we want to see how well it can\\nrepresent the prior knowledge that Parr is able to represent using the HAM. We begin by describing\\nParr’s HAM for his maze task, and then we will present a MAXQ hierarchy that captures much of\\nthe same prior knowledge.2\\n\\nParr’s top level machine, MRoot, consists of a loop with a single choice state that chooses among\\nfour possible child machines: MGo(East), MGo(South), MGo(W est), and MGo(N orth). The loop\\nterminates when the agent reaches a goal state. MRoot will only invoke a particular machine if there\\nis a hallway in the speciﬁed direction. Hence, in the start state, it will only consider MGo(South)\\n\\n2The author thanks Ron Parr for providing the details of the HAM for this task.\\n\\n52\\n\\n\\x0cand MGo(East).\\n\\nThe MGo(d) machine begins executing when the agent is in an intersection. So the ﬁrst thing it\\ntries to do is to exit the intersection into a hallway in speciﬁed direction d. Then it attempts\\nto traverse the hallway until it reaches another intersection.\\nIt does this by ﬁrst invoking a\\nExitIntersection(d) machine. When that machine returns, it then invokes a MExitHallway(d) ma-\\nchine. When that machine returns, MGo also returns.\\n\\nThe MExitIntersection and MExitHallway machines are identical except for their termination con-\\nditions. Both machines consist of a loop with one choice state that chooses among four possible sub-\\nroutines. To simplify their description, suppose that MGo(East) has chosen MExitIntersection(East).\\nThen the four possible subroutines are MSniﬀ(East, N orth), MSniﬀ(East, South), MBack(East, N orth),\\nand MBack(East, South).\\n\\nThe MSniﬀ(d, p) machine always moves in direction d until it encounters a wall (either part of\\nan obstacle or part of the walls of the maze). Then it moves in perpendicular direction p until it\\nreaches the end of the wall. A wall can “end” in two ways: either the agent is now trapped in a\\ncorner with walls in both directions d and p or else there is no longer a wall in direction d. In the\\nﬁrst case, the MSniﬀ machine terminates; in the second case, it resumes moving in direction d.\\n\\nThe MBack(d, p) machine moves one step backwards (in the direction opposite from d) and\\nthen moves ﬁve steps in direction p. These moves may or may not succeed, because the actions are\\nstochastic and there may be walls blocking the way. But the actions are carried out in any case,\\nand then the MBack machine returns.\\n\\nThe MSniﬀ and MBack machines also terminate if they reach the end of a hall or the end of an\\n\\nintersection.\\n\\nThese ﬁnite-state controllers deﬁne a highly constrained partial policy. The MBack, MSniﬀ,\\nand MGo machines contain no choice states at all. The only choice points are in MRoot, which\\nmust choose the direction in which to move, and in MExitIntersection and MExitHall, which must\\ndecide when to call MSniﬀ, when to call MBack, and which “perpendicular” direction to tell these\\nmachines to try when they cannot move forward.\\n\\nFigure 14 shows a MAXQ graph that encodes a similar set of constraints on the policy. The\\n\\nsubtasks are deﬁned as follows:\\n\\n• Root. This is exactly the same as the MRoot machine. It must choose a direction d and invoke\\nGo. It terminates when the agent enters a terminal state. This is also its goal condition (of\\ncourse).\\n\\n• Go(d, r). The parameter r is bound to the current 12-by-12 “room” in which the agent is\\nlocated. Go terminates when the agent enters the room at the end of the hallway in direction\\nd or when it leaves the desired hallway (e.g., in the wrong direction). The goal condition for\\nGo is satisﬁed only if the agent reaches the desired intersection.\\n\\n• ExitInter(d, r). This terminates when the agent has exited room r. The goal condition is that\\n\\nthe agent exit room r in direction d.\\n\\n• ExitHall(d, r). This terminates when the agent has exited the current hall (into some intersec-\\ntion). The goal condition is that the agent has entered the desired intersection in direction\\nd.\\n\\n• Sniﬀ(d, r). This encodes a subtask that is equivalent to the MSniﬀ machine. However, Sniﬀ\\nmust have two child subtasks, ToWall and FollowWall that were simply internal states of\\nMSniﬀ. This is necessary, because a subtask in the MAXQ framework cannot contain any\\n\\n53\\n\\n\\x0cinternal state, whereas a ﬁnite-state controller in the HAM representation can contain as\\nmany internal states as necessary. In particular, it can have one state for when it is moving\\nforward and another state for when it is following a wall sideways.\\n\\n• ToWall(d). This is equivalent to part of MSniﬀ, and it terminates when there is a wall\\nin “front” of the agent in direction d. The goal condition is the same as the termination\\ncondition.\\n\\n• FollowWall(d, p). This is equivalent to the other part of MSniﬀ. It moves in direction p until\\nthe wall in direction d ends (or until it is stuck in a corner with walls in both directions d\\nand p). The goal condition is the same as the termination condition.\\n\\n• Back(d, p, x, y). This attempts to encode the same information as the MBack machine, but\\nthis is a case where the MAXQ hierarchy cannot capture the same information. MBack simply\\nexecutes a sequence of 6 primitive actions (one step back, ﬁve stes in direction p). But to\\ndo this, MBack must have 6 internal states, which MAXQ does not allow. Instead, the Back\\nsubtask is has the subgoal of moving the agent at least one square backwards and at least 3\\nsquares in the direction p. In order to determine whether it has achieved this subgoal, it must\\nremember the x and y position where it started to execute, so these are bound as parameters\\nto Back. Back terminates if it achieves this subgoal or if it runs into walls that prevent it\\nfrom achieving the subgoal. The goal condition is the same as the termination condition.\\n\\n• BackOne(d, x, y). This moves the agent one step backwards (in the direction opposite to d.\\nIt needs the starting x and y position in order to tell when it has succeeded. It terminates\\nif it has moved at least one unit in direction d or if there is a wall in this direction. Its goal\\ncondition is the same as its termination condition.\\n\\n• PerpThree(p, x, y). This moves the agent three steps in the direction p. It needs the starting\\nx and y positions in order to tell when it has succeeded. It terminates when it has moved at\\nleast three units in the direction p or if there is a wall in that direction. The goal condition\\nis the same as the termination condition.\\n\\n• Move(d). This is a “parameterized primitive” action.\\n\\nIt executes one primitive move in\\n\\ndirection d and terminates immediately.\\n\\nFrom this, we can see that there are three major diﬀerences between the MAXQ representation\\nand the HAM representation. First, a HAM ﬁnite-state controller can contain internal states. To\\nconvert them into a MAXQ subtask graph, we must make a separate subtask for each internal state\\nin the HAM. Second, a HAM can terminate based on an “amount of eﬀort” (e.g., performing 5\\nactions), whereas a MAXQ subtask must terminate based on some change in the state of the world.\\nIt is impossible to deﬁne a MAXQ subtask that performs k steps and then terminate regardless of\\nthe eﬀects of those steps (i.e., without adding some kind of “counter” to the state of the MDP).\\nThird, it is more diﬃcult to formulate the termination conditions for MAXQ subtasks than for\\nHAM machines. For example, in the HAM, it was not necessary to specify that the MExitHallway\\nmachine terminates when it has entered a diﬀerent intersection than the one where the MGo was\\nexecuted. However, this is important for the MAXQ method, because in MAXQ, each subtask\\nlearns its own value function and policy—independent of its parent tasks. For example, without\\nthe requirement to enter a diﬀerent intersection, the learning algorithms for MAXQ will always\\nprefer to have MaxExitHall take one step backward and return to the room in which the Go action\\nwas started (because that is a much easier terminated state to reach). This problem does not arise\\n\\n54\\n\\n\\x0cin the HAM approach, because the policy learned for a subtask depends on the whole “ﬂattened”\\nhierarchy of machines, and returning to the state where the Go action was started does not help\\nsolve the overall problem of reaching the goal state in the lower right corner.\\n\\nTo construct the MAXQ graph for this problem, we have introduced three programming tricks:\\n(a) binding parameters to aspects of the current state (in order to serve as a kind of “local memory”\\nfor where the subtask began executing), (b) having a parameterized primitive action (in order to be\\nable to pass a parameter value that speciﬁes which primitive action to perform), and (c) employing\\n“inheritance of termination conditions”—that is, each subtask in this MAXQ graph (but not the\\nothers in this paper) inherits the termination conditions of all of its ancestor tasks. Hence, if the\\nagent is in the middle of executing a ToWall action when it leaves an intersection, the ToWall\\nsubroutine terminates because the ExitInter subroutine has terminated.\\nIf this satisﬁes the goal\\ncondition of ExitInter, then it is also considered to satisfy the goal condition of ToWall. This\\ninheritance made it easier to write the MAXQ graph, because the parents did not need to pass\\ndown to their children all of the information necessary to deﬁne the complete termination and goal\\npredicates.\\n\\nThere are essentially no opportunities for state abstraction in this task, because there are no ir-\\nrelevant features of the state. There are some opportunities to apply the Shielding and Termination\\nproperties, however. In particular, ExitHall(d) is guaranteed to cause its parent task, MaxGo(d) to\\nterminate, so it does not require any stored C values. There are many states where some subtasks\\nare terminated (e.g., Go(East) in any state where there is a wall on the east side of the room), and\\nso no C values need to be stored.\\n\\nNonetheless, even after applying the state elimination conditions, the MAXQ representation for\\nthis task requires much more space than a ﬂat representation. An exact computation is diﬃcult,\\nbut after applying MAXQ-Q learning, the MAXQ representation required 52,043 values, whereas\\nﬂat Q learning requires fewer than 16,704 values. Parr states that his method requires only 4,300\\nvalues.\\n\\nTo test the relative eﬀectiveness of the MAXQ representation, we compare MAXQ-Q learning\\nwith ﬂat Q learning. Because of the very large negative values that some states acquire (particularly\\nduring the early phases of learning), we were unable to get Boltzmann exploration to work well—\\none very bad trial would cause an action to receive such a low Q value, that it would never be tried\\nagain. Hence, we experimented with both ǫ-greedy exploration and counter-based exploration. The\\nǫ-greedy exploration policy is an ordered, abstract GLIE policy in which a random action is chosen\\nwith probability ǫ, and ǫ is gradually decreased over time. The counter-based exploration policy\\nkeeps track of how many times each action a has been executed in each state s. To choose an\\naction in state s, it selects the action that has been executed the fewest times until all actions\\nhave been executed T times. Then it switches to greedy execution. Hence, it is not a genuine\\nGLIE policy. Parr employed counter-based exploration policies in his experiments with this task.\\nlearning rate 0.50, initial value for ǫ of\\nFor Flat Q learning, we chose the following parameters:\\n1.0, ǫ decreased by 0.001 after each successful execution of a Max node, and initial Q values of\\n−200.123. For MAXQ-Q learning, we chose the following parameters: counter-based exploration\\nwith T = 10, learning rate equal to the reciprocal of the number of times an action had been\\nperformed, and initial values for the C values selected carefully to provide underestimates of the\\ntrue C values. For example, the initial values for QExitInter were −40.123, because in the worst\\ncase, after completing an ExitInter task, it takes about 40 steps to complete the subsequent ExitHall\\ntask and hence, complete the Go parent task.\\n\\nFigure 15 plots the results. We can see that MAXQ-Q learning converges about 10 times faster\\nthan Flat Q learning. We do not know whether MAXQ-Q has converged to a recursively optimal\\npolicy. For comparison, we also show the performance of a hierarchical policy that we coded\\n\\n55\\n\\n\\x0cby hand, but in our hand-coded policy, we used knowledge of contextual information to choose\\noperators, so this policy is surely better than the best recursively optimal policy. HAMQ learning\\nshould converge to a policy equal to or slightly better than our hand-coded policy.\\n\\nThis experiment demonstrates that the MAXQ representation can capture most—but not all—\\nof the prior knowledge that can be represented by the HAMQ hierarchy. It also shows that the\\nMAXQ representation requires much more care in the design of the goal conditions for the subtasks.\\n\\n7.4 Other Domains\\n\\nIn addition to the three domains discussed above, we have developed MAXQ graphs for Singh’s\\n(1992b) “ﬂag task”, the treasure hunter task described by Tadepalli and Dietterich (Tadepalli &\\nDietterich, 1997), and Dayan and Hinton’s (1993) Fuedal-Q learning task. All of these tasks can\\nbe easily and naturally placed into the MAXQ framework—indeed, all of them ﬁt more easily than\\nthe Parr and Russell maze task.\\n\\nMAXQ is able to exactly duplicate Singh’s work and his decomposition of the value function—\\nwhile using exactly the same amount of space to represent the value function. MAXQ can also\\nduplicate the results from Tadepalli and Dietterich—however, because MAXQ is not an explanation-\\nbased method, it is considerably slower and requires substantially more space to represent the value\\nfunction.\\n\\nIn the Feudal-Q task, MAXQ is able to give better performance than Feudal-Q learning. The\\nreason is that in Feudal-Q learning, each subroutine makes decisions using only a Q function\\nlearned at that level—that is, without information about the estimated costs of the actions of its\\nIn contrast, the MAXQ value function decomposition permits each Max node to\\ndescendants.\\nmake decisions based on the sum of its completion function, C(i, s, j), and the costs estimated by\\nits descendants, V (j, s). Of course, MAXQ also supports non-hierarchical execution, which is not\\npossible for Feudal-Q, because it does not learn a value function decomposition.\\n\\n8 Discussion: Design Tradeoﬀs in Hierarchical Reinforcement\\n\\nLearning\\n\\nAt the start of this paper, we discussed four issues concerning the design of hierarchical reinforce-\\nment learning architectures. In this section, we want to highlight a tradeoﬀ between two of those\\nissues: the method for deﬁning subtasks and the use of state abstraction.\\n\\nMAXQ deﬁnes subtasks using a termination predicate Ti and a pseudo-reward function ˜R. There\\nare at least two drawbacks of this method. First, it can be hard for the programmer to deﬁne Ti\\nand ˜R correctly, since this essentially requires guessing the value function of the optimal policy\\nfor the MDP at all states where the subtask terminates. Second, it leads us to seek a recursively\\noptimal policy rather than a hierarchically optimal policy. Recursively optimal policies may be\\nmuch worse than hierarchically optimal ones, so we may be giving up substantial performance.\\n\\nHowever, in return for these two drawbacks, MAXQ obtains a very important beneﬁt: the\\npolicies and value functions for subtasks become context-free. In other words, they do not depend\\non their parent tasks or the larger context in which they are invoked. To understand this point,\\nconsider again the MDP shown in Figure 6.\\nIt is clear that the optimal policy for exiting the\\nleft-hand room (the Exit subtask) depends on the location of the goal. If it is at the top of the\\nright-hand room, then the agent should prefer to exit via the upper door, whereas if it is at the\\nbottom of the right-hand room, the agent should prefer to exit by the lower door. However, if\\nwe deﬁne the subtask of exiting the left-hand room using a pseudo-reward of zero for both doors,\\n\\n56\\n\\n\\x0cthen we obtain a policy that is not optimal in either case, but a policy that we can re-use in both\\ncases. Furthermore, this policy does not depend on the location of the goal. Hence, we can apply\\nMax node irrelevance to solve the Exit subtask using only the location of the robot and ignore the\\nlocation of the goal.\\n\\nThis example shows that we obtain the beneﬁts of subtask reuse and state abstraction because\\nwe deﬁne the subtask using a termination predicate and a pseudo-reward function. The termination\\npredicate and pseudo-reward function provide a barrier that prevents “communication” of value\\ninformation between the Exit subtask and its context.\\n\\nCompare this to Parr’s HAM method. The HAMQ algorithm ﬁnds the best policy consistent\\nwith the hierarchy. To achieve this, it must permit information to propagate “into” the Exit subtask\\n(i.e., the Exit ﬁnite-state controller) from its environment. But this means that if any state that is\\nreached after leaving the Exit subtask has diﬀerent values depending on the location of the goal,\\nthen these diﬀerent values will propagate back into the Exit subtask. To represent these diﬀerent\\nvalues, the Exit subtask must know the location of the goal. In short, to achieve a hierarchically\\noptimal policy within the Exit subtask, we must (in general) represent its value function using the\\nentire state space.\\n\\nWe can see, therefore, that there is a direct tradeoﬀ between achieving hierarchical optimality\\nand achieving recursive optimality. Methods for hierarchical optimality have more freedom in\\ndeﬁning subtasks (e.g., using complete policies, as in the option approach, or using partial policies,\\nas in the HAM approach). But they cannot employ state abstractions within subtasks, and in\\ngeneral, they cannot reuse the solution of one subtask in multiple contexts. Methods for recursive\\noptimality, on the other hand, must deﬁne subtasks using some method (such as pseudo-reward\\nfunctions) that isolates the subtask from its context. But in return, they can apply state abstraction\\nand the learned policy can be reused in many contexts (where it will be more or less optimal).\\n\\nIt is interesting that the iterative method described by Dean and Lin (1995) can be viewed as\\na method for moving along this tradeoﬀ. In the Dean and Lin method, the programmer makes an\\ninitial guess for the values of the terminal states of each subtask (i.e., the doorways in Figure 6).\\nBased on this initial guess, the locally optimal policies for the subtasks are computed. Then\\nthe locally optimal policy for the parent task is computed—while holding the subtask policies\\nﬁxed (i.e., treating them as options). At this point, their algorithm has computed the recursively\\noptimal solution to the original problem, given the initial guesses. Instead of solving the various\\nsubproblems sequentially via an oﬄine algorithm, we could use the MAXQ-Q learning algorithm.\\nBut the method of Dean and Lin does not stop here. Instead, it computes new values of the\\nterminal states of each subtask based on the learned value function for the entire problem. This\\nallows it to update its “guesses” for the values of the terminal states. The entire solution process\\ncan now be repeated. To obtain a new recursively optimal solution, based on the new guesses.\\nThey prove that if this process is iterated indeﬁnitely, it will converge to the recursively optimal\\npolicy (provided, of course, that no state abstractions are used within the subtasks).\\n\\nThis suggests an extension to MAXQ-Q learning that adapts the ˜R values online. Each time a\\nsubtask terminates, we could update the ˜R function based on the computed value of the terminated\\nstate. To be precise, if j is a subtask of i, then when j terminates in state s′, we should update\\n˜R(j, s′) to be equal to ˜V (i, s′) = maxa′ ˜Q(i, s′, a′). However, this will only work if ˜R(j, s′) is\\nrepresented using the full state s′. If subtask j is employing state abstractions, x = χ(s), then\\n˜R(j, x′) will need to be the average value of ˜V (i, s′), where the average is taken over all states s′ such\\nthat x′ = χ(s′) (weighted by the probability of visiting those states). This is easily accomplished\\nby performing a stochastic approximation update of the form\\n\\n′\\n˜R(j, x\\n\\n′\\n) = (1 − αt) ˜R(j, x\\n\\n) + αt ˜V (i, s\\n\\n′\\n\\n)\\n\\n57\\n\\n\\x0ceach time subtask j terminates. Such an algorithm could be expected to converge to the best\\nhierarchical policy consistent with the given state abstractions.\\n\\nThis also suggests that in some problems, it may be worthwhile to ﬁrst learn a recursively\\noptimal policy using very aggressive state abstractions and then use the learned value function\\nto initialize a MAXQ representation with a more detailed representation of the states. These\\nprogressive reﬁnements of the state space could be guided by monitoring the degree to which the\\nvalues of ˜V (i, s′) vary for a single abstract state x′. If they have a large variance, this means that\\nthe state abstractions are failing to make important distinctions in the values of the states, and\\nthey should be reﬁned.\\n\\nBoth of these kinds of adaptive algorithms will take longer to converge than the basic MAXQ\\nmethod described in this paper. But for tasks that an agent must solve many times in its lifetime,\\nit is worthwhile to have learning algorithms that provide an initial useful solution but gradually\\nimprove that solution until it is optimal. An important goal for future research is to ﬁnd methods\\nfor diagnosing and repairing errors (or sub-optimalities) in the initial hierarchy so that ultimately\\nthe optimal policy is discovered.\\n\\n9 Concluding Remarks\\n\\nThis paper has introduced a new representation for the value function in hierarchical reinforcement\\nlearning—the MAXQ value function decomposition. We have proved that the MAXQ decompo-\\nsition can represent the value function of any hierarchical policy under both the ﬁnite-horizon\\nundiscounted, cumulative reward criterion and the inﬁnite-horizon discounted reward criterion.\\nThis representation supports subtask sharing and re-use, because the overall value function is de-\\ncomposed into value functions for individual subtasks.\\n\\nThe paper introduced a learning algorithm, MAXQ-Q learning, and proved that it converges\\nwith probability 1 to a recursively optimal policy. The paper argued that although recursive\\noptimality is weaker than either hierarchical optimality or global optimality, it is an important\\nform of optimality because it permits each subtask to learn a locally optimal policy while ignoring\\nthe behavior of its ancestors in the MAXQ graph. This increases the opportunities for subtask\\nsharing and state abstraction.\\n\\nWe have shown that the MAXQ decomposition creates opportunities for state abstraction, and\\nwe identiﬁed a set of ﬁve properties (Max Node Irrelevance, Leaf Irrelevance, Result Distribution\\nIrrelevance, Shielding, and Termination) that allow us to ignore large parts of the state space\\nwithin subtasks. We proved that MAXQ-Q still converges in the presence of these forms of state\\nabstraction, and we showed experimentally that state abstraction is important in practice for the\\nsuccessful application of MAXQ-Q learning—at least in the Taxi and Kaelbling HDG tasks.\\n\\nThe paper presented two diﬀerent methods for deriving improved non-hierarchical policies from\\nthe MAXQ value function representation, and it has formalized the conditions under which these\\nmethods can improve over the hierarchical policy. The paper veriﬁed experimentally that non-\\nhierarchical execution gives improved performance in the Fickle Taxi Task (where it achieves opti-\\nmal performance) and in the HDG task (where it gives a substantial improvement).\\n\\nFinally, the paper has argued that there is a tradeoﬀ governing the design of hierarchical rein-\\nforcement learning methods. At one end of the design spectrum are “context free” methods such\\nas MAXQ. They provide good support for state abstraction and subtask sharing but they can only\\nlearn recursively optimal policies. At the other end of the spectrum are “context-sensitive” meth-\\nods such as HAMQ, the options framework, and the early work of Dean and Lin. These methods\\ncan discover hierarchically optimal policies (or, in some cases, globally optimal policies), but their\\n\\n58\\n\\n\\x0cdrawback is that they cannot easily exploit state abstractions or share subtasks. Because of the\\ngreat speedups that are enabled by state abstraction, this paper has argued that the context-free\\napproach is to be preferred—and that it can be relaxed as needed to obtain improved policies.\\n\\nAcknowledgements\\n\\nThe author gratefully acknowledges the support of the National Science Foundation under grant\\nnumber IRI-9626584, the Oﬃce of Naval Research under grant number N00014-95-1-0557, the Air\\nForce Oﬃce of Scientiﬁc Research under grant number F49620-98-1-0375, and the Spanish Council\\nfor Scientiﬁc Research. In addition, the author is indebted to many colleagues for helping develop\\nand clarify the ideas in this paper including Valentina Bayer, Leslie Kaelbling, Bill Langford,\\nWes Pinchot, Rich Sutton, Prasad Tadepalli, and Sebastian Thrun. I particularly want to thank\\nEric Chown for encouraging me to study Feudal reinforcement learning, Ron Parr for providing the\\ndetails of his HAM machines, and Sebastian Thrun encouraging me to write a single comprehensive\\npaper. I also thank the anonymous reviewers of previous drafts of this paper for their suggestions\\nand careful reading, which have improved the paper immeasurably.\\n\\nReferences\\n\\nBelmont, MA.\\n\\nBellman, R. E. (1957). Dynamic Programming. Princeton University Press.\\n\\nBertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientiﬁc,\\n\\nBoutilier, C., Shoham, Y., & Wellman, M. (1997). Economic principles of multi-agent systems\\n\\n(Editorial). Artiﬁcial Intelligence, 94 (1–2), 1–6.\\n\\nBoutilier, C., Dean, T., & Hanks, S. (1999). Decision theoretic planning: Structural assumptions\\n\\nand computational leverage. Journal of Artiﬁcial Intelligence Research, To appear.\\n\\nCurrie, K., & Tate, A. (1991). O-plan: The open planning architecture. Artiﬁcial Intelligence,\\n\\n52 (1), 49–86.\\n\\nDayan, P., & Hinton, G. (1993). Feudal reinforcement learning. In Advances in Neural Information\\n\\nProcessing Systems, 5, pp. 271–278. Morgan Kaufmann, San Francisco, CA.\\n\\nDean, T., & Lin, S.-H. (1995). Decomposition techniques for planning in stochastic domains.\\nTech. rep. CS-95-10, Department of Computer Science, Brown University, Providence, Rhode\\nIsland.\\n\\nDietterich, T. G. (1998). The MAXQ method for hierarchical reinforcement learning. In Fifteenth\\n\\nInternational Conference on Machine Learning.\\n\\nFikes, R. E., Hart, P. E., & Nilsson, N. J. (1972). Learning and executing generalized robot plans.\\n\\nArtiﬁcial Intelligence, 3, 251–288.\\n\\nHauskrecht, M., Meuleau, N., Boutilier, C., Kaelbling, L., & Dean, T. (1998). Hierarchical solution\\nof Markov decision processes using macro-actions. Tech. rep., Brown University, Department\\nof Computer Science, Providence, RI.\\n\\nHoward, R. A. (1960). Dynamic Programming and Markov Processes. MIT Press, Cambridge, MA.\\n\\n59\\n\\n\\x0cJaakkola, T., Jordan, M. I., & Singh, S. P. (1994). On the convergence of stochastic iterative\\n\\ndynamic programming algorithms. Neural Computation, 6 (6), 1185–1201.\\n\\nKaelbling, L. P. (1993). Hierarchical reinforcement learning: Preliminary results. In Proceedings\\nof the Tenth International Conference on Machine Learning, pp. 167–173 San Francisco, CA.\\nMorgan Kaufmann.\\n\\nKnoblock, C. A. (1990). Learning abstraction hierarchies for problem solving. In Proceedings of the\\nEighth National Conference on Artiﬁcial Intelligence, pp. 923–928 Boston, MA. AAAI Press.\\n\\nKorf, R. E. (1985). Macro-operators: A weak method for learning. Artiﬁcial Intelligence, 26 (1),\\n\\n35–77.\\n\\nLin, L.-J. (1993). Reinforcement learning for robots using neural networks. Ph.D. thesis, Carnegie\\n\\nMellon University, Department of Computer Science, Pittsburgh, PA.\\n\\nParr, R. (1998a). Flexible decomposition algorithms for weakly coupled markov decision problems.\\nIn Proceedings of the Fourteenth Annual Conference on Uncertainty in Artiﬁcial Intelligence\\n(UAI–98), pp. 422–430 San Francisco, CA. Morgan Kaufmann Publishers.\\n\\nParr, R. (1998b). Hierarchical control and learning for Markov decision processes. Ph.D. thesis,\\n\\nUniversity of California, Berkeley, California.\\n\\nParr, R., & Russell, S. (1998). Reinforcement learning with hierarchies of machines. In Advances\\n\\nin Neural Information Processing Systems, Vol. 10 Cambridge, MA. MIT Press.\\n\\nPearl, J. (1988). Probabilistic Inference in Intelligent Systems. Networks of Plausible Inference.\\n\\nMorgan Kaufmann, San Mateo, CA.\\n\\nRummery, G. A., & Niranjan, M. (1994). Online Qlearning using connectionist systems. Tech.\\nrep. CUED/FINFENG/TR 166, Cambridge University Engineering Department, Cambridge,\\nEngland.\\n\\nSacerdoti, E. D. (1974). Planning in a hierarchy of abstraction spaces. Artiﬁcial Intelligence, 5 (2),\\n\\n115–135.\\n\\nSingh, S., Jaakkola, T., Littman, M. L., & Szepesv´ari, C. (1998). Convergence results for single-step\\non-policy reinforcement-learning algorithms. Tech. rep., University of Colorado, Department\\nof Computer Science, Boulder, CO.\\n\\nSingh, S. P. (1992a). Transfer of learning by composing solutions of elemental sequential tasks.\\n\\nSingh, S. P. (1992b). Transfer of learning by composing solutions of elemental sequential tasks.\\n\\nMachine Learning, 8, 323–339.\\n\\nMachine Learning, 8, 323.\\n\\nSutton, R. S., Singh, S., Precup, D., & Ravindran, B. (1999). Improved switching among temporally\\nabstract actions. In Advances in Neural Information Processing Systems, Vol. 11. MIT Press.\\n\\nSutton, R., & Barto, A. G. (1998). Introduction to Reinforcement Learning. MIT Press, Cambridge,\\n\\nMA.\\n\\n60\\n\\n\\x0cSutton, R. S., Precup, D., & Singh, S. (1998). Between MDPs and Semi-MDPs: Learning, plan-\\nning, and representing knowledge at multiple temporal scales. Tech. rep., University of Mas-\\nsachusetts, Department of Computer and Information Sciences, Amherst, MA.\\n\\nTadepalli, P., & Dietterich, T. G. (1997). Hierarchical explanation-based reinforcement learning.\\nIn Proceedings of the Fourteenth International Conference on Machine Learning, pp. 358–366\\nSan Francisco, CA. Morgan Kaufmann.\\n\\nWatkins, C. J. C. H. (1989). Learning from Delayed Rewards. Ph.D. thesis, King’s College, Oxford.\\n\\n(To be reprinted by MIT Press.).\\n\\nWatkins, C. J., & Dayan, P. (1992). Technical note q-learning. Machine Learning, 8, 279.\\n\\n61\\n\\n\\x0cGoal\\n\\nFigure 13: Parr’s maze problem. The start state is in the upper left corner, and all states in the\\nlower right-hand room are terminal states.\\n\\n62\\n\\n\\x0cQExitInter(d,r)\\n\\nQExitHall(d,r)\\n\\nMaxExitInter(d,r)\\n\\nMaxExitHall(d,r)\\n\\nQSniffEI(d,p)\\n\\nQBackEI(d,p)\\n\\nQSniffEH(d,p)\\n\\nQBackEH(d,p)\\n\\nMaxSniff(d,p)\\n\\nMaxBack(d,p,x,y)\\n\\nx/X\\n\\ny/Y\\n\\nx/X\\n\\ny/Y\\n\\nQFollowWall(d,p)\\n\\nQToWall(d)\\n\\nQBackOne(d)\\n\\nQPerpThree(p)\\n\\nMaxFollowWall(d,p)\\n\\nMaxToWall(d)\\n\\nMaxBackOne(d)\\n\\nMaxPerpThree(p)\\n\\nd/p\\n\\nd/d\\n\\nd/Inv(d)\\n\\nd/p\\n\\nQMoveFW(d)\\n\\nQMoveTW(d)\\n\\nQMoveBO(d)\\n\\nQMoveP3(d)\\n\\nFigure 14: MAXQ graph for Parr’s maze task.\\n\\nMaxRoot\\n\\nGo(d)\\n\\nr/ROOM\\n\\nMaxGo(d,r)\\n\\nMaxMove(d)\\n\\n63\\n\\n\\x0cHand-coded hierarchical policy\\n\\n-350\\n\\nMAXQ Q Learning\\n\\nFlat Q Learning\\n\\nl\\na\\ni\\nr\\nT\\n \\nr\\ne\\nP\\n \\nd\\nr\\na\\nw\\ne\\nR\\n \\nn\\na\\ne\\n\\nM\\n\\n-100\\n\\n-150\\n\\n-200\\n\\n-250\\n\\n-300\\n\\n-400\\n\\n-450\\n\\n-500\\n\\n0\\n\\n1e+06\\n\\n2e+06\\n\\n4e+06\\n\\n5e+06\\n\\n6e+06\\n\\n3e+06\\nPrimitive Steps\\n\\nFigure 15: Comparison of Flat Q learning and MAXQ-Q learning in the Parr maze task.\\n\\n64\\n\\n\\x0c'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedText = text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedText = re.sub(r'-\\n','-', cleanedText) #Remove -\\n issues\n",
    "cleanedText = re.sub(r'\\n',' ',cleanedText)#Get rid of new lines replace with spaces\n",
    "\n",
    "cleanedText = re.sub(r'- ','',cleanedText) #To get rid of hyphens next lines\n",
    "\n",
    "cleanedText = re.sub(r'\\x0c','', cleanedText) #Remove page breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedText = cleanedText.split(\"Abstract \",1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This paper presents a new approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The decomposition, known as the MAXQ decomposition, has both a procedural semantics—as a subroutine hierarchy—and a declarative semantics—as a representation of the value function of a hierarchical policy. MAXQ uniﬁes and extends previous work on hierar-chical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. It is based on the assumption that the programmer can identify useful subgoals and deﬁne subtasks that achieve these subgoals. By deﬁning such subgoals, the programmer constrains the set of policies that need to be considered during reinforcement learning. The MAXQ value function decomposition can represent the value function of any policy that is consistent with the given hierarchy. The decomposition also creates opportunities to exploit state abstractions, so that individual MDPs within the hierarchy can ignore large parts of the state space. This is important for the practical application of the method. This paper deﬁnes the MAXQ hierarchy, proves formal results on its representational power, and establishes ﬁve conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the ﬁve kinds of state abstraction. The paper evaluates the MAXQ represen-tation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than ﬂat Q learning. The fact that MAXQ learns a representation of the value function has an important beneﬁt: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the eﬀectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoﬀs in hierarchical reinforcement learning.  1  1 Introduction  A central goal of artiﬁcial intelligence is to develop techniques for constructing robust, autonomous agents that are able to achieve good performance in complex, real-world environments. One fruitful line of research views agents from an “economic” perspective (Boutilier, Shoham, & Wellman, 1997): An agent interacts with an environment and receives real-valued rewards and penalties. The agent’s goal is to maximize the total reward it receives. The economic view makes it easy to formalize traditional goals of achievement (“land this airplane”). But it also makes it easy to formulate goals of prevention (“don’t crash into any other airplanes”) and goals of maintenance (“keep the air-traﬃc control system working as long as possible”). Goals of achievement can be represented by giving a positive reward for achieving the goal. Goals of prevention can be represented by giving a negative reward when bad events occur, and goals of maintenance can be represented by giving a positive reward for each time step that the desireable state is maintained. Furthermore, the economic formalism makes it possible to incorporate uncertainty—we can require the agent to maximize the expected value of the total reward in the face of random events in the world.  This brief review shows that the economic approach is very expressive—a diﬃcult research chal-lenge, however, is to develop eﬃcient and scalable methods for reasoning, planning, and learning within the economic AI framework. The area of Stochastic Planning studies methods for ﬁnding optimal or near-optimal plans to maximize expected total reward in the case where the agent has complete knowledge of the probabilistic behavior of the environment and the reward function. The basic methods for this case were developed in the 1950s in the ﬁeld of “Dynamic Programming.” Unfortunately, these methods require time polynomial in the number of states in the state space, which makes them prohibitively expensive for most AI problems. Hence, recent research has fo-cused on methods that can exploit structure within the planning problem to work more eﬃciently (Boutilier, Dean, & Hanks, 1999).  The area of Reinforcement Learning (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998) stud-ies methods for learning optimal or near-optimal plans by interacting directly with the external environment (as opposed to analyzing a user-provided model of the environment). Again, the basic methods in reinforcement learning are based on dynamic programming algorithms. However, rein-forcement learning methods oﬀer two important advantages over classical dynamic programming. First, the methods are online. This permits them to focus their attention on the parts of the state space that are important and ignore the rest of the space. Second, the methods can employ function approximation algorithms (e.g., neural networks) to represent their knowledge. This allows them to generalize across the state space so that the learning time scales much better.  Despite the recent advances in both probabilistic planning and reinforcement learning, there are still many shortcomings. The biggest of these is the lack of a fully satisfactory method for incorporating hierarchies into these algorithms. Research in classical planning has shown that hierarchical methods such as hierarchical task networks (Currie & Tate, 1991), macro actions (Fikes, Hart, & Nilsson, 1972; Korf, 1985), and state abstraction methods (Sacerdoti, 1974; Knoblock, 1990) can provide exponential reductions in the computational cost of ﬁnding good plans. However, all of the basic algorithms for probabilistic planning and reinforcement learning are “ﬂat” methods— they treat the state space as one huge ﬂat search space. This means that the paths from the start state to the goal state are very long, and the length of these paths determines the cost of learning and planning, because information about future rewards must be propagated backward along these paths.  Many researchers (Singh, 1992a; Lin, 1993; Kaelbling, 1993; Dayan & Hinton, 1993; Hauskrecht, Meuleau, Boutilier, Kaelbling, & Dean, 1998; Parr & Russell, 1998; Sutton, Precup, & Singh, 1998) have experimented with diﬀerent methods of hierarchical reinforcement learning and hierarchical  2  probabilistic planning. This research has explored many diﬀerent points in the design space of hierarchical methods, but several of these systems were designed for speciﬁc situations. We lack crisp deﬁnitions of the main approaches and a clear understanding of the relative merits of the diﬀerent methods.  This paper formalizes and clariﬁes one approach and attempts to understand how it compares with the other techniques. The approach, called the MAXQ method, provides a hierarchical decom-position of the given reinforcement learning problem into a set of subproblems. It simultaneously provides a decomposition of the value function for the given problem into a set of value functions for the subproblems. Hence, it has both a declarative semantics (as a value function decomposition) and a procedural semantics (as a subroutine hierarchy).  A review of previous research shows that there are several important design decisions that must be made when constructing a hierarchical reinforcement learning system. As a way of providing an overview of the results in this paper, let us review these issues and see how the MAXQ method approaches each of them.  The ﬁrst issue is how subtasks should be speciﬁed. Hierarchical reinforcement learning involves breaking the target Markov decision problem into a hierarchy of subproblems or subtasks. There are three general approaches to deﬁning these subtasks. One approach is to deﬁne each subtask in terms of a ﬁxed policy that is provided by the programmer. The “option” method of Sutton, Precup, and Singh (1998) takes this approach. The second approach is to deﬁne each subtask in terms of a non-deterministic ﬁnite-state controller. The Hierarchy of Abstract Machines (HAM) method of Parr and Russell (1998) takes this approach. This method permits the programmer to provide a “partial policy” that constrains the set of permitted actions at each point, but does not specify a complete policy for each subtask. The third approach is to deﬁne each subtask in terms of a termination predicate and a local reward function. These deﬁne what it means for the subtask to be completed and what the ﬁnal reward should be for completing the subtask. The MAXQ method described in this paper follows this approach, building upon previous work by Singh (1992a), Kaelbling (1993), Dayan and Hinton (1993), and Dean and Lin (1995).  An advantage of the “option” and partial policy approaches is that the subtask can be deﬁned in terms of an amount of eﬀort or a course of action rather than in terms of achieving a particular goal condition. However, the “option” approach (at least in the simple form described here), requires the programmer to provide complete policies for the subtasks, which can be a diﬃcult programming task in real-world problems. On the other hand, the termination predicate method requires the programmer to guess the relative desirability of the diﬀerent states in which the subtask might terminate. This can also be diﬃcult, although Dean and Lin show how these guesses can be revised automatically by the learning algorithm.  A potential drawback of all hierarchical methods is that the learned policy may be suboptimal. The programmer-provided hierarchy constrains the set of possible policies that can be considered. If these constraints are poorly chosen, the resulting policy will be suboptimal. Nonetheless, the learn-ing algorithms that have been developed for the “option” and partial policy approaches guarantee that the learned policy will be the best possible policy consistent with these constraints.  The termination predicate method suﬀers from an additional source of suboptimality. The learning algorithm described in this paper converges to a form of local optimality that we call recursive optimality. This means that the policy of each subtask is locally optimal given the policies of its children. But there might exist better hierarchical policies where the policy for a subtask must be locally suboptimal so that the overall policy is optimal. This problem can be avoided by careful deﬁnition of termination predicates and local reward functions, but this is an added burden on the programmer. (It is interesting to note that this problem of recursive optimality has not been noticed previously. This is because previous work focused on subtasks with a single terminal state,  3  and in such cases, the problem does not arise.)  The second design issue is whether to employ state abstractions within subtasks. A subtask employs state abstraction if it ignores some aspects of the state of the environment. For example, in many robot navigation problems, choices about what route to take to reach a goal location are independent of what the robot is currently carrying. With few exceptions, state abstraction has not been explored previously. We will see that the MAXQ method creates many opportunities to exploit state abstraction, and that these abstractions can have a huge impact in accelerating learning. We will also see that there is an important design tradeoﬀ: the successful use of state abstraction requires that subtasks be deﬁned in terms of termination predicates rather than using the option or partial policy methods. This is why the MAXQ method must employ termination predicates, despite the problems that this can create.  The third design issue concerns the non-hierarchical “execution” of a learned hierarchical pol-icy. Kaelbling (1993) was the ﬁrst to point out that a value function learned from a hierarchical policy could be evaluated incrementally to yield a potentially much better non-hierarchical policy. Dietterich (1998) and Sutton, Singh, Precup, and Ravindran (1999) generalized this to show how arbitrary subroutines could be executed non-hierarchically to yield improved policies. However, in order to support this non-hierarchical execution, extra learning is required. Ordinarily, in hierar-chical reinforcement learning, the only states where learning is required at the higher levels of the hierarchy are states where one or more of the subroutines could terminate (plus all possible initial states). But to support non-hierarchical execution, learning is required in all states (and at all levels of the hierarchy). In general, this requires additional exploration as well as additional computation and memory. As a consequence of the hierarchical decomposition of the value function, the MAXQ method is able to support either form of execution, and we will see that there are many problems where the improvement from non-hierarchical execution is worth the added cost.  The fourth and ﬁnal issue is what form of learning algorithm to employ. An important advantage of reinforcement learning algorithms is that they typically operate online. However, ﬁnding online algorithms that work for general hierarchical reinforcement learning has been diﬃcult, particularly within the termination predicate family of methods. Singh’s method relied on each subtask having a unique terminal state; Kaelbling employed a mix of online and batch algorithms to train her hierarchy; and work within the “options” framework usually assumes that the policies for the subproblems are given and do not need to be learned at all. The best previous online algorithms are the HAMQ Q learning algorithm of Parr and Russell (for the partial policy method) and the Feudal Q algorithm of Dayan and Hinton. Unfortunately, the HAMQ method requires “ﬂattening” the hierarchy, and this has several undesirable consequences. The Feudal Q algorithm is tailored to a speciﬁc kind of problem, and it does not converge to any well-deﬁned optimal policy.  In this paper, we present a general algorithm, called MAXQ-Q, for fully-online learning of a hierarchical value function. We show experimentally and theoretically that the algorithm converges to a recursively optimal policy. We also show that it is substantially faster than “ﬂat” (i.e., non-hierarchical) Q learning when state abstractions are employed. Without state abstractions, it gives performance similar to (or even worse than) the HAMQ algorithm.  The remainder of this paper is organized as follows. After introducing our notation in Section 2, we deﬁne the MAXQ value function decomposition in Section 3 and illustrate it with a sim-ple example Markov decision problem. Section 4 presents an analytically tractable version of the MAXQ-Q learning algorithm called the MAXQ-0 algorithm and proves its convergence to a recur-sively optimal policy. It then shows how to extend MAXQ-0 to produce the MAXQ-Q algorithm, and shows how to extend the theorem similarly. Section 5 takes up the issue of state abstraction and formalizes a series of ﬁve conditions under which state abstractions can be safely incorporated into the MAXQ representation. State abstraction can give rise to a hierarchical credit assignment  4  problem, and the paper brieﬂy discusses one solution to this problem. Finally, Section 7 presents experiments with three example domains. These experiments give some idea of the generality of the MAXQ representation. They also provide results on the relative importance of temporal and state abstractions and on the importance of non-hierarchical execution. The paper concludes with further discussion of the design issues that were brieﬂy described above, and in particular, it tackles the question of the tradeoﬀ between the method of deﬁning subtasks (via termination predicates) and the ability to exploit state abstractions.  Some readers may be disappointed that MAXQ provides no way of learning the structure of the hierarchy. Our philosophy in developing MAXQ (which we share with other reinforcement learning researchers, notably Parr and Russell) has been to draw inspiration from the development of Belief Networks (Pearl, 1988). Belief networks were ﬁrst introduced as a formalism in which the knowledge engineer would describe the structure of the networks and domain experts would provide the necessary probability estimates. Subsequently, methods were developed for learning the probability values directly from observational data. Most recently, several methods have been developed for learning the structure of the belief networks from data, so that the dependence on the knowledge engineer is reduced.  In this paper, we will likewise require that the programmer provide the structure of the hierarchy. The programmer will also need to make several important design decisions. We will see below that a MAXQ representation is very much like a computer program, and we will rely on the programmer to design each of the modules and indicate the permissible ways in which the modules can invoke each other. Our learning algorithms will ﬁll in “implementations” of each module in such a way that the overall program will work well. We believe that this approach will provide a practical tool for solving large real-world MDPs. We also believe that it will help us understand the structure of hierarchical learning algorithms. It is our hope that subsequent research will be able to automate most of the work that we are currently requiring the programmer to do.  2 Formal Deﬁnitions  2.1 Markov Decision Problems and Semi-Markov Decision Problems  We employ the standard deﬁnitions for Markov Decision Problems and Semi-Markov Decision Problems.  In this paper, we restrict our attention to situations in which an agent is interacting with a fully-observable stochastic environment. This situation can be modeled as a Markov Decision Problem (MDP) hS, A, P, R, P0i deﬁned as follows:  • S: this is the set of states of the environment. At each point in time, the agent can observe  the complete state of the environment.  • A: this is a ﬁnite set of actions. Technically, the set of available actions depends on the  current state s, but we will suppress this dependence in our notation.  • P : When an action a ∈ A is performed, the environment makes a probabilistic transition from its current state s to a resulting state s′ according to the probability distribution P (s′|s, a).  • R: Similarly, when action a is performed and the environment makes its transition from s to s′, the agent receives a real-valued (possibly stochastic) reward R(s′|s, a). To simplify the notation, it is customary to treat this reward as being given at the time that action a is initiated, even though it may in general depend on s′ as well as on s and a.  5  • P0: This is the starting state distribution. When the MDP is initialized, it is in state s with  probability P0(s).  A policy, π, is a mapping from states to actions that tells what action a = π(s) to perform when the environment is in state s.  We will consider two settings: Episodic and Inﬁnite-Horizon. In the episodic setting, all rewards are ﬁnite and there is at least one zero-cost absorbing terminal state. An absorbing terminal state is a state in which all actions lead back to the same state with probability 1 and zero reward. We will only consider problems where all deterministic policies are “proper”—that is, all deterministic policies have a non-zero probability of reaching a terminal state when started in an arbitrary state. In this setting, the goal of the agent is to ﬁnd a policy that maximizes the expected cumulative reward. In the special case where all rewards are non-positive, these problems are referred to as stochastic shortest path problems, because the rewards can be viewed as costs (i.e., lengths), and the policy attempts to move the agent along the path of minimum expected cost.  In the inﬁnite horizon setting, all rewards are also ﬁnite. In addition, there is a discount factor γ, and the agent’s goal is to ﬁnd a policy that minimizes the inﬁnite discounted sum of future rewards.  The value function V π for policy π is a function that tells, for each state s, what the expected cumulative reward will be of executing that policy. Let rt be a random variable that tells the reward that the agent receives at time step t while following policy π. We can deﬁne the value function in the episodic setting as  V π(s) = E {rt + rt+1 + rt+2 + · · · |st = t, π} .  In the discounted setting, the value function is  V π(s) = E  rt + γrt+1 + γ2rt+2 + · · ·  st = t, π  .  We can see that this equation reduces to the previous one when γ = 1. However, in the inﬁnite horizon case, this inﬁnite sum will not converge unless γ < 1.  The value function satisﬁes the Bellman equation for a ﬁxed policy:  (cid:12) (cid:12) (cid:12)  o  V π(s) =  ′  P (s  |s, π(s))  R(s  ′  |s, π(s)) + γV π(s  ′  )  .  (cid:2) The quantity on the right-hand side is called the backed-up value of performing action a in state s. For each possible successor state s′, it computes the reward that would be received and the value of the resulting state and then weights those according to the probability of ending up in s′.  The optimal value function V ∗ is the value function that simultaneously maximizes the expected cumulative reward in all states s ∈ S. Bellman (1957) proved that it is the unique solution to what is now known as the Bellman equation:  (cid:3)  ∗  V  (s) = max  P (s  |s, a)  R(s  |s, a) + γV  ′  ′  ∗  ′  (s  )  .  (1)  a  Xs′  (cid:2)  (cid:3)  There may be many optimal policies that achieve this value. Any policy that chooses a in s to achieve the maximum on the right-hand side of this equation is an optimal policy. We will denote an optimal policy by π∗. Note that all optimal policies are “greedy” with respect to the backed-up value of the available actions.  Closely related to the value function is the so-called action-value function, or Q function (Watkins, 1989). This function, Qπ(s, a), gives the expected cumulative reward of performing  n  Xs′  6  Xs′  Xs′  (cid:2)  (cid:20)  action a in state s and then following policy π thereafter. The Q function also satisﬁes a Bellman equation:  Qπ(s, a) =  ′  P (s  |s, a)  R(s  ′  |s, a) + γQπ(s  ′  ′  , π(s  ))  .  The optimal action-value function is written Q∗(s, a), and it satisﬁes the equation  (cid:3)  ∗  Q  (s, a) =  ′  ′  P (s  |s, a)  R(s  |s, a) + γ max  ∗  ′  ′ , a  (s  a′ Q  .  ) (cid:21)  (2)  Note that any policy that is greedy with respect to Q∗ is an optimal policy. There may be many such optimal policies—they diﬀer only in how they break ties between actions with identical Q∗ values.  An action order, denoted ω, is a total order over the actions within an MDP. That is, ω is an anti-symmetric, transitive relation such that ω(a1, a2) is true iﬀ a1 is preferred to a2. An ordered greedy policy, πω is a greedy policy that breaks ties using ω. For example, suppose that the two best actions at state s are a1 and a2, that Q(s, a1) = Q(s, a2), and that ω(a1, a2). Then the ordered greedy policy πω will choose a1: πω(s) = a1. Note that although there may be many optimal policies for a given MDP, the ordered greedy policy, π∗  ω, is unique.  A discrete-time semi-Markov Decision Process (SMDP) is a generalization of the Markov Deci-sion Process in which the actions can take a variable amount of time to complete. In particular, let the random variable N denote the number of time steps that action a takes when it is executed in state s. We can extend the state transition probability function to be the joint distribution of the result states s′ and the number of time steps N when action a is performed in state s: P (s′, N |s, a). Similarly, the reward function can be changed to be R(s′, N |s, a).1  It is straightforward to modify the Bellman equation to deﬁne the value function for a ﬁxed  policy π as  V π(s) =  ′  P (s  , N |s, π(s))  R(s  ′  , N |s, π(s)) + γN V π(s  ′  )  .  Xs′,N  h The only change is that the expected value on the right-hand side is taken with respect to both s′ and N , and γ is raised to the power N to reﬂect the variable amount of time that may elapse while executing action a.  i  Note that because expectation is a linear operator, we can write each of these Bellman equations as the sum of the expected reward for performing action a and the expected value of the resulting state s. For example, we can rewrite the equation above as  V π(s) = R(s, π(s)) +  ′  P (s  , N |s, π(s))γN V π(s  ′  ).  (3)  where R(s, π(s)) is the expected reward of performing action π(s) in state s, where the expectation is taken with respect to s′ and N .  Note that for the episodic case, there is no diﬀerence between a MDP and a Semi-Markov  Decision Process.  1This formalization is slightly diﬀerent than the standard formulation of SMDPs, which separates P (s′|s, a) and F (t|s, a), where F is the cumulative distribution function for the probability that a will terminate in t time units, where t is real-valued rather than integer-valued. In our case, it is important to consider the joint distribution of s′ and N , but we do not need to consider actions with arbitrary real-valued durations.  Xs′,N  7  2.2 Reinforcement Learning Algorithms  A reinforcement learning algorithm is an algorithm that is given access to an unknown MDP via the following reinforcement learning protocol. At each time step t, the algorithm is told the current state s of the MDP and the set of actions A(s) ⊆ A that are executable in that state. The algorithm chooses an action a ∈ A(s), and the MDP executes this action (which causes it to move to state s’) and returns a real-valued reward r. If s is an absorbing terminal state, the set of actions A(s) contains only the special action reset, which causes the MDP to move to one of its initial states, drawn according to P0.  The learning algorithm is evaluated based on its observed cumulative reward. The cumulative reward of a good learning algorithm should converge to the cumulative reward of the optimal policy for the MDP.  In this paper, we will make use of two well-known learning algorithms: Q learning (Watkins, 1989; Watkins & Dayan, 1992) and SARSA(0) (Rummery & Niranjan, 1994). Both of these algorithms maintain a tabular representation of the action-value function Q(s, a). Every entry of the table is initialized arbitrarily.  In Q learning, after the algorithm has observed s, chosen a, received r, and observed s′, it  performs the following update:  Qt(s, a) := (1 − αt)Qt−1(s, a) + αt[r + γ max  a′ Qt−1(s  ′  ′ , a  )],  where αt is a learning rate parameter.  Jaakkola, Jordan and Singh (1994) and Bertsekas and Tsitsiklis (1996) prove that if the agent  follows an “exploration policy” that tries every action in every state inﬁnitely often and if  lim T →∞  T  t=1 X  αt = ∞ and  lim T→∞  T  Xt=1  α2  t < ∞  (4)  then Qt converges to the optimal action-value function Q∗ with probability 1. Their proof holds in both settings discussed in this paper (episodic and inﬁnite-horizon).  The SARSA(0) algorithm is very similar. After observing s, choosing a, observing r, observing  s′, and choosing a′, the algorithm performs the following update:  Qt(s, a) := (1 − αt)Qt−1(s, a) + αt(s, a)[r + γQt−1(s  ′  ′ , a  )],  where αt is a learning rate parameter. The key diﬀerence is that the Q value of the chosen action a′, Q(s′, a′), appears on the right-hand side in the place where Q learning uses the Q value of the best action. Singh, Jaakkola, Littman, and Szepesv´ari (1998) provide two important convergence results: First, if a ﬁxed policy π is employed to choose actions, SARSA(0) will converge to the value function of that policy provided αt decreases according to Equation (4). Second, if a so-called GLIE policy is employed to choose actions, SARSA(0) will converge to the value function of the optimal policy, provided again that αt decreases according to Equation (4). A GLIE policy is deﬁned as follows:  Deﬁnition 1 A GLIE (greedy in the limit with inﬁnite exploration) policy is any policy satisfying  1. Each action is executed inﬁnitely often in every state that is visited inﬁnitely often.  2. In the limit, the policy is greedy with respect to the Q-value function with probability 1.  8  4  3  2  1  0  R  Y  0  G  B  3  1  2  4  Figure 1: The Taxi Domain  3 The MAXQ Value Function Decomposition  At the center of the MAXQ method for hierarchical reinforcement learning is the MAXQ value function decomposition. MAXQ describes how to decompose the overall value function for a policy into a collection of value functions for individual subtasks (and subsubtasks, recursively).  3.1 A Motivating Example  To make the discussion concrete, let us consider the following simple example. Figure 1 shows a 5-by-5 grid world inhabited by a taxi agent. There are four specially-designated locations in In this world, marked as R(ed), B(lue), G(reen), and Y(ellow). The taxi problem is episodic. each episode, the taxi starts in a randomly-chosen square. There is a passenger at one of the four locations (chosen randomly), and that passenger wishes to be transported to one of the four locations (also chosen randomly). The taxi must go to the passenger’s location (the “source”), pick up the passenger, go to the destination location (the “destination”), and put down the passenger there. (To keep things uniform, the taxi must pick up and drop oﬀ the passenger even if he/she is already located at the destination!) The episode ends when the passenger is deposited at the destination location.  There are six primitive actions in this domain: (a) four navigation actions that move the taxi one square North, South, East, or West, (b) a Pickup action, and (c) a Putdown action. Each action is deterministic. There is a reward of −1 for each action and an additional reward of +20 for successfully delivering the passenger. There is a reward of −10 if the taxi attempts to execute the Putdown or Pickup actions illegally. If a navigation action would cause the taxi to hit a wall, the action is a no-op, and there is only the usual reward of −1.  We seek a policy that maximizes the total reward per episode. There are 500 possible states: 25 squares, 5 locations for the passenger (counting the four starting locations and the taxi), and 4 destinations.  This task has a simple hierarchical structure in which there are two main sub-tasks: Get the passenger and Deliver the passenger. Each of these subtasks in turn involves the subtask of navigating to one of the four locations and then performing a Pickup or Putdown action.  This task illustrates the need to support temporal abstraction, state abstraction, and subtask sharing. The temporal abstraction is obvious—for example, the process of navigating to the passen-ger’s location and picking up the passenger is a temporally extended action that can take diﬀerent numbers of steps to complete depending on the distance to the target. The top level policy (get passenger; deliver passenger) can be expressed very simply if these temporal abstractions can be  9  employed.  The need for state abstraction is perhaps less obvious. Consider the subtask of getting the passenger. While this subtask is being solved, the destination of the passenger is completely irrelevant—it cannot aﬀect any of the nagivation or pickup decisions. Perhaps more importantly, when navigating to a target location (either the source or destination location of the passenger), only the identity of the target location is important. The fact that in some cases the taxi is carrying the passenger and in other cases it is not is irrelevant. Finally, support for subtask sharing is critical.  If the system could learn how to solve the navigation subtask once, then the solution could be shared by both of the “Get the passenger” and “Deliver the passenger” subtasks. We will show below that the MAXQ method provides a value function representation and learning algorithm that supports temporal abstraction, state abstraction, and subtask sharing.  To construct a MAXQ decomposition for the taxi problem, we must identify a set of individual subtasks that we believe will be important for solving the overall task. In this case, let us deﬁne the following four tasks:  • Navigate(t). In this subtask, the goal is to move the taxi from its current location to one of  the four target locations, which will be indicated by the formal parameter t.  • Get. In this subtask, the goal is to move the taxi from its current location to the passenger’s  current location and pick up the passenger.  • Put. The goal of this subtask is to move the taxi from the current location to the passenger’s  destination location and drop oﬀ the passenger.  • Root. This is the whole taxi task.  Each of these subtasks is deﬁned by a subgoal, and each subtask terminates when the subgoal  is achieved.  After deﬁning these subtasks, we must indicate for each subtask which other subtasks or prim-itive actions it should employ to reach its goal. For example, the Navigate(t) subtask should use the four primitive actions North, South, East, and West. The Get subtask should use the Navigate subtask and the Pickup primitive action, and so on.  All of this information can be summarized by a directed acyclic graph called the task graph, which is shown in Figure 2. In this graph, each node corresponds to a subtask or a primitive action, and each edge corresponds to a potential way in which one subtask can “call” one of its child tasks. The notation f ormal/actual (e.g., t/source) tells how a formal parameter is to be bound to an actual parameter.  Now suppose that for each of these subtasks, we write a policy (e.g., as a computer program) to achieve the subtask. We will refer to the policy for a subtask as a “subroutine”, and we can view the parent subroutine as invoking the child subroutine via ordinary subroutine-call-and-return semantics. If we have a policy for each subtask, then this gives us an overall policy for the Taxi MDP. The Root subtask executes its policy by calling subroutines that are policies for the Get and Put subtasks. The Get policy calls subroutines for the Pickup primitive action and the Navigate(t) subtask. And so on. We will call this collection of policies a hierarchical policy. In a hierarchical policy, each subroutine executes until it enters a terminal state for its subtask.  3.2 Deﬁnitions  Let us formalize the discussion so far.  10  Pickup  Navigate(t)  Putdown  Root  Get  Put  t/source  t/destination  North  South  East  West  Figure 2: A task graph for the Taxi problem.  The MAXQ decomposition takes a given MDP M and decomposes it into a set of subtasks {M0, M1, . . . , Mn} with the convention that M0 is the root subtask (i.e., solving M0 solves the entire original MDP M ).  Deﬁnition 2 An unparameterized subtask is a three-tuple, hTi, Ai, ˜Rii, deﬁned as follows:  1. Ti(si) is a termination predicate that partitions S into a set of active states, Si and a set of terminal states, Ti. The policy for subtask Mi can only be executed if the current state s is in Si.  2. Ai is a set of actions that can be performed to achieve subtask Mi. These actions can either be primitive actions from A, the set of primitive actions for the MDP, or they can be other subtasks, which we will denote by their indexes i. We will refer to these actions as the “children” of subtask i. If a child subtask Mj has formal parameters, then it can occur multiple times in Ai, and each such occurrence must specify the actual values that will be bound to the formal parameters. The set of actions Ai may diﬀer from one state to another, so technically, Ai is a function of s. However, we will suppress this dependence in our notation. 3. ˜Ri(s′|s, a) is the pseudo-reward function, which speciﬁes a pseudo-reward for each transition from a state s ∈ Si to a terminal state s′ ∈ Ti. This pseudo-reward tells how desirable each of the terminal states is for this subtask. It is typically employed to give goal terminal states a pseudo-reward of 0 and any non-goal terminal states a negative reward.  Each primitive action a from M is a primitive subtask in the MAXQ decomposition such that a is always executable, it always terminates immediately after execution, and its pseudo-reward function is uniformly zero.  If a subtask has formal parameters, then each possible binding of actual values to the formal parameters speciﬁes a distinct subtask. We can think of the values of the formal parameters as being part of the “name” of the subtask. In practice, of course, we implement a parameterized subtask by parameterizing the various components of the task. If b speciﬁes the actual parameter values for task Mi, then we can deﬁne a parameterized termination predicate Ti(s, b) and a parameterized pseudo-reward function ˜Ri(s′|s, a, b). To simplify notation in the rest of the paper, we will usually omit these parameter bindings from our notation.  11  Table 1: Pseudo-Code for Execution of a Hierarchical Policy  st is the state of the world at time t  1 2 Kt is the state of the execution stack at time t  while top(Kt) is not a primitive action  Let (i, fi) := top(Kt), where  i is the name of the “current” subroutine, and fi gives the parameter bindings for i  3 4 5 6 7 8 9  Let (a, fa) := πi(s, fi), where  push (a, fa) onto the stack Kt  a is the action and fa gives the parameter bindings chosen by policy πi  10 Let (a, nil) := pop(Kt) be the primitive action on the top of the stack. 11 Execute primitive action a, and update st+1 to be the resulting state of the environment. 12  13 while top(Kt) speciﬁes a terminated subtask do 14  pop(Kt)  15 Kt+1 := Kt is the resulting execution stack.  Deﬁnition 3 A hierarchical policy, π, is a set containing a policy for each of the subtasks in the problem: π = {π0, . . . , πn}.  Each subtask policy πi takes a state and returns the name of a primitive action to execute or the name of a subroutine (and bindings for its formal parameters) to invoke. In the terminology of Sutton, Precup, and Singh (1998), a subtask policy is a deterministic “option”, and its probability of terminating in state s (which they denote by β(s)) is 0 if s ∈ Si, and 1 if s ∈ Ti.  In a parameterized task, the policy must be parameterized as well so that π takes a state and the bindings of formal parameters and returns a chosen action and the bindings (if any) of its formal parameters.  Table 1 gives a pseudo-code description of the procedure for executing a hierarchical policy. The hierarchical policy is executed using a stack discipline, as in ordinary programming languages. Let Kt denote the contents of the pushdown stack at time t. When a subroutine is invoked, its name and actual parameters are pushed onto the stack. When a subroutine terminates, its name and actual parameters are popped oﬀ the stack. It is sometimes useful to think of the contents of the stack as being an additional part of the state space for the problem. Hence, a hierarchical policy implicitly deﬁnes a mapping from the current state st and current stack contents Kt to a primitive action a. This action is executed, and this yields a resulting state st+1 and a resulting stack contents Kt+1. Because of the added state information in the stack, the hierarchical policy is non-Markovian with respect to the original MDP.  Because a hierarchical policy maps from states s and stack contents K to actions, the value function for a hierarchical policy must in general also assign values to all combinations of states s and stack contents K.  Deﬁnition 4 A hierarchical value function, denoted V π(hs, Ki), gives the expected cumulative reward of following the hierarchical policy π starting in state s with stack contents K.  In this paper, we will primarily be interested only in the “top level” value of the hierarchical policy—that is, the value when the stack K is empty: V π(hs, nili). This is the value of executing the hierarchical policy beginning in state s and starting at the top level of the hierarchy.  12  Deﬁnition 5 The projected value function, denoted V π(s), is the value of executing hierarchical policy π starting in state s and starting at the root of the task hierarchy.  3.3 Decomposition of the Projected Value Function  Now that we have deﬁned a hierarchical policy and its projected value function, we can show how that value function can be decomposed hierarchically. The decomposition is based on the following theorem:  Theorem 1 Given a task graph over tasks M0, . . . , Mn and a hierarchical policy π, each subtask Mi deﬁnes a semi-Markov decision process with states Si, actions Ai, probability transition function i (s′, N |s, a), and expected reward function R(s, a) = V π(a, s), where V π(a, s) is the projected P π value function for child task Ma in state s. If a is a primitive action, V π(a, s) is deﬁned as the expected immediate reward of executing a in s: V π(a, s) =  s′ P (s′|s, a)R(s′|s, a).  Proof: Consider all of the subroutines that are descendants of task Mi in the task graph. Be-cause all of these subroutines are executing ﬁxed policies (speciﬁed by hierarchical policy π), the i (s′, N |s, a) is a well deﬁned, stationary distribution for each child probability transition function P π subroutine a. The set of states Si and the set of actions Ai are obvious. The interesting part of this theorem is the fact that the expected reward function R(s, a) of the SMDP is the projected value function of the child task Ma.  P  To see this, let us write out the value of V π(i, s):  V π(i, s) = E{rt + γrt+1 + γ2rt+2 + · · · |st = s, π}  This sum continues until the subroutine for task Mi enters a state in Ti.  Now let us suppose that the ﬁrst action chosen by πi is a subroutine a. This subroutine is in-i (s′, N |s, a).  voked, and it executes for a number of steps N and terminates in state s′ according to P π We can rewrite Equation (5) as  V π(i, s) = E  γurt+u +  γurt+u  st = s, π  N −1  (  u=0 X  ∞  Xu=N  )  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  The ﬁrst summation on the right-hand side of Equation (6) is the discounted sum of rewards for executing subroutine a starting in state s until it terminates, in other words, it is V π(a, s), the projected value function for the child task Ma. The second term on the right-hand side of the equation is the value of s′ for the current task i, V π(i, s′), discounted by γN , where s′ is the current state when subroutine a terminates. We can write this in the form of a Bellman equation:  (5)  (6)  (7)  V π(i, s) = V π(πi(s), s) +  ′  P π  i (s  , N |s, πi(s))γN V π(i, s  ′  )  Xs′,N  This has the same form as Equation (3), which is the Bellman equation for an SMDP, where the ﬁrst term is the expected reward R(s, π(s)). Q.E.D.  To obtain a hierarchical decomposition of the projected value function, let us switch to the action-value (or Q) representation. First, we need to extend the Q notation to handle the task hierarchy. Let Qπ(i, s, a) be the expected cumulative reward for subtask Mi of performing action a in state s and then following hierarchical policy π until subtask Mi terminates. With this notation, we can re-state Equation (7) as follows:  Qπ(i, s, a) = V π(a, s) +  ′  P π  i (s  , N |s, a)γN Qπ(i, s  ′  ′  , π(s  )),  (8)  Xs′,N  13  The right-most term in this equation is the expected discounted reward of completing task Mi after executing action a in state s. This term only depends on i, s, and a, because the summation marginalizes away the dependence on s′ and N . Let us deﬁne C π(i, s, a) to be equal to this term:  Deﬁnition 6 The completion function, C π(i, s, a), is the expected discounted cumulative reward of completing subtask Mi after invoking the subroutine for subtask Ma in state s. The reward is discounted back to the point in time where a begins execution.  C π(i, s, a) =  ′  P π  i (s  , N |s, a)γN Qπ(i, s  ′  ′  , π(s  ))  Xs′,N  With this deﬁnition, we can express the Q function recursively as  Qπ(i, s, a) = V π(a, s) + C π(i, s, a).  Finally, we can re-express the deﬁnition for V π(i, s) as  V π(i, s) =  Qπ(i, s, πi(s))  s′ P (s′|s, i)R(s′|s, i)  (  if i is composite if i is primitive  (9)  (10)  (11)  P  We will refer to equations (9), (10), and (11) as the decomposition equations for the MAXQ hierarchy under a ﬁxed hierarchical policy π. These equations recursively decompose the projected value function for the root, V π(0, s) into the projected value functions for the individual subtasks, M1, . . . , Mn and the individual completion functions C π(j, s, a) for j = 1, . . . , n. The fundamental quantities that must be stored to represent the value function decomposition are just the C values for all non-primitive subtasks and the V values for all primitive actions.  To make it easier for programmers to design and debug MAXQ decompositions, we have de-veloped a graphical representation that we call the MAXQ graph. A MAXQ graph for the Taxi domain is shown in Figure 3. The graph contains two kinds of nodes, Max nodes and Q nodes. The Max nodes correspond to the subtasks in the task decomposition—there is one Max node for each primitive action and one Max node for each subtask (including the Root) task. Each primitive Max node i stores the value of V π(i, s). The Q nodes correspond to the actions that are available for each subtask. Each Q node for parent task i, state s and subtask a stores the value of C π(i, s, a). In addition to storing information, the Max nodes and Q nodes can be viewed as performing parts of the computation described by the decomposition equations. Speciﬁcally, each Max node i can be viewed as computing the projected value function V π(i, s) for its subtask. For primitive Max nodes, this information is stored in the node. For composite Max nodes, this information is obtained by “asking” the Q node corresponding to πi(s). Each Q node with parent task i and child task a can be viewed as computing the value of Qπ(i, s, a). It does this by “asking” its child task a for its projected value function V π(a, s) and then adding its completion function C π(i, s, a).  As an example, consider the situation shown in Figure 1, which we will denote by s1. Suppose that the passenger is at R and wishes to go to B. Let the hierarchical policy we are evaluating be an optimal policy denoted by π (we will omit the superscript * to reduce the clutter of the notation). The value of this state under π is 10, because it will cost 1 unit to move the taxi to R, 1 unit to pickup the passenger, 7 units to move the taxi to B, and 1 unit to putdown the passenger, for a total of 10 units (a reward of −10). When the passenger is delivered, the agent gets a reward of +20, so the net value is +10.  Figure 4 shows how the MAXQ hierarchy computes this value. To compute the value V π(Root, s1),  MaxRoot consults its policy and ﬁnds that πRoot(s1) is Get. Hence, it “asks” the Q node, QGet  14  MaxRoot  QGet  QPut  MaxGet  MaxPut  QPickup  QNavigateForGet  QNavigateForPut  QPutdown  t/source  t/destination  Pickup  Putdown  MaxNavigate(t)  QNorth(t)  QEast(t)  QSouth(t)  QWest(t)  North  East  South  West  Figure 3: A MAXQ graph for the Taxi Domain  to compute Qπ(Root, s1, Get). The completion cost for the Root task after performing a Get, C π(Root, s1, Get), is 12, because it will cost 8 units to deliver the customer (for a net reward of 20 − 8 = 12) after completing the Get subtask. However, this is just the reward after completing the Get, so it must ask MaxGet to estimate the expected reward of performing the Get itself.  The policy for MaxGet dictates that in s1, the Navigate subroutine should be invoked with t bound to R, so MaxGet consults the Q node, QNavigateForGet to compute the expected re-ward. QNavigateForGet knows that after completing the Navigate(R) task, one more action (the Pickup) will be required to complete the Get, so C π(MaxGet, s1, Navigate(R)) = −1. It then asks MaxNavigate(R) to compute the expected reward of performing a Navigate to location R.  The policy for MaxNavigate chooses the North action, so MaxNavigate asks QNorth to compute the value. QNorth looks up its completion cost, and ﬁnds that C π(Navigate, s1, North) is 0 (i.e., the Navigate task will be completed after performing the North action). It consults MaxNorth to determine the expected cost of performing the North action itself. Because MaxNorth is a primitive action, it looks up its expected reward, which is −1.  Now this series of recursive computations can conclude as follows:  • Qπ(Navigate(R), s1, North) = −1 + 0  15  10  MaxRoot  QPut  MaxPut  10  12  QGet  -2  MaxGet  -2  -1  -1  -1  QPickup  QNavigateForGet  QNavigateForPut  QPutdown  Pickup  Putdown  -1  MaxNavigate(t)  0  QNorth(t)  QEast(t)  QSouth(t)  QWest(t)  North  East  South  West  Figure 4: Computing the value of a state using the MAXQ hierarchy. The C value of each Q node is shown to the left of the node. All other numbers show the values being returned up the graph.  • V π(Navigate(R), s1) = −1  • Qπ(Get, s1, Navigate(R)) = −1 + −1  (−1 to perform the Navigate plus −1 to complete the Get.  • V π(Get, s1) = −2  • Qπ(Root, s1, Get) = −2 + 12  (−2 to perform the Get plus 12 to complete the Root task and collect the ﬁnal reward).  The end result of all of this is that the value of V π(Root, s1) is decomposed into a sum of C  terms plus the expected reward of the chosen primitive action:  V π(Root, s1) = V π(North, s1) + C π(Navigate(R), s1, North) + C π(Get, s1, Navigate(R)) + C π(Root, s1, Get)  = −1 + 0 + −1 + 12  = 10  16  V π(0, s)  XXXXXXX  (cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24) PPPPP  V π(a1, s)  (cid:16)(cid:16)(cid:16)(cid:16)(cid:16)  .  .  . V π(am−1, s) (cid:26) Z  (cid:26)  (cid:26)(cid:26)  Z  ZZ  V π(am, s) C π(am−1, s, am)  C π(a1, s, a2)  C π(0, s, a1)  r1  r2  r3  r4  r5  r8  r9  r10  r11  r12  r13  r14  .  .  .  Figure 5: The MAXQ decomposition; r1, . . . , r14 denote the sequence of rewards received from primitive actions at times 1, . . . , 14.  In general, the MAXQ value function decomposition has the form  V π(0, s) = V π(am, s) + C π(am−1, s, am) + . . . + C π(a1, s, a2) + C π(0, s, a1),  (12)  where a0, a1, . . . , am is the “path” of Max nodes chosen by the hierarchical policy going from the Root down to a primitive leaf node.  We can summarize the presentation of this section by the following theorem:  Theorem 2 Let π = {πi; i = 0, . . . , n} be a hierarchical policy deﬁned for a given MAXQ graph with subtasks M0, . . . , Mn, and let i = 0 be the root node of the graph. Then there exist values for C π(i, s, a) (for internal Max nodes) and V π(i, s) (for primitive, leaf Max nodes) such that V π(0, s) (as computed by the decomposition equations (9), (10), and (11)) is the expected discounted cumulative reward of following policy π starting in state s.  Proof: The proof is by induction on the number of levels in the task graph. At each level i, we compute values for C π(i, s, π(s)) (or V π(i, s), if i is primitive) according to the decomposition equations. We can apply the decomposition equations again to compute Qπ(i, s, π(s)) and apply Equation (8) and Theorem 1 to conclude that Qπ(i, s, π(s)) gives the value function for level i. When i = 0, we obtain the value function for the entire hierarchical policy. Q. E. D.  It is important to note that this representation theorem does not mention the pseudo-reward function, because the pseudo-reward is used only during learning. This theorem captures the representational power of the MAXQ decomposition, but it does not address the question of whether there is a learning algorithm that can ﬁnd a given policy. That is the subject of the next section.  4 A Learning Algorithm for the MAXQ Decomposition  In order to develop a learning algorithm for the MAXQ decomposition, we must consider exactly what we are hoping to achieve. Of course, for any MDP M , we would like to ﬁnd an optimal policy π∗. However, in the MAXQ method (and in hierarchical reinforcement learning in general), the programmer imposes a hierarchy on the problem. This hierarchy constrains the space of possible policies so that it may not be possible to represent the optimal policy or its value function.  17  In the MAXQ method, the constraints take two forms. First, within a subtask, only some of the possible primitive actions may be permitted. For example, in the taxi task, during a Navigate(t), only the North, South, East, and West actions are available—the Pickup and Putdown actions are not allowed. Second, consider a Max node Mj with child nodes {Mj1, . . . , Mjk}. The policy learned for Mj must involve executing the learned policies of these child nodes. When the policy for child node Mji is executed, it will run until it enters a state in Tji. Hence, any policy learned for Mj must pass through some subset of these terminal state sets {Tj1, . . . , Tjk}.  The HAM method shares these same two constraints and in addition, it imposes a partial policy on each node, so that the policy for any subtask Mi must be a deterministic reﬁnement of the given non-deterministic initial policy for node i.  In the “option” approach, the policy is even further constrained. In this approach, there are only two non-primitive levels in the hierarchy, and the subtasks at the lower level are given complete policies by the programmer. Hence, any learned policy must be constructed by “concatenating” the given lower level policies in some order.  The purpose of imposing these constraints on the policy is to incorporate prior knowledge and thereby reduce the size of the space that must be searched to ﬁnd a good policy. However, these constraints may make it impossible to learn the optimal policy.  If we can’t learn the optimal policy, the next best target would be to learn the best policy that  is consistent with (i.e., can be represented by) the given hierarchy.  Deﬁnition 7 A hierarchically optimal policy for MDP M is a policy that achieves the highest cumulative reward among all policies consistent with the given hierarchy.  Parr (1998b) proves that his HAMQ learning algorithm converges with probability 1 to a hier-archically optimal policy. Similarly, given a ﬁxed set of options, Sutton, Precup, and Singh (1998) prove that their SMDP learning algorithm converges to a hierarchically optimal value function. (Incidentally, they also show that if the primitive actions are also made available as “trivial” op-tions, then their SMDP method converges to the optimal policy. However, in this case, it is hard to say anything formal about how the options speed the learning process. They may in fact hinder it (Hauskrecht et al., 1998).)  With the MAXQ method, we will seek an even weaker form of optimality: recursive optimality.  Deﬁnition 8 A recursively optimal policy for MDP M with MAXQ decomposition {M0, . . . , Mk} is a hierarchical policy π = {π0, . . . , πk} such that for each subtask Mi, the corresponding policy πi is optimal for the SMDP deﬁned by the set of states Si, the set of actions Ai, the state transition probability function P π(s′, N |s, a), and the reward function given by the sum of the original reward function R(s′|s, a) and the pseudo-reward function ˜Ri(s′).  Note that in this deﬁnition, the state transition probability distribution is deﬁned by the locally optimal policies {πj} of all subtasks that are descendants of Mi in the MAXQ graph. Hence, recursive optimality is a kind of local optimality in which the policy at each node is optimal given the policies of its children.  The reason to seek recursive optimality rather than hierarchical optimality is that recursive optimality makes it possible to solve each subtask without reference to the context in which it is executed. This context-free property makes it easier to share and re-use subtasks. It will also turn out to be essential for the successful use of state abstraction.  Before we proceed to describe our learning algorithm for recursive optimality, let us see how  recursive optimality diﬀers from hierarchical optimality.  18  MaxRoot  QExit  QGotoGoal  MaxExit  MaxGotoGoal  QExitNorth  QExitSouth  QExitEast  QNorthG  QSouthG  QEastG  G  *  *  Figure 6: A simple MDP (left) and its associated MAXQ graph (right). The policy shown in the left diagram is recursively optimal but not hierarchically optimal. The shaded cells indicate points where the locally-optimal policy is not globally optimal.  North  South  East  It is easy to construct examples of policies that are recursively optimal but not hierarchically optimal. Consider the simple maze problem and its associated MAXQ graph shown in Figures 6. Suppose a robot starts somewhere in the left room, and it must reach the goal G in the right room. The robot has three actions, North, South, and East, and these actions are deterministic. The robot receives a reward of −1 for each move. Let us deﬁne two subtasks:  • Exit. This task terminates when the robot exits the left room. We can set the pseudo-reward  function ˜R to be 0 for the two terminal states (i.e., the two states indicated by *’s).  • GotoGoal. This task terminates when the robot reaches the goal G.  The arrows in Figure 6 show the locally optimal policy within each room. The arrows on the left seek to exit the left room by the shortest path, because this is what we speciﬁed when we set the pseudo-reward function to 0. The arrows on the right follow the shortest path to the goal, which is ﬁne. However, the resulting policy is neither hierarchically optimal nor optimal.  There exists a hierarchical policy that would always exit the left room by the upper door. The MAXQ value function decomposition can represent the value function of this policy, but such a policy would not be locally optimal (because, for example, the states in the “shaded” region would If we consider for a moment, we can see a way to not follow the shortest path to a doorway). ﬁx this problem. The value of the upper starred state under the optimal hierarchical policy is −2 and the value of the lower starred state is −6. Hence, if we set ˜R to have these values, then the recursively-optimal policy would be hierarchically optimal (and globally optimal). In other words, if the programmer can guess the right values for the terminal states of a subtask, then the recursively optimal policy will be hierarchically optimal (provided that all primitive actions are available within the subtask).  This basic idea was ﬁrst pointed out by Dean and Lin (1995). They describe an algorithm that makes initial guesses for the values of these starred states and then updates those guesses based on the computed values of the starred states under the resulting recursively-optimal policy. They proved that this will converge to a hierarchically optimal policy. The drawback of their method is  19  that it requires repeated solution of the resulting hierarchical learning problem, and this does not always yield a speedup over just solving the original, ﬂat problem.  Parr (1998a) proposed an interesting approach that constructs a set of diﬀerent ˜R functions and computes the recursively optimal policy under each of them for each subtask. His method chooses the ˜R functions in such a way that the hierarchically optimal policy can be approximated to any desired degree. Unfortunately, the method is quite ineﬃcient, because it relies on solving a series of linear programming problems each of which requires time polynomial in several parameters, including the number of states |Si| within the subtask.  This discussion suggests that while, in principle, it is possible to learn good values for the pseudo-reward function, in practice, we must rely on the programmer to specify a single pseudo-reward function, ˜R. If the programmer wishes to consider a small number of alternative pseudo-reward functions, they can be handled by deﬁning a small number of subtasks that are identical except for their ˜R functions, and permitting the learning algorithm to choose the one that gives the best recursively-optimal policy.  In practice, we have employed the following simpliﬁed approach to deﬁning ˜R. For each subtask Mi, we deﬁne two predicates: the termination predicate, Ti, and a goal predicate Gi. The goal predicate deﬁnes a subset of the terminated states that are “goal states”, and these have a pseudo-reward of 0. All other terminal states have a ﬁxed constant pseudo-reward (e.g., −100) that is set so that it is always better to terminate in a goal state than in a non-goal state. For the problems on which we have tested the MAXQ method, this worked very well.  In our experiments with MAXQ, we have found that it is easy to make mistakes in deﬁning Ti and Gi. If the goal is not deﬁned carefully, it is easy to create a set of subtasks that lead to inﬁnite looping. For example, consider again the problem in Figure 6. Suppose we permit a fourth action, West in the MDP and let us deﬁne the termination and goal predicates for the right hand room to be satisﬁed iﬀ either the robot reaches the goal or it exits the room. This is a very natural deﬁnition, since it is quite similar to the deﬁnition for the left-hand room. However, the resulting locally-optimal policy for this room will attempt to move to the nearest of these three locations: the goal, the upper door, or the lower door. We can easily see that for all but a few states near the goal, the only policies that can be constructed by MaxRoot will loop forever, ﬁrst trying to leave the left room by entering the right room, and then trying to leave the right room by entering the left room. This problem is easily ﬁxed by deﬁning the goal predicate Gi for the right room to be true if and only if the robot reaches the goal G. But avoiding such “undesired termination” bugs can be hard in more complex domains.  Now that we have an understanding of recursively optimal policies, we present two learning algorithms. The ﬁrst one, called MAXQ-0, applies only in the case when the pseudo-reward function ˜R is always zero. We will ﬁrst prove its convergence properties and then show how it can be extended to give the second algorithm, MAXQ-Q, which works with general pseudo-reward functions.  Table 2 gives pseudo-code for MAXQ-0. MAXQ-0 is a recursive function that executes the current exploration policy starting at Max node i in state s. It performs actions until it reaches a terminal state, at which point it returns a count of the total number of primitive actions that have been executed. To execute an action, MAXQ-0 calls itself recursively. When the recursive call returns, it updates the value of the completion function for node i. It uses the count of the number of primitive actions to appropriately discount the value of the resulting state s′. At leaf nodes, MAXQ-0 updates the estimated one-step expected reward, V (i, s). The value αt(i) is a “learning rate” parameter that should be gradually decreased to zero in the limit.  There are two things that must be speciﬁed in order to make this algorithm description complete. First, we must specify how to compute Vt(i, s′) in line 12, since it is not stored in the Max node. It is computed by the following modiﬁed versions of the decomposition equations:  20  Table 2: The MAXQ-0 learning algorithm.  1  2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  function MAXQ-0(MaxNode i, State s)  if i is a primitive MaxNode  execute i, receive r, and observe result state s′ Vt+1(i, s) := (1 − αt(i)) · Vt(i, s) + αt(i) · rt return 1  else  let count = 0 while Ti(s) is false do  choose an action a according to the current exploration policy πx(i, s) let N = MAXQ-0(a, s) observe result state s′ Ct+1(i, s, a) := (1 − αt(i)) · Ct(i, s, a) + αt(i) · γN Vt(i, s′ count := count + N s := s′ end return count  )  end MAXQ-0  Table 3: Pseudo-code for Greedy Execution of the MAXQ Graph  function EvaluateMaxNode(i, s)  if i is a primitive Max node return hVt(i, s), ii  else  for each j ∈ Ai,  1  2 3 4 5 6 7 8  let hV(j, s), aji = EvaluateMaxNode(j, s)  let jhg = argmaxj Vt(j, s) + Ct(i, s, j) return hVt(jhg, s), ajhg i  end // EvaluateMaxNode  Vt(i, s) =  maxa Qt(i, s, a) Vt(i, s)  if i is composite if i is primitive  (  Qt(i, s, a) = Vt(a, s) + Ct(i, s, a).  (13)  (14)  These equations reﬂect two important changes compared with Equations (10) and (11). First, in the ﬁrst equation, Vt(i, s) is deﬁned in terms of the Q value of the best action a, rather than of the action chosen by a ﬁxed hierarchical policy. Second, there are no π superscripts, because the current value function, Vt(i, s) is not based on a ﬁxed hierarchical policy π.  To compute Vt(i, s) using these equations, we must perform a complete search of all paths through the MAXQ graph starting at node i and ending at the leaf nodes. Table 3 gives pseudo-code for a recursive function, EvaluateMaxNode, that implements a depth-ﬁrst search. In addition to returning Vt(i, s), EvaluateMaxNode also returns the action at the leaf node that achieves this value. This information is not needed for MAXQ-0, but it will be useful later when we consider non-hierarchical execution of the learned recursively-optimal policy.  The second thing that must be speciﬁed to complete our deﬁnition of MAXQ-0 is the exploration  policy, πx. We require that πx be an ordered GLIE policy.  21  Deﬁnition 9 An ordered GLIE policy is a GLIE policy (Greedy in the Limit of Inﬁnite Explo-ration) that converges in the limit to an ordered greedy policy, which is a greedy policy that imposes an arbitrary ﬁxed order ω on the available actions and breaks ties in favor of the action a that ap-pears earliest in that order.  We need this property in order to ensure that MAXQ-0 converges to a uniquely-deﬁned recur-sively optimal policy. A fundamental problem with recursive optimality is that in general, each Max node i will have a choice of many diﬀerent locally optimal policies given the policies adopted by its descendant nodes. These diﬀerent locally optimal policies will all achieve the same locally optimal value function, but they can give rise to diﬀerent probability transition functions P (s′, N |s, i). The result will be that the Semi-Markov Decision Problem deﬁned at the next level above node i in the MAXQ graph will diﬀer depending on which of these various locally optimal policies is chosen by node i. However, if we establish a ﬁxed ordering over the Max nodes in the MAXQ graph (e.g., a left-to-right depth-ﬁrst numbering), and break ties in favor of the lowest-numbered action, then this deﬁnes a unique policy at each Max node. And consequently, by induction, it deﬁnes a unique policy for the entire MAXQ graph. Let us call this policy π∗ r . We will use the r subscript to denote recursively optimal quantities under an ordered greedy policy. Hence, the corresponding value function is V ∗ r denote the corresponding completion function and action-value function. We now prove that the MAXQ-0 algorithm converges to π∗ r .  r , and C ∗  r and Q∗  Theorem 3 Let M = hS, A, P, R, P0i be either an episodic MDP for which all deterministic policies are proper or a discounted inﬁnite horizon MDP with discount factor γ. Let H be a MAXQ graph deﬁned over subtasks {M0, . . . , Mk} such that the pseudo-reward function ˜Ri(s′|s, a) is zero for all i, s, a, and s′. Let αt(i) > 0 be a sequence of constants for each Max node i such that  lim T →∞  T  t=1 X  lim T →∞  T  t=1 X  αt(i) = ∞ and  α2  t (i) < ∞  (15)  Let πx(i, s) be an ordered GLIE policy at each node i and state s and assume that |Vt(i, s)| and |Ct(i, s, a)| are bounded for all t, i, s, and a. Then with probability 1, algorithm MAXQ-0 converges to π∗  r , the unique recursively optimal policy for M consistent with H and πx.  Proof: The proof follows an argument similar to those introduced to prove the convergence of Q learning and SARSA(0) (Bertsekas & Tsitsiklis, 1996; Jaakkola et al., 1994). We will employ the following result from stochastic approximation theory:  Lemma 1 (Proposition 4.5 from Bertsekas and Tsitsiklis, 1996) Consider the iteration  rt+1(x) := (1 − αt(x))rt(x) + αt(x)((U rt)(x) + wt(x) + ut(x)).  Let Ft = {r0(x), . . . , rt(x), w0(x), . . . , wt−1(x), α0(x), . . . , αt(x), ∀x} be the entire history of the it-eration. If  (a) The αt(i) ≥ 0 satisfy conditions (15)  (b) For every i and t the noise terms wt(i) satisfy E[wt(i)|Ft] = 0  (c) Given any norm ||·|| on Rn, there exist constants A and B such that E[w2  t (i)|Ft] ≤ A+B||rt||2.  22  (d) There exists a vector r∗, a positive vector ξ, and a scalar β ∈ [0, 1), such that for all t,  ||U rt − r  ||ξ ≤ β||rt − r  ||ξ  ∗  ∗  (e) There exists a nonnegative random sequence θt that converges to zero with probability 1 and  is such that for all t  |ut(x)| ≤ θt(||rt||ξ + 1)  then rt converges to r∗ with probability 1. The notation || · ||ξ denotes a weighted maximum norm  ||A||ξ = max  x  |A(x)| ξ(x)  .  The structure of the proof of Theorem 3 will be inductive, starting at the leaves of the MAXQ graph and working toward the root. We will employ a diﬀerent time clock at each node i to count the number of update steps performed by MAXQ-0 at that node. The variable t will always refer to the time clock of the current node i.  To prove the base case for any primitive Max node, we note that line 4 of MAXQ-0 is just the standard stochastic approximation algorithm for computing the expected reward for performing action a in state s, and therefore it converges under the conditions given above.  To prove the recursive case, consider any composite Max node i with child node j. Let Pt(s′, N |s, j) be the transition probability distribution for performing child action j in state s at time t (i.e., while following the exploration policy in all descendent nodes of node j). By the inductive assumption, MAXQ-0 applied to j will converge to the (unique) recursively optimal value function V ∗ r (j, s) with probability 1. Furthermore, because MAXQ-0 is following an ordered GLIE policy for j and its descendants, Pt(s′, N |s, j) will converge to P ∗ r (s′, N |s, j), the unique transition probability function for executing child j under the locally optimal policy π∗ r . What remains to be shown is that the update assignment for C (line 12 of the MAXQ-0 algorithm) will converge to the optimal C ∗  r function with probability 1.  To prove this, we will apply Lemma 1. We will identify the x in the lemma with a state-action pair (s, a). The vector rt will be the completion-cost table Ct(i, s, a) for all s, a and ﬁxed i after t update steps. The vector r∗ will be the optimal completion-cost C ∗ r (i, s, a) (again, for ﬁxed i). Deﬁne the mapping U to be  (U C)(i, s, a) =  ′  ∗ r (s  P  , N |s, a)γN (max  [C(i, s  ′  ′ , a  ) + V  ′ ∗ r (a  ′  , s  )])  a′  Xs′  This is a C update under the MDP Mi assuming that all descendant value functions, V ∗ transition probabilities, P ∗  r (s′, N |s, a), have converged.  r (a, s), and  To apply the lemma, we must ﬁrst express the C update formula in the form of the update rule in the lemma. Let s be the state that results from performing a in state s. Line 12 can be written  Ct+1(i, s, a)  := (1 − αt(i)) · Ct(i, s, a) + αt(i) · γN (max a′  ′ [Ct(i, s, a  ′ ) + Vt(a  , s)])  := (1 − αt(i)) · Ct(i, s, a) + αt(i) · [(U Ct)(i, s, a) + wt(i, s, a) + ut(i, s, a)]  where  wt(i, s, a) = γN  ′ [Ct(i, s, a  ′ ) + Vt(a  , s)]  −  max a′  (cid:18)  (cid:19)  23  ut(i, s, a) =  Pt(s  , N |s, a)γN  [Ct(i, s  ′  ′ , a  ′ ) + Vt(a  , s  )]  −  Pt(s  , N |s, a)γN  [Ct(i, s  ′  ′ , a  ′ ) + Vt(a  , s  )]  ′  ′  Xs′,N  Xs′,N  Xs′,N  max a′  max a′  (cid:18)  (cid:18)  max a′  (cid:18)  ′  ∗ r (s  P  , N |s, a)γN  [Ct(i, s  ′  ′ , a  ) + V  ′ ∗ r (a  ′  , s  )]  ′  ′  (cid:19)  (cid:19)  (cid:19)  Here wt(i, s, a) is the diﬀerence between doing an update at node i using the single sample point s drawn according to Pt(s′, N |s, a) and doing an update using the full distribution Pt(s′, N |s, a). The value of ut(i, s, a) captures the diﬀerence between doing an update using the current probability transitions Pt(s′, N |s, a) and current value functions of the children Vt(a′, s′) and doing an up-date using the optimal probability transitions P ∗ r (s′, N |s, a) and the optimal values of the children r (a′, s′). V ∗  We now verify the conditions of Lemma 1. Condition (a) is assumed in the conditions of the theorem with αt(s, a) = αt(i). Condition (b) is satisﬁed because s is sampled from Pt(s′, N |s, a), so the expected value of the  diﬀerence is zero.  Condition (c) follows directly from the assumption that the |Ct(i, s, a)| and |Vt(i, s)| are bounded. Condition (d) is the condition that U is a weighted max norm pseudo-contraction. We can derive this by starting with the weighted max norm for Q learning. It is well known that Q is a weighted max norm pseudo-contraction (Bertsekas & Tsitsiklis, 1996) in both the episodic case where all deterministic policies are proper (and the discount factor γ = 1) and in the inﬁnite horizon discounted case (with γ < 1). That is, there exists a positive vector ξ and a scalar β ∈ [0, 1), such that for all t,  ||T Qt − Q  ||ξ ≤ β||Qt − Q  ||ξ,  ∗  ∗  (16)  where T is the operator  (T Q)(s, a) =  ′  P (s  , N |s, a)γN [R(s  ′  |s, a) + max  a′ Q(s  ′  ′ , a  )].  Xs′,N  Now we will show how to derive the contraction for the C update operator U . Our plan is to show ﬁrst how to express the U operator for learning C in terms of the T operator for updating Q values. Then we will replace T Q in the contraction equation for Q learning with U C, and show that U is a weighted max-norm contraction under the same weights ξ and the same β.  Recall from Eqn. (10) that Q(i, s, a) = C(i, s, a)+V (a, s). Furthermore, the U operator performs its updates using the optimal value functions of the child nodes, so we can write this as Qt(i, s, a) = Ct(i, s, a) + V ∗(a, s). Now once the children of node i have converged, the Q-function version of the Bellman equation for MDP Mi can be written as  Q(i, s, a) =  ′  ∗ r (s  P  , N |s, a)γN [V  ∗ r (a, s) + max  a′ Q(i, s  ′  ′ , a  )].  As we have noted before, V ∗ for node i, the T operator can be rewritten as  r (a, s) plays the role of the immediate reward function for Mi. Therefore,  (T Q)(i, s, a) =  ′  ∗ r (s  P  |s, a)γN [V  ∗ r (a, s) + max  a′ Q(i, s  ′  ′ , a  )].  Xs′,N  Xs′,N  24  Now we replace Q(i, s, a) by C(i, s, a) + V ∗  r (a, s), and obtain  (T Q)(i, s, a) =  ′  ∗ r (s  P  , N |s, a)γN (V  ∗ r (a, s) + max  [C(i, s  ′  ′ , a  ) + V  ′ ∗ r (a  ′  , s  )]).  Xs′,N  Note that V ∗ obtain  r (a, s) does not depend on s′ or N , so we can move it outside the expectation and  (T Q)(i, s, a) = V  ∗ r (a, s) +  ′  ∗ r (s  P  |s, a)γN (max  [C(i, s  ′  ′ , a  ) + V  ′ ∗ r (a  ′  , s  )])  Xs′,N ∗ r (a, s) + (U C)(i, s, a)  = V  Abusing notation slightly, we will express this in vector form as T Q(i) = V ∗ we can write Qt(i, s, a) = Ct(i, s, a) + V ∗  r (a, s) in vector form as Qt(i) = Ct(i) + V ∗ r .  r + U C(i). Similarly,  Now we can substitute these two formulas into the max norm pseudo-contraction formula for  T , Eqn. (16) to obtain  a′  a′  ||V  ∗ r + U Ct(i) − (C  ∗ r (i) + V  ∗ r )||ξ ≤ β||V  ∗ r + Ct(i) − (V  ∗ r + C  ∗ r (i))||ξ.  The V ∗ terms cancel on both sides of the equation, and we get  ||U Ct(i) − C  ∗ r (i)||ξ ≤ β||Ct(i) − C  ∗ r (i)||ξ.  Finally, it is easy verify (e), the most important condition. By assumption, the ordered GLIE policies in the child nodes converge with probability 1 to locally optimal policies for the children. Therefore Pt(s′, N |s, a) converges to P ∗ r (s′, N |s, a) for all s′, N, s, and a with probability 1 and Vt(a, s) converges with probability 1 to V ∗ r (a, s) for all child actions a. Therefore, |ut| converges to zero with probability 1. We can trivially construct a sequence θt = |ut| that bounds this convergence, so  |ut(s, a)| ≤ θt ≤ θt(||Ct(s, a)||ξ + 1).  We have veriﬁed all of the conditions of Lemma 1, so we can conclude that Ct(i) converges to C ∗ r (i) with probability 1. By induction, we can conclude that this holds for all nodes in the MAXQ including the root node, so the value function represented by the MAXQ graph converges to the unique value function of the recursively optimal policy π∗  r . Q.E.D.  Algorithm MAXQ-0 can be extended to accelerate learning in the higher nodes of the graph by a technique that we call “all states updating”. When an action a is chosen for Max node i in state s, the execution of a will move the environment through a sequence of states s = s1, . . . , sN , sN +1 = s′. If a was indeed the best abstract action to choose in s1, then it should also be the best action to choose (at node i) in states s2 through sN . Hence, we can execute a version of line 12 in MAXQ-0 for each of these intermediate states as shown in this replacement pseudo-code:  12a 12b 12c  for j from 1 to N do  Ct+1(i, sj, a) := (1 − αt(i)) · Ct(i, sj, a) + αt(i) · γ(N +1−j)maxa′ Qt(i, s′, a′) end // for  In our implementation, as each composite action is executed by MAXQ-0, it constructs a linked list of the sequence of primitive states that were visited. This list is returned when the composite action terminates. The parent Max node can then process each state in this list as shown above. The parent Max node appends the state lists that it receives from its children and passes them to its parent when it terminates. All experiments in this paper employ all states updating.  25  Kaelbling (1993) introduced a related, but more powerful, method for accelerating hierarchical reinforcement learning that she calls “all goals updating.” This method is suitable for a MAXQ hierarchy containing only a root task and one level of composite tasks. To understand all goals updating, suppose that for each primitive action, there are several composite tasks that could have invoked that primitive action. In all goals updating, whenever a primitive action is executed, the equivalent of line 12 of MAXQ-0 is applied in every composite task that could have invoked that primitive action. Sutton, Precup, and Singh (1998) prove that each of the composite tasks will converge to the optimal Q values under all goals updating.  All goals updating would work in the MAXQ hierarchy for composite tasks all of whose children are primitive actions. However, as we have seen, at higher levels in the hierarchy, node i needs to obtain samples of result states drawn according to P ∗(s′, N |s, a) for composite tasks a. All goals updating cannot provide these samples, so it cannot be applied at these higher levels.  Now that we have shown the convergence of MAXQ-0, let us design a learning algorithm for arbitrary pseudo-reward functions, ˜Ri(s). We could just add the pseudo-reward into MAXQ-0, but this has the eﬀect of changing the MDP M to have a diﬀerent reward function. The pseudo-rewards “contaminate” the values of all of the completion functions computed in the hierarchy. The resulting learned policy will not be recursively optimal for the original MDP.  This problem can be solved by learning two completion functions. The ﬁrst one, C(i, s, a) is the completion function that we have been discussing so far in this paper. It computes the expected reward for completing task Mi after performing action a in state s and then following the learned policy for Mi. It is computed without any reference to ˜Ri. This completion function will be used by parent tasks to compute V (i, s), the expected reward for performing action i starting in state s. The second completion function ˜C(i, s, a) is a completion function that we will use only “inside” node i in order to discover the locally optimal policy for task Mi. This function will incorporate rewards both from the “real” reward function, R(s′|s, a) and from the pseudo-reward function ˜Ri(s).  We will employ two diﬀerent update rules to learn these two completion functions. The ˜C function will be learned using an update rule similar to the Q learning rule in line 12 of MAXQ-0. But the C function will be learned using an update rule similar to SARSA(0)—its purpose is to learn the value function for the policy that is discovered by optimizing ˜C. Pseudo-code for the resulting algorithm, MAXQ-Q is shown in Table 4.  The key step is at lines 16 and 17. In line 16, MAXQ-Q ﬁrst updates ˜C using the value of the greedy action, a∗, in the resulting state. This update includes the pseudo-reward ˜Ri. Then in line 17, MAXQ-Q updates C using this same greedy action a∗, even if this would not be the greedy action according to the “uncontaminated” value function. This update, of course, does not include the pseudo-reward function.  It is important to note that whereever Vt(a, s) appears in this pseudo-code, it refers to the “uncontaminated” value function of state s when executing the Max node a. This is computed recursively in exactly the same way as in MAXQ-0.  Finally, note that the pseudo-code also incorporates all-states updating, so each call to MAXQ-Q returns a list of all of the states that were visited during its execution, and the updates of lines 16 and 17 are performed for each of those states. The list of states is ordered most-recent-ﬁrst, so the states are updated starting with the last state visited and working backward to the starting state, which helps speed up the algorithm.  When MAXQ-Q has converged, the resulting recursively optimal policy is computed at each node by choosing the action a that maximizes ˜Q(i, s, a) = ˜C(i, s, a)+V (a, s) (breaking ties according to the ﬁxed ordering established by the ordered GLIE policy). It is for this reason that we gave the name “Max nodes” to the nodes that represent subtasks (and learned policies) within the MAXQ  26  Table 4: The MAXQ-Q learning algorithm.  function MAXQ-Q(MaxNode i, State s)  let seq = () be the sequence of states visited while executing i if i is a primitive MaxNode  execute i, receive r, and observe result state s′ Vt+1(i, s) := (1 − αt(i)) · Vt(i, s) + αt(i) · rt push s into the beginning of seq  else  let count = 0 while Ti(s) is false do  choose an action a according to the current exploration policy πx(i, s) let childSeq = MAXQ-Q(a, s), where childSeq is the sequence of states visited  while executing action a.  observe result state s′ let a∗ = argmaxa′ [ ˜Ct(i, s′, a′) + Vt(a′, s′)] let N = length(childSeq) for each s in childSeq do  ˜Ct+1(i, s, a) := (1 − αt(i)) · ˜Ct(i, s, a) + αt(i) · γN [ ˜Ri(s′ Ct+1(i, s, a) := (1 − αt(i)) · Ct(i, s, a) + αt(i) · γN [Ct(i, s′, a∗) + Vt(a∗, s′)] N := N − 1 end // for  ) + ˜Ct(i, s′, a∗  ) + Vt(a∗, s)]  append childSeq onto the front of seq s := s′ end // while  end // else  return seq end MAXQ-0  1  2 3 4 5 6 7 8 9 10 11  12 13 14 15 16 17 18 19 20 21 22 23 24 25  graph. Each Q node j with parent node i stores both ˜C(i, s, j) and C(i, s, j), and it computes both ˜Q(i, s, j) and Q(i, s, j) by invoking its child Max node j. Each Max node i takes the maximum of these Q values and computes either V (i, s) or computes the best action, a∗ using ˜Q.  Corollary 1 Under the same conditions as Theorem 3, MAXQ-Q converges the unique recursively optimal policy for MDP M deﬁned by MAXQ graph H, pseudo-reward functions ˜R, and ordered GLIE exploration policy πx.  Proof: The argument is identical to, but more tedious than, the proof of Theorem 3. The proof of convergence of the ˜C values is identical to the original proof for the C values, but it relies on proving convergence of the “new” C values as well, which follows from the same weighted max norm pseudo-contraction argument. Q.E.D.  5 State Abstraction  There are many reasons to introduce hierarchical reinforcement learning, but perhaps the most important reason is to create opportunities for state abstraction. When we introduced the simple taxi problem in Figure 1, we pointed out that within each subtask, we can ignore certain aspects of the state space. For example, while performing a MaxNavigate(t), the taxi should make the same navigation decisions regardless of whether the passenger is in the taxi. The purpose of this section is to formalize the conditions under which it is safe to introduce such state abstractions and to show  27  how the convergence proofs for MAXQ-Q can be extended to prove convergence in the presence of state abstraction. Speciﬁcally, we will identify ﬁve conditions that permit the “safe” introduction of state abstractions.  Throughout this section, we will use the taxi problem as a running example, and we will see how each of the ﬁve conditions will permit us to reduce the number of distinct values that must be stored in order to represent the MAXQ value function decomposition. To establish a starting point, let us compute the number of values that must be stored for the taxi problem without any state abstraction.  The MAXQ representation must have tables for each of the C functions at the internal nodes and the V functions at the leaves. First, at the six leaf nodes, to store V (i, s), we must store 500 values at each node (because there are 500 states; 25 locations, 4 possible destinations for the passenger, and 5 possible current locations for the passenger (the four special locations and inside the taxi itself)). Second, at the root node, there are two children, which requires 2 × 500 = 1000 values. Third, at the MaxGet and MaxPut nodes, we have 2 actions each, so each one requires 1000 values, for a total of 2000. Finally, at MaxNavigate(t), we have four actions, but now we must also consider the target parameter t, which can take four possible values. Hence, there are eﬀectively 2000 combinations of states and t values for each action, or 8000 total values that must be represented. In total, therefore, the MAXQ representation requires 14,000 separate quantities to represent the value function.  To place this number in perspective, consider that a ﬂat Q learning representation must store a separate value for each of the six primitive actions in each of the 500 possible states, for a total of 3,000 values. Hence, we can see that without state abstraction, the MAXQ representation requires more than four times the memory of a ﬂat Q table!  5.1 Five Conditions that Permit State Abstraction  We now introduce ﬁve conditions that permit the introduction of state abstractions. For each condition, we give a deﬁnition and then prove a lemma which states that if the condition is satisﬁed, then the value function for some corresponding class of policies can be represented abstractly (i.e., by abstract versions of the V and C functions). For each condition, we then provide some rules for identifying when that condition can be satisﬁed and give examples from the taxi domain.  We begin by introducing some deﬁnitions and notation.  Deﬁnition 10 Let M be a MDP and H be a MAXQ graph deﬁned over M . Suppose that each state s can be written as a vector of values of a set of state variables. At each Max node i, suppose the state variables are partitioned into two sets Xi and Yi, and let χi be a function that projects a state s onto only the values of the variables in Xi. Then H combined with χi is called a state-abstracted MAXQ graph.  In cases where the state variables can be partitioned, we will often write s = (x, y) to mean that a state s is represented by a vector of values for the state variables in X and a vector of values for the state variables in Y . Similarly, we will sometimes write P (x′, y′, N |x, y, a), V (a, x, y), and ˜Ra(x′, y′) in place of P (s′, N |s, a), V (a, s), and ˜Ra(s′), respectively.  Deﬁnition 11 An abstract hierarchical policy for MDP M with state-abstracted MAXQ graph H and associated abstraction functions χi, is a hierarchical policy in which each policy πi (correspond-ing to subtask Mi) satisﬁes the condition that for any two states s1 and s2 such that χi(s1) = χi(s2), πi(s1) = πi(s2). (When πi is a stationary stochastic policy, this is interpreted to mean that the prob-ability distributions for choosing actions are the same in both states.)  28  In order for MAXQ-Q to converge in the presence of state abstractions, we will require that at all times t its (instantaneous) exploration policy is an abstract hierarchical policy. One way to achieve this is to construct the exploration policy so that it only uses information from the relevant state variables in deciding what action to perform. Boltzmann exploration based on the (state-abstracted) Q values, ǫ-greedy exploration, and counter-based exploration based on abstracted states are all abstract exploration policies. Counter-based exploration based on the full state space is not an abstract exploration policy.  Now that we have introduced our notation, let us describe and analyze the ﬁve abstraction conditions. We have identiﬁed three diﬀerent kinds of conditions under which abstractions can be introduced. The ﬁrst kind involves eliminating irrelevant variables within a subtask of the MAXQ graph. Under this form of abstraction, nodes toward the leaves of the MAXQ graph tend to have very few relevant variables, and nodes higher in the graph have more relevant variables. Hence, this kind of abstraction is most useful at the lower levels of the MAXQ graph.  The second kind of abstraction arises from “funnel” actions. These are macro actions that move the environment from some large number of initial states to a small number of resulting states. The completion cost of such subtasks can be represented using a number of values proportional to the number of resulting states. Funnel actions tend to appear higher in the MAXQ graph, so this form of abstraction is most useful near the root of the graph.  The third kind of abstraction arises from the structure of the MAXQ graph itself. It exploits the fact that large parts of the state space for a subtask may not be reachable because of the termination conditions of its ancestors in the MAXQ graph.  We begin by describing two abstraction conditions of the ﬁrst type. Then we will present two  conditions of the second type. And ﬁnally, we describe one condition of the third type.  5.1.1 Condition 1: Max Node Irrelevance  The ﬁrst condition arises when a set of state variables is irrelevant to a Max node.  Deﬁnition 12 Let Mi be a Max node in a MAXQ graph H for MDP M . A set of state variables Y is irrelevant to node i if the state variables of M can be partitioned into two sets X and Y such that for any stationary abstract hierarchical policy π executed by the descendants of i, the following two properties hold:  • the state transition probability distribution P π(s′, N |s, a) at node i can be factored into the  product of two distributions:  ′  P π(x  , y  ′  , N |x, y, a) = P π(y  ′  ′ |y, a) · P π(x  , N |x, a),  (17)  where y and y′ give values for the variables in Y , and x and x′ give values for the variables in X.  • for any pair of states s1 = (x, y1) and s2 = (x, y2) such that χ(s1) = χ(s2) = x, and any child  action a, V π(a, s1) = V π(a, s2) and ˜Ri(s1) = ˜Ri(s2).  Lemma 2 Let M be an MDP with full-state MAXQ graph H, and suppose that state variables Yi are irrelevant for Max node i. Let χi(s) = x be the associated abstraction function that maps s onto the remaining relevant variables Xi. Let π be any abstract hierarchical policy. Then the action-value function Qπ at node i can be represented compactly, with only one value of the completion  29  function C π(i, s, j) for each equivalence class of states s that share the same values on the relevant variables.  Speciﬁcally Qπ(i, s, j) can be computed as follows:  Qπ(i, s, j) = V π(j, χi(s)) + C π(i, χi(s), j)  where  C π(i, x, j) =  ′  P π(x  , N |x, j) · γN [V π(π(x  ′  ′ ), x  ′ ) + ˜Ri(x  ′ ) + C π(i, x  ′  , π(x  ))],  Xx′,N  where V π(j′, x′) = V π(j′, x′, y0), ˜Ri(x′) = ˜Ri(x′, y0), and π(x) = π(x, y0) for some arbitrary value y0 for the irrelevant state variables Yi.  Proof: Deﬁne a new MDP χi(Mi) at node i as follows:  • States: X = {x | χi(s) = x, for some s ∈ S}.  • Actions: A.  • Transition probabilities: P π(x′, N |x, a)  • Reward function: V π(a, x) + ˜Ri(x′)  Because π is an abstract policy, its decisions are the same for all states s such that χi(s) = x for some x. Therefore, it is also a well-deﬁned policy over χi(Mi). The action-value function for π over χi(Mi) is the unique solution to the following Bellman equation:  Qπ(i, x, j) = V π(j, x) +  ′ P π(x  , N |x, j) · γN [ ˜Ri(x  ′  ′ ) + Qπ(i, x  ′ , π(x  ))]  (18)  Compare this to the Bellman equation over Mi:  Qπ(i, s, j) = V π(j, s) +  ′  P π(s  , N |s, j) · γN [ ˜Ri(s  ) + Qπ(i, s  ′  ′  ′  , π(s  ))]  (19)  and note that V π(j, s) = V π(j, χ(s)) = V π(j, x) and ˜Ri(s′) = ˜Ri(χ(s′)) = ˜Ri(x′). Furthermore, we know that the distribution P π can be factored into separate distributions for Yi and Xi. Hence, we can rewrite (19) as  Qπ(i, s, j) = V π(j, x) +  ′  P (y  |y, j)  ′ P π(x  , N |x, j) · γN [ ˜Ri(x  ) + Qπ(i, s  ′  ′  ′  , π(s  ))]  Xy′  Xx′,N  The right-most sum does not depend on y or y′, so the sum over y′ evaluates to 1, and can be eliminated to give  Qπ(i, s, j) = V π(j, x) +  P π(x  ′  ′ , N |x, j) · γN [ ˜Ri(x  ) + Qπ(i, s  ′  ′  , π(s  ))].  (20)  Xx′,N  Xs′,N  Xx′,N  Finally, note that equations (18) and (20) are identical except for the expressions for the Q  values. Since the solution to the Bellman equation is unique, we must conclude that  Qπ(i, s, j) = Qπ(i, χ(s), j).  30  We can rewrite the right-hand side to obtain  Qπ(i, s, j) = V π(j, χ(s)) + C π(i, χ(s), j),  C π(i, x, j) =  ′ P (x  ′ , N |x, j) · γN [V π(π(x  ′ ), x  ′ ) + ˜Ri(x  ′ ) + C π(i, x  ′  , π(x  ))].  where  Q.E.D.  Xx′,N  Of course we are primarily interested in being able to discover and represent the optimal policy at each node i. The following corollary shows that the optimal policy is an abstract policy, and hence, that it can be represented abstractly.  Corollary 2 Consider the same conditions as Lemma 2, but with the change that the abstract hierarchical policy π is executed only by the descendants of node i, but not by node i. Let ρ be an ordering over actions. Then the optimal ordered policy π∗ ρ at node i is an abstract policy, and its action-value function can be represented abstracted.  Proof: Deﬁne the policy ω∗ ρ to be the optimal ordered policy over the abstract MDP χ(M ), and let Q∗(i, x, j) be the corresponding optimal action-value function. Then by the same argument given above, Q∗ is also a solution to the optimal Bellman equation for the original MDP. This means that the policy π∗ ρ deﬁned by π∗ ρ(s) = ω∗(χ(s)) is an optimal ordered policy, and by construction, it is an abstract policy. Q.E.D.  As stated, this condition appears quite diﬃcult to satisfy, since it requires that the state transi-tion probability distribution factor into X and Y components for all possible abstract hierarchical policies. However, in practice, this condition is often satisﬁed.  For example, let us consider the Navigate(t) subtask. The source and destination of the passenger are irrelevant to the achievement of this subtask. Any policy that successfully completes this subtask will have the same value function regardless of the source and destination locations of the passenger. (Any policy that does not complete the subtask will have the same value function also, but all states will have a value of −∞.) By abstracting away the passenger source and destination, we obtain a huge savings in space. Instead of requiring 8000 values to represent the C functions for this task, we require only 400 values (4 actions, 25 locations, 4 possible values for t).  One rule for noticing cases where this abstraction condition holds is to examine the subgraph rooted at the given Max node i. If a set of state variables is irrelevant to the leaf state transi-tion probabilities and reward functions and also to all pseudo-reward functions and termination conditions in the subgraph, then those variables satisfy the Max Node Irrelevance condition:  Lemma 3 Let M be an MDP with associated MAXQ graph H, and let i be a Max node in H. Let Xi and Yi be a partition of the state variables for M . A set of state variables Yi is irrelevant to node i if  • For each primitive leaf node a that is a descendant of i, P (x′, y′|x, y, a) = P (y′|y, a)P (x′|x, a)  and R(x′, y′|x, y, a) = R(x′|x, a),  • For each internal node j that is equal to node i or is a descendent of i , ˜Rj(x′, y′) = ˜Rj(x′)  and the termination predicate Tj(x′, y′) is true iﬀ Tj(x′).  31  Proof: We must show that any abstract hierarchical policy will give rise to an SMDP at node i whose transition probability distribution factors and whose reward function depends only on Xi. By deﬁnition, any abstract hierarchical policy will choose actions based only upon information in Xi. Because the primitive probability transition functions factor into an independent component for Xi and since the termination conditions at all nodes below i are based only on the variables in Xi, the probability transition function Pi(x′, y′, N |x, y, a) must also factor into Pi(y′|y, a) and Pi(x′, N |x, a). Similarly, all of the reward functions V (j, x, y) must be equal to V (j, x), because all rewards received within the subtree (either at the leaves or through pseudo-rewards) depend only on the variables in Xi. Therefore, the variables in Yi are irrelevant for Max node i. Q.E.D.  In the Taxi task, the primitive navigation actions, North, South, East, and West only depend on the location of the taxi and not on the location of the passenger. The pseudo-reward function and termination condition for the MaxNavigate(t) node only depend on the location of the taxi (and the parameter t). Hence, this lemma applies, and the passenger source and destination are irrelevant for the MaxNavigate node.  5.1.2 Condition 2: Leaf Irrelevance  The second abstraction condition describes situations under which we can apply state abstractions to leaf nodes of the MAXQ graph. For leaf nodes, we can obtain a stronger result than Lemma 2 by using a slightly weaker deﬁnition of irrelevance.  Deﬁnition 13 (Leaf Irrelevance) A set of state variables Y is irrelevant for a primitive action a of a MAXQ graph if for all states s the expected value of the reward function,  V (a, s) =  P (s  |s, a)R(s  |s, a)  ′  ′  Xs′  does not depend on any of the values of the state variables in Y . In other words, for any pair of states s1 and s2 that diﬀer only in their values for the variables in Y ,  P (s  ′ 1|s1, a)R(s  ′ 1|s1, a) =  P (s  ′ 2|s2, a)R(s  ′ 2|s2, a).  Xs′ If this condition is satisﬁed at leaf a, then the following lemma shows that we can represent its  Xs′  1  2  value function V (a, s) compactly.  Lemma 4 Let M be an MDP with full-state MAXQ graph H, and suppose that state variables Y are irrelevant for leaf node a. Let χ(s) = x be the associated abstraction function that maps s onto the remaining relevant variables X. Then we can represent V (a, s) for any state s by an abstracted value function V (a, χ(s)) = V (a, x).  Proof: According to the deﬁnition of Leaf Irrelevance, any two states that diﬀer only on the irrelevant state variables have the same value for V (a, s). Hence, we can represent this unique value by V (a, x). Q.E.D.  Here are two rules for ﬁnding cases where Leaf Irrelevance applies. The ﬁrst rule shows that if  the probability distribution factors, then we have Leaf Irrelevance.  Lemma 5 Suppose the probability transition function for primitive action a, P (s′|s, a), factors as P (x′, y′|x, y, a) = P (y′|y, a)P (x′|x, a) and the reward function satisﬁes R(s′|s, a) = R(x′|x, a). Then the variables in Y are irrelevant to the leaf node a.  32  Proof: Plug in to the deﬁnition of V (a, s) and simplify.  V (a, s) =  P (s  |s, a)R(s  |s, a)  ′  ′  ′  =  =  =  Xs′  Xx′,y′  Xy′  Xx′  P (y  ′  ′ |y, a)P (x  |x, a)R(x  |x, a)  ′  ′ P (x  ′ |x, a)R(x  |x, a)  P (y  |y, a)  Xx′ |x, a)R(x  ′  ′ P (x  |x, a)  Hence, the expected reward for the action a depends only on the variables in X and not on the variables in Y . Q.E.D.  Lemma 6 Let R(s′|s, a) = ra be the reward function for action a in MDP M , which is always equal to a constant ra. Then the entire state s is irrelevant to the primitive action a.  Proof:  V (a, s) =  P (s  |s, a)R(s  |s, a)  ′  ′  ′  =  P (s  |s, a)ra  Xs′  Xs′ = ra.  This does not depend on s, so the entire state is irrelevant to the primitive action a. Q.E.D.  This lemma is satisﬁed by the four leaf nodes North, South, East, and West in the taxi task, because their one-step reward is a constant (−1). Hence, instead of requiring 2000 values to store the V functions, we only need 4 values—one for each action. Similarly, the expected rewards of the Pickup and Putdown actions each require only 2 values, depending on whether the corresponding actions are legal or illegal. Hence, together, they require 4 values, instead of 1000 values.  5.1.3 Condition 3: Result Distribution Irrelevance  Now we consider a condition that results from “funnel” actions.  Deﬁnition 14 (Result Distribution Irrelevance). A set of state variables Yj is irrelevant for the result distribution of action j if, for all abstract policies π executed by node j and its descendants in the MAXQ hierarchy, the following holds: for all pairs of states s1 and s2 that diﬀer only in their values for the state variables in Yj,  ′  P π(s  , N |s1, j) = P π(s  ′  , N |s2, j)  for all s′ and N .  Lemma 7 Let M be an MDP with full-state MAXQ graph H, and suppose that the set of state variables Yj is irrelevant to the result distribution of action j, which is a child of Max node i. Let χij be the associated abstraction function: χij(s) = x. Then we can deﬁne an abstract completion cost function C π(i, χij(s), j) such that for all states s,  C π(i, s, j) = C π(i, χij (s), j).  33  Proof: The completion function for ﬁxed policy π is deﬁned as follows:  C π(i, s, j) =  ′  P (s  , N |s, j) · γN [ ˜Ri(s  ′  ) + Qπ(i, s  )].  ′  (21)  Xs′,N  Consider any two states s1 and s2, such that χij(s1) = χij(s2) = x. Under Result Distribution Irrelevance, their transition probability distributions are the same. Hence, the right-hand sides of (21) have the same value, and we can conclude that  C π(i, s1, j) = C π(i, s2, j). Therefore, we can deﬁne an abstract completion function, C π(i, x, j) to represent this quantity. Q.E.D.  It might appear that this condition would rarely be satisﬁed, and indeed, for inﬁnite horizon discounted problems, this is true. Consider, for example, the Get subroutine under an optimal policy for the taxi task. No matter what location that taxi has in state s, the taxi will be at the passenger’s starting location when the Get ﬁnishes executing (i.e., because the taxi will have just completed picking up the passenger). Hence, the starting location is irrelevant to the resulting location of the taxi. In the discounted cumulative reward setting, however, the number of steps N required to complete the Get action will depend very much on the starting location of the taxi. Consequently, P (s′, N |s, a) is not necessarily the same for any two states s with diﬀerent starting locations even though s′ is always the same.  The important lesson to draw from this is that discounting interferes with introducing state abstractions based on “funnel” operators—the MAXQ framework is therefore less eﬀective when applied in the discounted setting.  However, if we restrict attention to the episodic, undiscounted setting, then the result dis-tribution, P (s′|s, a), no longer depends on N , and the Result Distribution Irrelevance condition is satisﬁed. Fortunately, the Taxi task is an undiscounted, ﬁnite-horizon task, so we can repre-sent C(Root, s, Get) using 16 distinct values, because there are 16 equivalence classes of states (4 source locations times 4 destination locations). This is much less than the 500 quantities in the unabstracted representation.  “Funnel” actions arise in many hierarchical reinforcement learning problems. For example, abstract actions that move a robot to a doorway or that move a car onto the entrance ramp of a freeway have this property. The Result Distribution Irrelevance condition is applicable in all such situations as long as we are in the undiscounted setting.  5.1.4 Condition 4: Termination  The fourth condition is closely related to the “funnel’ property. It applies when a subtask is guaranteed to cause its parent task to terminate in a goal state. In a sense, the subtask is funneling the environment into the set of states described by the goal predicate of the parent task.  Lemma 8 (Termination). Let Mi be a task in a MAXQ graph such that for all states s where the goal predicate Gi(s) is true, the pseudo-reward function ˜Ri(s) = 0. Suppose there is a child task a and state s such that for all hierarchical policies π,  ′  ∀ s  ′  P π  i (s  , N |s, a) > 0 ⇒ Gi(s  ).  ′  (i.e., if s′ is a possible result state of applying a in s, then s′ is a goal terminal state for task i.)  Then for any policy executed at node i, the completion cost C(i, s, a) is zero and does not need  to be explicitly represented.  34  Proof: By the assumptions in the lemma, with probability 1 the completion cost is zero for any action that results in a goal terminal state. Q.E.D.  For example, in the Taxi task, in all states where the taxi is holding the passenger, the Put subroutine will succeed and result in a goal terminal state for Root. This is because the termination predicate for Put (i.e., that the passenger is at his or her destination location) implies the goal condition for Root (which is the same). This means that C(Root, s, Put) is uniformly zero, for all states s where Put is not terminated.  It is easy to detect cases where the Termination condition is satisﬁed. We only need to compare the termination predicate of a subtask with the goal predicate of the parent task. If the ﬁrst implies the second, then the termination condition is satisﬁed.  5.1.5 Condition 5: Shielding  The shielding condition arises from the structure of the MAXQ graph.  Lemma 9 (Shielding). Let Mi be a task in a MAXQ graph and s be a state such that for all paths from the root of the graph down to node Mi there exists a subtask j (possibly equal to i) whose termination predicate Tj(s) is true, then the Q nodes of Mi do not need to represent C values for state s.  Proof: Task i cannot be executed in state s, so no C values need to be estimated. Q.E.D.  As with the Termination condition, the Shielding condition can be veriﬁed by analyzing the  structure of the MAXQ graph and identifying nodes whose ancestor tasks are terminated.  In the Taxi task, a simple example of this arises in the Put task, which is terminated in all states where the passenger is not in the taxi. This means that we do not need to represent C(Root, s, Put) in these states. The result is that, when combined with the Termination condition above, we do not need to explcitly represent the completion function for Put at all!  5.1.6 Dicussion  By applying these ﬁve abstraction conditions, we obtain the following “safe” state abstractions for the Taxi task:  • North, South, East, and West. These terminal nodes require one quantity each, for a total of  four values. (Leaf Irrelevance).  • Pickup and Putdown each require 2 values (legal and illegal states), for a total of four. (Leaf  Irrelevance.)  • QNorth(t), QSouth(t), QEast(t), and QWest(t) each require 100 values (four values for t and  25 locations). (Max Node Irrelevance.)  • QNavigateForGet requires 4 values (for the four possible source locations). (The passenger destination is Max Node Irrelevant for MaxGet, and the taxi starting location is Result Dis-tribution Irrelevant for the Navigate action.)  • QPickup requires 100 possible values, 4 possible source locations and 25 possible taxi locations.  (Passenger destination is Max Node Irrelevant to MaxGet.)  35  • QGet requires 16 possible values (4 source locations, 4 destination locations). (Result Distribution Irrelevance.)  • QNavigateForPut requires only 4 values (for the four possible destination locations). (The passenger source and destination are Max Node Irrelevant to MaxPut; the taxi location is Result Distribution Irrelevant for the Navigate action.)  • QPutdown requires 100 possible values (25 taxi locations, 4 possible destination locations).  (Passenger source is Max Node Irrelevant for MaxPut.)  • QPut requires 0 values. (Termination and Shielding.)  This gives a total of 632 distinct values, which is much less than the 3000 values required by ﬂat Q learning. Hence, we can see that by applying state abstractions, the MAXQ representation can give a much more compact representation of the value function. A key thing to note is that these state abstractions cannot be exploited with the ﬂat representation of the value function.  What prior knowledge is required on the part of a programmer in order to introduce these state abstractions? It suﬃces to know some general constraints on the one-step reward functions, the one-step transition probabilities, and termination predicates, goal predicates, and pseudo-reward functions within the MAXQ graph. Speciﬁcally, the Max Node Irrelevance and Leaf Irrelevance conditions require simple analysis of the one-step transition function and the reward and pseudo-reward functions. Opportunities to apply the Result Distribution Irrelevance condition can be found by identifying “funnel” eﬀects that result from the deﬁnitions of the termination conditions for operators. Similarly, the Shielding and Termination conditions only require analysis of the termination predicates of the various subtasks. Hence, applying these ﬁve conditions to introduce state abstractions is a straightforward process, and once a model of the one-step transition and reward functions has been learned, the abstraction conditions can be checked to see if they were satisﬁed.  5.2 Convergence of MAXQ-Q with State Abstraction  We have shown that state abstractions can be safely introduced into the MAXQ value function decomposition under the ﬁve conditions described above. However, these conditions only guarantee that the value function of any ﬁxed abstract hierarchical policy can be represented—they do not show that the optimal policy can be represented, nor do they show that the MAXQ-Q learning algorithm will ﬁnd the optimal policy. The goal of this section is to prove these two results: (a) that the ordered recursively-optimal policy is an abstract policy and (b) that MAXQ-Q will converge to this policy when applied to a MAXQ graph with safe state abstractions.  Lemma 10 Let M be an MDP with full-state MAXQ graph H and abstract-state MAXQ graph χ(H) where the abstractions satisfy the ﬁve conditions given above. Let ρ be an ordering over all actions in the MAXQ graph. Then the following statements are true:  • The unique ordered recursively-optimal policy π∗  r deﬁned by M , H, and ρ is an abstract policy  (i.e., it depends only on the relevant state variables at each node),  • The C and V functions in χ(H) can represent the projected value function of π∗ r .  36  Proof: The ﬁve abstraction lemmas tell us that if the ordered recursively-optimal policy is ab-stract, then the C and V functions of χ(H) can represent its value function. Hence, the heart of this lemma is the ﬁrst claim. The last two forms of abstraction (Shielding and Termination) do not place any restrictions on abstract policies, so we ignore them in this proof.  The proof is by induction on the levels of the MAXQ graph, starting at the leaves. As a base case, let us consider a Max node i all of whose children are primitive actions. In this case, there are no policies executed within the children of the Max node. Hence if variables Yi are irrelevant for node i, then we can apply our abstraction lemmas to represent the value function of any policy at node i—not just abstract policies. Consequently, the value function of any optimal policy for node i can be represented, and it will have the property that Q∗(i, s1, a) = Q∗(i, s2, a) for any states s1 and s2 such that χi(s1) = χi(s2).  Now let us impose the action ordering ρ to compute the optimal ordered policy. Consider two actions a1 and a2 such that ρ(a1, a2) (i.e., ρ prefers a1), and suppose that there is a “tie” in the Q∗ function at state s1 such that the values  ∗  Q  (i, s1, a1) = Q  (i, s1, a2)  ∗  and they are the only two actions that maximize Q∗ in this state. Then the optimal ordered policy must choose a1. Now in all other states s2 such that χi(s1) = χi(s2), we know that the Q∗ values will be the same. Hence, the same tie will exist between a1 and a2, and hence, the optimal ordered policy must make the same choice in all such states. Hence, the optimal ordered policy for node i is an abstract policy.  Now let us turn to the recursive case at Max node i. Make the inductive assumption that the ordered recursively-optimal policy is abstract within all descendant nodes and consider the locally optimal policy at node i. If Y is a set of state variables that are irrelevant to node i, Corollary 2 tells us that Q∗(i, s1, j) = Q∗(i, s2, j) for all states s1 and s2 such that χi(s1) = χi(s2). Similarly, if Y is a set of variables irrelevant to the result distribution of a particular action j, then Lemma 7 tells us the same thing. Hence, by the same ordering argument given above, the ordered optimal policy at node i must be abstract. By induction, this proves the lemma. Q.E.D.  With this lemma, we have established that the combination of an MDP M , an abstract MAXQ graph H, and an action ordering deﬁnes a unique recursively-optimal ordered abstract policy. We are now ready to prove that MAXQ-Q will converge to this policy.  Theorem 4 Let M = hS, A, P, R, P0i be either an episodic MDP for which all deterministic policies are proper or a discounted inﬁnite horizon MDP with discount factor γ < 1. Let H be an unab-stracted MAXQ graph deﬁned over subtasks {M0, . . . , Mk} with pseudo-reward functions ˜Ri(s′|s, a). Let χ(H) be a state-abstracted MAXQ graph deﬁned by applying state abstractions χi to each node i of H under the ﬁve conditions given above. Let πx(i, χi(s)) be an abstract ordered GLIE exploration policy at each node i and state s whose decisions depend only on the “relevant” state variables at each node i. Let π∗ r be the unique recursively-optimal hierarchical policy deﬁned by πx, M , and ˜R. Then with probability 1, algorithm MAXQ-Q applied to χ(H) converges to π∗ r provided that the learning rates αt(i) satisfy Equation (15) and |Vt(i, χi(s))| and |Ct(i, χi(s), a)| are bounded for all t, i, χi(s), and a.  Proof: Rather than repeating the entire proof for MAXQ-Q, we will only describe what must change under state abstraction. The last two forms of state abstraction refer to states whose values can be inferred from the structure of the MAXQ graph, and therefore do not need to be represented  37  at all. Since these values are not updated by MAXQ-Q, we can ignore them. We will now consider the ﬁrst three forms of state abstraction in turn.  We begin by considering primitive leaf nodes. Let a be a leaf node and let Y be a set of state variables that are Leaf Irrelevant for a. Let s1 = (x, y1) and s2 = (x, y2) be two states that diﬀer only in their values for Y . Under Leaf Irrelevance, the probability transitions P (s′ 1|s1, a) and P (s′ 2|s2, a) need not be the same, but the expected reward of performing a in both states must be the same. When MAXQ-Q visits an abstract state x, it does not “know” the value of y, the part of the state that has been abstracted away. Nonetheless, it draws a sample according to P (s′|x, y, a), receives a reward R(s′|x, y, a), and updates its estimate of V (a, x) (line 5 of MAXQ-Q). Let Pt(y) be the probability that MAXQ-Q is visiting (x, y) given that the unabstracted part of the state is x. Then Line 5 of MAXQ-Q is computing a stochastic approximation to  We can write this as  Pt(y)  ′  Pt(s  , N |x, y, a)R(s  |x, y, a).  Pt(y)Pt(s  , N |x, y, a)R(s  |x, y, a).  ′  ′  ′  Xs′,N,y  y X  Xs′,N  According to Leaf Irrelevance, the inner sum has the same value for all states s such that χ(s) = x. Call this value r0(x). This gives  Pt(y)r0(x),  y X  which is equal to r0(x) for any distribution Pt(y). Hence, MAXQ-Q converges under Leaf Irrelevance abstractions.  Now let us turn to the two forms of abstraction that apply to internal nodes: Node Irrelevance and Result Distribution Irrelevance. Consider the SMDP deﬁned at each node i of the abstracted MAXQ graph at time t during MAXQ-Q. This would be an ordinary SMDP with transition prob-ability function Pt(x′, N |x, a) and reward function Vt(a, x) + ˜Ri(x′) except that when MAXQ-Q draws samples of state transitions, they are drawn according to the distribution Pt(s′, N |s, a) over the original state space. To prove the theorem, we must show that drawing (s′, N ) according to this second distribution is equivalent to drawing (x′, N ) according to the ﬁrst distribution.  For Max Node Irrelevance, we know that for all abstract policies applied to node i and its  descendants, the transition probability distribution factors as  ′  P (s  , N |s, a) = P (y  ′  ′ |y, a)P (x  , N |x, a).  Because the exploration policy is an abstract policy, Pt(s′, N |s, a) factors in this way. This means that the Xi and Yi components of the state are independent of each other, and hence, sampling from Pt(s′, N |s, a) gives samples for Pt(x′, N |x, a). Therefore, MAXQ-Q will converge under Max Node Irrelevance abstractions.  Finally, consider Result Distribution Irrelevance. Let j be a child of node i, and suppose Yj is a set of state variables that are irrelevant to the result distribution of j. When the SMDP at node i wishes to draw a sample from Pt(x′, N |x, j), it does not “know” the current value of y, the irrelevant part of the current state. However, this does not matter, because Result Distribution Irrelevance means that for all possible values of y, Pt(x′, y′, N |x, y, j) is the same. Hence, MAXQ-Q will converge under Result Distribution Irrelevance abstractions.  In each of these three cases, MAXQ-Q will converge to a locally-optimal ordered policy at node i in the MAXQ graph. By Lemma 10, this can be extended to produce a locally-optimal ordered policy for the unabstracted SMDP at node i. Hence, by induction, MAXQ-Q will converge to  38  the unique ordered recursively optimal policy π∗ exploration policy πx. Q.E.D.  r deﬁned by MAXQ-Q H, MDP M , and ordered  5.3 The Hierarchical Credit Assignment Problem  There are still some situations where we would like to introduce state abstractions but where the ﬁve properties described above do not permit them. Consider the following modiﬁcation of the taxi problem. Suppose that the taxi has a fuel tank and that each time the taxi moves one square, it costs one unit of fuel. If the taxi runs out of fuel before delivering the passenger to his or her destination, it receives a reward of −20, and the trial ends. Fortunately, there is a ﬁlling station where the taxi can execute a Fillup action to ﬁll the fuel tank.  To solve this modiﬁed problem using the MAXQ hierarchy, we can introduce another subtask, Refuel, which has the goal of moving the taxi to the ﬁlling station and ﬁlling the tank. MaxRefuel is a child of MaxRoot, and it invokes Navigate(t) (with t bound to the location of the ﬁlling station) to move the taxi to the ﬁlling station.  The introduction of fuel and the possibility that we might run out of fuel means that we must include the current amount of fuel as a feature in representing every C value (for internal nodes) and V value (for leaf nodes). This is unfortunate, because our intuition tells us that the amount of fuel should have no inﬂuence on our decisions inside the Navigate(t) subtask. The amount of fuel should be taken into account by the top-level Q nodes, which must decide whether to go refuel, go pick up the passenger, or go deliver the passenger.  Given this intuition, it is natural to try abstracting away the “amount of remaining fuel” within the Navigate(t) subtask. However, this doesn’t work, because when the taxi runs out of fuel and a −20 reward is given, the QNorth, QSouth, QEast, and QWest nodes cannot “explain” why this reward was received—that is, they have no consistent way of setting their C tables to predict when this negative reward will occur. Stated more formally, the diﬃculty is that the Max Node Irrelevance condition is not satisﬁed because the one-step reward function R(s′|s, a) for these actions depends on the amount of fuel.  We call this the hierarchical credit assignment problem. The fundamental issue here is that in the MAXQ decomposition all information about rewards is stored in the leaf nodes of the hierarchy. We would like to separate out the basic rewards received for navigation (i.e., −1 for each action) from the reward received for exhausting fuel (−20). If we make the reward at the leaves only depend on the location of the taxi, then the Max Node Irrelevance condition will be satisﬁed.  One way to do this is to have the programmer manually decompose the reward function and indicate which nodes in the hierarchy will “receive” each reward. Let R(s′|s, a) = i R(i, s′|s, a) be a decomposition of the reward function, such that R(i, s′|s, a) speciﬁes that part of the reward that must be handled by Max node i. In the modiﬁed taxi problem, for example, we can decompose the reward so that the leaf nodes receive all of the original penalties, but the out-of-fuel rewards must be handled by MaxRoot. Lines 16 and 17 of the MAXQ-Q algorithm are easily modiﬁed to include R(i, s′|s, a).  P  In most domains, we believe it will be easy for the designer of the hierarchy to decompose the reward function. It has been straightforward in all of the problems we have studied. However, an interesting problem for future research is to develop an algorithm that can solve the hierarchical credit assignment problem autonomously.  39  6 Non-Hierarchical Execution of the MAXQ Hierarchy  Up to this point in the paper, we have focused exclusively on representing and learning hierarchical policies. However, often the optimal policy for a MDP is not a strictly hierarchical policy. Kaelbling (1993) ﬁrst introduced the idea of deriving a non-hierarchical policy from the value function of a hierarchical policy. In this section, we exploit the MAXQ decomposition to generalize her ideas and apply them recursively at all levels of the hierarchy.  The ﬁrst method is based on the dynamic programming algorithm known as policy iteration. The policy iteration algorithm starts with an initial policy π0. It then repeats the following two steps until the policy converges. In the policy evaluation step, it computes the value function V πk of the current policy πk. Then, in the policy improvement step, it computes a new policy, πk+1 according to the rule  πk+1(s) := argmax  ′  P (s  |s, a)[R(s  ′  |s, a) + γV πk(s  )].  ′  (22)  a  Xs′  Howard (1960) proved that if πk is not an optimal policy, then πk+1 is guaranteed to be an im-provement. Note that in order to apply this method, we need to know the transition probability distribution P (s′|s, a) and the reward function R(s′|s, a).  If we know P (s′|s, a) and R(s′|s, a), we can use the MAXQ representation of the value function to perform one step of policy iteration. We start with a hierarchical policy π and represent its value function using the MAXQ hierarchy (e.g., π could have been learned via MAXQ-Q). Then, we can perform one step of policy improvement by applying Equation (22) using V π(0, s′) (computed by the MAXQ hierarchy) to compute V π(s′).  s′ P (s′|s, a)[R(s′|s, a) + γV π(0, s)], where V π(0, s) is the value Corollary 3 Let πg(s) = argmaxa function computed by the MAXQ hierarchy. Then, if π was not an optimal policy, πg is strictly better for at least one state in S.  P  Proof: This is a direct consequence of Howard’s policy improvement theorem. Q.E.D.  Unfortunately, we can’t iterate this policy improvement process, because the new policy, πg is very unlikely to be a hierarchical policy (i.e., it is unlikely to be representable in terms of local policies for each node of the MAXQ graph). Nonetheless, one step of policy improvement can give very signiﬁcant improvements.  This approach to non-hierarchical execution ignores the internal structure of the MAXQ graph. In eﬀect, the MAXQ hierarchy is just viewed as a kind of function approximator for representing V π—any other representation would give the same one-step improved policy πg.  The second approach to non-hierarchical execution borrows an idea from Q learning. One of the great beauties of the Q representation for value functions is that we can compute one step of policy improvement without knowing P (s′|s, a), simply by taking the new policy to be πg(s) := argmaxa Q(s, a). This gives us the same one-step greedy policy as we computed above using one-step lookahead. With the MAXQ decomposition, we can perform these policy improvement steps at all levels of the hierarchy.  We have already deﬁned the function that we need.  In Table 3 we presented the function EvaluateMaxNode, which, given the current state s, conducts a search along all paths from a given Max node i to the leaves of the MAXQ graph and ﬁnds the path with the best value (i.e., with the maximum sum of C values along the path, plus the V value at the leaf). In addition, EvaluateMaxNode returns the primitive action a at the end of this best path. This action a would  40  Table 5: The procedure for executing the one-step greedy policy.  procedure ExecuteHGPolicy(s)  repeat  Let hV (0, s), ai := EvaluateMaxNode(0, s) execute primitive action a Let s be the resulting state  end // ExecuteHGPolicy  1 2 3 4  be the ﬁrst primitive action to be executed if the learned hierarchical policy were executed starting in the current state s. Our second method for non-hierarchical execution of the MAXQ graph is to call EvaluateMaxNode in each state, and execute the primitive action a that is returned. The pseudo-code is shown in Table 5.  We will call the policy computed by ExecuteHGPolicy the hierarchical greedy policy, and denote it πhg∗, where the superscript * indicates that we are computing the greedy action at each time step. The following theorem shows that this can give a better policy than the original, hierarchical policy.  Theorem 5 Let G be a MAXQ graph representing the value function of hierarchical policy π (i.e., in terms of C π(i, s, j), computed for all i, s, and j). Let V hg(0, s) be the value computed by ExecuteHGPolicy, and let πhg∗ be the resulting policy. Deﬁne V hg∗ to be the value function of πhg∗. Then for all states s, it is the case that  V π(s) ≤ V hg(0, s) ≤ V hg∗  (s).  (23)  Proof: (sketch) The left inequality in Equation (23) is satisﬁed by construction by line 7 of EvaluateMaxNode. To see this, consider that the original hierarchical policy, π, can be viewed as choosing a “path” through the MAXQ graph running from the root to one of the leaf nodes, and V π(0, s) is the sum of the C π values along this chosen path (plus the V π value at the leaf node). In contrast, EvaluateMaxNode performs a traversal of all paths through the MAXQ graph and ﬁnds the best path, that is, the path with the largest sum of C π (and leaf V π) values. Hence, V hg(0, s) must be at least as large as V π(0, s).  To establish the right inequality, note that by construction V hg(0, s) is the value function of a policy, call it πhg, that chooses one action greedily at each level of the MAXQ graph (recursively), and then follows π thereafter. This is a consequence of the fact that line 7 of EvaluateMaxNode has C π on its right-hand side, and C π represents the cost of “completing” each subroutine by following π, not by following some other, greedier, policy. (In Table 3, C π is written as Ct.) However, when we execute ExecuteHGPolicy (and hence, execute πhg∗), we have an opportunity to improve upon π and πhg at each time step. Hence, V hg(0, s) is an underestimate of the actual value of πhg∗. Q.E.D.  Note that this theorem only works in one direction. It says that if we can ﬁnd a state where V hg(0, s) > V π(s), then the greedy policy, πhg∗, will be strictly better than π. However, it could be that π is not an optimal policy and yet the structure of the MAXQ graph prevents us from considering an action (either primitive or composite) that would improve π. Hence, unlike the policy improvement theorem of Howard, we do not have a guarantee that if π is suboptimal, then the hierarchically greedy policy is a strict improvement.  41  In contrast, if we perform one-step policy improvement as discussed at the start of this section, Corollary 3 guarantees that we will improve the policy. So we can see that in general, neither of these two methods for non-hierarchical execution dominates the other. Nonetheless, the ﬁrst method only operates at the level of individual primitive actions, so it is not able to produce very large improvements in the policy. In contrast, the hierarchical greedy method can obtain very large improvements in the policy by changing which actions (i.e., subroutines) are chosen near the root of the hierarchy. Hence, in general, hierarchical greedy execution is probably the better method. (Of course, the value functions of both methods could be computed, and the one with the better estimated value could be executed.)  Sutton, Singh, Precup and Ravindran (1999) have simultaneously developed a closely-related method for non-hierarchical execution of macros. Their method is equivalent to ExecuteHGPolicy for the special case where the MAXQ hierarchy has only one level of subtasks. The interesting aspect of ExecuteHGPolicy is that it permits greedy improvements at all levels of the tree to inﬂuence which action is chosen.  Some care must be taken in applying Theorem 5 to a MAXQ hierarchy whose C values have been learned via MAXQ-Q. Being an online algorithm, MAXQ-Q will not have correctly learned the values of all states at all nodes of the MAXQ graph. For example, in the taxi problem, the value of C(Put, s, QPutdown) will not have been learned very well except at the four special locations. This is because the Put subtask cannot be executed until the passenger is in the taxi, and this usually means that a Get has just been completed, so the taxi is at the passenger’s source location. During exploration, both children of Put will be tried in such states. The PutDown will usually fail, whereas the Navigate will eventually succeed (perhaps after lengthy exploration) and take the taxi to the destination location. Now because of all states updating, the values for C(Put, s, Navigate(t)) will have been learned at all of the states, but the C values for the Putdown action will not. Hence, if we train the MAXQ representation using hierarchical execution (as in MAXQ-Q), and then switch to hierarchically-greedy execution, the results will be quite bad. In particular, we need to introduce hierarchically-greedy execution early enough so that the exploration policy is still actively exploring. (In theory, a GLIE exploration policy never ceases to explore, but in practice, we want to ﬁnd a good policy quickly, not just asymptotically).  Of course an alternative would be to use hierarchically-greedy execution from the very beginning of learning. However, remember that the higher nodes in the MAXQ hierarchy need to obtain samples of P (s′, N |s, a) for each child action a. If the hierarchical greedy execution interrupts child a before it has reached a terminal state, then these samples cannot be obtained. Hence, it is important to begin with purely hierarchical execution during training, and make a transition to greedy execution at some point.  The approach we have taken is to implement MAXQ-Q in such a way that we can specify a number of primitive actions L that can be taken hierarchically before the hierarchical execution is “interrupted” and control returns to the top level (where a new action can be chosen greedily). We start with L set very large, so that execution is completely hierarchical—when a child action is invoked, we are committed to execute that action until it terminates. However, gradually, we reduce L until it becomes 1, at which point we have hierarchical greedy execution. We time this so that it reaches 1 at about the same time our Boltzmann exploration cools to a temperature of 0.1 (which is where exploration eﬀectively has halted). As the experimental results will show, this generally gives excellent results with very little added exploration cost.  42  7 Experimental Evaluation of the MAXQ Method  We have performed a series of experiments with the MAXQ method with three goals in mind: (a) to understand the expressive power of the value function decomposition, (b) to characterize the behavior of the MAXQ-Q learning algorithm, and (c) to assess the relative importance of temporal abstraction, state abstraction, and non-hierarchical execution. In this section, we describe these experiments and present the results.  7.1 The Fickle Taxi Task  Our ﬁrst experiments were performed on a modiﬁed version of the taxi task. This version incor-porates two changes to the task described in Section 3.1. First, each of the four navigation actions is noisy, so that with probability 0.8 it moves in the intended direction, but with probability 0.1 it instead moves to the right (of the intended direction) and with probability 0.1 it moves to the left. The second change is that after the taxi has picked up the passenger and moved one square away from the passenger’s source location, the passenger changes his or her destination location with probability 0.3. The purpose of this change is to create a situation where the optimal policy is not a hierarchical policy so that the eﬀectiveness of non-hierarchical execution can be measured. We compared four diﬀerent conﬁgurations of the learning algorithm: (a) ﬂat Q learning, (b) MAXQ-Q learning without any form of state abstraction, (c) MAXQ-Q learning with state abstrac-tion, and (d) MAXQ-Q learning with state abstraction and greedy execution. These conﬁgurations are controlled by many parameters. These include the following: (a) the initial values of the Q and C functions, (b) the learning rate (we employed a ﬁxed learning rate), (c) the cooling schedule for Boltzmann exploration (the GLIE policy that we employed), and (d) for non-hierarchical ex-ecution, the schedule for decreasing L, the number of steps of consecutive hierarchical execution. We optimized these settings separately for each conﬁguration with the goal of matching or exceed-ing (with as few primitive actions as possible) the best policy that we could code by hand. For Boltzmann exploration, we established an initial temperature and then a cooling rate. A separate temperature is maintained for each Max node in the MAXQ graph, and its temperature is reduced by multiplying by the cooling rate each time that subtask terminates in a goal state.  The following parameters were chosen. For ﬂat Q learning: initial Q values of 0.123, learning rate 0.25, and Boltzmann exploration with an initial temperature of 50 and a cooling rate of 0.9879. (We use initial values that end in .123 as a “signature” to aid debugging.)  For MAXQ-Q learning without state abstraction, we used initial values of 0.123, a learning rate of 0.50, and Boltzmann exploration with an initial temperature of 50 and cooling rates of .9996 at MaxRoot and MaxPut, 0.9939 at MaxGet, and 0.9879 at MaxNavigate.  For MAXQ-Q learning with state abstraction, we used initial values of 0.123, a learning rate of 0.25, and Boltzmann exploration with an initial temperature of 50 and cooling rates of 0.9074 at MaxRoot, 0.9526 at MaxPut, 0.9526 at MaxGet, and 0.9879 at MaxNavigate.  For MAXQ-Q learning with non-hierarchical execution, we used the same settings as with state In addition, we initialized L to 500 and decreased it by 10 with each trial until it  abstraction. reached 1. So after 50 trials, execution was completely greedy.  Figure 7 shows the averaged results of 100 training trials. The ﬁrst thing to note is that all forms of MAXQ learning have better initial performance than ﬂat Q learning. This is because of the constraints introduced by the MAXQ hierarchy. For example, while the agent is executing a Navigate subtask, it will never attempt to pickup or putdown the passenger. Similarly, it will never attempt to putdown the passenger until it has ﬁrst picked up the passenger (and vice versa).  The second thing to notice is that without state abstractions, MAXQ-Q learning actually takes  43  d r a w e R   e v i t a l u m u C   n a e  M  200  0  -200  -400  -600  -800  -1000  0  Hierarchical Q Learning with State Abstraction and Greedy Execution Hierarchical Q Learning with State Abstraction Hierarchical Q Learning without State Abstraction Flat Q Learning  50000  100000  150000  200000  250000  Primitive Actions  Figure 7: Comparison of performance of hierarchical Q learning with ﬂat Q learning, with and without state abstractions, and with and without greedy evaluation.  longer to converge, so that the Flat Q curve crosses the MAXQ/no abstraction curve. This shows that without state abstraction, the cost of learning the huge number of parameters in the MAXQ representation is not really worth the beneﬁts.  The third thing to notice is that with state abstractions, MAXQ-Q converges very quickly to a hierarchically optimal policy. This can be seen more clearly in Figure 8, which focuses on the range of reward values in the neighborhood of the optimal policy. Here we can see that MAXQ with abstractions attains the hierarchically optimal policy after approximately 40,000 steps, whereas ﬂat Q learning requires roughly twice as long to reach the same level. However, ﬂat Q learning, of course, can continue onward and reach optimal performance, whereas with the MAXQ hierarchy, the best hierarchical policy is slow to respond to the “ﬁckle” behavior of the passenger when he/she changes the destination.  The last thing to notice is that with greedy execution, the MAXQ policy is also able to attain optimal performance. But as the execution becomes “more greedy”, there is a drop in performance, because MAXQ-Q must learn C values in new regions of the state space that were not visited by the recursively optimal policy. Despite this drop in performance, greedy MAXQ-Q recovers rapidly and reaches hierarchically optimal performance faster than purely-hierarchical MAXQ-Q learning. Hence, there is no added cost—in terms of exploration—for introducing greedy execution.  This experiment presents evidence in favor of three claims: ﬁrst, that hierarchical reinforcement learning can be much faster than ﬂat Q learning; second, that state abstraction is required by MAXQ for good performance; and third, that non-hierarchical execution can produce signiﬁcant improvements in performance with little or no added exploration cost.  44  MAXQ Abstract+Greedy  Flat Q  Optimal Policy  Hier-Optimal Policy  MAXQ Abstract  MAXQ No Abstract  d r a w e R   e v i t a l u m u C   n a e  M  10  5  0  -5  -10  -15  0  50000  100000  150000  200000  250000  300000  Primitive Actions  Figure 8: Close-up view of the previous ﬁgure. This ﬁgure also shows two horizontal lines indicating optimal performance and hierarchically optimal performance in this domain. To make this ﬁgure more readable, we have applied a 100-step moving average to the data points.  7.2 Kaelbling’s HDG Method  The second task that we will consider is a simple maze task introduced by Leslie Kaelbling (1993) and shown in Figure 10. In each trial of this task, the agent starts in a randomly-chosen state and must move to a randomly-chosen goal state using the usual North, South, East, and West operators (we employed deterministic operators). There is a small cost for each move, and the agent must maximize the undiscounted sum of these costs.  Because the goal state can be in any of 100 diﬀerent locations, there are actually 100 diﬀerent MDPs. Kaelbling’s HDG method starts by choosing an arbitrary set of landmark states and deﬁning a Voronoi partition of the state space based on the Manhattan distances to these landmarks (i.e., two states belong to the same Voronoi cell iﬀ they have the same nearest landmark). The method then deﬁnes one subtask for each landmark l. The subtask is to move from any state in the current Voronoi cell or in any neighboring Voronoi cell to the landmark l. Optimal policies for these subtasks are then computed.  Once HDG has the policies for these subtasks, it can solve the abstract Markov Decision Problem of moving from each landmark state to any other landmark state using the subtask solutions as macro actions (subroutines). So it computes a value function for this MDP.  Finally, for each possible destination location g within a Voronoi cell for landmark l, the HDG  method computes the optimal policy of getting from l to g.  By combining these subtasks, the HDG method can construct a good approximation to the optimal policy as follows. In addition to the value functions discussed above, the agent maintains two other functions: N L(s), the name of the landmark nearest to state s, and N (l), a list of the  45  10  9  8  7  6  5  4  3  2  1  1  2  3  4  5  6  7  8  9  10  Figure 9: Kaelbling’s 10-by-10 navigation task. Each circled state is a landmark state, and the heavy lines show the boundaries of the Voronoi cells. In each episode, a start state and a goal state are chosen at random. In this ﬁgure, the start state is shown by the shaded hexagon, and the goal state is shown by the shaded square.  landmarks that are in the cells that are immediate neighbors of cell l. By combining these, the agent can build a list for each state s of the current landmark and the landmarks of the neighboring cells. For each such landmark, the agent computes the sum of three terms:  (t1) the expected cost of reaching that landmark,  (t2) the expected cost of moving from that landmark to the landmark in the goal cell, and  (t3) the expected cost of moving from the goal-cell landmark to the goal state.  Note that while terms (t1) and (t3) can be exact estimates, term (t2) is computed using the landmark subtasks as subroutines. This means that the corresponding path must pass through the intermediate landmark states rather than going directly to the goal landmark. Hence, term (t2) is typically an overestimate of the required distance. (Also note that (t3) is the same for all choices of the intermediate landmarks, so it does not need to be explicitly included in the computation.)  Given this information, the agent then chooses to move toward the best of the landmarks (unless the agent is already in the goal Voronoi cell, in which case the agent moves toward the goal state). For example, in Figure 9, term (t1) is the cost of reaching the landmark in row 7, column 4, which is 4. Term (t2) is the cost of getting from row 7, column 4 to the landmark at row 1 column 4 (by going from one landmark to another). In this case, the best landmark-to-landmark path is from row 7, column 1 to row 5 column 6, and then to row 1 column 4. Hence, term (t2) is 12. Term (t3) is the cost of getting from row 1 column 4 to the goal, which is 2. The sum of these is 4 + 12 + 2 = 18. For comparison, the optimal path has length 10.  In Kaelbling’s experiments, she employed a variation of Q learning to learn terms (t1) and (t3), and she computed (t2) at regular intervals via the Floyd-Warshall all-sources shortest paths algorithm.  46  MaxRoot(g)  gl/NL(g)  QGotoGoalLmk(gl)  QGotoGoal(g)  MaxGotoGoalLmk(gl)  QGotoLmk(l,gl)  MaxGotoLmk(l)  MaxGotoGoal(g)  QNorthLmk(l)  QSouthLmk(l)  QEastLmk(l)  QWestLmk(l)  QNorthG(g)  QSouthG(g)  QEastG(g)  QWestG(g)  North  South  East  West  Figure 10: A MAXQ graph for the HDG navigation task.  Figure 10 shows a MAXQ approach to solving this problem. The overall task Root, takes one  argument g, which speciﬁes the goal cell. There are three subtasks:  • GotoGoalLmk, go to the landmark nearest to the goal location. The termination for the predicate is true if the agent reaches the landmark nearest to the goal. The goal predicate is the same as the termination predicate.  • GotoLmk(l), go to landmark l. The termination predicate for this is true if either (a) the agent reaches landmark l or (b) the agent is outside of the region deﬁned by the Voronoi cell for l and the neighboring Voronoi cells, N (l). The goal predicate for this subtask is true only for condition (a).  • GotoGoal(g), go to the goal location g. The termination predicate for this subtask is true if either the agent is in the goal location or the agent is outside of the Voronoi cell N L(g) that contains g. The goal predicate for this subtask is true if the agent is in the goal location.  The MAXQ decomposition is essentially the same as Kaelbling’s method, but somewhat redun-dant. Consider a state where the agent is not inside the same Voronoi cell as the goal g. In such  47  states, HDG decomposes the value function into three terms (t1), (t2), and (t3). Similarly, MAXQ also decomposes it into these same three terms:  • V (GotoLmk(l), s, a) the cost of getting to landmark l.  (Actually the sum of V (a, s) and  C(GotoLmk(l), s, a).)  mark gl nearest the goal.  • C(GotoGoalLmk(gl), s, M axGotoLmk(l)) the cost of getting from landmark l to the land• C(Root, s, GotoGoalLmk(gl)) the cost of getting to the goal location after reaching gl.  When the agent is inside the goal Voronoi cell, then again HDG and MAXQ store essentially the same information. HDG stores Q(GotoGoal(g), s, a), while MAXQ breaks this into two terms: C(GotoGoal(g), s, a) and V (a, s) and then sums these two quantities to compute the Q value.  Note that this MAXQ decomposition stores some information twice—speciﬁcally, the cost of getting from the goal landmark gl to the goal is stored both as C(Root, s, GotoGoalLmk(gl)) and as C(GotoGoal(g), s, a) + V (a, s).  Let us compare the amount of memory required by ﬂat Q learning, HDG, and MAXQ. There are 100 locations, 4 possible actions, and 100 possible goal states, so ﬂat Q learning must store 40,000 values.  To compute quantity (t1), HDG must store 4 Q values (for the four actions) for each state s with respect to its own landmark and the landmarks in N (N L(s)). This gives a total of 2,028 values that must be stored.  To compute quantity (t2), HDG must store, for each landmark, information on the shortest path to every other landmark. There are 12 landmarks. Consider the landmark at row 6, column 1. It has 5 neighboring landmarks which constitute the ﬁve macro actions that the agent can perform to move to another landmark. The nearest landmark to the goal cell could be any of the other 11 landmarks, so this gives a total of 55 Q values that must be stored. Similar computations for all 12 landmarks give a total of 506 values that must be stored.  Finally, to compute quantity (t3), HDG must store information, for each square inside each Voronoi cell, about how to get to each of the other squares inside the same Voronoi cell. This requires 3,536 values.  Hence, the grand total for HDG is 6,070, which is a huge savings over ﬂat Q learning. Now let’s consider the MAXQ hierarchy with and without state abstractions.  • V (a, s): This is the expected reward of each primitive action in each state. There are 100 states and 4 primitive actions, so this requires 400 values. However, because the reward is constant (−1), we can apply Leaf Irrelevance to store only a single value.  • C(GotoLmk(l), s, a), where a is one of the four primitive actions. This requires the same amount of space as (t1) in Kaelbling’s representation—indeed, combined with V (a, a), this represents exactly the same information as (t1). It requires 2,028 values. No state abstractions can be applied.  • C(GotoGoalLmk(gl), s, GotoLmk(l)): This is the cost of completing the GotoGoalLmk task after going to landmark l. If the primitive actions are deterministic, then GotoLmk(l) will always terminate at location l, and hence, we only need to store this for each pair of l and gl. This is exactly the same as Kaelbling’s quantity (t2), which requires 506 values. However, if the primitive actions are stochastic—as they were in Kaelbling’s original paper—then we must store this value for each possible terminal state of each GotoLmk action. Each of these actions  48  Table 6: Comparison of the number of values that must be stored to represent the value function using the HDG and MAXQ methods.  HDG MAXQ item item  (t1) (t2) (t3)  V (a, s) C(GotoLmk(l), s, a) C(GotoGoalLmk, s, GotoLmk(l)) C(GotoGoal(g), s, a) C(Root, s, GotoGoalLmk) C(Root, s, GotoGoal)  Total Number of Values Required  MAXQ  HDG MAXQ MAXQ no abs values 400 0 2,028 2,028 6,600 506 3,536 3,536 100 0 96 0 12,760 6,070  safe abs unsafe abs 1 2,028 506 3,536 100 0 6,171  1 2,028 6,600 3,536 100 96 12,361  could terminate at its target landmark l or in one of the states bordering the set of Voronoi cells that are the neighbors of the cell for l. This requires 6,600 values. When Kaelbling stores values only for (t2), she is eﬀectively making the assumption that GotoLmk(l) will never fail to reach landmark l. This is an approximation which we can introduce into the MAXQ representation by our choice of state abstraction at this node.  • C(GotoGoal, s, a): This is the cost of completing the GotoGoal task after making one of the primitive actions a. This is the same as quantity (t3) in the HDG representation, and it requires the same amoount of space: 3,536 values.  • C(Root, s, GotoGoalLmk): This is the cost of reaching the goal once we have reached the landmark nearest the goal. MAXQ must represent this for all combinations of goal landmarks and goals. This requires 100 values. Note that these values are the same as the values of C(GotoGoal(g), s, a) + V (a, s) for each of the primitive actions. This means that the MAXQ representation stores this information twice, whereas the HDG representation only stores it once (as term (t3)).  • C(Root, s, GotoGoal). This is the cost of completing the Root task after we have executed the GotoGoal task. If the primitive action are deterministic, this is always zero, because GotoGoal will have reached the goal. Hence, we can apply the Termination condition and not store any values at all. However, if the primitive actions are stochastic, then we must store this value for each possible state that borders the Voronoi cell that contains the goal. This requires 96 diﬀerent values. Again, in Kaelbling’s HDG representation of the value function, she is ignoring the probability that GotoGoal will terminate in a non-goal state. Because MAXQ is an exact representation of the value function, it does not ignore this possibility. If we (incorrectly) apply the Termination condition in this case, the MAXQ representation becomes a function approximation.  In the stochastic case, without state abstractions, the MAXQ representation requires 12,760 values. With safe state abstractions, it requires 12,361 values. With the approximations employed by Kaelbling (or equivalently, if the primitive actions are deterministic), the MAXQ representation with state abstractions requires 6,171 values. These numbers are summarized in Table 6. We can see that, with the unsafe state abstractions, the MAXQ representation requires only slightly more space than the HDG representation (because of the redundancy in storing C(Root, s, GotoGoalLmk).  49  This example shows that for the HDG task, we can start with the fully-general formulation provided by MAXQ and impose assumptions to obtain a method that is similar to HDG. The MAXQ formulation guarantees that the value function of the hierarchical policy will be represented exactly. The assumptions will introduce approximations into the value function representation. This might be useful as a general design methodology for building application-speciﬁc hierarchical representations. Our long-term goal is to develop such methods so that each new application does not require inventing a new set of techniques. Instead, oﬀ-the-shelf tools (e.g., based on MAXQ) could be specialized by imposing assumptions and state abstractions to produce more eﬃcient special-purpose systems.  One of the most important contributions of the HDG method was that it introduced a form of non-hierarchical execution. As soon as the agent crosses from one Voronoi cell into another, the current subtask is “interrupted”, and the agent recomputes the “current target landmark”. The eﬀect of this is that (until it reaches the goal Voronoi cell), the agent is always aiming for a landmark outside of its current Voronoi cell. Hence, although the agent “aims for” a sequence of landmark states, it typically does not visit many of these states on its way to the goal. The states just provide a convenient set of intermediate targets. By taking these “shortcuts”, HDG compensates for the fact that, in general, it has overestimated the cost of getting to the goal, because its computed value function is based on a policy where the agent goes from one landmark to another.  The same eﬀect is obtained by hierarchical greedy execution of the MAXQ graph (which was directly inspired by the HDG method). Note that by storing the N L (nearest landmark) function, Kaelbing’s HDG method can detect very eﬃciently when the current subtask should be interrupted. This technique only works for navigation problems in a space with a distance metric. In contrast, ExecuteHGPolicy performs a kind of “polling”, where it checks after each primitive action whether it should interrupt the current subroutine and invoke a new one. An important goal for future research on MAXQ is to ﬁnd a general purpose mechanism for avoiding unnecessary “polling”— that is, a mechanism that can discover eﬃciently-evaluable interrupt conditions.  Figure 11 shows the results of our experiments with HDG using the MAXQ-Q learning al-gorithm. We employed the following parameters: for Flat Q learning, initial values of 0.123, a learning rate of 1.0, initial temperature of 50, and cooling rate of .9074; for MAXQ-Q without initial values of −25.123, learning rate of 1.0, initial temperature of 50, and state abstractions: cooling rates of .9074 for MaxRoot, .9999 for MaxGotoGoalLmk, .9074 for MaxGotoGoal, and .9526 for MaxGotoLmk; for MAXQ-Q with state abstractions: initial values of −20.123, learning rate of 1.0, initial temperature of 50, and cooling rates of .9760 for MaxRoot, .9969 for MaxGotoGoal, .9984 for MaxGotoGoalLmk, and .9969 for MaxGotoLmk. Hierarchical greedy execution was introduced by starting with 3000 primitive actions per trial, and reducing this every trial by 2 actions, so that after 1500 trials, execution is completely greedy.  The ﬁgure conﬁrms the observations made in our experiments with the Fickle Taxi task. With-out state abstractions, MAXQ-Q converges much more slowly than ﬂat Q learning. With state abstractions, it converges roughly three times as fast. Figure 12 shows a close-up view of Figure 11 that allows us to compare the diﬀerences in the ﬁnal levels of performance of the methods. Here, we can see that MAXQ-Q with no state abstractions was not able to reach the quality of our hand-coded hierarchical policy—presumably even more exploration would be required to achieve this, whereas with state abstractions, MAXQ-Q is able to do slightly better than our hand-coded policy. With hierarchical greedy execution, MAXQ-Q is able to reach the goal using one fewer action, on the average—so that it approaches the performance of the best hierarchical greedy policy (as computed by value iteration). Notice however, that the best performance that can be obtained by hierarchical greedy execution of the best recursively-optimal policy cannot match optimal perfor-mance. Hence, Flat Q learning achieves a policy that reaches the goal state, on the average, with  50  MAXQ  Flat Q  MAXQ No Abstractions  0  -20  -40  -60  -80  -100  -120  -140  d r a w e R   e v i t a l u m u C   n a e  M  0  200000  400000  1e+06  1.2e+06  1.4e+06  600000 800000 Primitive Actions  Figure 11: Comparison of Flat Q learning with MAXQ-Q learning with and without state abstrac-tion. (Average of 100 runs.)  about one fewer primitive action. Finally notice that as in the taxi domain, there was no added exploration cost for shifting to greedy execution.  7.3 Parr and Russell: Hierarchies of Abstract Machines  In his (1998b) dissertation work, Ron Parr considered an approach to hierarchical reinforcement learning in which the programmer encodes prior knowledge in the form of a hierarchy of ﬁnite-state controllers called a HAM (Hierarchy of Abstract Machines). The hierarchy is executed using a procedure-call-and-return discipline, and it provides a partial policy for the task. The policy is partial because each machine can include non-deterministic, “choice” machine states, in which the machine lists several options for action but does not specify which one should be chosen. The programmer puts “choice” states at any point where he/she does not know what action should be performed. Given this partial policy, Parr’s goal is to ﬁnd the best policy for making choices in the choice states. In other words, his goal is to learn a hierarchical value function V (hs, mi), where s is a state (of the external environment) and m contains all of the internal state of the hierarchy (i.e., the contents of the procedure call stack and the values of the current machine states for all machines appearing in the stack). A key observation is that it is only necessary to learn this value function at choice states hs, mi. Parr’s algorithm does not learn a decomposition of the value function. Instead, it “ﬂattens” the hierarchy to create a new Markov decision problem over the choice states hs, mi. Hence, it is hierarchical primarily in the sense that the programmer structures the prior knowledge hierarchically. An advantage of this is that Parr’s method can ﬁnd the optimal hierarchical policy subject to constraints provided by the programmer. A disadvantage is that the method cannot be executed “non-hierarchically” to produce a better policy.  51  MAXQ Greedy  MAXQ  Optimal Policy  Hierarchical Greedy Optimal Policy  Hierarchical Hand-coded Policy  Flat Q  MAXQ No Abstractions  d r a w e R   e v i t a l u m u C   n a e  M  -6  -8  -10  -12  -14  0  200000  400000  1e+06  1.2e+06  1.4e+06  600000 800000 Primitive Actions  Figure 12: Expanded view comparing Flat Q learning with MAXQ-Q learning with and without state abstraction and with and without hierarchical greedy execution. (Average of 100 runs.)  Parr illustrated his work using the maze shown in Figure 13. This maze has a high-level structure (i.e., as a series of hallways and intersections), and a low-level structure (a series of obstacles that must be avoided in order to move through the hallways and intersections). In each trial, the agent starts in the top left corner, and it must move to any state in the bottom right corner room. The agent has the usual four primitive actions, North, South, East, and West. The actions are stochastic: with probability 0.8, they succeed, but with probability 0.1 the action will move to the “left” and with probability 0.1 the action will move to the “right” instead (e.g., a North action will move east with probability 0.1 and west with probability 0.1). If an action would collide with a wall or an obstacle, it has no eﬀect.  The maze is structured as a series of “rooms”, each containing a 12-by-12 block of states (and various obstacles). Some rooms are parts of “hallways”, because they contain walls on two opposite sides, and they are open on the other two sides. Other rooms are “intersections”, where two or more hallways meet.  To test the representational power of the MAXQ hierarchy, we want to see how well it can represent the prior knowledge that Parr is able to represent using the HAM. We begin by describing Parr’s HAM for his maze task, and then we will present a MAXQ hierarchy that captures much of the same prior knowledge.2  Parr’s top level machine, MRoot, consists of a loop with a single choice state that chooses among four possible child machines: MGo(East), MGo(South), MGo(W est), and MGo(N orth). The loop terminates when the agent reaches a goal state. MRoot will only invoke a particular machine if there is a hallway in the speciﬁed direction. Hence, in the start state, it will only consider MGo(South)  2The author thanks Ron Parr for providing the details of the HAM for this task.  52  and MGo(East).  The MGo(d) machine begins executing when the agent is in an intersection. So the ﬁrst thing it tries to do is to exit the intersection into a hallway in speciﬁed direction d. Then it attempts to traverse the hallway until it reaches another intersection. It does this by ﬁrst invoking a ExitIntersection(d) machine. When that machine returns, it then invokes a MExitHallway(d) ma-chine. When that machine returns, MGo also returns.  The MExitIntersection and MExitHallway machines are identical except for their termination con-ditions. Both machines consist of a loop with one choice state that chooses among four possible sub-routines. To simplify their description, suppose that MGo(East) has chosen MExitIntersection(East). Then the four possible subroutines are MSniﬀ(East, N orth), MSniﬀ(East, South), MBack(East, N orth), and MBack(East, South).  The MSniﬀ(d, p) machine always moves in direction d until it encounters a wall (either part of an obstacle or part of the walls of the maze). Then it moves in perpendicular direction p until it reaches the end of the wall. A wall can “end” in two ways: either the agent is now trapped in a corner with walls in both directions d and p or else there is no longer a wall in direction d. In the ﬁrst case, the MSniﬀ machine terminates; in the second case, it resumes moving in direction d.  The MBack(d, p) machine moves one step backwards (in the direction opposite from d) and then moves ﬁve steps in direction p. These moves may or may not succeed, because the actions are stochastic and there may be walls blocking the way. But the actions are carried out in any case, and then the MBack machine returns.  The MSniﬀ and MBack machines also terminate if they reach the end of a hall or the end of an  intersection.  These ﬁnite-state controllers deﬁne a highly constrained partial policy. The MBack, MSniﬀ, and MGo machines contain no choice states at all. The only choice points are in MRoot, which must choose the direction in which to move, and in MExitIntersection and MExitHall, which must decide when to call MSniﬀ, when to call MBack, and which “perpendicular” direction to tell these machines to try when they cannot move forward.  Figure 14 shows a MAXQ graph that encodes a similar set of constraints on the policy. The  subtasks are deﬁned as follows:  • Root. This is exactly the same as the MRoot machine. It must choose a direction d and invoke Go. It terminates when the agent enters a terminal state. This is also its goal condition (of course).  • Go(d, r). The parameter r is bound to the current 12-by-12 “room” in which the agent is located. Go terminates when the agent enters the room at the end of the hallway in direction d or when it leaves the desired hallway (e.g., in the wrong direction). The goal condition for Go is satisﬁed only if the agent reaches the desired intersection.  • ExitInter(d, r). This terminates when the agent has exited room r. The goal condition is that  the agent exit room r in direction d.  • ExitHall(d, r). This terminates when the agent has exited the current hall (into some intersec-tion). The goal condition is that the agent has entered the desired intersection in direction d.  • Sniﬀ(d, r). This encodes a subtask that is equivalent to the MSniﬀ machine. However, Sniﬀ must have two child subtasks, ToWall and FollowWall that were simply internal states of MSniﬀ. This is necessary, because a subtask in the MAXQ framework cannot contain any  53  internal state, whereas a ﬁnite-state controller in the HAM representation can contain as many internal states as necessary. In particular, it can have one state for when it is moving forward and another state for when it is following a wall sideways.  • ToWall(d). This is equivalent to part of MSniﬀ, and it terminates when there is a wall in “front” of the agent in direction d. The goal condition is the same as the termination condition.  • FollowWall(d, p). This is equivalent to the other part of MSniﬀ. It moves in direction p until the wall in direction d ends (or until it is stuck in a corner with walls in both directions d and p). The goal condition is the same as the termination condition.  • Back(d, p, x, y). This attempts to encode the same information as the MBack machine, but this is a case where the MAXQ hierarchy cannot capture the same information. MBack simply executes a sequence of 6 primitive actions (one step back, ﬁve stes in direction p). But to do this, MBack must have 6 internal states, which MAXQ does not allow. Instead, the Back subtask is has the subgoal of moving the agent at least one square backwards and at least 3 squares in the direction p. In order to determine whether it has achieved this subgoal, it must remember the x and y position where it started to execute, so these are bound as parameters to Back. Back terminates if it achieves this subgoal or if it runs into walls that prevent it from achieving the subgoal. The goal condition is the same as the termination condition.  • BackOne(d, x, y). This moves the agent one step backwards (in the direction opposite to d. It needs the starting x and y position in order to tell when it has succeeded. It terminates if it has moved at least one unit in direction d or if there is a wall in this direction. Its goal condition is the same as its termination condition.  • PerpThree(p, x, y). This moves the agent three steps in the direction p. It needs the starting x and y positions in order to tell when it has succeeded. It terminates when it has moved at least three units in the direction p or if there is a wall in that direction. The goal condition is the same as the termination condition.  • Move(d). This is a “parameterized primitive” action.  It executes one primitive move in  direction d and terminates immediately.  From this, we can see that there are three major diﬀerences between the MAXQ representation and the HAM representation. First, a HAM ﬁnite-state controller can contain internal states. To convert them into a MAXQ subtask graph, we must make a separate subtask for each internal state in the HAM. Second, a HAM can terminate based on an “amount of eﬀort” (e.g., performing 5 actions), whereas a MAXQ subtask must terminate based on some change in the state of the world. It is impossible to deﬁne a MAXQ subtask that performs k steps and then terminate regardless of the eﬀects of those steps (i.e., without adding some kind of “counter” to the state of the MDP). Third, it is more diﬃcult to formulate the termination conditions for MAXQ subtasks than for HAM machines. For example, in the HAM, it was not necessary to specify that the MExitHallway machine terminates when it has entered a diﬀerent intersection than the one where the MGo was executed. However, this is important for the MAXQ method, because in MAXQ, each subtask learns its own value function and policy—independent of its parent tasks. For example, without the requirement to enter a diﬀerent intersection, the learning algorithms for MAXQ will always prefer to have MaxExitHall take one step backward and return to the room in which the Go action was started (because that is a much easier terminated state to reach). This problem does not arise  54  in the HAM approach, because the policy learned for a subtask depends on the whole “ﬂattened” hierarchy of machines, and returning to the state where the Go action was started does not help solve the overall problem of reaching the goal state in the lower right corner.  To construct the MAXQ graph for this problem, we have introduced three programming tricks: (a) binding parameters to aspects of the current state (in order to serve as a kind of “local memory” for where the subtask began executing), (b) having a parameterized primitive action (in order to be able to pass a parameter value that speciﬁes which primitive action to perform), and (c) employing “inheritance of termination conditions”—that is, each subtask in this MAXQ graph (but not the others in this paper) inherits the termination conditions of all of its ancestor tasks. Hence, if the agent is in the middle of executing a ToWall action when it leaves an intersection, the ToWall subroutine terminates because the ExitInter subroutine has terminated. If this satisﬁes the goal condition of ExitInter, then it is also considered to satisfy the goal condition of ToWall. This inheritance made it easier to write the MAXQ graph, because the parents did not need to pass down to their children all of the information necessary to deﬁne the complete termination and goal predicates.  There are essentially no opportunities for state abstraction in this task, because there are no ir-relevant features of the state. There are some opportunities to apply the Shielding and Termination properties, however. In particular, ExitHall(d) is guaranteed to cause its parent task, MaxGo(d) to terminate, so it does not require any stored C values. There are many states where some subtasks are terminated (e.g., Go(East) in any state where there is a wall on the east side of the room), and so no C values need to be stored.  Nonetheless, even after applying the state elimination conditions, the MAXQ representation for this task requires much more space than a ﬂat representation. An exact computation is diﬃcult, but after applying MAXQ-Q learning, the MAXQ representation required 52,043 values, whereas ﬂat Q learning requires fewer than 16,704 values. Parr states that his method requires only 4,300 values.  To test the relative eﬀectiveness of the MAXQ representation, we compare MAXQ-Q learning with ﬂat Q learning. Because of the very large negative values that some states acquire (particularly during the early phases of learning), we were unable to get Boltzmann exploration to work well— one very bad trial would cause an action to receive such a low Q value, that it would never be tried again. Hence, we experimented with both ǫ-greedy exploration and counter-based exploration. The ǫ-greedy exploration policy is an ordered, abstract GLIE policy in which a random action is chosen with probability ǫ, and ǫ is gradually decreased over time. The counter-based exploration policy keeps track of how many times each action a has been executed in each state s. To choose an action in state s, it selects the action that has been executed the fewest times until all actions have been executed T times. Then it switches to greedy execution. Hence, it is not a genuine GLIE policy. Parr employed counter-based exploration policies in his experiments with this task. learning rate 0.50, initial value for ǫ of For Flat Q learning, we chose the following parameters: 1.0, ǫ decreased by 0.001 after each successful execution of a Max node, and initial Q values of −200.123. For MAXQ-Q learning, we chose the following parameters: counter-based exploration with T = 10, learning rate equal to the reciprocal of the number of times an action had been performed, and initial values for the C values selected carefully to provide underestimates of the true C values. For example, the initial values for QExitInter were −40.123, because in the worst case, after completing an ExitInter task, it takes about 40 steps to complete the subsequent ExitHall task and hence, complete the Go parent task.  Figure 15 plots the results. We can see that MAXQ-Q learning converges about 10 times faster than Flat Q learning. We do not know whether MAXQ-Q has converged to a recursively optimal policy. For comparison, we also show the performance of a hierarchical policy that we coded  55  by hand, but in our hand-coded policy, we used knowledge of contextual information to choose operators, so this policy is surely better than the best recursively optimal policy. HAMQ learning should converge to a policy equal to or slightly better than our hand-coded policy.  This experiment demonstrates that the MAXQ representation can capture most—but not all— of the prior knowledge that can be represented by the HAMQ hierarchy. It also shows that the MAXQ representation requires much more care in the design of the goal conditions for the subtasks.  7.4 Other Domains  In addition to the three domains discussed above, we have developed MAXQ graphs for Singh’s (1992b) “ﬂag task”, the treasure hunter task described by Tadepalli and Dietterich (Tadepalli & Dietterich, 1997), and Dayan and Hinton’s (1993) Fuedal-Q learning task. All of these tasks can be easily and naturally placed into the MAXQ framework—indeed, all of them ﬁt more easily than the Parr and Russell maze task.  MAXQ is able to exactly duplicate Singh’s work and his decomposition of the value function— while using exactly the same amount of space to represent the value function. MAXQ can also duplicate the results from Tadepalli and Dietterich—however, because MAXQ is not an explanation-based method, it is considerably slower and requires substantially more space to represent the value function.  In the Feudal-Q task, MAXQ is able to give better performance than Feudal-Q learning. The reason is that in Feudal-Q learning, each subroutine makes decisions using only a Q function learned at that level—that is, without information about the estimated costs of the actions of its In contrast, the MAXQ value function decomposition permits each Max node to descendants. make decisions based on the sum of its completion function, C(i, s, j), and the costs estimated by its descendants, V (j, s). Of course, MAXQ also supports non-hierarchical execution, which is not possible for Feudal-Q, because it does not learn a value function decomposition.  8 Discussion: Design Tradeoﬀs in Hierarchical Reinforcement  Learning  At the start of this paper, we discussed four issues concerning the design of hierarchical reinforce-ment learning architectures. In this section, we want to highlight a tradeoﬀ between two of those issues: the method for deﬁning subtasks and the use of state abstraction.  MAXQ deﬁnes subtasks using a termination predicate Ti and a pseudo-reward function ˜R. There are at least two drawbacks of this method. First, it can be hard for the programmer to deﬁne Ti and ˜R correctly, since this essentially requires guessing the value function of the optimal policy for the MDP at all states where the subtask terminates. Second, it leads us to seek a recursively optimal policy rather than a hierarchically optimal policy. Recursively optimal policies may be much worse than hierarchically optimal ones, so we may be giving up substantial performance.  However, in return for these two drawbacks, MAXQ obtains a very important beneﬁt: the policies and value functions for subtasks become context-free. In other words, they do not depend on their parent tasks or the larger context in which they are invoked. To understand this point, consider again the MDP shown in Figure 6. It is clear that the optimal policy for exiting the left-hand room (the Exit subtask) depends on the location of the goal. If it is at the top of the right-hand room, then the agent should prefer to exit via the upper door, whereas if it is at the bottom of the right-hand room, the agent should prefer to exit by the lower door. However, if we deﬁne the subtask of exiting the left-hand room using a pseudo-reward of zero for both doors,  56  then we obtain a policy that is not optimal in either case, but a policy that we can re-use in both cases. Furthermore, this policy does not depend on the location of the goal. Hence, we can apply Max node irrelevance to solve the Exit subtask using only the location of the robot and ignore the location of the goal.  This example shows that we obtain the beneﬁts of subtask reuse and state abstraction because we deﬁne the subtask using a termination predicate and a pseudo-reward function. The termination predicate and pseudo-reward function provide a barrier that prevents “communication” of value information between the Exit subtask and its context.  Compare this to Parr’s HAM method. The HAMQ algorithm ﬁnds the best policy consistent with the hierarchy. To achieve this, it must permit information to propagate “into” the Exit subtask (i.e., the Exit ﬁnite-state controller) from its environment. But this means that if any state that is reached after leaving the Exit subtask has diﬀerent values depending on the location of the goal, then these diﬀerent values will propagate back into the Exit subtask. To represent these diﬀerent values, the Exit subtask must know the location of the goal. In short, to achieve a hierarchically optimal policy within the Exit subtask, we must (in general) represent its value function using the entire state space.  We can see, therefore, that there is a direct tradeoﬀ between achieving hierarchical optimality and achieving recursive optimality. Methods for hierarchical optimality have more freedom in deﬁning subtasks (e.g., using complete policies, as in the option approach, or using partial policies, as in the HAM approach). But they cannot employ state abstractions within subtasks, and in general, they cannot reuse the solution of one subtask in multiple contexts. Methods for recursive optimality, on the other hand, must deﬁne subtasks using some method (such as pseudo-reward functions) that isolates the subtask from its context. But in return, they can apply state abstraction and the learned policy can be reused in many contexts (where it will be more or less optimal).  It is interesting that the iterative method described by Dean and Lin (1995) can be viewed as a method for moving along this tradeoﬀ. In the Dean and Lin method, the programmer makes an initial guess for the values of the terminal states of each subtask (i.e., the doorways in Figure 6). Based on this initial guess, the locally optimal policies for the subtasks are computed. Then the locally optimal policy for the parent task is computed—while holding the subtask policies ﬁxed (i.e., treating them as options). At this point, their algorithm has computed the recursively optimal solution to the original problem, given the initial guesses. Instead of solving the various subproblems sequentially via an oﬄine algorithm, we could use the MAXQ-Q learning algorithm. But the method of Dean and Lin does not stop here. Instead, it computes new values of the terminal states of each subtask based on the learned value function for the entire problem. This allows it to update its “guesses” for the values of the terminal states. The entire solution process can now be repeated. To obtain a new recursively optimal solution, based on the new guesses. They prove that if this process is iterated indeﬁnitely, it will converge to the recursively optimal policy (provided, of course, that no state abstractions are used within the subtasks).  This suggests an extension to MAXQ-Q learning that adapts the ˜R values online. Each time a subtask terminates, we could update the ˜R function based on the computed value of the terminated state. To be precise, if j is a subtask of i, then when j terminates in state s′, we should update ˜R(j, s′) to be equal to ˜V (i, s′) = maxa′ ˜Q(i, s′, a′). However, this will only work if ˜R(j, s′) is represented using the full state s′. If subtask j is employing state abstractions, x = χ(s), then ˜R(j, x′) will need to be the average value of ˜V (i, s′), where the average is taken over all states s′ such that x′ = χ(s′) (weighted by the probability of visiting those states). This is easily accomplished by performing a stochastic approximation update of the form  ′ ˜R(j, x  ′ ) = (1 − αt) ˜R(j, x  ) + αt ˜V (i, s  ′  )  57  each time subtask j terminates. Such an algorithm could be expected to converge to the best hierarchical policy consistent with the given state abstractions.  This also suggests that in some problems, it may be worthwhile to ﬁrst learn a recursively optimal policy using very aggressive state abstractions and then use the learned value function to initialize a MAXQ representation with a more detailed representation of the states. These progressive reﬁnements of the state space could be guided by monitoring the degree to which the values of ˜V (i, s′) vary for a single abstract state x′. If they have a large variance, this means that the state abstractions are failing to make important distinctions in the values of the states, and they should be reﬁned.  Both of these kinds of adaptive algorithms will take longer to converge than the basic MAXQ method described in this paper. But for tasks that an agent must solve many times in its lifetime, it is worthwhile to have learning algorithms that provide an initial useful solution but gradually improve that solution until it is optimal. An important goal for future research is to ﬁnd methods for diagnosing and repairing errors (or sub-optimalities) in the initial hierarchy so that ultimately the optimal policy is discovered.  9 Concluding Remarks  This paper has introduced a new representation for the value function in hierarchical reinforcement learning—the MAXQ value function decomposition. We have proved that the MAXQ decompo-sition can represent the value function of any hierarchical policy under both the ﬁnite-horizon undiscounted, cumulative reward criterion and the inﬁnite-horizon discounted reward criterion. This representation supports subtask sharing and re-use, because the overall value function is de-composed into value functions for individual subtasks.  The paper introduced a learning algorithm, MAXQ-Q learning, and proved that it converges with probability 1 to a recursively optimal policy. The paper argued that although recursive optimality is weaker than either hierarchical optimality or global optimality, it is an important form of optimality because it permits each subtask to learn a locally optimal policy while ignoring the behavior of its ancestors in the MAXQ graph. This increases the opportunities for subtask sharing and state abstraction.  We have shown that the MAXQ decomposition creates opportunities for state abstraction, and we identiﬁed a set of ﬁve properties (Max Node Irrelevance, Leaf Irrelevance, Result Distribution Irrelevance, Shielding, and Termination) that allow us to ignore large parts of the state space within subtasks. We proved that MAXQ-Q still converges in the presence of these forms of state abstraction, and we showed experimentally that state abstraction is important in practice for the successful application of MAXQ-Q learning—at least in the Taxi and Kaelbling HDG tasks.  The paper presented two diﬀerent methods for deriving improved non-hierarchical policies from the MAXQ value function representation, and it has formalized the conditions under which these methods can improve over the hierarchical policy. The paper veriﬁed experimentally that non-hierarchical execution gives improved performance in the Fickle Taxi Task (where it achieves opti-mal performance) and in the HDG task (where it gives a substantial improvement).  Finally, the paper has argued that there is a tradeoﬀ governing the design of hierarchical rein-forcement learning methods. At one end of the design spectrum are “context free” methods such as MAXQ. They provide good support for state abstraction and subtask sharing but they can only learn recursively optimal policies. At the other end of the spectrum are “context-sensitive” meth-ods such as HAMQ, the options framework, and the early work of Dean and Lin. These methods can discover hierarchically optimal policies (or, in some cases, globally optimal policies), but their  58  drawback is that they cannot easily exploit state abstractions or share subtasks. Because of the great speedups that are enabled by state abstraction, this paper has argued that the context-free approach is to be preferred—and that it can be relaxed as needed to obtain improved policies.  Acknowledgements  The author gratefully acknowledges the support of the National Science Foundation under grant number IRI-9626584, the Oﬃce of Naval Research under grant number N00014-95-1-0557, the Air Force Oﬃce of Scientiﬁc Research under grant number F49620-98-1-0375, and the Spanish Council for Scientiﬁc Research. In addition, the author is indebted to many colleagues for helping develop and clarify the ideas in this paper including Valentina Bayer, Leslie Kaelbling, Bill Langford, Wes Pinchot, Rich Sutton, Prasad Tadepalli, and Sebastian Thrun. I particularly want to thank Eric Chown for encouraging me to study Feudal reinforcement learning, Ron Parr for providing the details of his HAM machines, and Sebastian Thrun encouraging me to write a single comprehensive paper. I also thank the anonymous reviewers of previous drafts of this paper for their suggestions and careful reading, which have improved the paper immeasurably.  References  Belmont, MA.  Bellman, R. E. (1957). Dynamic Programming. Princeton University Press.  Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientiﬁc,  Boutilier, C., Shoham, Y., & Wellman, M. (1997). Economic principles of multi-agent systems  (Editorial). Artiﬁcial Intelligence, 94 (1–2), 1–6.  Boutilier, C., Dean, T., & Hanks, S. (1999). Decision theoretic planning: Structural assumptions  and computational leverage. Journal of Artiﬁcial Intelligence Research, To appear.  Currie, K., & Tate, A. (1991). O-plan: The open planning architecture. Artiﬁcial Intelligence,  52 (1), 49–86.  Dayan, P., & Hinton, G. (1993). Feudal reinforcement learning. In Advances in Neural Information  Processing Systems, 5, pp. 271–278. Morgan Kaufmann, San Francisco, CA.  Dean, T., & Lin, S.-H. (1995). Decomposition techniques for planning in stochastic domains. Tech. rep. CS-95-10, Department of Computer Science, Brown University, Providence, Rhode Island.  Dietterich, T. G. (1998). The MAXQ method for hierarchical reinforcement learning. In Fifteenth  International Conference on Machine Learning.  Fikes, R. E., Hart, P. E., & Nilsson, N. J. (1972). Learning and executing generalized robot plans.  Artiﬁcial Intelligence, 3, 251–288.  Hauskrecht, M., Meuleau, N., Boutilier, C., Kaelbling, L., & Dean, T. (1998). Hierarchical solution of Markov decision processes using macro-actions. Tech. rep., Brown University, Department of Computer Science, Providence, RI.  Howard, R. A. (1960). Dynamic Programming and Markov Processes. MIT Press, Cambridge, MA.  59  Jaakkola, T., Jordan, M. I., & Singh, S. P. (1994). On the convergence of stochastic iterative  dynamic programming algorithms. Neural Computation, 6 (6), 1185–1201.  Kaelbling, L. P. (1993). Hierarchical reinforcement learning: Preliminary results. In Proceedings of the Tenth International Conference on Machine Learning, pp. 167–173 San Francisco, CA. Morgan Kaufmann.  Knoblock, C. A. (1990). Learning abstraction hierarchies for problem solving. In Proceedings of the Eighth National Conference on Artiﬁcial Intelligence, pp. 923–928 Boston, MA. AAAI Press.  Korf, R. E. (1985). Macro-operators: A weak method for learning. Artiﬁcial Intelligence, 26 (1),  35–77.  Lin, L.-J. (1993). Reinforcement learning for robots using neural networks. Ph.D. thesis, Carnegie  Mellon University, Department of Computer Science, Pittsburgh, PA.  Parr, R. (1998a). Flexible decomposition algorithms for weakly coupled markov decision problems. In Proceedings of the Fourteenth Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI–98), pp. 422–430 San Francisco, CA. Morgan Kaufmann Publishers.  Parr, R. (1998b). Hierarchical control and learning for Markov decision processes. Ph.D. thesis,  University of California, Berkeley, California.  Parr, R., & Russell, S. (1998). Reinforcement learning with hierarchies of machines. In Advances  in Neural Information Processing Systems, Vol. 10 Cambridge, MA. MIT Press.  Pearl, J. (1988). Probabilistic Inference in Intelligent Systems. Networks of Plausible Inference.  Morgan Kaufmann, San Mateo, CA.  Rummery, G. A., & Niranjan, M. (1994). Online Qlearning using connectionist systems. Tech. rep. CUED/FINFENG/TR 166, Cambridge University Engineering Department, Cambridge, England.  Sacerdoti, E. D. (1974). Planning in a hierarchy of abstraction spaces. Artiﬁcial Intelligence, 5 (2),  115–135.  Singh, S., Jaakkola, T., Littman, M. L., & Szepesv´ari, C. (1998). Convergence results for single-step on-policy reinforcement-learning algorithms. Tech. rep., University of Colorado, Department of Computer Science, Boulder, CO.  Singh, S. P. (1992a). Transfer of learning by composing solutions of elemental sequential tasks.  Singh, S. P. (1992b). Transfer of learning by composing solutions of elemental sequential tasks.  Machine Learning, 8, 323–339.  Machine Learning, 8, 323.  Sutton, R. S., Singh, S., Precup, D., & Ravindran, B. (1999). Improved switching among temporally abstract actions. In Advances in Neural Information Processing Systems, Vol. 11. MIT Press.  Sutton, R., & Barto, A. G. (1998). Introduction to Reinforcement Learning. MIT Press, Cambridge,  MA.  60  Sutton, R. S., Precup, D., & Singh, S. (1998). Between MDPs and Semi-MDPs: Learning, plan-ning, and representing knowledge at multiple temporal scales. Tech. rep., University of Mas-sachusetts, Department of Computer and Information Sciences, Amherst, MA.  Tadepalli, P., & Dietterich, T. G. (1997). Hierarchical explanation-based reinforcement learning. In Proceedings of the Fourteenth International Conference on Machine Learning, pp. 358–366 San Francisco, CA. Morgan Kaufmann.  Watkins, C. J. C. H. (1989). Learning from Delayed Rewards. Ph.D. thesis, King’s College, Oxford.  (To be reprinted by MIT Press.).  Watkins, C. J., & Dayan, P. (1992). Technical note q-learning. Machine Learning, 8, 279.  61  Goal  Figure 13: Parr’s maze problem. The start state is in the upper left corner, and all states in the lower right-hand room are terminal states.  62  QExitInter(d,r)  QExitHall(d,r)  MaxExitInter(d,r)  MaxExitHall(d,r)  QSniffEI(d,p)  QBackEI(d,p)  QSniffEH(d,p)  QBackEH(d,p)  MaxSniff(d,p)  MaxBack(d,p,x,y)  x/X  y/Y  x/X  y/Y  QFollowWall(d,p)  QToWall(d)  QBackOne(d)  QPerpThree(p)  MaxFollowWall(d,p)  MaxToWall(d)  MaxBackOne(d)  MaxPerpThree(p)  d/p  d/d  d/Inv(d)  d/p  QMoveFW(d)  QMoveTW(d)  QMoveBO(d)  QMoveP3(d)  Figure 14: MAXQ graph for Parr’s maze task.  MaxRoot  Go(d)  r/ROOM  MaxGo(d,r)  MaxMove(d)  63  Hand-coded hierarchical policy  -350  MAXQ Q Learning  Flat Q Learning  l a i r T   r e P   d r a w e R   n a e  M  -100  -150  -200  -250  -300  -400  -450  -500  0  1e+06  2e+06  4e+06  5e+06  6e+06  3e+06 Primitive Steps  Figure 15: Comparison of Flat Q learning and MAXQ-Q learning in the Parr maze task.  64  '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Calculate Freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens= nltk.sent_tokenize(cleanedText)\n",
    "word_tokens = nltk.word_tokenize(cleanedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate frequency\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wordFreqs = {}\n",
    "\n",
    "for word in word_tokens:\n",
    "    if word not in stopwords:\n",
    "        if word not in wordFreqs.keys():\n",
    "            wordFreqs[word] = 1\n",
    "        else:\n",
    "            wordFreqs[word] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': 114,\n",
       " 'paper': 37,\n",
       " 'presents': 5,\n",
       " 'new': 16,\n",
       " 'approach': 25,\n",
       " 'hierarchical': 117,\n",
       " 'reinforcement': 30,\n",
       " 'learning': 160,\n",
       " 'based': 23,\n",
       " 'decomposing': 2,\n",
       " 'target': 11,\n",
       " 'Markov': 12,\n",
       " 'decision': 8,\n",
       " 'process': 9,\n",
       " '(': 1338,\n",
       " 'MDP': 50,\n",
       " ')': 1335,\n",
       " 'hierarchy': 58,\n",
       " 'smaller': 2,\n",
       " 'MDPs': 6,\n",
       " 'value': 183,\n",
       " 'function': 203,\n",
       " 'additive': 1,\n",
       " 'combination': 2,\n",
       " 'functions': 45,\n",
       " '.': 1496,\n",
       " 'The': 230,\n",
       " 'decomposition': 43,\n",
       " ',': 2563,\n",
       " 'known': 5,\n",
       " 'MAXQ': 214,\n",
       " 'procedural': 2,\n",
       " 'semantics—as': 2,\n",
       " 'subroutine': 25,\n",
       " 'hierarchy—and': 1,\n",
       " 'declarative': 2,\n",
       " 'representation': 49,\n",
       " 'policy': 334,\n",
       " 'uniﬁes': 1,\n",
       " 'extends': 1,\n",
       " 'previous': 8,\n",
       " 'work': 17,\n",
       " 'hierar-chical': 2,\n",
       " 'Singh': 21,\n",
       " 'Kaelbling': 24,\n",
       " 'Dayan': 8,\n",
       " 'Hinton': 6,\n",
       " 'It': 45,\n",
       " 'assumption': 6,\n",
       " 'programmer': 24,\n",
       " 'identify': 4,\n",
       " 'useful': 7,\n",
       " 'subgoals': 3,\n",
       " 'deﬁne': 23,\n",
       " 'subtasks': 58,\n",
       " 'achieve': 12,\n",
       " 'By': 13,\n",
       " 'deﬁning': 11,\n",
       " 'constrains': 4,\n",
       " 'set': 49,\n",
       " 'policies': 60,\n",
       " 'need': 26,\n",
       " 'considered': 4,\n",
       " 'represent': 30,\n",
       " 'consistent': 8,\n",
       " 'given': 32,\n",
       " 'also': 37,\n",
       " 'creates': 3,\n",
       " 'opportunities': 7,\n",
       " 'exploit': 6,\n",
       " 'state': 310,\n",
       " 'abstractions': 55,\n",
       " 'individual': 7,\n",
       " 'within': 24,\n",
       " 'ignore': 8,\n",
       " 'large': 11,\n",
       " 'parts': 6,\n",
       " 'space': 28,\n",
       " 'important': 27,\n",
       " 'practical': 2,\n",
       " 'application': 3,\n",
       " 'method': 64,\n",
       " 'deﬁnes': 9,\n",
       " 'proves': 5,\n",
       " 'formal': 12,\n",
       " 'results': 18,\n",
       " 'representational': 3,\n",
       " 'power': 5,\n",
       " 'establishes': 1,\n",
       " 'ﬁve': 18,\n",
       " 'conditions': 37,\n",
       " 'safe': 7,\n",
       " 'use': 15,\n",
       " 'online': 8,\n",
       " 'model-free': 1,\n",
       " 'algorithm': 58,\n",
       " 'MAXQ-Q': 75,\n",
       " 'converges': 30,\n",
       " 'wih': 1,\n",
       " 'probability': 60,\n",
       " '1': 91,\n",
       " 'kind': 11,\n",
       " 'locally-optimal': 5,\n",
       " 'recursively': 34,\n",
       " 'optimal': 134,\n",
       " 'even': 10,\n",
       " 'presence': 4,\n",
       " 'kinds': 4,\n",
       " 'abstraction': 65,\n",
       " 'evaluates': 2,\n",
       " 'represen-tation': 1,\n",
       " 'series': 8,\n",
       " 'experiments': 12,\n",
       " 'three': 20,\n",
       " 'domains': 6,\n",
       " 'shows': 21,\n",
       " 'experimentally': 5,\n",
       " 'much': 15,\n",
       " 'faster': 5,\n",
       " 'ﬂat': 23,\n",
       " 'Q': 108,\n",
       " 'fact': 7,\n",
       " 'learns': 2,\n",
       " 'beneﬁt': 2,\n",
       " ':': 160,\n",
       " 'makes': 12,\n",
       " 'possible': 40,\n",
       " 'compute': 19,\n",
       " 'execute': 14,\n",
       " 'improved': 7,\n",
       " 'non-hierarchical': 25,\n",
       " 'via': 9,\n",
       " 'procedure': 5,\n",
       " 'similar': 9,\n",
       " 'improvement': 13,\n",
       " 'step': 17,\n",
       " 'iteration': 6,\n",
       " 'demonstrates': 2,\n",
       " 'eﬀectiveness': 3,\n",
       " 'execution': 52,\n",
       " 'Finally': 13,\n",
       " 'concludes': 2,\n",
       " 'comparison': 3,\n",
       " 'related': 4,\n",
       " 'discussion': 5,\n",
       " 'design': 16,\n",
       " 'tradeoﬀs': 1,\n",
       " 'Introduction': 2,\n",
       " 'A': 46,\n",
       " 'central': 1,\n",
       " 'goal': 102,\n",
       " 'artiﬁcial': 1,\n",
       " 'intelligence': 1,\n",
       " 'develop': 6,\n",
       " 'techniques': 4,\n",
       " 'constructing': 2,\n",
       " 'robust': 1,\n",
       " 'autonomous': 1,\n",
       " 'agents': 2,\n",
       " 'able': 13,\n",
       " 'good': 9,\n",
       " 'performance': 21,\n",
       " 'complex': 2,\n",
       " 'real-world': 3,\n",
       " 'environments': 1,\n",
       " 'One': 7,\n",
       " 'fruitful': 1,\n",
       " 'line': 12,\n",
       " 'research': 9,\n",
       " 'views': 1,\n",
       " '“': 102,\n",
       " 'economic': 5,\n",
       " '”': 101,\n",
       " 'perspective': 2,\n",
       " 'Boutilier': 6,\n",
       " 'Shoham': 2,\n",
       " '&': 32,\n",
       " 'Wellman': 2,\n",
       " '1997': 4,\n",
       " 'An': 13,\n",
       " 'agent': 58,\n",
       " 'interacts': 1,\n",
       " 'environment': 17,\n",
       " 'receives': 8,\n",
       " 'real-valued': 5,\n",
       " 'rewards': 15,\n",
       " 'penalties': 2,\n",
       " '’': 41,\n",
       " 'maximize': 5,\n",
       " 'total': 18,\n",
       " 'reward': 96,\n",
       " 'view': 5,\n",
       " 'easy': 8,\n",
       " 'formalize': 3,\n",
       " 'traditional': 1,\n",
       " 'goals': 12,\n",
       " 'achievement': 3,\n",
       " 'land': 1,\n",
       " 'airplane': 1,\n",
       " 'But': 13,\n",
       " 'formulate': 2,\n",
       " 'prevention': 2,\n",
       " 'crash': 1,\n",
       " 'airplanes': 1,\n",
       " 'maintenance': 2,\n",
       " 'keep': 2,\n",
       " 'air-traﬃc': 1,\n",
       " 'control': 3,\n",
       " 'system': 3,\n",
       " 'working': 3,\n",
       " 'long': 4,\n",
       " 'Goals': 2,\n",
       " 'represented': 19,\n",
       " 'giving': 4,\n",
       " 'positive': 4,\n",
       " 'achieving': 5,\n",
       " 'negative': 4,\n",
       " 'bad': 3,\n",
       " 'events': 2,\n",
       " 'occur': 3,\n",
       " 'time': 30,\n",
       " 'desireable': 1,\n",
       " 'maintained': 2,\n",
       " 'Furthermore': 5,\n",
       " 'formalism': 2,\n",
       " 'incorporate': 3,\n",
       " 'uncertainty—we': 1,\n",
       " 'require': 15,\n",
       " 'expected': 40,\n",
       " 'face': 1,\n",
       " 'random': 6,\n",
       " 'world': 5,\n",
       " 'brief': 1,\n",
       " 'review': 3,\n",
       " 'expressive—a': 1,\n",
       " 'diﬃcult': 7,\n",
       " 'chal-lenge': 1,\n",
       " 'however': 4,\n",
       " 'eﬃcient': 2,\n",
       " 'scalable': 1,\n",
       " 'methods': 31,\n",
       " 'reasoning': 1,\n",
       " 'planning': 10,\n",
       " 'AI': 2,\n",
       " 'framework': 5,\n",
       " 'area': 2,\n",
       " 'Stochastic': 1,\n",
       " 'Planning': 2,\n",
       " 'studies': 1,\n",
       " 'ﬁnding': 4,\n",
       " 'near-optimal': 2,\n",
       " 'plans': 4,\n",
       " 'case': 28,\n",
       " 'complete': 19,\n",
       " 'knowledge': 12,\n",
       " 'probabilistic': 5,\n",
       " 'behavior': 4,\n",
       " 'basic': 6,\n",
       " 'developed': 6,\n",
       " '1950s': 1,\n",
       " 'ﬁeld': 1,\n",
       " 'Dynamic': 3,\n",
       " 'Programming.': 1,\n",
       " 'Unfortunately': 4,\n",
       " 'polynomial': 2,\n",
       " 'number': 27,\n",
       " 'states': 123,\n",
       " 'prohibitively': 1,\n",
       " 'expensive': 1,\n",
       " 'problems': 16,\n",
       " 'Hence': 50,\n",
       " 'recent': 2,\n",
       " 'fo-cused': 1,\n",
       " 'structure': 16,\n",
       " 'problem': 44,\n",
       " 'eﬃciently': 2,\n",
       " 'Dean': 12,\n",
       " 'Hanks': 2,\n",
       " '1999': 5,\n",
       " 'Reinforcement': 6,\n",
       " 'Learning': 21,\n",
       " 'Bertsekas': 6,\n",
       " 'Tsitsiklis': 6,\n",
       " '1996': 6,\n",
       " ';': 23,\n",
       " 'Sutton': 12,\n",
       " 'Barto': 2,\n",
       " '1998': 18,\n",
       " 'stud-ies': 1,\n",
       " 'interacting': 2,\n",
       " 'directly': 5,\n",
       " 'external': 2,\n",
       " 'opposed': 1,\n",
       " 'analyzing': 2,\n",
       " 'user-provided': 1,\n",
       " 'model': 2,\n",
       " 'Again': 2,\n",
       " 'dynamic': 4,\n",
       " 'programming': 8,\n",
       " 'algorithms': 20,\n",
       " 'However': 35,\n",
       " 'rein-forcement': 2,\n",
       " 'oﬀer': 1,\n",
       " 'two': 57,\n",
       " 'advantages': 1,\n",
       " 'classical': 2,\n",
       " 'First': 10,\n",
       " 'permits': 5,\n",
       " 'focus': 1,\n",
       " 'attention': 3,\n",
       " 'rest': 2,\n",
       " 'Second': 7,\n",
       " 'employ': 11,\n",
       " 'approximation': 8,\n",
       " 'e.g.': 12,\n",
       " 'neural': 2,\n",
       " 'networks': 6,\n",
       " 'allows': 3,\n",
       " 'generalize': 2,\n",
       " 'across': 1,\n",
       " 'scales': 2,\n",
       " 'better': 15,\n",
       " 'Despite': 2,\n",
       " 'advances': 1,\n",
       " 'still': 4,\n",
       " 'many': 19,\n",
       " 'shortcomings': 1,\n",
       " 'biggest': 1,\n",
       " 'lack': 2,\n",
       " 'fully': 1,\n",
       " 'satisfactory': 1,\n",
       " 'incorporating': 1,\n",
       " 'hierarchies': 3,\n",
       " 'Research': 5,\n",
       " 'shown': 20,\n",
       " 'task': 90,\n",
       " 'Currie': 2,\n",
       " 'Tate': 2,\n",
       " '1991': 2,\n",
       " 'macro': 4,\n",
       " 'actions': 97,\n",
       " 'Fikes': 2,\n",
       " 'Hart': 2,\n",
       " 'Nilsson': 2,\n",
       " '1972': 2,\n",
       " 'Korf': 2,\n",
       " '1985': 2,\n",
       " 'Sacerdoti': 2,\n",
       " '1974': 2,\n",
       " 'Knoblock': 2,\n",
       " '1990': 2,\n",
       " 'provide': 15,\n",
       " 'exponential': 1,\n",
       " 'reductions': 1,\n",
       " 'computational': 2,\n",
       " 'cost': 34,\n",
       " 'methods—': 1,\n",
       " 'treat': 2,\n",
       " 'one': 64,\n",
       " 'huge': 5,\n",
       " 'search': 4,\n",
       " 'means': 14,\n",
       " 'paths': 8,\n",
       " 'start': 10,\n",
       " 'length': 3,\n",
       " 'determines': 1,\n",
       " 'information': 24,\n",
       " 'future': 5,\n",
       " 'must': 79,\n",
       " 'propagated': 1,\n",
       " 'backward': 3,\n",
       " 'along': 6,\n",
       " 'Many': 1,\n",
       " 'researchers': 2,\n",
       " '1992a': 3,\n",
       " 'Lin': 10,\n",
       " '1993': 13,\n",
       " 'Hauskrecht': 3,\n",
       " 'Meuleau': 2,\n",
       " 'Parr': 27,\n",
       " 'Russell': 7,\n",
       " 'Precup': 9,\n",
       " 'experimented': 2,\n",
       " 'diﬀerent': 25,\n",
       " '2': 36,\n",
       " 'explored': 2,\n",
       " 'points': 4,\n",
       " 'several': 8,\n",
       " 'systems': 4,\n",
       " 'designed': 1,\n",
       " 'speciﬁc': 2,\n",
       " 'situations': 5,\n",
       " 'We': 74,\n",
       " 'crisp': 1,\n",
       " 'deﬁnitions': 4,\n",
       " 'main': 2,\n",
       " 'approaches': 6,\n",
       " 'clear': 2,\n",
       " 'understanding': 2,\n",
       " 'relative': 5,\n",
       " 'merits': 1,\n",
       " 'formalizes': 2,\n",
       " 'clariﬁes': 1,\n",
       " 'attempts': 5,\n",
       " 'understand': 5,\n",
       " 'compares': 1,\n",
       " 'called': 8,\n",
       " 'provides': 5,\n",
       " 'decom-position': 1,\n",
       " 'subproblems': 5,\n",
       " 'simultaneously': 3,\n",
       " 'semantics': 3,\n",
       " 'decisions': 9,\n",
       " 'made': 4,\n",
       " 'As': 9,\n",
       " 'way': 14,\n",
       " 'providing': 3,\n",
       " 'overview': 1,\n",
       " 'let': 33,\n",
       " 'us': 33,\n",
       " 'issues': 4,\n",
       " 'see': 23,\n",
       " 'ﬁrst': 35,\n",
       " 'issue': 6,\n",
       " 'speciﬁed': 7,\n",
       " 'Hierarchical': 14,\n",
       " 'involves': 3,\n",
       " 'breaking': 2,\n",
       " 'There': 24,\n",
       " 'general': 18,\n",
       " 'subtask': 131,\n",
       " 'terms': 20,\n",
       " 'ﬁxed': 20,\n",
       " 'provided': 8,\n",
       " 'option': 8,\n",
       " 'takes': 11,\n",
       " 'second': 18,\n",
       " 'non-deterministic': 3,\n",
       " 'ﬁnite-state': 6,\n",
       " 'controller': 4,\n",
       " 'Hierarchy': 3,\n",
       " 'Abstract': 5,\n",
       " 'Machines': 3,\n",
       " 'HAM': 17,\n",
       " 'partial': 11,\n",
       " 'permitted': 2,\n",
       " 'point': 13,\n",
       " 'specify': 7,\n",
       " 'third': 6,\n",
       " 'termination': 42,\n",
       " 'predicate': 27,\n",
       " 'local': 6,\n",
       " 'These': 15,\n",
       " 'completed': 4,\n",
       " 'ﬁnal': 4,\n",
       " 'completing': 12,\n",
       " 'described': 12,\n",
       " 'follows': 17,\n",
       " 'building': 2,\n",
       " 'upon': 3,\n",
       " '1995': 4,\n",
       " 'advantage': 3,\n",
       " 'deﬁned': 29,\n",
       " 'amount': 12,\n",
       " 'eﬀort': 2,\n",
       " 'course': 11,\n",
       " 'action': 138,\n",
       " 'rather': 7,\n",
       " 'particular': 8,\n",
       " 'condition': 51,\n",
       " 'least': 10,\n",
       " 'simple': 9,\n",
       " 'form': 18,\n",
       " 'requires': 38,\n",
       " 'On': 2,\n",
       " 'hand': 5,\n",
       " 'guess': 4,\n",
       " 'desirability': 1,\n",
       " 'might': 5,\n",
       " 'terminate': 13,\n",
       " 'although': 4,\n",
       " 'show': 21,\n",
       " 'guesses': 6,\n",
       " 'revised': 1,\n",
       " 'automatically': 1,\n",
       " 'potential': 2,\n",
       " 'drawback': 3,\n",
       " 'learned': 27,\n",
       " 'may': 19,\n",
       " 'suboptimal': 4,\n",
       " 'programmer-provided': 1,\n",
       " 'If': 31,\n",
       " 'constraints': 10,\n",
       " 'poorly': 1,\n",
       " 'chosen': 22,\n",
       " 'resulting': 22,\n",
       " 'Nonetheless': 5,\n",
       " 'learn-ing': 1,\n",
       " 'guarantee': 3,\n",
       " 'best': 25,\n",
       " 'suﬀers': 1,\n",
       " 'additional': 5,\n",
       " 'source': 15,\n",
       " 'suboptimality': 1,\n",
       " 'optimality': 22,\n",
       " 'call': 15,\n",
       " 'recursive': 18,\n",
       " 'locally': 16,\n",
       " 'children': 14,\n",
       " 'exist': 4,\n",
       " 'overall': 8,\n",
       " 'avoided': 2,\n",
       " 'careful': 2,\n",
       " 'deﬁnition': 12,\n",
       " 'predicates': 10,\n",
       " 'added': 7,\n",
       " 'burden': 1,\n",
       " 'interesting': 6,\n",
       " 'note': 12,\n",
       " 'noticed': 1,\n",
       " 'previously': 2,\n",
       " 'focused': 2,\n",
       " 'single': 7,\n",
       " 'terminal': 28,\n",
       " '3': 32,\n",
       " 'cases': 10,\n",
       " 'arise': 3,\n",
       " 'whether': 8,\n",
       " 'employs': 1,\n",
       " 'ignores': 2,\n",
       " 'aspects': 3,\n",
       " 'For': 38,\n",
       " 'example': 28,\n",
       " 'robot': 12,\n",
       " 'navigation': 11,\n",
       " 'choices': 3,\n",
       " 'route': 1,\n",
       " 'take': 8,\n",
       " 'reach': 10,\n",
       " 'location': 44,\n",
       " 'independent': 3,\n",
       " 'currently': 2,\n",
       " 'carrying': 2,\n",
       " 'With': 10,\n",
       " 'exceptions': 1,\n",
       " 'impact': 1,\n",
       " 'accelerating': 2,\n",
       " 'tradeoﬀ': 6,\n",
       " 'successful': 4,\n",
       " 'using': 47,\n",
       " 'despite': 1,\n",
       " 'create': 5,\n",
       " 'concerns': 1,\n",
       " 'pol-icy': 1,\n",
       " 'could': 20,\n",
       " 'evaluated': 2,\n",
       " 'incrementally': 1,\n",
       " 'yield': 3,\n",
       " 'potentially': 1,\n",
       " 'Dietterich': 5,\n",
       " 'Ravindran': 3,\n",
       " 'generalized': 2,\n",
       " 'arbitrary': 7,\n",
       " 'subroutines': 10,\n",
       " 'executed': 28,\n",
       " 'non-hierarchically': 2,\n",
       " 'order': 20,\n",
       " 'support': 7,\n",
       " 'extra': 1,\n",
       " 'required': 12,\n",
       " 'Ordinarily': 1,\n",
       " 'higher': 7,\n",
       " 'levels': 12,\n",
       " 'plus': 6,\n",
       " 'initial': 31,\n",
       " 'In': 95,\n",
       " 'exploration': 42,\n",
       " 'well': 10,\n",
       " 'computation': 4,\n",
       " 'memory': 4,\n",
       " 'consequence': 3,\n",
       " 'either': 15,\n",
       " 'worth': 2,\n",
       " 'fourth': 3,\n",
       " 'typically': 4,\n",
       " 'operate': 1,\n",
       " 'particularly': 3,\n",
       " 'family': 1,\n",
       " 'relied': 1,\n",
       " 'unique': 17,\n",
       " 'employed': 14,\n",
       " 'mix': 1,\n",
       " 'batch': 1,\n",
       " 'train': 2,\n",
       " 'options': 6,\n",
       " 'usually': 4,\n",
       " 'assumes': 1,\n",
       " 'HAMQ': 8,\n",
       " 'Feudal': 4,\n",
       " 'ﬂattening': 1,\n",
       " 'undesirable': 1,\n",
       " 'consequences': 1,\n",
       " 'tailored': 1,\n",
       " 'converge': 23,\n",
       " 'well-deﬁned': 2,\n",
       " 'present': 5,\n",
       " 'fully-online': 1,\n",
       " 'theoretically': 1,\n",
       " 'substantially': 2,\n",
       " 'i.e.': 25,\n",
       " 'Without': 1,\n",
       " 'gives': 20,\n",
       " 'worse': 2,\n",
       " 'remainder': 1,\n",
       " 'organized': 1,\n",
       " 'After': 3,\n",
       " 'introducing': 4,\n",
       " 'notation': 14,\n",
       " 'Section': 6,\n",
       " 'illustrate': 1,\n",
       " 'sim-ple': 1,\n",
       " '4': 48,\n",
       " 'analytically': 1,\n",
       " 'tractable': 1,\n",
       " 'version': 5,\n",
       " 'MAXQ-0': 30,\n",
       " 'convergence': 11,\n",
       " 'recur-sively': 2,\n",
       " 'extend': 4,\n",
       " 'produce': 6,\n",
       " 'theorem': 12,\n",
       " 'similarly': 1,\n",
       " '5': 29,\n",
       " 'safely': 2,\n",
       " 'incorporated': 1,\n",
       " 'State': 9,\n",
       " 'give': 17,\n",
       " 'rise': 3,\n",
       " 'credit': 3,\n",
       " 'assignment': 4,\n",
       " 'brieﬂy': 2,\n",
       " 'discusses': 1,\n",
       " 'solution': 14,\n",
       " '7': 22,\n",
       " 'idea': 4,\n",
       " 'generality': 1,\n",
       " 'They': 6,\n",
       " 'importance': 3,\n",
       " 'temporal': 7,\n",
       " 'tackles': 1,\n",
       " 'question': 2,\n",
       " 'ability': 1,\n",
       " 'Some': 3,\n",
       " 'readers': 1,\n",
       " 'disappointed': 1,\n",
       " 'Our': 6,\n",
       " 'philosophy': 1,\n",
       " 'developing': 1,\n",
       " 'share': 4,\n",
       " 'notably': 1,\n",
       " 'draw': 3,\n",
       " 'inspiration': 1,\n",
       " 'development': 1,\n",
       " 'Belief': 2,\n",
       " 'Networks': 2,\n",
       " 'Pearl': 2,\n",
       " '1988': 2,\n",
       " 'introduced': 15,\n",
       " 'engineer': 2,\n",
       " 'would': 24,\n",
       " 'describe': 7,\n",
       " 'domain': 6,\n",
       " 'experts': 1,\n",
       " 'necessary': 6,\n",
       " 'estimates': 2,\n",
       " 'Subsequently': 1,\n",
       " 'values': 135,\n",
       " 'observational': 1,\n",
       " 'data': 3,\n",
       " 'Most': 1,\n",
       " 'recently': 1,\n",
       " 'belief': 1,\n",
       " 'dependence': 4,\n",
       " 'reduced': 2,\n",
       " 'likewise': 1,\n",
       " 'make': 15,\n",
       " 'like': 4,\n",
       " 'computer': 2,\n",
       " 'program': 3,\n",
       " 'rely': 2,\n",
       " 'modules': 2,\n",
       " 'indicate': 4,\n",
       " 'permissible': 1,\n",
       " 'ways': 2,\n",
       " 'invoke': 5,\n",
       " 'ﬁll': 2,\n",
       " 'implementations': 1,\n",
       " 'module': 1,\n",
       " 'believe': 4,\n",
       " 'tool': 1,\n",
       " 'solving': 8,\n",
       " 'help': 2,\n",
       " 'hope': 1,\n",
       " 'subsequent': 2,\n",
       " 'automate': 1,\n",
       " 'requiring': 3,\n",
       " 'Formal': 1,\n",
       " 'Deﬁnitions': 2,\n",
       " '2.1': 1,\n",
       " 'Decision': 10,\n",
       " 'Problems': 4,\n",
       " 'Semi-Markov': 4,\n",
       " 'standard': 3,\n",
       " 'restrict': 2,\n",
       " 'fully-observable': 1,\n",
       " 'stochastic': 14,\n",
       " 'situation': 3,\n",
       " 'modeled': 1,\n",
       " 'Problem': 4,\n",
       " 'hS': 3,\n",
       " 'P': 91,\n",
       " 'R': 64,\n",
       " 'P0i': 3,\n",
       " '•': 57,\n",
       " 'S': 3,\n",
       " 'At': 10,\n",
       " 'observe': 5,\n",
       " 'ﬁnite': 3,\n",
       " 'Technically': 1,\n",
       " 'available': 6,\n",
       " 'depends': 9,\n",
       " 'current': 38,\n",
       " 'suppress': 2,\n",
       " 'When': 18,\n",
       " '∈': 11,\n",
       " 'performed': 10,\n",
       " 'transition': 25,\n",
       " 's′': 92,\n",
       " 'according': 18,\n",
       " 'distribution': 24,\n",
       " 's′|s': 29,\n",
       " 'Similarly': 11,\n",
       " 'possibly': 2,\n",
       " 'To': 38,\n",
       " 'simplify': 4,\n",
       " 'customary': 1,\n",
       " 'initiated': 1,\n",
       " 'though': 2,\n",
       " 'depend': 13,\n",
       " 'P0': 3,\n",
       " 'starting': 26,\n",
       " 'initialized': 3,\n",
       " 'π': 207,\n",
       " 'mapping': 3,\n",
       " 'tells': 8,\n",
       " '=': 178,\n",
       " 'perform': 11,\n",
       " 's.': 15,\n",
       " 'consider': 26,\n",
       " 'settings': 4,\n",
       " 'Episodic': 1,\n",
       " 'Inﬁnite-Horizon': 1,\n",
       " 'episodic': 9,\n",
       " 'setting': 10,\n",
       " 'zero-cost': 1,\n",
       " 'absorbing': 3,\n",
       " 'lead': 2,\n",
       " 'back': 4,\n",
       " 'zero': 13,\n",
       " 'deterministic': 13,\n",
       " 'proper': 4,\n",
       " '—that': 2,\n",
       " 'non-zero': 1,\n",
       " 'reaching': 6,\n",
       " 'started': 4,\n",
       " 'ﬁnd': 12,\n",
       " 'maximizes': 4,\n",
       " 'cumulative': 15,\n",
       " 'special': 5,\n",
       " 'non-positive': 1,\n",
       " 'referred': 1,\n",
       " 'shortest': 6,\n",
       " 'path': 17,\n",
       " 'viewed': 7,\n",
       " 'costs': 5,\n",
       " 'lengths': 1,\n",
       " 'move': 30,\n",
       " 'minimum': 1,\n",
       " 'inﬁnite': 10,\n",
       " 'horizon': 6,\n",
       " 'addition': 9,\n",
       " 'discount': 5,\n",
       " 'factor': 7,\n",
       " 'γ': 11,\n",
       " 'minimizes': 1,\n",
       " 'discounted': 15,\n",
       " 'sum': 18,\n",
       " 'V': 144,\n",
       " 'executing': 21,\n",
       " 'Let': 52,\n",
       " 'rt': 12,\n",
       " 'variable': 5,\n",
       " 'following': 36,\n",
       " 'E': 6,\n",
       " '{': 14,\n",
       " '+': 105,\n",
       " 'rt+1': 2,\n",
       " 'rt+2': 1,\n",
       " '·': 34,\n",
       " '|st': 2,\n",
       " '}': 14,\n",
       " 'γrt+1': 2,\n",
       " 'γ2rt+2': 2,\n",
       " 'st': 4,\n",
       " 'equation': 20,\n",
       " 'reduces': 1,\n",
       " 'unless': 2,\n",
       " '<': 5,\n",
       " 'satisﬁes': 6,\n",
       " 'Bellman': 14,\n",
       " 'cid:12': 8,\n",
       " '′': 148,\n",
       " '|s': 63,\n",
       " 'γV': 4,\n",
       " 'cid:2': 3,\n",
       " 'quantity': 8,\n",
       " 'right-hand': 12,\n",
       " 'side': 9,\n",
       " 'backed-up': 2,\n",
       " 'performing': 21,\n",
       " 'successor': 1,\n",
       " 'computes': 13,\n",
       " 'received': 6,\n",
       " 'weights': 2,\n",
       " 'ending': 2,\n",
       " '∗': 56,\n",
       " 'S.': 12,\n",
       " '1957': 2,\n",
       " 'proved': 6,\n",
       " 'cid:3': 3,\n",
       " 'max': 20,\n",
       " 'Xs′': 29,\n",
       " 'Any': 3,\n",
       " 'chooses': 8,\n",
       " 'maximum': 4,\n",
       " 'denote': 10,\n",
       " 'π∗': 16,\n",
       " 'Note': 13,\n",
       " 'greedy': 42,\n",
       " 'respect': 7,\n",
       " 'Closely': 1,\n",
       " 'so-called': 2,\n",
       " 'action-value': 10,\n",
       " 'Watkins': 5,\n",
       " '1989': 3,\n",
       " 'Qπ': 30,\n",
       " 'n': 7,\n",
       " '6': 26,\n",
       " 'cid:20': 1,\n",
       " 'thereafter': 2,\n",
       " 'γQπ': 1,\n",
       " 'written': 5,\n",
       " 'Q∗': 14,\n",
       " 'a′': 22,\n",
       " 'cid:21': 1,\n",
       " 'policies—they': 1,\n",
       " 'diﬀer': 7,\n",
       " 'break': 2,\n",
       " 'ties': 5,\n",
       " 'identical': 6,\n",
       " 'denoted': 4,\n",
       " 'ω': 7,\n",
       " 'That': 3,\n",
       " 'anti-symmetric': 1,\n",
       " 'transitive': 1,\n",
       " 'relation': 1,\n",
       " 'a1': 19,\n",
       " 'a2': 11,\n",
       " 'true': 13,\n",
       " 'iﬀ': 4,\n",
       " 'preferred': 1,\n",
       " 'ordered': 32,\n",
       " 'πω': 3,\n",
       " 'breaks': 3,\n",
       " 'suppose': 11,\n",
       " 'Then': 28,\n",
       " 'choose': 14,\n",
       " 'discrete-time': 1,\n",
       " 'semi-Markov': 2,\n",
       " 'Process': 3,\n",
       " 'SMDP': 11,\n",
       " 'generalization': 1,\n",
       " 'Deci-sion': 1,\n",
       " 'N': 118,\n",
       " 'steps': 16,\n",
       " 'joint': 2,\n",
       " 'result': 19,\n",
       " 'changed': 1,\n",
       " '.1': 1,\n",
       " 'straightforward': 3,\n",
       " 'modify': 1,\n",
       " 'γN': 27,\n",
       " 'h': 1,\n",
       " 'change': 6,\n",
       " 'taken': 7,\n",
       " 'raised': 1,\n",
       " 'reﬂect': 2,\n",
       " 'elapse': 1,\n",
       " 'a.': 3,\n",
       " 'expectation': 3,\n",
       " 'linear': 2,\n",
       " 'operator': 7,\n",
       " 'write': 11,\n",
       " 'equations': 12,\n",
       " 'rewrite': 4,\n",
       " 'diﬀerence': 5,\n",
       " '1This': 1,\n",
       " 'formalization': 1,\n",
       " 'slightly': 6,\n",
       " 'formulation': 3,\n",
       " 'SMDPs': 1,\n",
       " 'separates': 1,\n",
       " 'F': 2,\n",
       " 't|s': 1,\n",
       " 'units': 5,\n",
       " 'integer-valued': 1,\n",
       " 'durations': 1,\n",
       " '2.2': 1,\n",
       " 'Algorithms': 1,\n",
       " 'access': 1,\n",
       " 'unknown': 1,\n",
       " 'protocol': 1,\n",
       " 'told': 1,\n",
       " '⊆': 1,\n",
       " 'executable': 2,\n",
       " 'executes': 7,\n",
       " 'causes': 2,\n",
       " 'returns': 13,\n",
       " 'r.': 2,\n",
       " 'contains': 5,\n",
       " 'reset': 1,\n",
       " 'drawn': 4,\n",
       " 'observed': 3,\n",
       " 'well-known': 1,\n",
       " '1992': 2,\n",
       " 'SARSA': 6,\n",
       " '0': 61,\n",
       " 'Rummery': 2,\n",
       " 'Niranjan': 2,\n",
       " '1994': 5,\n",
       " 'Both': 3,\n",
       " 'maintain': 1,\n",
       " 'tabular': 1,\n",
       " 'Every': 1,\n",
       " 'entry': 1,\n",
       " 'table': 3,\n",
       " 'arbitrarily': 1,\n",
       " 'r': 80,\n",
       " 'performs': 7,\n",
       " 'update': 22,\n",
       " 'Qt': 10,\n",
       " '−': 24,\n",
       " 'αt': 37,\n",
       " 'Qt−1': 3,\n",
       " '[': 30,\n",
       " ']': 28,\n",
       " 'rate': 17,\n",
       " 'parameter': 14,\n",
       " 'Jaakkola': 5,\n",
       " 'Jordan': 2,\n",
       " 'prove': 15,\n",
       " 'tries': 2,\n",
       " 'every': 8,\n",
       " 'inﬁnitely': 3,\n",
       " 'often': 6,\n",
       " 'lim': 4,\n",
       " 'T': 20,\n",
       " '→∞': 3,\n",
       " 't=1': 3,\n",
       " 'X': 13,\n",
       " '∞': 5,\n",
       " 'T→∞': 1,\n",
       " 'Xt=1': 1,\n",
       " 'α2': 2,\n",
       " 'Their': 2,\n",
       " 'proof': 10,\n",
       " 'holds': 4,\n",
       " 'discussed': 5,\n",
       " 'inﬁnite-horizon': 2,\n",
       " 'observing': 3,\n",
       " 'choosing': 6,\n",
       " 'γQt−1': 1,\n",
       " 'key': 4,\n",
       " 'appears': 3,\n",
       " 'place': 4,\n",
       " 'uses': 3,\n",
       " 'Littman': 2,\n",
       " 'Szepesv´ari': 2,\n",
       " 'decreases': 2,\n",
       " 'Equation': 10,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordFreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': 0.0444791260241904, 'paper': 0.01443620756925478, 'presents': 0.0019508388607101053, 'new': 0.006242684354272337, 'approach': 0.009754194303550527, 'hierarchical': 0.04564962934061646, 'reinforcement': 0.011705033164260631, 'learning': 0.06242684354272337, 'based': 0.008973858759266484, 'decomposing': 0.0007803355442840422, 'target': 0.004291845493562232, 'Markov': 0.0046820132657042525, 'decision': 0.0031213421771361686, 'process': 0.00351150994927819, '(': 0.5220444791260241, 'MDP': 0.019508388607101055, ')': 0.5208739758095982, 'hierarchy': 0.02262973078423722, 'smaller': 0.0007803355442840422, 'MDPs': 0.0023410066328521262, 'value': 0.07140070230198986, 'function': 0.07920405774483028, 'additive': 0.0003901677721420211, 'combination': 0.0007803355442840422, 'functions': 0.017557549746390948, '.': 0.5836909871244635, 'The': 0.08973858759266484, 'decomposition': 0.016777214202106906, ',': 1.0, 'known': 0.0019508388607101053, 'MAXQ': 0.08349590323839251, 'procedural': 0.0007803355442840422, 'semantics—as': 0.0007803355442840422, 'subroutine': 0.009754194303550527, 'hierarchy—and': 0.0003901677721420211, 'declarative': 0.0007803355442840422, 'representation': 0.01911822083495903, 'policy': 0.13031603589543503, 'uniﬁes': 0.0003901677721420211, 'extends': 0.0003901677721420211, 'previous': 0.0031213421771361686, 'work': 0.006632852126414358, 'hierar-chical': 0.0007803355442840422, 'Singh': 0.008193523214982443, 'Kaelbling': 0.009364026531408505, 'Dayan': 0.0031213421771361686, 'Hinton': 0.0023410066328521262, 'It': 0.017557549746390948, 'assumption': 0.0023410066328521262, 'programmer': 0.009364026531408505, 'identify': 0.0015606710885680843, 'useful': 0.0027311744049941474, 'subgoals': 0.0011705033164260631, 'deﬁne': 0.008973858759266484, 'subtasks': 0.02262973078423722, 'achieve': 0.0046820132657042525, 'By': 0.005072181037846274, 'deﬁning': 0.004291845493562232, 'constrains': 0.0015606710885680843, 'set': 0.01911822083495903, 'policies': 0.023410066328521262, 'need': 0.010144362075692548, 'considered': 0.0015606710885680843, 'represent': 0.011705033164260631, 'consistent': 0.0031213421771361686, 'given': 0.012485368708544674, 'also': 0.01443620756925478, 'creates': 0.0011705033164260631, 'opportunities': 0.0027311744049941474, 'exploit': 0.0023410066328521262, 'state': 0.12095200936402653, 'abstractions': 0.02145922746781116, 'individual': 0.0027311744049941474, 'within': 0.009364026531408505, 'ignore': 0.0031213421771361686, 'large': 0.004291845493562232, 'parts': 0.0023410066328521262, 'space': 0.01092469761997659, 'important': 0.010534529847834569, 'practical': 0.0007803355442840422, 'application': 0.0011705033164260631, 'method': 0.02497073741708935, 'deﬁnes': 0.00351150994927819, 'proves': 0.0019508388607101053, 'formal': 0.0046820132657042525, 'results': 0.00702301989855638, 'representational': 0.0011705033164260631, 'power': 0.0019508388607101053, 'establishes': 0.0003901677721420211, 'ﬁve': 0.00702301989855638, 'conditions': 0.01443620756925478, 'safe': 0.0027311744049941474, 'use': 0.005852516582130316, 'online': 0.0031213421771361686, 'model-free': 0.0003901677721420211, 'algorithm': 0.02262973078423722, 'MAXQ-Q': 0.02926258291065158, 'converges': 0.011705033164260631, 'wih': 0.0003901677721420211, 'probability': 0.023410066328521262, '1': 0.035505267264923916, 'kind': 0.004291845493562232, 'locally-optimal': 0.0019508388607101053, 'recursively': 0.013265704252828716, 'optimal': 0.05228248146703082, 'even': 0.0039016777214202106, 'presence': 0.0015606710885680843, 'kinds': 0.0015606710885680843, 'abstraction': 0.02536090518923137, 'evaluates': 0.0007803355442840422, 'represen-tation': 0.0003901677721420211, 'series': 0.0031213421771361686, 'experiments': 0.0046820132657042525, 'three': 0.007803355442840421, 'domains': 0.0023410066328521262, 'shows': 0.008193523214982443, 'experimentally': 0.0019508388607101053, 'much': 0.005852516582130316, 'faster': 0.0019508388607101053, 'ﬂat': 0.008973858759266484, 'Q': 0.042138119391338276, 'fact': 0.0027311744049941474, 'learns': 0.0007803355442840422, 'beneﬁt': 0.0007803355442840422, ':': 0.06242684354272337, 'makes': 0.0046820132657042525, 'possible': 0.015606710885680842, 'compute': 0.0074131876706984, 'execute': 0.005462348809988295, 'improved': 0.0027311744049941474, 'non-hierarchical': 0.009754194303550527, 'via': 0.00351150994927819, 'procedure': 0.0019508388607101053, 'similar': 0.00351150994927819, 'improvement': 0.005072181037846274, 'step': 0.006632852126414358, 'iteration': 0.0023410066328521262, 'demonstrates': 0.0007803355442840422, 'eﬀectiveness': 0.0011705033164260631, 'execution': 0.020288724151385096, 'Finally': 0.005072181037846274, 'concludes': 0.0007803355442840422, 'comparison': 0.0011705033164260631, 'related': 0.0015606710885680843, 'discussion': 0.0019508388607101053, 'design': 0.006242684354272337, 'tradeoﬀs': 0.0003901677721420211, 'Introduction': 0.0007803355442840422, 'A': 0.01794771751853297, 'central': 0.0003901677721420211, 'goal': 0.03979711275848615, 'artiﬁcial': 0.0003901677721420211, 'intelligence': 0.0003901677721420211, 'develop': 0.0023410066328521262, 'techniques': 0.0015606710885680843, 'constructing': 0.0007803355442840422, 'robust': 0.0003901677721420211, 'autonomous': 0.0003901677721420211, 'agents': 0.0007803355442840422, 'able': 0.005072181037846274, 'good': 0.00351150994927819, 'performance': 0.008193523214982443, 'complex': 0.0007803355442840422, 'real-world': 0.0011705033164260631, 'environments': 0.0003901677721420211, 'One': 0.0027311744049941474, 'fruitful': 0.0003901677721420211, 'line': 0.0046820132657042525, 'research': 0.00351150994927819, 'views': 0.0003901677721420211, '“': 0.03979711275848615, 'economic': 0.0019508388607101053, '”': 0.03940694498634413, 'perspective': 0.0007803355442840422, 'Boutilier': 0.0023410066328521262, 'Shoham': 0.0007803355442840422, '&': 0.012485368708544674, 'Wellman': 0.0007803355442840422, '1997': 0.0015606710885680843, 'An': 0.005072181037846274, 'agent': 0.02262973078423722, 'interacts': 0.0003901677721420211, 'environment': 0.006632852126414358, 'receives': 0.0031213421771361686, 'real-valued': 0.0019508388607101053, 'rewards': 0.005852516582130316, 'penalties': 0.0007803355442840422, '’': 0.015996878657822865, 'maximize': 0.0019508388607101053, 'total': 0.00702301989855638, 'reward': 0.03745610612563402, 'view': 0.0019508388607101053, 'easy': 0.0031213421771361686, 'formalize': 0.0011705033164260631, 'traditional': 0.0003901677721420211, 'goals': 0.0046820132657042525, 'achievement': 0.0011705033164260631, 'land': 0.0003901677721420211, 'airplane': 0.0003901677721420211, 'But': 0.005072181037846274, 'formulate': 0.0007803355442840422, 'prevention': 0.0007803355442840422, 'crash': 0.0003901677721420211, 'airplanes': 0.0003901677721420211, 'maintenance': 0.0007803355442840422, 'keep': 0.0007803355442840422, 'air-traﬃc': 0.0003901677721420211, 'control': 0.0011705033164260631, 'system': 0.0011705033164260631, 'working': 0.0011705033164260631, 'long': 0.0015606710885680843, 'Goals': 0.0007803355442840422, 'represented': 0.0074131876706984, 'giving': 0.0015606710885680843, 'positive': 0.0015606710885680843, 'achieving': 0.0019508388607101053, 'negative': 0.0015606710885680843, 'bad': 0.0011705033164260631, 'events': 0.0007803355442840422, 'occur': 0.0011705033164260631, 'time': 0.011705033164260631, 'desireable': 0.0003901677721420211, 'maintained': 0.0007803355442840422, 'Furthermore': 0.0019508388607101053, 'formalism': 0.0007803355442840422, 'incorporate': 0.0011705033164260631, 'uncertainty—we': 0.0003901677721420211, 'require': 0.005852516582130316, 'expected': 0.015606710885680842, 'face': 0.0003901677721420211, 'random': 0.0023410066328521262, 'world': 0.0019508388607101053, 'brief': 0.0003901677721420211, 'review': 0.0011705033164260631, 'expressive—a': 0.0003901677721420211, 'diﬃcult': 0.0027311744049941474, 'chal-lenge': 0.0003901677721420211, 'however': 0.0015606710885680843, 'eﬃcient': 0.0007803355442840422, 'scalable': 0.0003901677721420211, 'methods': 0.012095200936402654, 'reasoning': 0.0003901677721420211, 'planning': 0.0039016777214202106, 'AI': 0.0007803355442840422, 'framework': 0.0019508388607101053, 'area': 0.0007803355442840422, 'Stochastic': 0.0003901677721420211, 'Planning': 0.0007803355442840422, 'studies': 0.0003901677721420211, 'ﬁnding': 0.0015606710885680843, 'near-optimal': 0.0007803355442840422, 'plans': 0.0015606710885680843, 'case': 0.01092469761997659, 'complete': 0.0074131876706984, 'knowledge': 0.0046820132657042525, 'probabilistic': 0.0019508388607101053, 'behavior': 0.0015606710885680843, 'basic': 0.0023410066328521262, 'developed': 0.0023410066328521262, '1950s': 0.0003901677721420211, 'ﬁeld': 0.0003901677721420211, 'Dynamic': 0.0011705033164260631, 'Programming.': 0.0003901677721420211, 'Unfortunately': 0.0015606710885680843, 'polynomial': 0.0007803355442840422, 'number': 0.010534529847834569, 'states': 0.047990635973468594, 'prohibitively': 0.0003901677721420211, 'expensive': 0.0003901677721420211, 'problems': 0.006242684354272337, 'Hence': 0.019508388607101055, 'recent': 0.0007803355442840422, 'fo-cused': 0.0003901677721420211, 'structure': 0.006242684354272337, 'problem': 0.017167381974248927, 'eﬃciently': 0.0007803355442840422, 'Dean': 0.0046820132657042525, 'Hanks': 0.0007803355442840422, '1999': 0.0019508388607101053, 'Reinforcement': 0.0023410066328521262, 'Learning': 0.008193523214982443, 'Bertsekas': 0.0023410066328521262, 'Tsitsiklis': 0.0023410066328521262, '1996': 0.0023410066328521262, ';': 0.008973858759266484, 'Sutton': 0.0046820132657042525, 'Barto': 0.0007803355442840422, '1998': 0.00702301989855638, 'stud-ies': 0.0003901677721420211, 'interacting': 0.0007803355442840422, 'directly': 0.0019508388607101053, 'external': 0.0007803355442840422, 'opposed': 0.0003901677721420211, 'analyzing': 0.0007803355442840422, 'user-provided': 0.0003901677721420211, 'model': 0.0007803355442840422, 'Again': 0.0007803355442840422, 'dynamic': 0.0015606710885680843, 'programming': 0.0031213421771361686, 'algorithms': 0.007803355442840421, 'However': 0.013655872024970737, 'rein-forcement': 0.0007803355442840422, 'oﬀer': 0.0003901677721420211, 'two': 0.0222395630120952, 'advantages': 0.0003901677721420211, 'classical': 0.0007803355442840422, 'First': 0.0039016777214202106, 'permits': 0.0019508388607101053, 'focus': 0.0003901677721420211, 'attention': 0.0011705033164260631, 'rest': 0.0007803355442840422, 'Second': 0.0027311744049941474, 'employ': 0.004291845493562232, 'approximation': 0.0031213421771361686, 'e.g.': 0.0046820132657042525, 'neural': 0.0007803355442840422, 'networks': 0.0023410066328521262, 'allows': 0.0011705033164260631, 'generalize': 0.0007803355442840422, 'across': 0.0003901677721420211, 'scales': 0.0007803355442840422, 'better': 0.005852516582130316, 'Despite': 0.0007803355442840422, 'advances': 0.0003901677721420211, 'still': 0.0015606710885680843, 'many': 0.0074131876706984, 'shortcomings': 0.0003901677721420211, 'biggest': 0.0003901677721420211, 'lack': 0.0007803355442840422, 'fully': 0.0003901677721420211, 'satisfactory': 0.0003901677721420211, 'incorporating': 0.0003901677721420211, 'hierarchies': 0.0011705033164260631, 'Research': 0.0019508388607101053, 'shown': 0.007803355442840421, 'task': 0.035115099492781895, 'Currie': 0.0007803355442840422, 'Tate': 0.0007803355442840422, '1991': 0.0007803355442840422, 'macro': 0.0015606710885680843, 'actions': 0.03784627389777604, 'Fikes': 0.0007803355442840422, 'Hart': 0.0007803355442840422, 'Nilsson': 0.0007803355442840422, '1972': 0.0007803355442840422, 'Korf': 0.0007803355442840422, '1985': 0.0007803355442840422, 'Sacerdoti': 0.0007803355442840422, '1974': 0.0007803355442840422, 'Knoblock': 0.0007803355442840422, '1990': 0.0007803355442840422, 'provide': 0.005852516582130316, 'exponential': 0.0003901677721420211, 'reductions': 0.0003901677721420211, 'computational': 0.0007803355442840422, 'cost': 0.013265704252828716, 'methods—': 0.0003901677721420211, 'treat': 0.0007803355442840422, 'one': 0.02497073741708935, 'huge': 0.0019508388607101053, 'search': 0.0015606710885680843, 'means': 0.005462348809988295, 'paths': 0.0031213421771361686, 'start': 0.0039016777214202106, 'length': 0.0011705033164260631, 'determines': 0.0003901677721420211, 'information': 0.009364026531408505, 'future': 0.0019508388607101053, 'must': 0.030823253999219664, 'propagated': 0.0003901677721420211, 'backward': 0.0011705033164260631, 'along': 0.0023410066328521262, 'Many': 0.0003901677721420211, 'researchers': 0.0007803355442840422, '1992a': 0.0011705033164260631, 'Lin': 0.0039016777214202106, '1993': 0.005072181037846274, 'Hauskrecht': 0.0011705033164260631, 'Meuleau': 0.0007803355442840422, 'Parr': 0.010534529847834569, 'Russell': 0.0027311744049941474, 'Precup': 0.00351150994927819, 'experimented': 0.0007803355442840422, 'diﬀerent': 0.009754194303550527, '2': 0.01404603979711276, 'explored': 0.0007803355442840422, 'points': 0.0015606710885680843, 'several': 0.0031213421771361686, 'systems': 0.0015606710885680843, 'designed': 0.0003901677721420211, 'speciﬁc': 0.0007803355442840422, 'situations': 0.0019508388607101053, 'We': 0.02887241513850956, 'crisp': 0.0003901677721420211, 'deﬁnitions': 0.0015606710885680843, 'main': 0.0007803355442840422, 'approaches': 0.0023410066328521262, 'clear': 0.0007803355442840422, 'understanding': 0.0007803355442840422, 'relative': 0.0019508388607101053, 'merits': 0.0003901677721420211, 'formalizes': 0.0007803355442840422, 'clariﬁes': 0.0003901677721420211, 'attempts': 0.0019508388607101053, 'understand': 0.0019508388607101053, 'compares': 0.0003901677721420211, 'called': 0.0031213421771361686, 'provides': 0.0019508388607101053, 'decom-position': 0.0003901677721420211, 'subproblems': 0.0019508388607101053, 'simultaneously': 0.0011705033164260631, 'semantics': 0.0011705033164260631, 'decisions': 0.00351150994927819, 'made': 0.0015606710885680843, 'As': 0.00351150994927819, 'way': 0.005462348809988295, 'providing': 0.0011705033164260631, 'overview': 0.0003901677721420211, 'let': 0.012875536480686695, 'us': 0.012875536480686695, 'issues': 0.0015606710885680843, 'see': 0.008973858759266484, 'ﬁrst': 0.013655872024970737, 'issue': 0.0023410066328521262, 'speciﬁed': 0.0027311744049941474, 'Hierarchical': 0.005462348809988295, 'involves': 0.0011705033164260631, 'breaking': 0.0007803355442840422, 'There': 0.009364026531408505, 'general': 0.00702301989855638, 'subtask': 0.05111197815060476, 'terms': 0.007803355442840421, 'ﬁxed': 0.007803355442840421, 'provided': 0.0031213421771361686, 'option': 0.0031213421771361686, 'takes': 0.004291845493562232, 'second': 0.00702301989855638, 'non-deterministic': 0.0011705033164260631, 'ﬁnite-state': 0.0023410066328521262, 'controller': 0.0015606710885680843, 'Hierarchy': 0.0011705033164260631, 'Abstract': 0.0019508388607101053, 'Machines': 0.0011705033164260631, 'HAM': 0.006632852126414358, 'partial': 0.004291845493562232, 'permitted': 0.0007803355442840422, 'point': 0.005072181037846274, 'specify': 0.0027311744049941474, 'third': 0.0023410066328521262, 'termination': 0.016387046429964885, 'predicate': 0.010534529847834569, 'local': 0.0023410066328521262, 'These': 0.005852516582130316, 'completed': 0.0015606710885680843, 'ﬁnal': 0.0015606710885680843, 'completing': 0.0046820132657042525, 'described': 0.0046820132657042525, 'follows': 0.006632852126414358, 'building': 0.0007803355442840422, 'upon': 0.0011705033164260631, '1995': 0.0015606710885680843, 'advantage': 0.0011705033164260631, 'deﬁned': 0.01131486539211861, 'amount': 0.0046820132657042525, 'eﬀort': 0.0007803355442840422, 'course': 0.004291845493562232, 'action': 0.053843152555598905, 'rather': 0.0027311744049941474, 'particular': 0.0031213421771361686, 'condition': 0.019898556379243076, 'least': 0.0039016777214202106, 'simple': 0.00351150994927819, 'form': 0.00702301989855638, 'requires': 0.0148263753413968, 'On': 0.0007803355442840422, 'hand': 0.0019508388607101053, 'guess': 0.0015606710885680843, 'desirability': 0.0003901677721420211, 'might': 0.0019508388607101053, 'terminate': 0.005072181037846274, 'although': 0.0015606710885680843, 'show': 0.008193523214982443, 'guesses': 0.0023410066328521262, 'revised': 0.0003901677721420211, 'automatically': 0.0003901677721420211, 'potential': 0.0007803355442840422, 'drawback': 0.0011705033164260631, 'learned': 0.010534529847834569, 'may': 0.0074131876706984, 'suboptimal': 0.0015606710885680843, 'programmer-provided': 0.0003901677721420211, 'If': 0.012095200936402654, 'constraints': 0.0039016777214202106, 'poorly': 0.0003901677721420211, 'chosen': 0.008583690987124463, 'resulting': 0.008583690987124463, 'Nonetheless': 0.0019508388607101053, 'learn-ing': 0.0003901677721420211, 'guarantee': 0.0011705033164260631, 'best': 0.009754194303550527, 'suﬀers': 0.0003901677721420211, 'additional': 0.0019508388607101053, 'source': 0.005852516582130316, 'suboptimality': 0.0003901677721420211, 'optimality': 0.008583690987124463, 'call': 0.005852516582130316, 'recursive': 0.00702301989855638, 'locally': 0.006242684354272337, 'children': 0.005462348809988295, 'exist': 0.0015606710885680843, 'overall': 0.0031213421771361686, 'avoided': 0.0007803355442840422, 'careful': 0.0007803355442840422, 'deﬁnition': 0.0046820132657042525, 'predicates': 0.0039016777214202106, 'added': 0.0027311744049941474, 'burden': 0.0003901677721420211, 'interesting': 0.0023410066328521262, 'note': 0.0046820132657042525, 'noticed': 0.0003901677721420211, 'previously': 0.0007803355442840422, 'focused': 0.0007803355442840422, 'single': 0.0027311744049941474, 'terminal': 0.01092469761997659, '3': 0.012485368708544674, 'cases': 0.0039016777214202106, 'arise': 0.0011705033164260631, 'whether': 0.0031213421771361686, 'employs': 0.0003901677721420211, 'ignores': 0.0007803355442840422, 'aspects': 0.0011705033164260631, 'For': 0.0148263753413968, 'example': 0.01092469761997659, 'robot': 0.0046820132657042525, 'navigation': 0.004291845493562232, 'choices': 0.0011705033164260631, 'route': 0.0003901677721420211, 'take': 0.0031213421771361686, 'reach': 0.0039016777214202106, 'location': 0.017167381974248927, 'independent': 0.0011705033164260631, 'currently': 0.0007803355442840422, 'carrying': 0.0007803355442840422, 'With': 0.0039016777214202106, 'exceptions': 0.0003901677721420211, 'impact': 0.0003901677721420211, 'accelerating': 0.0007803355442840422, 'tradeoﬀ': 0.0023410066328521262, 'successful': 0.0015606710885680843, 'using': 0.01833788529067499, 'despite': 0.0003901677721420211, 'create': 0.0019508388607101053, 'concerns': 0.0003901677721420211, 'pol-icy': 0.0003901677721420211, 'could': 0.007803355442840421, 'evaluated': 0.0007803355442840422, 'incrementally': 0.0003901677721420211, 'yield': 0.0011705033164260631, 'potentially': 0.0003901677721420211, 'Dietterich': 0.0019508388607101053, 'Ravindran': 0.0011705033164260631, 'generalized': 0.0007803355442840422, 'arbitrary': 0.0027311744049941474, 'subroutines': 0.0039016777214202106, 'executed': 0.01092469761997659, 'non-hierarchically': 0.0007803355442840422, 'order': 0.007803355442840421, 'support': 0.0027311744049941474, 'extra': 0.0003901677721420211, 'required': 0.0046820132657042525, 'Ordinarily': 0.0003901677721420211, 'higher': 0.0027311744049941474, 'levels': 0.0046820132657042525, 'plus': 0.0023410066328521262, 'initial': 0.012095200936402654, 'In': 0.037065938353492, 'exploration': 0.016387046429964885, 'well': 0.0039016777214202106, 'computation': 0.0015606710885680843, 'memory': 0.0015606710885680843, 'consequence': 0.0011705033164260631, 'either': 0.005852516582130316, 'worth': 0.0007803355442840422, 'fourth': 0.0011705033164260631, 'typically': 0.0015606710885680843, 'operate': 0.0003901677721420211, 'particularly': 0.0011705033164260631, 'family': 0.0003901677721420211, 'relied': 0.0003901677721420211, 'unique': 0.006632852126414358, 'employed': 0.005462348809988295, 'mix': 0.0003901677721420211, 'batch': 0.0003901677721420211, 'train': 0.0007803355442840422, 'options': 0.0023410066328521262, 'usually': 0.0015606710885680843, 'assumes': 0.0003901677721420211, 'HAMQ': 0.0031213421771361686, 'Feudal': 0.0015606710885680843, 'ﬂattening': 0.0003901677721420211, 'undesirable': 0.0003901677721420211, 'consequences': 0.0003901677721420211, 'tailored': 0.0003901677721420211, 'converge': 0.008973858759266484, 'well-deﬁned': 0.0007803355442840422, 'present': 0.0019508388607101053, 'fully-online': 0.0003901677721420211, 'theoretically': 0.0003901677721420211, 'substantially': 0.0007803355442840422, 'i.e.': 0.009754194303550527, 'Without': 0.0003901677721420211, 'gives': 0.007803355442840421, 'worse': 0.0007803355442840422, 'remainder': 0.0003901677721420211, 'organized': 0.0003901677721420211, 'After': 0.0011705033164260631, 'introducing': 0.0015606710885680843, 'notation': 0.005462348809988295, 'Section': 0.0023410066328521262, 'illustrate': 0.0003901677721420211, 'sim-ple': 0.0003901677721420211, '4': 0.01872805306281701, 'analytically': 0.0003901677721420211, 'tractable': 0.0003901677721420211, 'version': 0.0019508388607101053, 'MAXQ-0': 0.011705033164260631, 'convergence': 0.004291845493562232, 'recur-sively': 0.0007803355442840422, 'extend': 0.0015606710885680843, 'produce': 0.0023410066328521262, 'theorem': 0.0046820132657042525, 'similarly': 0.0003901677721420211, '5': 0.01131486539211861, 'safely': 0.0007803355442840422, 'incorporated': 0.0003901677721420211, 'State': 0.00351150994927819, 'give': 0.006632852126414358, 'rise': 0.0011705033164260631, 'credit': 0.0011705033164260631, 'assignment': 0.0015606710885680843, 'brieﬂy': 0.0007803355442840422, 'discusses': 0.0003901677721420211, 'solution': 0.005462348809988295, '7': 0.008583690987124463, 'idea': 0.0015606710885680843, 'generality': 0.0003901677721420211, 'They': 0.0023410066328521262, 'importance': 0.0011705033164260631, 'temporal': 0.0027311744049941474, 'tackles': 0.0003901677721420211, 'question': 0.0007803355442840422, 'ability': 0.0003901677721420211, 'Some': 0.0011705033164260631, 'readers': 0.0003901677721420211, 'disappointed': 0.0003901677721420211, 'Our': 0.0023410066328521262, 'philosophy': 0.0003901677721420211, 'developing': 0.0003901677721420211, 'share': 0.0015606710885680843, 'notably': 0.0003901677721420211, 'draw': 0.0011705033164260631, 'inspiration': 0.0003901677721420211, 'development': 0.0003901677721420211, 'Belief': 0.0007803355442840422, 'Networks': 0.0007803355442840422, 'Pearl': 0.0007803355442840422, '1988': 0.0007803355442840422, 'introduced': 0.005852516582130316, 'engineer': 0.0007803355442840422, 'would': 0.009364026531408505, 'describe': 0.0027311744049941474, 'domain': 0.0023410066328521262, 'experts': 0.0003901677721420211, 'necessary': 0.0023410066328521262, 'estimates': 0.0007803355442840422, 'Subsequently': 0.0003901677721420211, 'values': 0.05267264923917284, 'observational': 0.0003901677721420211, 'data': 0.0011705033164260631, 'Most': 0.0003901677721420211, 'recently': 0.0003901677721420211, 'belief': 0.0003901677721420211, 'dependence': 0.0015606710885680843, 'reduced': 0.0007803355442840422, 'likewise': 0.0003901677721420211, 'make': 0.005852516582130316, 'like': 0.0015606710885680843, 'computer': 0.0007803355442840422, 'program': 0.0011705033164260631, 'rely': 0.0007803355442840422, 'modules': 0.0007803355442840422, 'indicate': 0.0015606710885680843, 'permissible': 0.0003901677721420211, 'ways': 0.0007803355442840422, 'invoke': 0.0019508388607101053, 'ﬁll': 0.0007803355442840422, 'implementations': 0.0003901677721420211, 'module': 0.0003901677721420211, 'believe': 0.0015606710885680843, 'tool': 0.0003901677721420211, 'solving': 0.0031213421771361686, 'help': 0.0007803355442840422, 'hope': 0.0003901677721420211, 'subsequent': 0.0007803355442840422, 'automate': 0.0003901677721420211, 'requiring': 0.0011705033164260631, 'Formal': 0.0003901677721420211, 'Deﬁnitions': 0.0007803355442840422, '2.1': 0.0003901677721420211, 'Decision': 0.0039016777214202106, 'Problems': 0.0015606710885680843, 'Semi-Markov': 0.0015606710885680843, 'standard': 0.0011705033164260631, 'restrict': 0.0007803355442840422, 'fully-observable': 0.0003901677721420211, 'stochastic': 0.005462348809988295, 'situation': 0.0011705033164260631, 'modeled': 0.0003901677721420211, 'Problem': 0.0015606710885680843, 'hS': 0.0011705033164260631, 'P': 0.035505267264923916, 'R': 0.02497073741708935, 'P0i': 0.0011705033164260631, '•': 0.0222395630120952, 'S': 0.0011705033164260631, 'At': 0.0039016777214202106, 'observe': 0.0019508388607101053, 'ﬁnite': 0.0011705033164260631, 'Technically': 0.0003901677721420211, 'available': 0.0023410066328521262, 'depends': 0.00351150994927819, 'current': 0.0148263753413968, 'suppress': 0.0007803355442840422, 'When': 0.00702301989855638, '∈': 0.004291845493562232, 'performed': 0.0039016777214202106, 'transition': 0.009754194303550527, 's′': 0.03589543503706594, 'according': 0.00702301989855638, 'distribution': 0.009364026531408505, 's′|s': 0.01131486539211861, 'Similarly': 0.004291845493562232, 'possibly': 0.0007803355442840422, 'To': 0.0148263753413968, 'simplify': 0.0015606710885680843, 'customary': 0.0003901677721420211, 'initiated': 0.0003901677721420211, 'though': 0.0007803355442840422, 'depend': 0.005072181037846274, 'P0': 0.0011705033164260631, 'starting': 0.010144362075692548, 'initialized': 0.0011705033164260631, 'π': 0.08076472883339836, 'mapping': 0.0011705033164260631, 'tells': 0.0031213421771361686, '=': 0.06944986344127975, 'perform': 0.004291845493562232, 's.': 0.005852516582130316, 'consider': 0.010144362075692548, 'settings': 0.0015606710885680843, 'Episodic': 0.0003901677721420211, 'Inﬁnite-Horizon': 0.0003901677721420211, 'episodic': 0.00351150994927819, 'setting': 0.0039016777214202106, 'zero-cost': 0.0003901677721420211, 'absorbing': 0.0011705033164260631, 'lead': 0.0007803355442840422, 'back': 0.0015606710885680843, 'zero': 0.005072181037846274, 'deterministic': 0.005072181037846274, 'proper': 0.0015606710885680843, '—that': 0.0007803355442840422, 'non-zero': 0.0003901677721420211, 'reaching': 0.0023410066328521262, 'started': 0.0015606710885680843, 'ﬁnd': 0.0046820132657042525, 'maximizes': 0.0015606710885680843, 'cumulative': 0.005852516582130316, 'special': 0.0019508388607101053, 'non-positive': 0.0003901677721420211, 'referred': 0.0003901677721420211, 'shortest': 0.0023410066328521262, 'path': 0.006632852126414358, 'viewed': 0.0027311744049941474, 'costs': 0.0019508388607101053, 'lengths': 0.0003901677721420211, 'move': 0.011705033164260631, 'minimum': 0.0003901677721420211, 'inﬁnite': 0.0039016777214202106, 'horizon': 0.0023410066328521262, 'addition': 0.00351150994927819, 'discount': 0.0019508388607101053, 'factor': 0.0027311744049941474, 'γ': 0.004291845493562232, 'minimizes': 0.0003901677721420211, 'discounted': 0.005852516582130316, 'sum': 0.00702301989855638, 'V': 0.05618415918845104, 'executing': 0.008193523214982443, 'Let': 0.020288724151385096, 'rt': 0.0046820132657042525, 'variable': 0.0019508388607101053, 'following': 0.01404603979711276, 'E': 0.0023410066328521262, '{': 0.005462348809988295, '+': 0.040967616074912214, 'rt+1': 0.0007803355442840422, 'rt+2': 0.0003901677721420211, '·': 0.013265704252828716, '|st': 0.0007803355442840422, '}': 0.005462348809988295, 'γrt+1': 0.0007803355442840422, 'γ2rt+2': 0.0007803355442840422, 'st': 0.0015606710885680843, 'equation': 0.007803355442840421, 'reduces': 0.0003901677721420211, 'unless': 0.0007803355442840422, '<': 0.0019508388607101053, 'satisﬁes': 0.0023410066328521262, 'Bellman': 0.005462348809988295, 'cid:12': 0.0031213421771361686, '′': 0.05774483027701912, '|s': 0.024580569644947328, 'γV': 0.0015606710885680843, 'cid:2': 0.0011705033164260631, 'quantity': 0.0031213421771361686, 'right-hand': 0.0046820132657042525, 'side': 0.00351150994927819, 'backed-up': 0.0007803355442840422, 'performing': 0.008193523214982443, 'successor': 0.0003901677721420211, 'computes': 0.005072181037846274, 'received': 0.0023410066328521262, 'weights': 0.0007803355442840422, 'ending': 0.0007803355442840422, '∗': 0.02184939523995318, 'S.': 0.0046820132657042525, '1957': 0.0007803355442840422, 'proved': 0.0023410066328521262, 'cid:3': 0.0011705033164260631, 'max': 0.007803355442840421, 'Xs′': 0.01131486539211861, 'Any': 0.0011705033164260631, 'chooses': 0.0031213421771361686, 'maximum': 0.0015606710885680843, 'denote': 0.0039016777214202106, 'π∗': 0.006242684354272337, 'Note': 0.005072181037846274, 'greedy': 0.016387046429964885, 'respect': 0.0027311744049941474, 'Closely': 0.0003901677721420211, 'so-called': 0.0007803355442840422, 'action-value': 0.0039016777214202106, 'Watkins': 0.0019508388607101053, '1989': 0.0011705033164260631, 'Qπ': 0.011705033164260631, 'n': 0.0027311744049941474, '6': 0.010144362075692548, 'cid:20': 0.0003901677721420211, 'thereafter': 0.0007803355442840422, 'γQπ': 0.0003901677721420211, 'written': 0.0019508388607101053, 'Q∗': 0.005462348809988295, 'a′': 0.008583690987124463, 'cid:21': 0.0003901677721420211, 'policies—they': 0.0003901677721420211, 'diﬀer': 0.0027311744049941474, 'break': 0.0007803355442840422, 'ties': 0.0019508388607101053, 'identical': 0.0023410066328521262, 'denoted': 0.0015606710885680843, 'ω': 0.0027311744049941474, 'That': 0.0011705033164260631, 'anti-symmetric': 0.0003901677721420211, 'transitive': 0.0003901677721420211, 'relation': 0.0003901677721420211, 'a1': 0.0074131876706984, 'a2': 0.004291845493562232, 'true': 0.005072181037846274, 'iﬀ': 0.0015606710885680843, 'preferred': 0.0003901677721420211, 'ordered': 0.012485368708544674, 'πω': 0.0011705033164260631, 'breaks': 0.0011705033164260631, 'suppose': 0.004291845493562232, 'Then': 0.01092469761997659, 'choose': 0.005462348809988295, 'discrete-time': 0.0003901677721420211, 'semi-Markov': 0.0007803355442840422, 'Process': 0.0011705033164260631, 'SMDP': 0.004291845493562232, 'generalization': 0.0003901677721420211, 'Deci-sion': 0.0003901677721420211, 'N': 0.04603979711275848, 'steps': 0.006242684354272337, 'joint': 0.0007803355442840422, 'result': 0.0074131876706984, 'changed': 0.0003901677721420211, '.1': 0.0003901677721420211, 'straightforward': 0.0011705033164260631, 'modify': 0.0003901677721420211, 'γN': 0.010534529847834569, 'h': 0.0003901677721420211, 'change': 0.0023410066328521262, 'taken': 0.0027311744049941474, 'raised': 0.0003901677721420211, 'reﬂect': 0.0007803355442840422, 'elapse': 0.0003901677721420211, 'a.': 0.0011705033164260631, 'expectation': 0.0011705033164260631, 'linear': 0.0007803355442840422, 'operator': 0.0027311744049941474, 'write': 0.004291845493562232, 'equations': 0.0046820132657042525, 'rewrite': 0.0015606710885680843, 'diﬀerence': 0.0019508388607101053, '1This': 0.0003901677721420211, 'formalization': 0.0003901677721420211, 'slightly': 0.0023410066328521262, 'formulation': 0.0011705033164260631, 'SMDPs': 0.0003901677721420211, 'separates': 0.0003901677721420211, 'F': 0.0007803355442840422, 't|s': 0.0003901677721420211, 'units': 0.0019508388607101053, 'integer-valued': 0.0003901677721420211, 'durations': 0.0003901677721420211, '2.2': 0.0003901677721420211, 'Algorithms': 0.0003901677721420211, 'access': 0.0003901677721420211, 'unknown': 0.0003901677721420211, 'protocol': 0.0003901677721420211, 'told': 0.0003901677721420211, '⊆': 0.0003901677721420211, 'executable': 0.0007803355442840422, 'executes': 0.0027311744049941474, 'causes': 0.0007803355442840422, 'returns': 0.005072181037846274, 'r.': 0.0007803355442840422, 'contains': 0.0019508388607101053, 'reset': 0.0003901677721420211, 'drawn': 0.0015606710885680843, 'observed': 0.0011705033164260631, 'well-known': 0.0003901677721420211, '1992': 0.0007803355442840422, 'SARSA': 0.0023410066328521262, '0': 0.023800234100663287, 'Rummery': 0.0007803355442840422, 'Niranjan': 0.0007803355442840422, '1994': 0.0019508388607101053, 'Both': 0.0011705033164260631, 'maintain': 0.0003901677721420211, 'tabular': 0.0003901677721420211, 'Every': 0.0003901677721420211, 'entry': 0.0003901677721420211, 'table': 0.0011705033164260631, 'arbitrarily': 0.0003901677721420211, 'r': 0.031213421771361684, 'performs': 0.0027311744049941474, 'update': 0.008583690987124463, 'Qt': 0.0039016777214202106, '−': 0.009364026531408505, 'αt': 0.01443620756925478, 'Qt−1': 0.0011705033164260631, '[': 0.011705033164260631, ']': 0.01092469761997659, 'rate': 0.006632852126414358, 'parameter': 0.005462348809988295, 'Jaakkola': 0.0019508388607101053, 'Jordan': 0.0007803355442840422, 'prove': 0.005852516582130316, 'tries': 0.0007803355442840422, 'every': 0.0031213421771361686, 'inﬁnitely': 0.0011705033164260631, 'often': 0.0023410066328521262, 'lim': 0.0015606710885680843, 'T': 0.007803355442840421, '→∞': 0.0011705033164260631, 't=1': 0.0011705033164260631, 'X': 0.005072181037846274, '∞': 0.0019508388607101053, 'T→∞': 0.0003901677721420211, 'Xt=1': 0.0003901677721420211, 'α2': 0.0007803355442840422, 'Their': 0.0007803355442840422, 'proof': 0.0039016777214202106, 'holds': 0.0015606710885680843, 'discussed': 0.0019508388607101053, 'inﬁnite-horizon': 0.0007803355442840422, 'observing': 0.0011705033164260631, 'choosing': 0.0023410066328521262, 'γQt−1': 0.0003901677721420211, 'key': 0.0015606710885680843, 'appears': 0.0011705033164260631, 'place': 0.0015606710885680843, 'uses': 0.0011705033164260631, 'Littman': 0.0007803355442840422, 'Szepesv´ari': 0.0007803355442840422, 'decreases': 0.0007803355442840422, 'Equation': 0.0039016777214202106, 'GLIE': 0.006242684354272337, 'Deﬁnition': 0.005462348809988295, 'limit': 0.0015606710885680843, 'satisfying': 0.0003901677721420211, 'Each': 0.005462348809988295, 'visited': 0.0027311744049941474, 'Q-value': 0.0003901677721420211, '8': 0.0074131876706984, 'Y': 0.006632852126414358, 'G': 0.0019508388607101053, 'B': 0.0023410066328521262, 'Figure': 0.01404603979711276, 'Taxi': 0.005462348809988295, 'Domain': 0.0007803355442840422, 'Value': 0.0007803355442840422, 'Function': 0.0007803355442840422, 'Decomposition': 0.0015606710885680843, 'center': 0.0003901677721420211, 'describes': 0.0007803355442840422, 'decompose': 0.0019508388607101053, 'collection': 0.0007803355442840422, 'subsubtasks': 0.0003901677721420211, '3.1': 0.0007803355442840422, 'Motivating': 0.0003901677721420211, 'Example': 0.0003901677721420211, 'concrete': 0.0003901677721420211, '5-by-5': 0.0003901677721420211, 'grid': 0.0003901677721420211, 'inhabited': 0.0003901677721420211, 'taxi': 0.0222395630120952, 'four': 0.01131486539211861, 'specially-designated': 0.0003901677721420211, 'locations': 0.01092469761997659, 'marked': 0.0003901677721420211, 'ed': 0.0003901677721420211, 'lue': 0.0003901677721420211, 'reen': 0.0003901677721420211, 'ellow': 0.0003901677721420211, 'episode': 0.0015606710885680843, 'starts': 0.0023410066328521262, 'randomly-chosen': 0.0011705033164260631, 'square': 0.0027311744049941474, 'passenger': 0.02145922746781116, 'randomly': 0.0007803355442840422, 'wishes': 0.0015606710885680843, 'transported': 0.0003901677721420211, 'go': 0.00351150994927819, 'pick': 0.0015606710885680843, 'destination': 0.009364026531408505, 'put': 0.0003901677721420211, 'things': 0.0007803355442840422, 'uniform': 0.0003901677721420211, 'drop': 0.0015606710885680843, 'oﬀ': 0.0011705033164260631, 'he/she': 0.0011705033164260631, 'already': 0.0011705033164260631, 'located': 0.0007803355442840422, '!': 0.0011705033164260631, 'ends': 0.0011705033164260631, 'deposited': 0.0003901677721420211, 'six': 0.0011705033164260631, 'primitive': 0.030823253999219664, 'North': 0.008583690987124463, 'South': 0.00702301989855638, 'East': 0.008973858759266484, 'West': 0.005072181037846274, 'b': 0.0046820132657042525, 'Pickup': 0.0046820132657042525, 'c': 0.0027311744049941474, 'Putdown': 0.0039016777214202106, '−1': 0.006632852126414358, '+20': 0.0007803355442840422, 'successfully': 0.0007803355442840422, 'delivering': 0.0007803355442840422, '−10': 0.0007803355442840422, 'illegally': 0.0003901677721420211, 'cause': 0.0015606710885680843, 'hit': 0.0003901677721420211, 'wall': 0.0046820132657042525, 'no-op': 0.0003901677721420211, 'usual': 0.0011705033164260631, 'seek': 0.0019508388607101053, 'per': 0.0007803355442840422, '500': 0.0027311744049941474, '25': 0.0031213421771361686, 'squares': 0.0011705033164260631, 'counting': 0.0003901677721420211, 'destinations': 0.0007803355442840422, 'sub-tasks': 0.0003901677721420211, 'Get': 0.010534529847834569, 'Deliver': 0.0007803355442840422, 'turn': 0.0019508388607101053, 'navigating': 0.0011705033164260631, 'illustrates': 0.0003901677721420211, 'sharing': 0.0023410066328521262, 'obvious—for': 0.0003901677721420211, 'passen-ger': 0.0003901677721420211, 'picking': 0.0007803355442840422, 'temporally': 0.0007803355442840422, 'extended': 0.0019508388607101053, 'numbers': 0.0011705033164260631, 'depending': 0.0015606710885680843, 'distance': 0.0011705033164260631, 'top': 0.004291845493562232, 'level': 0.005852516582130316, 'get': 0.0015606710885680843, 'deliver': 0.0011705033164260631, 'expressed': 0.0003901677721420211, 'simply': 0.0015606710885680843, '9': 0.005462348809988295, 'perhaps': 0.0011705033164260631, 'less': 0.0019508388607101053, 'obvious': 0.0007803355442840422, 'Consider': 0.0046820132657042525, 'getting': 0.00351150994927819, 'While': 0.0003901677721420211, 'solved': 0.0007803355442840422, 'completely': 0.0015606710885680843, 'irrelevant—it': 0.0003901677721420211, 'aﬀect': 0.0003901677721420211, 'nagivation': 0.0003901677721420211, 'pickup': 0.0011705033164260631, 'Perhaps': 0.0003901677721420211, 'importantly': 0.0003901677721420211, 'identity': 0.0003901677721420211, 'irrelevant': 0.009754194303550527, 'critical': 0.0003901677721420211, 'learn': 0.006242684354272337, 'solve': 0.0031213421771361686, 'shared': 0.0003901677721420211, 'supports': 0.0011705033164260631, 'construct': 0.0023410066328521262, 'tasks': 0.00702301989855638, 'Navigate': 0.010534529847834569, 'indicated': 0.0007803355442840422, 't.': 0.0007803355442840422, 'Put': 0.005462348809988295, 'Root': 0.01131486539211861, 'whole': 0.0007803355442840422, 'subgoal': 0.0023410066328521262, 'terminates': 0.011705033164260631, 'achieved': 0.0007803355442840422, 'prim-itive': 0.0003901677721420211, 'All': 0.0027311744049941474, 'summarized': 0.0007803355442840422, 'directed': 0.0003901677721420211, 'acyclic': 0.0003901677721420211, 'graph': 0.03199375731564573, 'node': 0.04486929379633242, 'corresponds': 0.0007803355442840422, 'edge': 0.0003901677721420211, 'child': 0.01131486539211861, 'f': 0.0003901677721420211, 'ormal/actual': 0.0003901677721420211, 't/source': 0.0011705033164260631, 'bound': 0.0023410066328521262, 'actual': 0.0027311744049941474, 'Now': 0.00702301989855638, 'refer': 0.0019508388607101053, 'parent': 0.006242684354272337, 'invoking': 0.0015606710885680843, 'ordinary': 0.0011705033164260631, 'subroutine-call-and-return': 0.0003901677721420211, 'calling': 0.0003901677721420211, 'calls': 0.0011705033164260631, 'And': 0.0011705033164260631, 'enters': 0.0019508388607101053, '3.2': 0.0003901677721420211, 'far': 0.0007803355442840422, '10': 0.01092469761997659, 't/destination': 0.0007803355442840422, 'M': 0.013265704252828716, 'decomposes': 0.0011705033164260631, 'M0': 0.0031213421771361686, 'M1': 0.0007803355442840422, 'Mn': 0.0015606710885680843, 'convention': 0.0003901677721420211, 'root': 0.0046820132657042525, 'solves': 0.0003901677721420211, 'entire': 0.0039016777214202106, 'original': 0.005072181037846274, 'unparameterized': 0.0003901677721420211, 'three-tuple': 0.0003901677721420211, 'hTi': 0.0003901677721420211, 'Ai': 0.00351150994927819, '˜Rii': 0.0003901677721420211, 'Ti': 0.0046820132657042525, 'si': 0.0003901677721420211, 'partitions': 0.0003901677721420211, 'active': 0.0003901677721420211, 'Si': 0.0027311744049941474, 'Mi': 0.011705033164260631, 'indexes': 0.0003901677721420211, 'Mj': 0.0015606710885680843, 'parameters': 0.0074131876706984, 'multiple': 0.0011705033164260631, 'times': 0.0046820132657042525, 'occurrence': 0.0003901677721420211, 'another': 0.0031213421771361686, 'technically': 0.0003901677721420211, '˜Ri': 0.010534529847834569, 'pseudo-reward': 0.01404603979711276, 'speciﬁes': 0.0027311744049941474, 'desirable': 0.0003901677721420211, 'non-goal': 0.0011705033164260631, 'always': 0.005462348809988295, 'immediately': 0.0007803355442840422, 'uniformly': 0.0007803355442840422, 'binding': 0.0007803355442840422, 'distinct': 0.0015606710885680843, 'think': 0.0007803355442840422, 'part': 0.0046820132657042525, 'name': 0.0031213421771361686, 'practice': 0.0023410066328521262, 'implement': 0.0007803355442840422, 'parameterized': 0.0027311744049941474, 'parameterizing': 0.0003901677721420211, 'various': 0.0019508388607101053, 'components': 0.0011705033164260631, 'omit': 0.0007803355442840422, 'bindings': 0.0023410066328521262, '11': 0.005462348809988295, 'Table': 0.005462348809988295, 'Pseudo-Code': 0.0003901677721420211, 'Execution': 0.0015606710885680843, 'Policy': 0.0023410066328521262, 'Kt': 0.0039016777214202106, 'stack': 0.00702301989855638, 'fi': 0.0011705033164260631, 'fa': 0.0011705033164260631, 'πi': 0.005462348809988295, 'push': 0.0007803355442840422, 'onto': 0.0027311744049941474, 'nil': 0.0003901677721420211, 'pop': 0.0007803355442840422, 'Execute': 0.0003901677721420211, 'st+1': 0.0007803355442840422, '12': 0.009364026531408505, '13': 0.0031213421771361686, 'terminated': 0.00351150994927819, '14': 0.00351150994927819, '15': 0.00351150994927819, 'Kt+1': 0.0007803355442840422, 'containing': 0.0011705033164260631, 'π0': 0.0011705033164260631, 'πn': 0.0003901677721420211, 'terminology': 0.0003901677721420211, 'terminating': 0.0003901677721420211, 'β': 0.0015606710885680843, 'pseudo-code': 0.0027311744049941474, 'description': 0.0011705033164260631, 'discipline': 0.0007803355442840422, 'languages': 0.0003901677721420211, 'contents': 0.0031213421771361686, 'pushdown': 0.0003901677721420211, 'invoked': 0.0023410066328521262, 'pushed': 0.0003901677721420211, 'popped': 0.0003901677721420211, 'sometimes': 0.0007803355442840422, 'implicitly': 0.0003901677721420211, 'yields': 0.0003901677721420211, 'Because': 0.0039016777214202106, 'non-Markovian': 0.0003901677721420211, 'maps': 0.0011705033164260631, 'K': 0.0007803355442840422, 'assign': 0.0003901677721420211, 'combinations': 0.0011705033164260631, 'K.': 0.0011705033164260631, 'hs': 0.0019508388607101053, 'Ki': 0.0003901677721420211, 'primarily': 0.0011705033164260631, 'interested': 0.0007803355442840422, 'policy—that': 0.0003901677721420211, 'empty': 0.0003901677721420211, 'nili': 0.0003901677721420211, 'beginning': 0.0011705033164260631, 'projected': 0.004291845493562232, '3.3': 0.0003901677721420211, 'Projected': 0.0003901677721420211, 'decomposed': 0.0007803355442840422, 'hierarchically': 0.008193523214982443, 'Theorem': 0.0039016777214202106, 'Given': 0.0019508388607101053, 'Ma': 0.0015606710885680843, 'immediate': 0.0011705033164260631, 'Proof': 0.006632852126414358, 'descendants': 0.00351150994927819, 'Be-cause': 0.0003901677721420211, 'stationary': 0.0011705033164260631, 'continues': 0.0003901677721420211, 'in-i': 0.0003901677721420211, 'voked': 0.0003901677721420211, 'γurt+u': 0.0007803355442840422, 'u=0': 0.0003901677721420211, 'Xu=N': 0.0003901677721420211, 'summation': 0.0007803355442840422, 'words': 0.0019508388607101053, 'term': 0.0039016777214202106, 'Q.E.D': 0.006242684354272337, 'obtain': 0.006632852126414358, 'switch': 0.0007803355442840422, 'handle': 0.0003901677721420211, 're-state': 0.0003901677721420211, 'right-most': 0.0007803355442840422, 'marginalizes': 0.0003901677721420211, 'away': 0.0019508388607101053, 'C': 0.046429964884900504, 'equal': 0.00351150994927819, 'completion': 0.008973858759266484, 'begins': 0.0007803355442840422, 'express': 0.0015606710885680843, 're-express': 0.0003901677721420211, 'composite': 0.005072181037846274, 'j': 0.03589543503706594, 'n.': 0.0003901677721420211, 'fundamental': 0.0011705033164260631, 'quantities': 0.0019508388607101053, 'stored': 0.005072181037846274, 'non-primitive': 0.0007803355442840422, 'easier': 0.0015606710885680843, 'programmers': 0.0003901677721420211, 'debug': 0.0003901677721420211, 'decompositions': 0.0003901677721420211, 'de-veloped': 0.0003901677721420211, 'graphical': 0.0003901677721420211, 'nodes': 0.019508388607101055, 'Max': 0.02301989855637924, 'correspond': 0.0007803355442840422, 'decomposition—there': 0.0003901677721420211, 'including': 0.0015606710885680843, 'stores': 0.0031213421771361686, 'storing': 0.0011705033164260631, 'Speciﬁcally': 0.0015606710885680843, 'computing': 0.0019508388607101053, 'obtained': 0.0015606710885680843, 'asking': 0.0007803355442840422, 'corresponding': 0.0031213421771361686, 'adding': 0.0007803355442840422, 's1': 0.017167381974248927, 'Suppose': 0.0027311744049941474, 'evaluating': 0.0003901677721420211, 'superscript': 0.0007803355442840422, '*': 0.0019508388607101053, 'reduce': 0.0015606710885680843, 'clutter': 0.0003901677721420211, 'unit': 0.0019508388607101053, 'putdown': 0.0011705033164260631, 'delivered': 0.0003901677721420211, 'gets': 0.0003901677721420211, 'net': 0.0007803355442840422, '+10': 0.0003901677721420211, 'MaxRoot': 0.005072181037846274, 'consults': 0.0011705033164260631, 'ﬁnds': 0.0019508388607101053, 'πRoot': 0.0003901677721420211, 'asks': 0.0011705033164260631, 'QGet': 0.0015606710885680843, 'QPut': 0.0011705033164260631, 'MaxGet': 0.004291845493562232, 'MaxPut': 0.0027311744049941474, 'QPickup': 0.0011705033164260631, 'QNavigateForGet': 0.0019508388607101053, 'QNavigateForPut': 0.0011705033164260631, 'QPutdown': 0.0015606710885680843, 'MaxNavigate': 0.004291845493562232, 'QNorth': 0.0023410066328521262, 'QEast': 0.0015606710885680843, 'QSouth': 0.0015606710885680843, 'QWest': 0.0015606710885680843, 'customer': 0.0003901677721420211, '20': 0.0019508388607101053, 'ask': 0.0003901677721420211, 'estimate': 0.0007803355442840422, 'dictates': 0.0003901677721420211, 're-ward': 0.0003901677721420211, 'knows': 0.0003901677721420211, 'R.': 0.004291845493562232, 'looks': 0.0007803355442840422, 'MaxNorth': 0.0007803355442840422, 'determine': 0.0007803355442840422, 'computations': 0.0007803355442840422, 'conclude': 0.0023410066328521262, '-2': 0.0007803355442840422, '-1': 0.0015606710885680843, 'Computing': 0.0003901677721420211, 'left': 0.005852516582130316, 'returned': 0.0011705033164260631, '−2': 0.0015606710885680843, 'collect': 0.0003901677721420211, 'end': 0.0074131876706984, '16': 0.0046820132657042525, 'XXXXXXX': 0.0003901677721420211, 'cid:24': 0.0027311744049941474, 'PPPPP': 0.0003901677721420211, 'cid:16': 0.0019508388607101053, 'am−1': 0.0011705033164260631, 'cid:26': 0.0015606710885680843, 'Z': 0.0007803355442840422, 'ZZ': 0.0003901677721420211, 'r1': 0.0007803355442840422, 'r2': 0.0003901677721420211, 'r3': 0.0003901677721420211, 'r4': 0.0003901677721420211, 'r5': 0.0003901677721420211, 'r8': 0.0003901677721420211, 'r9': 0.0003901677721420211, 'r10': 0.0003901677721420211, 'r11': 0.0003901677721420211, 'r12': 0.0003901677721420211, 'r13': 0.0003901677721420211, 'r14': 0.0007803355442840422, 'sequence': 0.0039016777214202106, 'a0': 0.0003901677721420211, 'going': 0.0015606710885680843, 'leaf': 0.008973858759266484, 'summarize': 0.0003901677721420211, 'presentation': 0.0003901677721420211, 'section': 0.00351150994927819, 'internal': 0.005072181037846274, 'computed': 0.008973858759266484, 'induction': 0.0023410066328521262, 'apply': 0.006242684354272337, 'Q.': 0.0003901677721420211, 'E.': 0.0023410066328521262, 'D.': 0.0019508388607101053, 'mention': 0.0003901677721420211, 'used': 0.0027311744049941474, 'captures': 0.0011705033164260631, 'address': 0.0003901677721420211, 'subject': 0.0007803355442840422, 'next': 0.0011705033164260631, 'Algorithm': 0.0007803355442840422, 'exactly': 0.0031213421771361686, 'hoping': 0.0003901677721420211, 'Of': 0.0019508388607101053, 'imposes': 0.0011705033164260631, '17': 0.0031213421771361686, 'forms': 0.0027311744049941474, 'available—the': 0.0003901677721420211, 'allowed': 0.0003901677721420211, 'Mj1': 0.0003901677721420211, 'Mjk': 0.0003901677721420211, 'involve': 0.0003901677721420211, 'Mji': 0.0003901677721420211, 'run': 0.0007803355442840422, 'Tji': 0.0003901677721420211, 'pass': 0.0015606710885680843, 'subset': 0.0007803355442840422, 'sets': 0.0011705033164260631, 'Tj1': 0.0003901677721420211, 'Tjk': 0.0003901677721420211, 'shares': 0.0003901677721420211, 'reﬁnement': 0.0003901677721420211, 'constrained': 0.0007803355442840422, 'lower': 0.0031213421771361686, 'constructed': 0.0007803355442840422, 'concatenating': 0.0003901677721420211, 'purpose': 0.0019508388607101053, 'imposing': 0.0007803355442840422, 'prior': 0.0027311744049941474, 'thereby': 0.0003901677721420211, 'size': 0.0003901677721420211, 'searched': 0.0003901677721420211, 'impossible': 0.0007803355442840422, 'achieves': 0.0019508388607101053, 'highest': 0.0003901677721420211, 'among': 0.0015606710885680843, '1998b': 0.0011705033164260631, 'hier-archically': 0.0003901677721420211, 'Incidentally': 0.0003901677721420211, 'trivial': 0.0003901677721420211, 'op-tions': 0.0003901677721420211, 'hard': 0.0011705033164260631, 'say': 0.0003901677721420211, 'anything': 0.0003901677721420211, 'speed': 0.0007803355442840422, 'hinder': 0.0003901677721420211, 'et': 0.0007803355442840422, 'al.': 0.0007803355442840422, 'weaker': 0.0011705033164260631, 'Mk': 0.0011705033164260631, 'πk': 0.0019508388607101053, 'πj': 0.0003901677721420211, 'reason': 0.0015606710885680843, 'without': 0.008193523214982443, 'reference': 0.0007803355442840422, 'context': 0.0019508388607101053, 'context-free': 0.0011705033164260631, 'property': 0.0019508388607101053, 're-use': 0.0011705033164260631, 'essential': 0.0003901677721420211, 'Before': 0.0003901677721420211, 'proceed': 0.0003901677721420211, 'diﬀers': 0.0003901677721420211, '18': 0.0019508388607101053, 'QExit': 0.0003901677721420211, 'QGotoGoal': 0.0007803355442840422, 'MaxExit': 0.0003901677721420211, 'MaxGotoGoal': 0.0015606710885680843, 'QExitNorth': 0.0003901677721420211, 'QExitSouth': 0.0003901677721420211, 'QExitEast': 0.0003901677721420211, 'QNorthG': 0.0007803355442840422, 'QSouthG': 0.0007803355442840422, 'QEastG': 0.0007803355442840422, 'associated': 0.0027311744049941474, 'right': 0.005072181037846274, 'diagram': 0.0003901677721420211, 'shaded': 0.0015606710885680843, 'cells': 0.0023410066328521262, 'globally': 0.0011705033164260631, 'examples': 0.0007803355442840422, 'maze': 0.004291845493562232, 'Figures': 0.0003901677721420211, 'somewhere': 0.0003901677721420211, 'room': 0.010534529847834569, 'Exit': 0.0039016777214202106, 'exits': 0.0007803355442840422, '˜R': 0.0074131876706984, 'GotoGoal': 0.005462348809988295, 'reaches': 0.005462348809988295, 'G.': 0.0027311744049941474, 'arrows': 0.0011705033164260631, 'exit': 0.0023410066328521262, 'follow': 0.0007803355442840422, 'ﬁne': 0.0003901677721420211, 'neither': 0.0007803355442840422, 'exists': 0.0019508388607101053, 'upper': 0.0019508388607101053, 'door': 0.0019508388607101053, 'region': 0.0007803355442840422, 'moment': 0.0003901677721420211, 'doorway': 0.0007803355442840422, 'ﬁx': 0.0003901677721420211, 'starred': 0.0015606710885680843, '−6': 0.0003901677721420211, 'recursively-optimal': 0.004291845493562232, 'pointed': 0.0007803355442840422, 'updates': 0.0031213421771361686, '19': 0.0015606710885680843, 'repeated': 0.0007803355442840422, 'speedup': 0.0003901677721420211, '1998a': 0.0007803355442840422, 'proposed': 0.0003901677721420211, 'constructs': 0.0007803355442840422, 'His': 0.0003901677721420211, 'approximated': 0.0003901677721420211, 'desired': 0.0015606710885680843, 'degree': 0.0007803355442840422, 'quite': 0.0015606710885680843, 'ineﬃcient': 0.0003901677721420211, 'relies': 0.0007803355442840422, '|Si|': 0.0003901677721420211, 'suggests': 0.0011705033164260631, 'principle': 0.0003901677721420211, 'small': 0.0015606710885680843, 'alternative': 0.0007803355442840422, 'handled': 0.0011705033164260631, 'except': 0.0019508388607101053, 'permitting': 0.0003901677721420211, 'simpliﬁed': 0.0003901677721420211, 'Gi': 0.0019508388607101053, 'constant': 0.0015606710885680843, '−100': 0.0003901677721420211, 'tested': 0.0003901677721420211, 'worked': 0.0003901677721420211, 'found': 0.0007803355442840422, 'mistakes': 0.0003901677721420211, 'carefully': 0.0007803355442840422, 'looping': 0.0003901677721420211, 'permit': 0.0023410066328521262, 'satisﬁed': 0.006242684354272337, 'natural': 0.0007803355442840422, 'since': 0.0019508388607101053, 'left-hand': 0.0011705033164260631, 'attempt': 0.0011705033164260631, 'nearest': 0.00351150994927819, 'easily': 0.0027311744049941474, 'near': 0.0011705033164260631, 'loop': 0.0015606710885680843, 'forever': 0.0003901677721420211, 'trying': 0.0007803355442840422, 'leave': 0.0007803355442840422, 'entering': 0.0007803355442840422, 'avoiding': 0.0007803355442840422, 'undesired': 0.0003901677721420211, 'bugs': 0.0003901677721420211, 'applies': 0.0015606710885680843, 'properties': 0.0019508388607101053, 'works': 0.0011705033164260631, 'count': 0.0031213421771361686, 'appropriately': 0.0003901677721420211, 'estimated': 0.0019508388607101053, 'one-step': 0.0046820132657042525, 'gradually': 0.0015606710885680843, 'decreased': 0.0015606710885680843, 'Vt': 0.008973858759266484, 'modiﬁed': 0.0019508388607101053, 'versions': 0.0007803355442840422, 'MaxNode': 0.0015606710885680843, 'receive': 0.0019508388607101053, 'Vt+1': 0.0007803355442840422, 'return': 0.0031213421771361686, 'else': 0.0019508388607101053, 'false': 0.0007803355442840422, 'πx': 0.0039016777214202106, 'Ct+1': 0.0015606710885680843, 'Ct': 0.008583690987124463, 'Pseudo-code': 0.0007803355442840422, 'Greedy': 0.0019508388607101053, 'Graph': 0.0003901677721420211, 'EvaluateMaxNode': 0.0046820132657042525, 'hVt': 0.0007803355442840422, 'ii': 0.0003901677721420211, 'hV': 0.0007803355442840422, 'aji': 0.0003901677721420211, 'jhg': 0.0007803355442840422, 'argmaxj': 0.0003901677721420211, 'ajhg': 0.0003901677721420211, '//': 0.0023410066328521262, 'maxa': 0.0003901677721420211, 'changes': 0.0015606710885680843, 'compared': 0.0007803355442840422, 'Equations': 0.0003901677721420211, 'superscripts': 0.0003901677721420211, 'implements': 0.0003901677721420211, 'depth-ﬁrst': 0.0007803355442840422, 'returning': 0.0007803355442840422, 'needed': 0.0007803355442840422, 'later': 0.0003901677721420211, 'thing': 0.0031213421771361686, '21': 0.0015606710885680843, 'Limit': 0.0003901677721420211, 'Inﬁnite': 0.0003901677721420211, 'Explo-ration': 0.0003901677721420211, 'favor': 0.0011705033164260631, 'ap-pears': 0.0003901677721420211, 'earliest': 0.0003901677721420211, 'ensure': 0.0003901677721420211, 'uniquely-deﬁned': 0.0003901677721420211, 'choice': 0.0046820132657042525, 'adopted': 0.0003901677721420211, 'descendant': 0.0015606710885680843, 'establish': 0.0011705033164260631, 'ordering': 0.0027311744049941474, 'left-to-right': 0.0003901677721420211, 'numbering': 0.0003901677721420211, 'lowest-numbered': 0.0003901677721420211, 'consequently': 0.0003901677721420211, 'subscript': 0.0003901677721420211, 'H': 0.008583690987124463, '>': 0.0011705033164260631, 'constants': 0.0007803355442840422, 'assume': 0.0003901677721420211, '|Vt': 0.0011705033164260631, '|': 0.0039016777214202106, '|Ct': 0.0011705033164260631, 'bounded': 0.0011705033164260631, 'argument': 0.0023410066328521262, 'theory': 0.0007803355442840422, 'Lemma': 0.006632852126414358, 'Proposition': 0.0003901677721420211, '4.5': 0.0003901677721420211, 'x': 0.03823644166991806, 'U': 0.005072181037846274, 'wt': 0.0023410066328521262, 'ut': 0.0015606710885680843, 'Ft': 0.0003901677721420211, 'r0': 0.0015606710885680843, 'w0': 0.0003901677721420211, 'wt−1': 0.0003901677721420211, 'α0': 0.0003901677721420211, '∀x': 0.0003901677721420211, 'history': 0.0003901677721420211, 'it-eration': 0.0003901677721420211, '≥': 0.0003901677721420211, 'satisfy': 0.0027311744049941474, 'noise': 0.0003901677721420211, '|Ft': 0.0007803355442840422, 'norm': 0.0027311744049941474, '||·||': 0.0003901677721420211, 'Rn': 0.0003901677721420211, 'w2': 0.0003901677721420211, '≤': 0.0039016777214202106, 'A+B||rt||2': 0.0003901677721420211, '22': 0.0015606710885680843, 'vector': 0.0039016777214202106, 'r∗': 0.0011705033164260631, 'ξ': 0.0015606710885680843, 'scalar': 0.0007803355442840422, '||U': 0.0007803355442840422, '||ξ': 0.0039016777214202106, 'β||rt': 0.0003901677721420211, 'e': 0.006632852126414358, 'nonnegative': 0.0003901677721420211, 'θt': 0.0019508388607101053, '|ut': 0.0007803355442840422, '||rt||ξ': 0.0003901677721420211, '||': 0.0003901677721420211, 'denotes': 0.0003901677721420211, 'weighted': 0.0027311744049941474, '||A||ξ': 0.0003901677721420211, '|A': 0.0003901677721420211, 'inductive': 0.0011705033164260631, 'leaves': 0.00351150994927819, 'toward': 0.0015606710885680843, 'clock': 0.0007803355442840422, 'base': 0.0007803355442840422, 'therefore': 0.0019508388607101053, 'Pt': 0.008973858759266484, 'descendent': 0.0007803355442840422, 'applied': 0.00351150994927819, 'What': 0.0007803355442840422, 'remains': 0.0003901677721420211, 'lemma': 0.004291845493562232, 'state-action': 0.0003901677721420211, 'pair': 0.0015606710885680843, 'completion-cost': 0.0007803355442840422, 'Deﬁne': 0.0015606710885680843, 'assuming': 0.0003901677721420211, 'probabilities': 0.0015606710885680843, 'converged': 0.0015606710885680843, 'formula': 0.0007803355442840422, 'rule': 0.0027311744049941474, 'Line': 0.0007803355442840422, 'cid:18': 0.0015606710885680843, 'cid:19': 0.0015606710885680843, '23': 0.0015606710885680843, 'Here': 0.0015606710885680843, 'sample': 0.0011705033164260631, 'full': 0.0011705033164260631, 'transitions': 0.0015606710885680843, 'up-date': 0.0003901677721420211, 'verify': 0.0007803355442840422, 'Condition': 0.00351150994927819, 'assumed': 0.0003901677721420211, 'sampled': 0.0003901677721420211, 'pseudo-contraction': 0.0015606710885680843, 'derive': 0.0007803355442840422, '||T': 0.0003901677721420211, 'β||Qt': 0.0003901677721420211, 'contraction': 0.0011705033164260631, 'plan': 0.0003901677721420211, 'updating': 0.0039016777214202106, 'replace': 0.0007803355442840422, 'max-norm': 0.0003901677721420211, 'Recall': 0.0003901677721420211, 'Eqn': 0.0007803355442840422, '+V': 0.0007803355442840422, 'Q-function': 0.0003901677721420211, 'noted': 0.0003901677721420211, 'rewritten': 0.0003901677721420211, 'plays': 0.0003901677721420211, 'role': 0.0003901677721420211, 'Therefore': 0.0027311744049941474, '24': 0.0007803355442840422, 'outside': 0.0015606710885680843, 'Abusing': 0.0003901677721420211, 'substitute': 0.0003901677721420211, 'formulas': 0.0003901677721420211, '||V': 0.0003901677721420211, 'β||V': 0.0003901677721420211, 'cancel': 0.0003901677721420211, 'sides': 0.0015606710885680843, 'β||Ct': 0.0003901677721420211, '|ut|': 0.0007803355442840422, 'trivially': 0.0003901677721420211, 'bounds': 0.0003901677721420211, '||Ct': 0.0003901677721420211, 'veriﬁed': 0.0011705033164260631, 'accelerate': 0.0003901677721420211, 'technique': 0.0007803355442840422, 'sN': 0.0011705033164260631, '+1': 0.0003901677721420211, 'indeed': 0.0007803355442840422, 'abstract': 0.015216543113538821, 's2': 0.008583690987124463, 'intermediate': 0.0015606710885680843, 'replacement': 0.0003901677721420211, '12a': 0.0003901677721420211, '12b': 0.0003901677721420211, '12c': 0.0003901677721420211, 'sj': 0.0007803355442840422, '+1−j': 0.0003901677721420211, 'maxa′': 0.0007803355442840422, 'implementation': 0.0003901677721420211, 'linked': 0.0003901677721420211, 'list': 0.0027311744049941474, 'appends': 0.0003901677721420211, 'lists': 0.0007803355442840422, 'passes': 0.0003901677721420211, 'powerful': 0.0003901677721420211, 'updating.': 0.0003901677721420211, 'suitable': 0.0003901677721420211, 'whenever': 0.0003901677721420211, 'equivalent': 0.0023410066328521262, 'whose': 0.00351150994927819, 'seen': 0.0007803355442840422, 'needs': 0.0011705033164260631, 'samples': 0.0023410066328521262, 'add': 0.0003901677721420211, 'eﬀect': 0.0019508388607101053, 'changing': 0.0007803355442840422, 'pseudo-rewards': 0.0007803355442840422, 'contaminate': 0.0003901677721420211, 'discussing': 0.0003901677721420211, '˜C': 0.0027311744049941474, 'inside': 0.0027311744049941474, 'discover': 0.0015606710885680843, 'real': 0.0003901677721420211, 'rules': 0.0011705033164260631, '—its': 0.0003901677721420211, 'discovered': 0.0007803355442840422, 'optimizing': 0.0003901677721420211, 'lines': 0.0015606710885680843, 'a∗': 0.0031213421771361686, 'includes': 0.0003901677721420211, 'uncontaminated': 0.0007803355442840422, 'include': 0.0019508388607101053, 'whereever': 0.0003901677721420211, 'refers': 0.0003901677721420211, 'incorporates': 0.0003901677721420211, 'all-states': 0.0003901677721420211, 'most-recent-ﬁrst': 0.0003901677721420211, 'updated': 0.0007803355442840422, 'last': 0.0015606710885680843, 'helps': 0.0003901677721420211, '˜Q': 0.0015606710885680843, 'established': 0.0011705033164260631, 'gave': 0.0003901677721420211, '26': 0.0007803355442840422, 'seq': 0.0015606710885680843, 'childSeq': 0.0019508388607101053, 'argmaxa′': 0.0003901677721420211, '˜Ct': 0.0011705033164260631, '˜Ct+1': 0.0003901677721420211, 'append': 0.0003901677721420211, 'front': 0.0007803355442840422, 'Corollary': 0.0019508388607101053, 'Under': 0.0015606710885680843, 'tedious': 0.0003901677721420211, 'proving': 0.0003901677721420211, 'Abstraction': 0.0023410066328521262, 'reasons': 0.0003901677721420211, 'introduce': 0.0039016777214202106, 'certain': 0.0003901677721420211, 'regardless': 0.0011705033164260631, '27': 0.0003901677721420211, 'proofs': 0.0003901677721420211, 'introduction': 0.0011705033164260631, 'Throughout': 0.0003901677721420211, 'running': 0.0007803355442840422, 'tables': 0.0007803355442840422, 'store': 0.005462348809988295, '×': 0.0003901677721420211, '1000': 0.0011705033164260631, 'Third': 0.0007803355442840422, '2000': 0.0011705033164260631, 'eﬀectively': 0.0011705033164260631, '8000': 0.0007803355442840422, '14,000': 0.0003901677721420211, 'separate': 0.0023410066328521262, '3,000': 0.0003901677721420211, '5.1': 0.0003901677721420211, 'Five': 0.0003901677721420211, 'Conditions': 0.0003901677721420211, 'Permit': 0.0003901677721420211, 'class': 0.0007803355442840422, 'abstractly': 0.0007803355442840422, 'identifying': 0.0011705033164260631, 'begin': 0.0019508388607101053, 'variables': 0.017557549746390948, 'partitioned': 0.0011705033164260631, 'Xi': 0.004291845493562232, 'Yi': 0.00351150994927819, 'χi': 0.009364026531408505, 'projects': 0.0003901677721420211, 'combined': 0.0011705033164260631, 'state-abstracted': 0.0015606710885680843, 'mean': 0.0007803355442840422, 'x′': 0.01092469761997659, 'y′': 0.0039016777214202106, '|x': 0.010144362075692548, '˜Ra': 0.0007803355442840422, 'respectively': 0.0003901677721420211, 'correspond-ing': 0.0003901677721420211, 'interpreted': 0.0003901677721420211, 'prob-ability': 0.0007803355442840422, 'distributions': 0.0015606710885680843, '28': 0.0003901677721420211, 'instantaneous': 0.0003901677721420211, 'relevant': 0.0031213421771361686, 'deciding': 0.0003901677721420211, 'Boltzmann': 0.0031213421771361686, 'ǫ-greedy': 0.0011705033164260631, 'counter-based': 0.0019508388607101053, 'abstracted': 0.0019508388607101053, 'Counter-based': 0.0003901677721420211, 'analyze': 0.0003901677721420211, 'identiﬁed': 0.0007803355442840422, 'eliminating': 0.0003901677721420211, 'tend': 0.0007803355442840422, 'arises': 0.0019508388607101053, 'funnel': 0.0019508388607101053, 'proportional': 0.0003901677721420211, 'Funnel': 0.0007803355442840422, 'appear': 0.0011705033164260631, 'exploits': 0.0003901677721420211, 'reachable': 0.0003901677721420211, 'ancestors': 0.0007803355442840422, 'describing': 0.0007803355442840422, 'type': 0.0011705033164260631, 'ﬁnally': 0.0003901677721420211, '5.1.1': 0.0003901677721420211, 'Node': 0.005462348809988295, 'Irrelevance': 0.013655872024970737, 'hold': 0.0003901677721420211, 'factored': 0.0007803355442840422, 'product': 0.0003901677721420211, '|y': 0.0019508388607101053, 'y1': 0.0007803355442840422, 'y2': 0.0007803355442840422, 'χ': 0.0074131876706984, 'full-state': 0.0015606710885680843, 'remaining': 0.0011705033164260631, 'compactly': 0.0007803355442840422, '29': 0.0003901677721420211, 'equivalence': 0.0007803355442840422, 'Xx′': 0.0031213421771361686, 'j′': 0.0007803355442840422, 'y0': 0.0015606710885680843, 'States': 0.0003901677721420211, 'Actions': 0.0019508388607101053, 'Transition': 0.0003901677721420211, 'Reward': 0.0003901677721420211, 'Compare': 0.0007803355442840422, 'know': 0.004291845493562232, 'Xy′': 0.0007803355442840422, 'eliminated': 0.0003901677721420211, 'expressions': 0.0003901677721420211, 'Since': 0.0007803355442840422, '30': 0.0003901677721420211, 'corollary': 0.0003901677721420211, 'hence': 0.0023410066328521262, 'ρ': 0.0039016777214202106, 'ω∗': 0.0007803355442840422, 'construction': 0.0011705033164260631, 'stated': 0.0003901677721420211, 'transi-tion': 0.0007803355442840422, 'completes': 0.0003901677721420211, '−∞': 0.0003901677721420211, 'abstracting': 0.0007803355442840422, 'savings': 0.0007803355442840422, 'Instead': 0.0023410066328521262, '400': 0.0011705033164260631, 'noticing': 0.0003901677721420211, 'examine': 0.0003901677721420211, 'subgraph': 0.0007803355442840422, 'rooted': 0.0003901677721420211, 'H.': 0.0007803355442840422, 'partition': 0.0007803355442840422, 'y′|x': 0.0011705033164260631, 'y′|y': 0.0011705033164260631, 'x′|x': 0.0015606710885680843, '˜Rj': 0.0007803355442840422, 'Tj': 0.0011705033164260631, '31': 0.0003901677721420211, 'factors': 0.0019508388607101053, 'component': 0.0003901677721420211, 'Pi': 0.0011705033164260631, 'subtree': 0.0003901677721420211, 'i.': 0.0003901677721420211, '5.1.2': 0.0003901677721420211, 'Leaf': 0.005462348809988295, 'stronger': 0.0003901677721420211, 'irrelevance': 0.0007803355442840422, '1|s1': 0.0011705033164260631, '2|s2': 0.0011705033164260631, 'According': 0.0007803355442840422, '32': 0.0003901677721420211, 'Plug': 0.0003901677721420211, 'ra': 0.0015606710885680843, 'instead': 0.0015606710885680843, 'values—one': 0.0003901677721420211, 'legal': 0.0007803355442840422, 'illegal': 0.0007803355442840422, 'together': 0.0003901677721420211, '5.1.3': 0.0003901677721420211, 'Result': 0.005462348809988295, 'Distribution': 0.005072181037846274, 'Yj': 0.0015606710885680843, 'pairs': 0.0003901677721420211, '|s1': 0.0003901677721420211, '|s2': 0.0003901677721420211, 'χij': 0.0023410066328521262, '33': 0.0003901677721420211, 'rarely': 0.0003901677721420211, 'No': 0.0019508388607101053, 'matter': 0.0007803355442840422, 'ﬁnishes': 0.0003901677721420211, 'Consequently': 0.0007803355442840422, 'necessarily': 0.0003901677721420211, 'lesson': 0.0003901677721420211, 'discounting': 0.0003901677721420211, 'interferes': 0.0003901677721420211, 'operators—the': 0.0003901677721420211, 'eﬀective': 0.0003901677721420211, 'undiscounted': 0.0019508388607101053, 'dis-tribution': 0.0003901677721420211, 'longer': 0.0015606710885680843, 'Fortunately': 0.0007803355442840422, 'ﬁnite-horizon': 0.0007803355442840422, 'repre-sent': 0.0003901677721420211, 'classes': 0.0003901677721420211, 'unabstracted': 0.0011705033164260631, 'car': 0.0003901677721420211, 'entrance': 0.0003901677721420211, 'ramp': 0.0003901677721420211, 'freeway': 0.0003901677721420211, 'applicable': 0.0003901677721420211, '5.1.4': 0.0003901677721420211, 'Termination': 0.0046820132657042525, 'closely': 0.0003901677721420211, 'guaranteed': 0.0011705033164260631, 'sense': 0.0007803355442840422, 'funneling': 0.0003901677721420211, '∀': 0.0003901677721420211, '⇒': 0.0003901677721420211, 'applying': 0.00351150994927819, 'explicitly': 0.0007803355442840422, '34': 0.0003901677721420211, 'assumptions': 0.0019508388607101053, 'holding': 0.0007803355442840422, 'succeed': 0.0015606710885680843, 'implies': 0.0007803355442840422, 'detect': 0.0007803355442840422, 'compare': 0.0015606710885680843, '5.1.5': 0.0003901677721420211, 'Shielding': 0.0031213421771361686, 'shielding': 0.0003901677721420211, 'Task': 0.0011705033164260631, 'ancestor': 0.0007803355442840422, 'explcitly': 0.0003901677721420211, '5.1.6': 0.0003901677721420211, 'Dicussion': 0.0003901677721420211, '100': 0.005852516582130316, 'Irrelevant': 0.0027311744049941474, 'Dis-tribution': 0.0003901677721420211, 'Passenger': 0.0007803355442840422, '35': 0.0003901677721420211, '632': 0.0003901677721420211, '3000': 0.0007803355442840422, 'compact': 0.0003901677721420211, 'exploited': 0.0003901677721420211, '?': 0.0003901677721420211, 'suﬃces': 0.0003901677721420211, 'analysis': 0.0007803355442840422, 'Opportunities': 0.0003901677721420211, 'eﬀects': 0.0007803355442840422, 'operators': 0.0015606710885680843, 'checked': 0.0003901677721420211, '5.2': 0.0003901677721420211, 'Convergence': 0.0007803355442840422, 'represented—they': 0.0003901677721420211, 'abstract-state': 0.0003901677721420211, 'statements': 0.0003901677721420211, '36': 0.0003901677721420211, 'lemmas': 0.0007803355442840422, 'tell': 0.0015606710885680843, 'ab-stract': 0.0003901677721420211, 'heart': 0.0003901677721420211, 'claim': 0.0003901677721420211, 'restrictions': 0.0003901677721420211, 'i—not': 0.0003901677721420211, 'impose': 0.0007803355442840422, 'prefers': 0.0003901677721420211, 'tie': 0.0007803355442840422, 'Make': 0.0003901677721420211, 'ready': 0.0003901677721420211, 'unab-stracted': 0.0003901677721420211, 'rates': 0.0019508388607101053, 'Rather': 0.0003901677721420211, 'repeating': 0.0003901677721420211, 'inferred': 0.0003901677721420211, '37': 0.0003901677721420211, 'considering': 0.0007803355442840422, 'visits': 0.0003901677721420211, 'draws': 0.0007803355442840422, 's′|x': 0.0007803355442840422, 'visiting': 0.0007803355442840422, 'inner': 0.0003901677721420211, 'Call': 0.0003901677721420211, 'drawing': 0.0007803355442840422, 'sampling': 0.0003901677721420211, '38': 0.0003901677721420211, '5.3': 0.0003901677721420211, 'Credit': 0.0003901677721420211, 'Assignment': 0.0003901677721420211, 'modiﬁcation': 0.0003901677721420211, 'fuel': 0.005072181037846274, 'tank': 0.0011705033164260631, 'moves': 0.005072181037846274, 'runs': 0.0019508388607101053, '−20': 0.0011705033164260631, 'trial': 0.0027311744049941474, 'ﬁlling': 0.0019508388607101053, 'station': 0.0015606710885680843, 'Fillup': 0.0003901677721420211, 'Refuel': 0.0003901677721420211, 'moving': 0.00351150994927819, 'MaxRefuel': 0.0003901677721420211, 'invokes': 0.0007803355442840422, 'possibility': 0.0007803355442840422, 'feature': 0.0003901677721420211, 'representing': 0.0019508388607101053, 'unfortunate': 0.0003901677721420211, 'intuition': 0.0007803355442840422, 'inﬂuence': 0.0007803355442840422, 'account': 0.0003901677721420211, 'top-level': 0.0003901677721420211, 'decide': 0.0007803355442840422, 'refuel': 0.0003901677721420211, 'try': 0.0007803355442840422, 'explain': 0.0003901677721420211, 'received—that': 0.0003901677721420211, 'predict': 0.0003901677721420211, 'Stated': 0.0003901677721420211, 'formally': 0.0003901677721420211, 'diﬃculty': 0.0003901677721420211, 'exhausting': 0.0003901677721420211, 'manually': 0.0003901677721420211, 'out-of-fuel': 0.0003901677721420211, 'Lines': 0.0003901677721420211, 'designer': 0.0003901677721420211, 'studied': 0.0003901677721420211, 'autonomously': 0.0003901677721420211, '39': 0.0003901677721420211, 'Non-Hierarchical': 0.0003901677721420211, 'Up': 0.0003901677721420211, 'exclusively': 0.0003901677721420211, 'strictly': 0.0011705033164260631, 'deriving': 0.0007803355442840422, 'ideas': 0.0007803355442840422, 'repeats': 0.0003901677721420211, 'evaluation': 0.0007803355442840422, 'πk+1': 0.0011705033164260631, 'argmax': 0.0003901677721420211, 'Howard': 0.0015606710885680843, '1960': 0.0007803355442840422, 'im-provement': 0.0003901677721420211, 'πg': 0.0019508388607101053, 'argmaxa': 0.0007803355442840422, 'direct': 0.0007803355442840422, 'iterate': 0.0003901677721420211, 'unlikely': 0.0007803355442840422, 'representable': 0.0003901677721420211, 'signiﬁcant': 0.0007803355442840422, 'improvements': 0.0019508388607101053, 'approximator': 0.0003901677721420211, 'π—any': 0.0003901677721420211, 'borrows': 0.0003901677721420211, 'great': 0.0007803355442840422, 'beauties': 0.0003901677721420211, 'knowing': 0.0003901677721420211, 'taking': 0.0007803355442840422, 'lookahead': 0.0003901677721420211, 'presented': 0.0007803355442840422, 'conducts': 0.0003901677721420211, '40': 0.0007803355442840422, 'ExecuteHGPolicy': 0.0031213421771361686, 'repeat': 0.0003901677721420211, 'ai': 0.0003901677721420211, 'πhg∗': 0.0023410066328521262, 'indicates': 0.0003901677721420211, 'hg': 0.0023410066328521262, 'hg∗': 0.0007803355442840422, 'sketch': 0.0003901677721420211, 'inequality': 0.0007803355442840422, 'contrast': 0.0019508388607101053, 'traversal': 0.0003901677721420211, 'largest': 0.0003901677721420211, 'πhg': 0.0007803355442840422, 'greedily': 0.0007803355442840422, 'represents': 0.0007803355442840422, 'greedier': 0.0003901677721420211, 'Ct.': 0.0003901677721420211, 'opportunity': 0.0003901677721420211, 'improve': 0.0019508388607101053, 'underestimate': 0.0003901677721420211, 'direction': 0.011705033164260631, 'says': 0.0003901677721420211, 'yet': 0.0003901677721420211, 'prevents': 0.0007803355442840422, 'unlike': 0.0003901677721420211, 'strict': 0.0003901677721420211, '41': 0.0003901677721420211, 'guarantees': 0.0007803355442840422, 'So': 0.0015606710885680843, 'dominates': 0.0003901677721420211, 'operates': 0.0003901677721420211, 'probably': 0.0003901677721420211, 'closely-related': 0.0003901677721420211, 'macros': 0.0003901677721420211, 'aspect': 0.0003901677721420211, 'tree': 0.0003901677721420211, 'care': 0.0007803355442840422, 'Being': 0.0003901677721420211, 'correctly': 0.0007803355442840422, 'During': 0.0003901677721420211, 'tried': 0.0007803355442840422, 'PutDown': 0.0003901677721420211, 'fail': 0.0007803355442840422, 'whereas': 0.00351150994927819, 'eventually': 0.0003901677721420211, 'lengthy': 0.0003901677721420211, 'hierarchically-greedy': 0.0011705033164260631, 'early': 0.0011705033164260631, 'enough': 0.0003901677721420211, 'actively': 0.0003901677721420211, 'exploring': 0.0003901677721420211, 'never': 0.0019508388607101053, 'ceases': 0.0003901677721420211, 'explore': 0.0003901677721420211, 'want': 0.0015606710885680843, 'quickly': 0.0007803355442840422, 'asymptotically': 0.0003901677721420211, 'remember': 0.0007803355442840422, 'interrupts': 0.0003901677721420211, 'reached': 0.0019508388607101053, 'purely': 0.0003901677721420211, 'training': 0.0007803355442840422, 'L': 0.00351150994927819, 'interrupted': 0.0011705033164260631, 'hierarchical—when': 0.0003901677721420211, 'committed': 0.0003901677721420211, 'becomes': 0.0011705033164260631, 'cools': 0.0003901677721420211, 'temperature': 0.0039016777214202106, '0.1': 0.0027311744049941474, 'halted': 0.0003901677721420211, 'experimental': 0.0003901677721420211, 'generally': 0.0003901677721420211, 'excellent': 0.0003901677721420211, 'little': 0.0007803355442840422, '42': 0.0003901677721420211, 'Experimental': 0.0003901677721420211, 'Evaluation': 0.0003901677721420211, 'Method': 0.0007803355442840422, 'mind': 0.0003901677721420211, 'expressive': 0.0003901677721420211, 'characterize': 0.0003901677721420211, 'assess': 0.0003901677721420211, '7.1': 0.0003901677721420211, 'Fickle': 0.0011705033164260631, 'incor-porates': 0.0003901677721420211, 'noisy': 0.0003901677721420211, '0.8': 0.0007803355442840422, 'intended': 0.0007803355442840422, 'picked': 0.0007803355442840422, 'moved': 0.0011705033164260631, '0.3': 0.0003901677721420211, 'measured': 0.0003901677721420211, 'conﬁgurations': 0.0007803355442840422, 'abstrac-tion': 0.0007803355442840422, 'controlled': 0.0003901677721420211, 'cooling': 0.00351150994927819, 'schedule': 0.0007803355442840422, 'ex-ecution': 0.0003901677721420211, 'decreasing': 0.0003901677721420211, 'consecutive': 0.0003901677721420211, 'optimized': 0.0003901677721420211, 'separately': 0.0003901677721420211, 'conﬁguration': 0.0003901677721420211, 'matching': 0.0003901677721420211, 'exceed-ing': 0.0003901677721420211, 'code': 0.0003901677721420211, 'multiplying': 0.0003901677721420211, '0.123': 0.0015606710885680843, '0.25': 0.0007803355442840422, '50': 0.0031213421771361686, '0.9879': 0.0011705033164260631, '.123': 0.0003901677721420211, 'signature': 0.0003901677721420211, 'aid': 0.0003901677721420211, 'debugging': 0.0003901677721420211, '0.50': 0.0007803355442840422, '.9996': 0.0003901677721420211, '0.9939': 0.0003901677721420211, '0.9074': 0.0003901677721420211, '0.9526': 0.0007803355442840422, 'trials': 0.0011705033164260631, 'averaged': 0.0003901677721420211, 'vice': 0.0003901677721420211, 'versa': 0.0003901677721420211, 'notice': 0.0015606710885680843, 'actually': 0.0007803355442840422, '43': 0.0003901677721420211, 'w': 0.0019508388607101053, 'v': 0.0015606710885680843, 'l': 0.012095200936402654, 'u': 0.0031213421771361686, '200': 0.0003901677721420211, '-200': 0.0007803355442840422, '-400': 0.0007803355442840422, '-600': 0.0003901677721420211, '-800': 0.0003901677721420211, '-1000': 0.0003901677721420211, 'Flat': 0.005072181037846274, '50000': 0.0007803355442840422, '100000': 0.0007803355442840422, '150000': 0.0007803355442840422, '200000': 0.0015606710885680843, '250000': 0.0007803355442840422, 'Primitive': 0.0019508388607101053, 'Comparison': 0.0015606710885680843, 'curve': 0.0007803355442840422, 'crosses': 0.0007803355442840422, 'MAXQ/no': 0.0003901677721420211, 'really': 0.0003901677721420211, 'beneﬁts': 0.0007803355442840422, 'clearly': 0.0003901677721420211, 'focuses': 0.0003901677721420211, 'range': 0.0003901677721420211, 'neighborhood': 0.0003901677721420211, 'attains': 0.0003901677721420211, 'approximately': 0.0003901677721420211, '40,000': 0.0007803355442840422, 'roughly': 0.0007803355442840422, 'twice': 0.0007803355442840422, 'continue': 0.0003901677721420211, 'onward': 0.0003901677721420211, 'slow': 0.0003901677721420211, 'respond': 0.0003901677721420211, 'ﬁckle': 0.0003901677721420211, 'attain': 0.0003901677721420211, 'regions': 0.0003901677721420211, 'recovers': 0.0003901677721420211, 'rapidly': 0.0003901677721420211, 'purely-hierarchical': 0.0003901677721420211, 'cost—in': 0.0003901677721420211, 'exploration—for': 0.0003901677721420211, 'experiment': 0.0007803355442840422, 'evidence': 0.0003901677721420211, 'claims': 0.0003901677721420211, '44': 0.0003901677721420211, 'Abstract+Greedy': 0.0003901677721420211, 'Optimal': 0.0015606710885680843, 'Hier-Optimal': 0.0003901677721420211, '-5': 0.0003901677721420211, '-10': 0.0007803355442840422, '-15': 0.0003901677721420211, '300000': 0.0003901677721420211, 'Close-up': 0.0003901677721420211, 'ﬁgure': 0.0019508388607101053, 'horizontal': 0.0003901677721420211, 'indicating': 0.0003901677721420211, 'readable': 0.0003901677721420211, '100-step': 0.0003901677721420211, 'average': 0.0015606710885680843, '7.2': 0.0003901677721420211, 'HDG': 0.011705033164260631, 'Leslie': 0.0007803355442840422, 'landmark': 0.016777214202106906, 'Voronoi': 0.0074131876706984, 'Manhattan': 0.0003901677721420211, 'distances': 0.0003901677721420211, 'landmarks': 0.004291845493562232, 'belong': 0.0003901677721420211, 'cell': 0.007803355442840421, 'l.': 0.0031213421771361686, 'neighboring': 0.0015606710885680843, 'Once': 0.0003901677721420211, 'solutions': 0.0011705033164260631, 'g': 0.006632852126414358, 'g.': 0.0015606710885680843, 'combining': 0.0007803355442840422, 'maintains': 0.0003901677721420211, '45': 0.0003901677721420211, '10-by-10': 0.0003901677721420211, 'circled': 0.0003901677721420211, 'heavy': 0.0003901677721420211, 'boundaries': 0.0003901677721420211, 'hexagon': 0.0003901677721420211, 'neighbors': 0.0007803355442840422, 'build': 0.0003901677721420211, 't1': 0.00351150994927819, 't2': 0.004291845493562232, 't3': 0.0039016777214202106, 'goal-cell': 0.0003901677721420211, 'exact': 0.0011705033164260631, 'overestimate': 0.0003901677721420211, 'Also': 0.0003901677721420211, 'included': 0.0003901677721420211, 'row': 0.0031213421771361686, 'column': 0.0031213421771361686, 'Term': 0.0007803355442840422, 'landmark-to-landmark': 0.0003901677721420211, 'variation': 0.0003901677721420211, 'regular': 0.0003901677721420211, 'intervals': 0.0003901677721420211, 'Floyd-Warshall': 0.0003901677721420211, 'all-sources': 0.0003901677721420211, '46': 0.0003901677721420211, 'gl/NL': 0.0003901677721420211, 'QGotoGoalLmk': 0.0003901677721420211, 'gl': 0.004291845493562232, 'MaxGotoGoalLmk': 0.0011705033164260631, 'QGotoLmk': 0.0003901677721420211, 'MaxGotoLmk': 0.0011705033164260631, 'QNorthLmk': 0.0003901677721420211, 'QSouthLmk': 0.0003901677721420211, 'QEastLmk': 0.0003901677721420211, 'QWestLmk': 0.0003901677721420211, 'QWestG': 0.0003901677721420211, 'GotoGoalLmk': 0.0039016777214202106, 'GotoLmk': 0.0039016777214202106, 'essentially': 0.0015606710885680843, 'somewhat': 0.0003901677721420211, 'redun-dant': 0.0003901677721420211, '47': 0.0003901677721420211, 'Actually': 0.0003901677721420211, 'mark': 0.0003901677721420211, 'axGotoLmk': 0.0003901677721420211, 'land•': 0.0003901677721420211, 'sums': 0.0003901677721420211, 'twice—speciﬁcally': 0.0003901677721420211, '2,028': 0.0023410066328521262, 'constitute': 0.0003901677721420211, '55': 0.0007803355442840422, 'Similar': 0.0003901677721420211, '506': 0.0015606710885680843, '3,536': 0.0023410066328521262, 'grand': 0.0003901677721420211, '6,070': 0.0007803355442840422, 'representation—indeed': 0.0003901677721420211, 'stochastic—as': 0.0003901677721420211, 'paper—then': 0.0003901677721420211, '48': 0.0003901677721420211, 'item': 0.0007803355442840422, 'Total': 0.0003901677721420211, 'Number': 0.0003901677721420211, 'Values': 0.0003901677721420211, 'Required': 0.0003901677721420211, 'abs': 0.0011705033164260631, '6,600': 0.0011705033164260631, '96': 0.0011705033164260631, '12,760': 0.0007803355442840422, 'unsafe': 0.0007803355442840422, '6,171': 0.0007803355442840422, '12,361': 0.0007803355442840422, 'bordering': 0.0003901677721420211, 'making': 0.0011705033164260631, 'amoount': 0.0003901677721420211, 'borders': 0.0003901677721420211, 'ignoring': 0.0007803355442840422, 'incorrectly': 0.0003901677721420211, 'approximations': 0.0007803355442840422, 'equivalently': 0.0003901677721420211, 'redundancy': 0.0003901677721420211, '49': 0.0003901677721420211, 'fully-general': 0.0003901677721420211, 'methodology': 0.0003901677721420211, 'application-speciﬁc': 0.0003901677721420211, 'representations': 0.0003901677721420211, 'long-term': 0.0003901677721420211, 'inventing': 0.0003901677721420211, 'oﬀ-the-shelf': 0.0003901677721420211, 'tools': 0.0003901677721420211, 'specialized': 0.0003901677721420211, 'special-purpose': 0.0003901677721420211, 'contributions': 0.0003901677721420211, 'soon': 0.0003901677721420211, 'recomputes': 0.0003901677721420211, 'aiming': 0.0003901677721420211, 'aims': 0.0003901677721420211, 'visit': 0.0003901677721420211, 'convenient': 0.0003901677721420211, 'targets': 0.0003901677721420211, 'shortcuts': 0.0003901677721420211, 'compensates': 0.0003901677721420211, 'overestimated': 0.0003901677721420211, 'goes': 0.0003901677721420211, 'inspired': 0.0003901677721420211, 'Kaelbing': 0.0003901677721420211, 'metric': 0.0003901677721420211, 'polling': 0.0007803355442840422, 'checks': 0.0003901677721420211, 'interrupt': 0.0007803355442840422, 'mechanism': 0.0007803355442840422, 'unnecessary': 0.0003901677721420211, '—': 0.0003901677721420211, 'eﬃciently-evaluable': 0.0003901677721420211, 'al-gorithm': 0.0003901677721420211, '1.0': 0.0015606710885680843, '.9074': 0.0011705033164260631, '−25.123': 0.0003901677721420211, '.9999': 0.0003901677721420211, '.9526': 0.0003901677721420211, '−20.123': 0.0003901677721420211, '.9760': 0.0003901677721420211, '.9969': 0.0007803355442840422, '.9984': 0.0003901677721420211, 'reducing': 0.0003901677721420211, '1500': 0.0003901677721420211, 'conﬁrms': 0.0003901677721420211, 'observations': 0.0003901677721420211, 'With-out': 0.0003901677721420211, 'slowly': 0.0003901677721420211, 'fast': 0.0003901677721420211, 'close-up': 0.0003901677721420211, 'diﬀerences': 0.0007803355442840422, 'quality': 0.0003901677721420211, 'hand-coded': 0.0015606710885680843, 'policy—presumably': 0.0003901677721420211, 'fewer': 0.0011705033164260631, 'average—so': 0.0003901677721420211, 'Notice': 0.0003901677721420211, 'match': 0.0003901677721420211, 'perfor-mance': 0.0003901677721420211, 'Abstractions': 0.0007803355442840422, '-20': 0.0003901677721420211, '-40': 0.0003901677721420211, '-60': 0.0003901677721420211, '-80': 0.0003901677721420211, '-100': 0.0007803355442840422, '-120': 0.0003901677721420211, '-140': 0.0003901677721420211, '400000': 0.0007803355442840422, '1e+06': 0.0011705033164260631, '1.2e+06': 0.0007803355442840422, '1.4e+06': 0.0007803355442840422, '600000': 0.0007803355442840422, '800000': 0.0007803355442840422, 'Average': 0.0007803355442840422, 'shifting': 0.0003901677721420211, '7.3': 0.0003901677721420211, 'Hierarchies': 0.0003901677721420211, 'dissertation': 0.0003901677721420211, 'Ron': 0.0011705033164260631, 'encodes': 0.0011705033164260631, 'controllers': 0.0007803355442840422, 'procedure-call-and-return': 0.0003901677721420211, 'machine': 0.00702301989855638, 'puts': 0.0003901677721420211, 'mi': 0.0011705033164260631, 'machines': 0.004291845493562232, 'appearing': 0.0003901677721420211, 'observation': 0.0003901677721420211, 'ﬂattens': 0.0003901677721420211, 'structures': 0.0003901677721420211, 'disadvantage': 0.0003901677721420211, '51': 0.0003901677721420211, 'Hand-coded': 0.0007803355442840422, '-6': 0.0003901677721420211, '-8': 0.0003901677721420211, '-12': 0.0003901677721420211, '-14': 0.0003901677721420211, 'Expanded': 0.0003901677721420211, 'comparing': 0.0003901677721420211, 'illustrated': 0.0003901677721420211, 'high-level': 0.0003901677721420211, 'hallways': 0.0015606710885680843, 'intersections': 0.0011705033164260631, 'low-level': 0.0003901677721420211, 'obstacles': 0.0007803355442840422, 'corner': 0.0023410066328521262, 'bottom': 0.0007803355442840422, 'east': 0.0007803355442840422, 'west': 0.0003901677721420211, 'collide': 0.0003901677721420211, 'obstacle': 0.0007803355442840422, 'structured': 0.0003901677721420211, 'rooms': 0.0011705033164260631, '12-by-12': 0.0007803355442840422, 'block': 0.0003901677721420211, 'contain': 0.0019508388607101053, 'walls': 0.0023410066328521262, 'opposite': 0.0011705033164260631, 'open': 0.0007803355442840422, 'Other': 0.0007803355442840422, 'meet': 0.0003901677721420211, 'test': 0.0007803355442840422, 'knowledge.2': 0.0003901677721420211, 'MRoot': 0.0015606710885680843, 'consists': 0.0003901677721420211, 'MGo': 0.004291845493562232, 'W': 0.0003901677721420211, 'est': 0.0003901677721420211, 'orth': 0.0011705033164260631, 'hallway': 0.0019508388607101053, '2The': 0.0003901677721420211, 'author': 0.0011705033164260631, 'thanks': 0.0003901677721420211, 'details': 0.0007803355442840422, '52': 0.0007803355442840422, 'intersection': 0.00351150994927819, 'd.': 0.0027311744049941474, 'traverse': 0.0003901677721420211, 'ExitIntersection': 0.0003901677721420211, 'MExitHallway': 0.0011705033164260631, 'ma-chine': 0.0003901677721420211, 'MExitIntersection': 0.0011705033164260631, 'con-ditions': 0.0003901677721420211, 'consist': 0.0003901677721420211, 'sub-routines': 0.0003901677721420211, 'MSniﬀ': 0.004291845493562232, 'MBack': 0.0039016777214202106, 'p': 0.008193523214982443, 'encounters': 0.0003901677721420211, 'perpendicular': 0.0007803355442840422, 'trapped': 0.0003901677721420211, 'directions': 0.0007803355442840422, 'resumes': 0.0003901677721420211, 'backwards': 0.0011705033164260631, 'p.': 0.0011705033164260631, 'blocking': 0.0003901677721420211, 'carried': 0.0003901677721420211, 'hall': 0.0007803355442840422, 'highly': 0.0003901677721420211, 'MExitHall': 0.0003901677721420211, 'forward': 0.0007803355442840422, 'Go': 0.00351150994927819, 'wrong': 0.0003901677721420211, 'ExitInter': 0.0015606710885680843, 'exited': 0.0007803355442840422, 'ExitHall': 0.0011705033164260631, 'intersec-tion': 0.0003901677721420211, 'entered': 0.0007803355442840422, 'Sniﬀ': 0.0007803355442840422, 'ToWall': 0.0019508388607101053, 'FollowWall': 0.0007803355442840422, '53': 0.0003901677721420211, 'sideways': 0.0003901677721420211, 'stuck': 0.0003901677721420211, 'Back': 0.0015606710885680843, 'encode': 0.0003901677721420211, 'capture': 0.0007803355442840422, 'stes': 0.0003901677721420211, 'allow': 0.0007803355442840422, 'position': 0.0007803355442840422, 'prevent': 0.0003901677721420211, 'BackOne': 0.0003901677721420211, 'succeeded': 0.0007803355442840422, 'Its': 0.0003901677721420211, 'PerpThree': 0.0003901677721420211, 'positions': 0.0003901677721420211, 'Move': 0.0003901677721420211, 'From': 0.0003901677721420211, 'major': 0.0003901677721420211, 'convert': 0.0003901677721420211, 'k': 0.0003901677721420211, 'counter': 0.0003901677721420211, 'policy—independent': 0.0003901677721420211, 'requirement': 0.0003901677721420211, 'enter': 0.0003901677721420211, 'prefer': 0.0011705033164260631, 'MaxExitHall': 0.0007803355442840422, '54': 0.0003901677721420211, 'ﬂattened': 0.0003901677721420211, 'tricks': 0.0003901677721420211, 'serve': 0.0003901677721420211, 'began': 0.0003901677721420211, 'employing': 0.0007803355442840422, 'inheritance': 0.0007803355442840422, 'others': 0.0003901677721420211, 'inherits': 0.0003901677721420211, 'middle': 0.0003901677721420211, 'parents': 0.0003901677721420211, 'ir-relevant': 0.0003901677721420211, 'features': 0.0003901677721420211, 'MaxGo': 0.0007803355442840422, 'elimination': 0.0003901677721420211, '52,043': 0.0003901677721420211, '16,704': 0.0003901677721420211, '4,300': 0.0003901677721420211, 'acquire': 0.0003901677721420211, 'phases': 0.0003901677721420211, 'unable': 0.0003901677721420211, 'well—': 0.0003901677721420211, 'low': 0.0003901677721420211, 'ǫ': 0.0015606710885680843, 'keeps': 0.0003901677721420211, 'track': 0.0003901677721420211, 'selects': 0.0003901677721420211, 'fewest': 0.0003901677721420211, 'switches': 0.0003901677721420211, 'genuine': 0.0003901677721420211, 'chose': 0.0007803355442840422, '0.001': 0.0003901677721420211, '−200.123': 0.0003901677721420211, 'reciprocal': 0.0003901677721420211, 'selected': 0.0003901677721420211, 'underestimates': 0.0003901677721420211, 'QExitInter': 0.0007803355442840422, '−40.123': 0.0003901677721420211, 'worst': 0.0003901677721420211, 'plots': 0.0003901677721420211, 'coded': 0.0003901677721420211, 'contextual': 0.0003901677721420211, 'surely': 0.0003901677721420211, 'most—but': 0.0003901677721420211, 'all—': 0.0003901677721420211, '7.4': 0.0003901677721420211, 'Domains': 0.0003901677721420211, 'graphs': 0.0003901677721420211, '1992b': 0.0007803355442840422, 'ﬂag': 0.0003901677721420211, 'treasure': 0.0003901677721420211, 'hunter': 0.0003901677721420211, 'Tadepalli': 0.0019508388607101053, 'Fuedal-Q': 0.0003901677721420211, 'naturally': 0.0003901677721420211, 'placed': 0.0003901677721420211, 'framework—indeed': 0.0003901677721420211, 'ﬁt': 0.0003901677721420211, 'duplicate': 0.0007803355442840422, 'function—': 0.0003901677721420211, 'Dietterich—however': 0.0003901677721420211, 'explanation-based': 0.0007803355442840422, 'considerably': 0.0003901677721420211, 'slower': 0.0003901677721420211, 'Feudal-Q': 0.0015606710885680843, 'level—that': 0.0003901677721420211, 'Discussion': 0.0003901677721420211, 'Design': 0.0003901677721420211, 'Tradeoﬀs': 0.0003901677721420211, 'concerning': 0.0003901677721420211, 'reinforce-ment': 0.0003901677721420211, 'architectures': 0.0003901677721420211, 'highlight': 0.0003901677721420211, 'drawbacks': 0.0007803355442840422, 'guessing': 0.0003901677721420211, 'leads': 0.0003901677721420211, 'Recursively': 0.0003901677721420211, 'ones': 0.0003901677721420211, 'substantial': 0.0007803355442840422, 'obtains': 0.0003901677721420211, 'become': 0.0003901677721420211, 'larger': 0.0003901677721420211, 'exiting': 0.0007803355442840422, 'doors': 0.0003901677721420211, '56': 0.0003901677721420211, 'reuse': 0.0007803355442840422, 'barrier': 0.0003901677721420211, 'communication': 0.0003901677721420211, 'propagate': 0.0007803355442840422, 'leaving': 0.0003901677721420211, 'short': 0.0003901677721420211, 'Methods': 0.0007803355442840422, 'freedom': 0.0003901677721420211, 'contexts': 0.0007803355442840422, 'isolates': 0.0003901677721420211, 'reused': 0.0003901677721420211, 'iterative': 0.0007803355442840422, 'doorways': 0.0003901677721420211, 'Based': 0.0003901677721420211, 'computed—while': 0.0003901677721420211, 'treating': 0.0003901677721420211, 'sequentially': 0.0003901677721420211, 'oﬄine': 0.0003901677721420211, 'stop': 0.0003901677721420211, 'iterated': 0.0003901677721420211, 'indeﬁnitely': 0.0003901677721420211, 'extension': 0.0003901677721420211, 'adapts': 0.0003901677721420211, 'precise': 0.0003901677721420211, '˜V': 0.0015606710885680843, 'accomplished': 0.0003901677721420211, '57': 0.0003901677721420211, 'Such': 0.0003901677721420211, 'worthwhile': 0.0007803355442840422, 'aggressive': 0.0003901677721420211, 'initialize': 0.0003901677721420211, 'detailed': 0.0003901677721420211, 'progressive': 0.0003901677721420211, 'reﬁnements': 0.0003901677721420211, 'guided': 0.0003901677721420211, 'monitoring': 0.0003901677721420211, 'vary': 0.0003901677721420211, 'variance': 0.0003901677721420211, 'failing': 0.0003901677721420211, 'distinctions': 0.0003901677721420211, 'reﬁned': 0.0003901677721420211, 'adaptive': 0.0003901677721420211, 'lifetime': 0.0003901677721420211, 'diagnosing': 0.0003901677721420211, 'repairing': 0.0003901677721420211, 'errors': 0.0003901677721420211, 'sub-optimalities': 0.0003901677721420211, 'ultimately': 0.0003901677721420211, 'Concluding': 0.0003901677721420211, 'Remarks': 0.0003901677721420211, 'learning—the': 0.0003901677721420211, 'decompo-sition': 0.0003901677721420211, 'criterion': 0.0007803355442840422, 'de-composed': 0.0003901677721420211, 'argued': 0.0011705033164260631, 'global': 0.0003901677721420211, 'increases': 0.0003901677721420211, 'showed': 0.0003901677721420211, 'learning—at': 0.0003901677721420211, 'formalized': 0.0003901677721420211, 'opti-mal': 0.0003901677721420211, 'governing': 0.0003901677721420211, 'spectrum': 0.0007803355442840422, 'free': 0.0003901677721420211, 'context-sensitive': 0.0003901677721420211, 'meth-ods': 0.0003901677721420211, '58': 0.0003901677721420211, 'speedups': 0.0003901677721420211, 'enabled': 0.0003901677721420211, 'preferred—and': 0.0003901677721420211, 'relaxed': 0.0003901677721420211, 'Acknowledgements': 0.0003901677721420211, 'gratefully': 0.0003901677721420211, 'acknowledges': 0.0003901677721420211, 'National': 0.0007803355442840422, 'Science': 0.0019508388607101053, 'Foundation': 0.0003901677721420211, 'grant': 0.0011705033164260631, 'IRI-9626584': 0.0003901677721420211, 'Oﬃce': 0.0007803355442840422, 'Naval': 0.0003901677721420211, 'N00014-95-1-0557': 0.0003901677721420211, 'Air': 0.0003901677721420211, 'Force': 0.0003901677721420211, 'Scientiﬁc': 0.0011705033164260631, 'F49620-98-1-0375': 0.0003901677721420211, 'Spanish': 0.0003901677721420211, 'Council': 0.0003901677721420211, 'indebted': 0.0003901677721420211, 'colleagues': 0.0003901677721420211, 'helping': 0.0003901677721420211, 'clarify': 0.0003901677721420211, 'Valentina': 0.0003901677721420211, 'Bayer': 0.0003901677721420211, 'Bill': 0.0003901677721420211, 'Langford': 0.0003901677721420211, 'Wes': 0.0003901677721420211, 'Pinchot': 0.0003901677721420211, 'Rich': 0.0003901677721420211, 'Prasad': 0.0003901677721420211, 'Sebastian': 0.0007803355442840422, 'Thrun': 0.0007803355442840422, 'I': 0.0007803355442840422, 'thank': 0.0007803355442840422, 'Eric': 0.0003901677721420211, 'Chown': 0.0003901677721420211, 'encouraging': 0.0007803355442840422, 'study': 0.0003901677721420211, 'comprehensive': 0.0003901677721420211, 'anonymous': 0.0003901677721420211, 'reviewers': 0.0003901677721420211, 'drafts': 0.0003901677721420211, 'suggestions': 0.0003901677721420211, 'reading': 0.0003901677721420211, 'immeasurably': 0.0003901677721420211, 'References': 0.0003901677721420211, 'Belmont': 0.0003901677721420211, 'MA': 0.0023410066328521262, 'Programming': 0.0011705033164260631, 'Princeton': 0.0003901677721420211, 'University': 0.0031213421771361686, 'Press': 0.0023410066328521262, 'P.': 0.00351150994927819, 'J.': 0.0011705033164260631, 'N.': 0.0011705033164260631, 'Neuro-Dynamic': 0.0003901677721420211, 'Athena': 0.0003901677721420211, 'C.': 0.0031213421771361686, 'Y.': 0.0003901677721420211, 'M.': 0.0019508388607101053, 'Economic': 0.0003901677721420211, 'principles': 0.0003901677721420211, 'multi-agent': 0.0003901677721420211, 'Editorial': 0.0003901677721420211, 'Artiﬁcial': 0.0031213421771361686, 'Intelligence': 0.0031213421771361686, '94': 0.0003901677721420211, '1–2': 0.0003901677721420211, '1–6': 0.0003901677721420211, 'T.': 0.0027311744049941474, 'theoretic': 0.0003901677721420211, 'Structural': 0.0003901677721420211, 'leverage': 0.0003901677721420211, 'Journal': 0.0003901677721420211, 'O-plan': 0.0003901677721420211, 'architecture': 0.0003901677721420211, '49–86': 0.0003901677721420211, 'Advances': 0.0011705033164260631, 'Neural': 0.0015606710885680843, 'Information': 0.0015606710885680843, 'Processing': 0.0011705033164260631, 'Systems': 0.0015606710885680843, 'pp': 0.0019508388607101053, '271–278': 0.0003901677721420211, 'Morgan': 0.0019508388607101053, 'Kaufmann': 0.0019508388607101053, 'San': 0.0019508388607101053, 'Francisco': 0.0015606710885680843, 'CA': 0.0019508388607101053, 'S.-H.': 0.0003901677721420211, 'Tech': 0.0019508388607101053, 'rep.': 0.0019508388607101053, 'CS-95-10': 0.0003901677721420211, 'Department': 0.0023410066328521262, 'Computer': 0.0019508388607101053, 'Brown': 0.0007803355442840422, 'Providence': 0.0007803355442840422, 'Rhode': 0.0003901677721420211, 'Island': 0.0003901677721420211, 'Fifteenth': 0.0003901677721420211, 'International': 0.0011705033164260631, 'Conference': 0.0019508388607101053, 'Machine': 0.0023410066328521262, 'J': 0.0007803355442840422, '251–288': 0.0003901677721420211, 'L.': 0.0011705033164260631, 'processes': 0.0007803355442840422, 'macro-actions': 0.0003901677721420211, 'RI': 0.0003901677721420211, 'Processes': 0.0003901677721420211, 'MIT': 0.0019508388607101053, 'Cambridge': 0.0019508388607101053, '59': 0.0003901677721420211, 'I.': 0.0003901677721420211, 'Computation': 0.0003901677721420211, '1185–1201': 0.0003901677721420211, 'Preliminary': 0.0003901677721420211, 'Proceedings': 0.0015606710885680843, 'Tenth': 0.0003901677721420211, '167–173': 0.0003901677721420211, 'Eighth': 0.0003901677721420211, '923–928': 0.0003901677721420211, 'Boston': 0.0003901677721420211, 'AAAI': 0.0003901677721420211, 'Macro-operators': 0.0003901677721420211, 'weak': 0.0003901677721420211, '35–77': 0.0003901677721420211, 'L.-J': 0.0003901677721420211, 'robots': 0.0003901677721420211, 'Ph.D.': 0.0011705033164260631, 'thesis': 0.0011705033164260631, 'Carnegie': 0.0003901677721420211, 'Mellon': 0.0003901677721420211, 'Pittsburgh': 0.0003901677721420211, 'PA.': 0.0003901677721420211, 'Flexible': 0.0003901677721420211, 'weakly': 0.0003901677721420211, 'coupled': 0.0003901677721420211, 'markov': 0.0003901677721420211, 'Fourteenth': 0.0007803355442840422, 'Annual': 0.0003901677721420211, 'Uncertainty': 0.0003901677721420211, 'UAI–98': 0.0003901677721420211, '422–430': 0.0003901677721420211, 'Publishers': 0.0003901677721420211, 'California': 0.0007803355442840422, 'Berkeley': 0.0003901677721420211, 'Vol': 0.0007803355442840422, 'Probabilistic': 0.0003901677721420211, 'Inference': 0.0007803355442840422, 'Intelligent': 0.0003901677721420211, 'Plausible': 0.0003901677721420211, 'Mateo': 0.0003901677721420211, 'A.': 0.0007803355442840422, 'Online': 0.0003901677721420211, 'Qlearning': 0.0003901677721420211, 'connectionist': 0.0003901677721420211, 'CUED/FINFENG/TR': 0.0003901677721420211, '166': 0.0003901677721420211, 'Engineering': 0.0003901677721420211, 'England': 0.0003901677721420211, 'spaces': 0.0003901677721420211, '115–135': 0.0003901677721420211, 'single-step': 0.0003901677721420211, 'on-policy': 0.0003901677721420211, 'reinforcement-learning': 0.0003901677721420211, 'Colorado': 0.0003901677721420211, 'Boulder': 0.0003901677721420211, 'CO.': 0.0003901677721420211, 'Transfer': 0.0007803355442840422, 'composing': 0.0007803355442840422, 'elemental': 0.0007803355442840422, 'sequential': 0.0007803355442840422, '323–339': 0.0003901677721420211, '323': 0.0003901677721420211, 'Improved': 0.0003901677721420211, 'switching': 0.0003901677721420211, '60': 0.0003901677721420211, 'Between': 0.0003901677721420211, 'Semi-MDPs': 0.0003901677721420211, 'plan-ning': 0.0003901677721420211, 'Mas-sachusetts': 0.0003901677721420211, 'Sciences': 0.0003901677721420211, 'Amherst': 0.0003901677721420211, '358–366': 0.0003901677721420211, 'Delayed': 0.0003901677721420211, 'Rewards': 0.0003901677721420211, 'King': 0.0003901677721420211, 'College': 0.0003901677721420211, 'Oxford': 0.0003901677721420211, 'reprinted': 0.0003901677721420211, 'Press.': 0.0003901677721420211, 'Technical': 0.0003901677721420211, 'q-learning': 0.0003901677721420211, '279': 0.0003901677721420211, '61': 0.0003901677721420211, 'Goal': 0.0003901677721420211, '62': 0.0003901677721420211, 'QExitHall': 0.0003901677721420211, 'MaxExitInter': 0.0003901677721420211, 'QSniffEI': 0.0003901677721420211, 'QBackEI': 0.0003901677721420211, 'QSniffEH': 0.0003901677721420211, 'QBackEH': 0.0003901677721420211, 'MaxSniff': 0.0003901677721420211, 'MaxBack': 0.0003901677721420211, 'x/X': 0.0007803355442840422, 'y/Y': 0.0007803355442840422, 'QFollowWall': 0.0003901677721420211, 'QToWall': 0.0003901677721420211, 'QBackOne': 0.0003901677721420211, 'QPerpThree': 0.0003901677721420211, 'MaxFollowWall': 0.0003901677721420211, 'MaxToWall': 0.0003901677721420211, 'MaxBackOne': 0.0003901677721420211, 'MaxPerpThree': 0.0003901677721420211, 'd/p': 0.0007803355442840422, 'd/d': 0.0003901677721420211, 'd/Inv': 0.0003901677721420211, 'QMoveFW': 0.0003901677721420211, 'QMoveTW': 0.0003901677721420211, 'QMoveBO': 0.0003901677721420211, 'QMoveP3': 0.0003901677721420211, 'r/ROOM': 0.0003901677721420211, 'MaxMove': 0.0003901677721420211, '63': 0.0003901677721420211, '-350': 0.0003901677721420211, '-150': 0.0003901677721420211, '-250': 0.0003901677721420211, '-300': 0.0003901677721420211, '-450': 0.0003901677721420211, '-500': 0.0003901677721420211, '2e+06': 0.0003901677721420211, '4e+06': 0.0003901677721420211, '5e+06': 0.0003901677721420211, '6e+06': 0.0003901677721420211, '3e+06': 0.0003901677721420211, 'Steps': 0.0003901677721420211, '64': 0.0003901677721420211}\n"
     ]
    }
   ],
   "source": [
    "mostFreqy = max(wordFreqs.values())\n",
    "\n",
    "for word in wordFreqs.keys():\n",
    "    wordFreqs[word] = (wordFreqs[word]/mostFreqy)\n",
    "    \n",
    "print(wordFreqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentance Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "\n",
    "for sentence in sentence_tokens:\n",
    "    for word in nltk.word_tokenize(sentence.lower()):\n",
    "        if word in wordFreqs.keys():\n",
    "            if len(sentence.split(' ')) <30:\n",
    "                if sentence not in sentence_scores.keys():\n",
    "                    sentence_scores[sentence] = wordFreqs[word]\n",
    "                else:\n",
    "                    sentence_scores[sentence] += wordFreqs[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The decomposition, known as the MAXQ decomposition, has both a procedural semantics—as a subroutine hierarchy—and a declarative semantics—as a representation of the value function of a hierarchical policy.': 2.9781506047600477,\n",
       " 'MAXQ uniﬁes and extends previous work on hierar-chical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton.': 2.669137729223566,\n",
       " 'It is based on the assumption that the programmer can identify useful subgoals and deﬁne subtasks that achieve these subgoals.': 0.647288333983613,\n",
       " 'By deﬁning such subgoals, the programmer constrains the set of policies that need to be considered during reinforcement learning.': 1.728443230589153,\n",
       " 'The MAXQ value function decomposition can represent the value function of any policy that is consistent with the given hierarchy.': 1.0819352321498246,\n",
       " 'The decomposition also creates opportunities to exploit state abstractions, so that individual MDPs within the hierarchy can ignore large parts of the state space.': 1.9399141630901289,\n",
       " 'This is important for the practical application of the method.': 0.6211470932500975,\n",
       " 'This paper deﬁnes the MAXQ hierarchy, proves formal results on its representational power, and establishes ﬁve conditions for the safe use of state abstractions.': 2.813889972688256,\n",
       " 'The paper demonstrates the eﬀectiveness of this non-hierarchical execution experimentally.': 0.6320717908700741,\n",
       " 'Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoﬀs in hierarchical reinforcement learning.': 1.7366367538041358,\n",
       " 'One fruitful line of research views agents from an “economic” perspective (Boutilier, Shoham, & Wellman, 1997): An agent interacts with an environment and receives real-valued rewards and penalties.': 4.86110027311744,\n",
       " 'The agent’s goal is to maximize the total reward it receives.': 0.7116660163870465,\n",
       " 'The economic view makes it easy to formalize traditional goals of achievement (“land this airplane”).': 1.7257120561841595,\n",
       " 'Goals of achievement can be represented by giving a positive reward for achieving the goal.': 0.6792820912992588,\n",
       " 'Hence, recent research has fo-cused on methods that can exploit structure within the planning problem to work more eﬃciently (Boutilier, Dean, & Hanks, 1999).': 5.7065938353492,\n",
       " 'Again, the basic methods in reinforcement learning are based on dynamic programming algorithms.': 1.6937182988685133,\n",
       " 'However, rein-forcement learning methods oﬀer two important advantages over classical dynamic programming.': 1.699570815450644,\n",
       " 'First, the methods are online.': 1.5989075302380023,\n",
       " 'This permits them to focus their attention on the parts of the state space that are important and ignore the rest of the space.': 0.7467811158798283,\n",
       " 'Second, the methods can employ function approximation algorithms (e.g., neural networks) to represent their knowledge.': 3.7643386656262194,\n",
       " 'This allows them to generalize across the state space so that the learning time scales much better.': 0.8045259461568475,\n",
       " 'Despite the recent advances in both probabilistic planning and reinforcement learning, there are still many shortcomings.': 1.6746000780335546,\n",
       " 'The biggest of these is the lack of a fully satisfactory method for incorporating hierarchies into these algorithms.': 0.6199765899336716,\n",
       " 'However, all of the basic algorithms for probabilistic planning and reinforcement learning are “ﬂat” methods— they treat the state space as one huge ﬂat search space.': 1.9449863441279753,\n",
       " 'This research has explored many diﬀerent points in the design space of hierarchical methods, but several of these systems were designed for speciﬁc situations.': 1.6894264533749512,\n",
       " 'We lack crisp deﬁnitions of the main approaches and a clear understanding of the relative merits of the diﬀerent methods.': 0.6152945766679673,\n",
       " 'This paper formalizes and clariﬁes one approach and attempts to understand how it compares with the other techniques.': 0.6398751463129145,\n",
       " 'The approach, called the MAXQ method, provides a hierarchical decom-position of the given reinforcement learning problem into a set of subproblems.': 2.7943815840811554,\n",
       " 'It simultaneously provides a decomposition of the value function for the given problem into a set of value functions for the subproblems.': 0.8938743659773702,\n",
       " 'Hence, it has both a declarative semantics (as a value function decomposition) and a procedural semantics (as a subroutine hierarchy).': 3.875536480686696,\n",
       " 'A review of previous research shows that there are several important design decisions that must be made when constructing a hierarchical reinforcement learning system.': 0.777214202106906,\n",
       " 'As a way of providing an overview of the results in this paper, let us review these issues and see how the MAXQ method approaches each of them.': 1.6769410846664066,\n",
       " 'The ﬁrst issue is how subtasks should be speciﬁed.': 0.6250487709715178,\n",
       " 'Hierarchical reinforcement learning involves breaking the target Markov decision problem into a hierarchy of subproblems or subtasks.': 0.777604369879048,\n",
       " 'There are three general approaches to deﬁning these subtasks.': 0.6277799453765119,\n",
       " 'One approach is to deﬁne each subtask in terms of a ﬁxed policy that is provided by the programmer.': 0.8369098712446352,\n",
       " 'The “option” method of Sutton, Precup, and Singh (1998) takes this approach.': 3.7549746390948107,\n",
       " 'The second approach is to deﬁne each subtask in terms of a non-deterministic ﬁnite-state controller.': 0.6734295747171284,\n",
       " 'The Hierarchy of Abstract Machines (HAM) method of Parr and Russell (1998) takes this approach.': 2.757705813499805,\n",
       " 'The third approach is to deﬁne each subtask in terms of a termination predicate and a local reward function.': 0.8095981271946937,\n",
       " 'These deﬁne what it means for the subtask to be completed and what the ﬁnal reward should be for completing the subtask.': 0.7456106125634023,\n",
       " 'The MAXQ method described in this paper follows this approach, building upon previous work by Singh (1992a), Kaelbling (1993), Dayan and Hinton (1993), and Dean and Lin (1995).': 8.840421381193913,\n",
       " 'On the other hand, the termination predicate method requires the programmer to guess the relative desirability of the diﬀerent states in which the subtask might terminate.': 1.7815060476004685,\n",
       " 'This can also be diﬃcult, although Dean and Lin show how these guesses can be revised automatically by the learning algorithm.': 1.6987904799063596,\n",
       " 'A potential drawback of all hierarchical methods is that the learned policy may be suboptimal.': 0.7932110807647288,\n",
       " 'The programmer-provided hierarchy constrains the set of possible policies that can be considered.': 0.6679672259071401,\n",
       " 'If these constraints are poorly chosen, the resulting policy will be suboptimal.': 1.7370269215762777,\n",
       " 'The termination predicate method suﬀers from an additional source of suboptimality.': 0.6441669918064769,\n",
       " 'The learning algorithm described in this paper converges to a form of local optimality that we call recursive optimality.': 0.7389777604369879,\n",
       " 'This means that the policy of each subtask is locally optimal given the policies of its children.': 0.870464299648849,\n",
       " 'But there might exist better hierarchical policies where the policy for a subtask must be locally suboptimal so that the overall policy is optimal.': 1.0678891923527116,\n",
       " 'This problem can be avoided by careful deﬁnition of termination predicates and local reward functions, but this is an added burden on the programmer.': 1.6972298088177917,\n",
       " '(It is interesting to note that this problem of recursive optimality has not been noticed previously.': 1.1467030823253999,\n",
       " 'This is because previous work focused on subtasks with a single terminal state,  3  and in such cases, the problem does not arise.)': 3.30706203667577,\n",
       " 'The second design issue is whether to employ state abstractions within subtasks.': 0.7811158798283262,\n",
       " 'A subtask employs state abstraction if it ignores some aspects of the state of the environment.': 0.9110417479516193,\n",
       " 'For example, in many robot navigation problems, choices about what route to take to reach a goal location are independent of what the robot is currently carrying.': 2.6902067889192356,\n",
       " 'With few exceptions, state abstraction has not been explored previously.': 1.731954740538432,\n",
       " 'We will see that the MAXQ method creates many opportunities to exploit state abstraction, and that these abstractions can have a huge impact in accelerating learning.': 1.8646117830667188,\n",
       " 'This is why the MAXQ method must employ termination predicates, despite the problems that this can create.': 1.6726492391728445,\n",
       " 'The third design issue concerns the non-hierarchical “execution” of a learned hierarchical pol-icy.': 0.760827155676941,\n",
       " 'Kaelbling (1993) was the ﬁrst to point out that a value function learned from a hierarchical policy could be evaluated incrementally to yield a potentially much better non-hierarchical policy.': 2.149824424502536,\n",
       " 'Dietterich (1998) and Sutton, Singh, Precup, and Ravindran (1999) generalized this to show how arbitrary subroutines could be executed non-hierarchically to yield improved policies.': 5.740928599297698,\n",
       " 'However, in order to support this non-hierarchical execution, extra learning is required.': 2.6933281310963713,\n",
       " 'But to support non-hierarchical execution, learning is required in all states (and at all levels of the hierarchy).': 2.8017947717518528,\n",
       " 'In general, this requires additional exploration as well as additional computation and memory.': 1.6328521264143583,\n",
       " 'The fourth and ﬁnal issue is what form of learning algorithm to employ.': 0.685134607881389,\n",
       " 'An important advantage of reinforcement learning algorithms is that they typically operate online.': 0.6824034334763949,\n",
       " 'However, ﬁnding online algorithms that work for general hierarchical reinforcement learning has been diﬃcult, particularly within the termination predicate family of methods.': 2.78384705423332,\n",
       " 'The best previous online algorithms are the HAMQ Q learning algorithm of Parr and Russell (for the partial policy method) and the Feudal Q algorithm of Dayan and Hinton.': 1.9176746000780334,\n",
       " 'Unfortunately, the HAMQ method requires “ﬂattening” the hierarchy, and this has several undesirable consequences.': 2.7296137339055786,\n",
       " 'The Feudal Q algorithm is tailored to a speciﬁc kind of problem, and it does not converge to any well-deﬁned optimal policy.': 1.8213031603589545,\n",
       " 'In this paper, we present a general algorithm, called MAXQ-Q, for fully-online learning of a hierarchical value function.': 3.89192352711666,\n",
       " 'We show experimentally and theoretically that the algorithm converges to a recursively optimal policy.': 0.8244245025360906,\n",
       " 'We also show that it is substantially faster than “ﬂat” (i.e., non-hierarchical) Q learning when state abstractions are employed.': 2.969957081545065,\n",
       " 'Without state abstractions, it gives performance similar to (or even worse than) the HAMQ algorithm.': 2.8240343347639483,\n",
       " 'The remainder of this paper is organized as follows.': 0.6055403823644168,\n",
       " 'After introducing our notation in Section 2, we deﬁne the MAXQ value function decomposition in Section 3 and illustrate it with a sim-ple example Markov decision problem.': 1.833008193523215,\n",
       " 'Section 4 presents an analytically tractable version of the MAXQ-Q learning algorithm called the MAXQ-0 algorithm and proves its convergence to a recur-sively optimal policy.': 0.9110417479516192,\n",
       " 'It then shows how to extend MAXQ-0 to produce the MAXQ-Q algorithm, and shows how to extend the theorem similarly.': 1.6332422941865001,\n",
       " 'Section 5 takes up the issue of state abstraction and formalizes a series of ﬁve conditions under which state abstractions can be safely incorporated into the MAXQ representation.': 0.9395239953179867,\n",
       " 'State abstraction can give rise to a hierarchical credit assignment  4  problem, and the paper brieﬂy discusses one solution to this problem.': 1.8852906749902458,\n",
       " 'Finally, Section 7 presents experiments with three example domains.': 1.6234880998829497,\n",
       " 'These experiments give some idea of the generality of the MAXQ representation.': 0.6160749122122513,\n",
       " 'They also provide results on the relative importance of temporal and state abstractions and on the importance of non-hierarchical execution.': 0.7904799063597348,\n",
       " 'Some readers may be disappointed that MAXQ provides no way of learning the structure of the hierarchy.': 0.6905969566913773,\n",
       " 'Belief networks were ﬁrst introduced as a formalism in which the knowledge engineer would describe the structure of the networks and domain experts would provide the necessary probability estimates.': 0.6773312524385486,\n",
       " 'Subsequently, methods were developed for learning the probability values directly from observational data.': 1.740148263753414,\n",
       " 'Most recently, several methods have been developed for learning the structure of the belief networks from data, so that the dependence on the knowledge engineer is reduced.': 2.6820132657042537,\n",
       " 'In this paper, we will likewise require that the programmer provide the structure of the hierarchy.': 1.6484588373000393,\n",
       " 'The programmer will also need to make several important design decisions.': 0.646898166211471,\n",
       " 'Our learning algorithms will ﬁll in “implementations” of each module in such a way that the overall program will work well.': 0.7549746390948108,\n",
       " 'We believe that this approach will provide a practical tool for solving large real-world MDPs.': 0.610612563402263,\n",
       " 'We also believe that it will help us understand the structure of hierarchical learning algorithms.': 0.7374170893484199,\n",
       " 'It is our hope that subsequent research will be able to automate most of the work that we are currently requiring the programmer to do.': 0.6117830667186891,\n",
       " '2 Formal Deﬁnitions  2.1 Markov Decision Problems and Semi-Markov Decision Problems  We employ the standard deﬁnitions for Markov Decision Problems and Semi-Markov Decision Problems.': 0.649629340616465,\n",
       " 'In this paper, we restrict our attention to situations in which an agent is interacting with a fully-observable stochastic environment.': 1.6379243074522045,\n",
       " 'At each point in time, the agent can observe  the complete state of the environment.': 1.7600468201326571,\n",
       " '• A: this is a ﬁnite set of actions.': 0.7264923917284433,\n",
       " 'Technically, the set of available actions depends on the  current state s, but we will suppress this dependence in our notation.': 2.790479906359735,\n",
       " '• R: Similarly, when action a is performed and the environment makes its transition from s to s′, the agent receives a real-valued (possibly stochastic) reward R(s′|s, a).': 6.014436207569255,\n",
       " '5  • P0: This is the starting state distribution.': 0.8201326570425282,\n",
       " 'When the MDP is initialized, it is in state s with  probability P0(s).': 2.77214202106906,\n",
       " 'In the episodic setting, all rewards are ﬁnite and there is at least one zero-cost absorbing terminal state.': 1.7604369879047992,\n",
       " 'An absorbing terminal state is a state in which all actions lead back to the same state with probability 1 and zero reward.': 1.1002731174404996,\n",
       " 'In this setting, the goal of the agent is to ﬁnd a policy that maximizes the expected cumulative reward.': 1.8454935622317594,\n",
       " 'In the inﬁnite horizon setting, all rewards are also ﬁnite.': 1.6152945766679672,\n",
       " 'In addition, there is a discount factor γ, and the agent’s goal is to ﬁnd a policy that minimizes the inﬁnite discounted sum of future rewards.': 2.834568864611783,\n",
       " 'The value function V π for policy π is a function that tells, for each state s, what the expected cumulative reward will be of executing that policy.': 3.4284042138119393,\n",
       " 'Let rt be a random variable that tells the reward that the agent receives at time step t while following policy π.': 0.9153335934451814,\n",
       " 'We can see that this equation reduces to the previous one when γ = 1.': 0.7381974248927039,\n",
       " 'However, in the inﬁnite horizon case, this inﬁnite sum will not converge unless γ < 1.': 2.6648458837300044,\n",
       " '(1)  a  Xs′  (cid:2)  (cid:3)  There may be many optimal policies that achieve this value.': 3.916894264533749,\n",
       " 'Any policy that chooses a in s to achieve the maximum on the right-hand side of this equation is an optimal policy.': 0.9219664455715958,\n",
       " 'We will denote an optimal policy by π∗.': 0.7764338665626219,\n",
       " 'Note that all optimal policies are “greedy” with respect to the backed-up value of the available actions.': 0.8747561451424113,\n",
       " 'Closely related to the value function is the so-called action-value function, or Q function (Watkins, 1989).': 3.943425673039407,\n",
       " '(cid:21)  (2)  Note that any policy that is greedy with respect to Q∗ is an optimal policy.': 3.0206788919235272,\n",
       " 'There may be many such optimal policies—they diﬀer only in how they break ties between actions with identical Q∗ values.': 0.7495122902848226,\n",
       " 'An action order, denoted ω, is a total order over the actions within an MDP.': 2.7116660163870465,\n",
       " 'That is, ω is an anti-symmetric, transitive relation such that ω(a1, a2) is true iﬀ a1 is preferred to a2.': 4.6636753804135775,\n",
       " 'An ordered greedy policy, πω is a greedy policy that breaks ties using ω.': 1.914943425673039,\n",
       " 'For example, suppose that the two best actions at state s are a1 and a2, that Q(s, a1) = Q(s, a2), and that ω(a1, a2).': 10.025751072961375,\n",
       " 'Then the ordered greedy policy πω will choose a1: πω(s) = a1.': 1.9403043308622707,\n",
       " 'Note that although there may be many optimal policies for a given MDP, the ordered greedy policy, π∗  ω, is unique.': 3.8677331252438547,\n",
       " 'A discrete-time semi-Markov Decision Process (SMDP) is a generalization of the Markov Deci-sion Process in which the actions can take a variable amount of time to complete.': 1.70464299648849,\n",
       " '(3)  where R(s, π(s)) is the expected reward of performing action π(s) in state s, where the expectation is taken with respect to s′ and N .': 7.2419040187280554,\n",
       " 'Note that for the episodic case, there is no diﬀerence between a MDP and a Semi-Markov  Decision Process.': 1.611392898946547,\n",
       " 'In our case, it is important to consider the joint distribution of s′ and N , but we do not need to consider actions with arbitrary real-valued durations.': 2.7272727272727266,\n",
       " 'The learning algorithm is evaluated based on its observed cumulative reward.': 0.7229808817791651,\n",
       " 'The cumulative reward of a good learning algorithm should converge to the cumulative reward of the optimal policy for the MDP.': 0.9504486929379633,\n",
       " 'In this paper, we will make use of two well-known learning algorithms: Q learning (Watkins, 1989; Watkins & Dayan, 1992) and SARSA(0) (Rummery & Niranjan, 1994).': 8.017947717518533,\n",
       " 'Both of these algorithms maintain a tabular representation of the action-value function Q(s, a).': 2.73741708934842,\n",
       " 'Every entry of the table is initialized arbitrarily.': 0.5899336714787359,\n",
       " 'Their proof holds in both settings discussed in this paper (episodic and inﬁnite-horizon).': 1.6543113538821694,\n",
       " 'The SARSA(0) algorithm is very similar.': 1.6765509168942645,\n",
       " 'A GLIE policy is deﬁned as follows:  Deﬁnition 1 A GLIE (greedy in the limit with inﬁnite exploration) policy is any policy satisfying  1.': 2.212251268045259,\n",
       " 'Each action is executed inﬁnitely often in every state that is visited inﬁnitely often.': 0.7822863831447523,\n",
       " '2.': 1.1954740538431525,\n",
       " 'In the limit, the policy is greedy with respect to the Q-value function with probability 1.': 1.872805306281701,\n",
       " 'MAXQ describes how to decompose the overall value function for a policy into a collection of value functions for individual subtasks (and subsubtasks, recursively).': 3.042138119391338,\n",
       " '3.1 A Motivating Example  To make the discussion concrete, let us consider the following simple example.': 1.66796722590714,\n",
       " 'Figure 1 shows a 5-by-5 grid world inhabited by a taxi agent.': 0.6753804135778385,\n",
       " 'There are four specially-designated locations in In this world, marked as R(ed), B(lue), G(reen), and Y(ellow).': 8.824424502536091,\n",
       " 'The taxi problem is episodic.': 0.6266094420600858,\n",
       " 'each episode, the taxi starts in a randomly-chosen square.': 1.613733905579399,\n",
       " 'There is a passenger at one of the four locations (chosen randomly), and that passenger wishes to be transported to one of the four locations (also chosen randomly).': 3.8419820522824812,\n",
       " 'The taxi must go to the passenger’s location (the “source”), pick up the passenger, go to the destination location (the “destination”), and put down the passenger there.': 6.029262582910652,\n",
       " '(To keep things uniform, the taxi must pick up and drop oﬀ the passenger even if he/she is already located at the destination!)': 2.1412407335154118,\n",
       " 'The episode ends when the passenger is deposited at the destination location.': 0.6348029652750683,\n",
       " 'Each action is deterministic.': 0.6426063207179087,\n",
       " 'There is a reward of −1 for each action and an additional reward of +20 for successfully delivering the passenger.': 0.7448302770191183,\n",
       " 'There is a reward of −10 if the taxi attempts to execute the Putdown or Pickup actions illegally.': 0.6921576277799454,\n",
       " 'If a navigation action would cause the taxi to hit a wall, the action is a no-op, and there is only the usual reward of −1.': 2.779555208739758,\n",
       " 'We seek a policy that maximizes the total reward per episode.': 0.7643386656262193,\n",
       " 'There are 500 possible states: 25 squares, 5 locations for the passenger (counting the four starting locations and the taxi), and 4 destinations.': 3.877877487319548,\n",
       " 'This task has a simple hierarchical structure in which there are two main sub-tasks: Get the passenger and Deliver the passenger.': 0.8056964494732735,\n",
       " 'Each of these subtasks in turn involves the subtask of navigating to one of the four locations and then performing a Pickup or Putdown action.': 0.7733125243854858,\n",
       " 'This task illustrates the need to support temporal abstraction, state abstraction, and subtask sharing.': 2.859929769801014,\n",
       " 'The top level policy (get passenger; deliver passenger) can be expressed very simply if these temporal abstractions can be  9  employed.': 1.8587592664845884,\n",
       " 'The need for state abstraction is perhaps less obvious.': 0.7440499414748343,\n",
       " 'Consider the subtask of getting the passenger.': 0.6699180647678502,\n",
       " 'While this subtask is being solved, the destination of the passenger is completely irrelevant—it cannot aﬀect any of the nagivation or pickup decisions.': 1.6738197424892705,\n",
       " 'Perhaps more importantly, when navigating to a target location (either the source or destination location of the passenger), only the identity of the target location is important.': 3.7428794381584076,\n",
       " 'The fact that in some cases the taxi is carrying the passenger and in other cases it is not is irrelevant.': 0.6484588373000391,\n",
       " 'Finally, support for subtask sharing is critical.': 1.6402653140850567,\n",
       " 'We will show below that the MAXQ method provides a value function representation and learning algorithm that supports temporal abstraction, state abstraction, and subtask sharing.': 3.1026141240733516,\n",
       " 'To construct a MAXQ decomposition for the taxi problem, we must identify a set of individual subtasks that we believe will be important for solving the overall task.': 1.7725321888412011,\n",
       " 'In this case, let us deﬁne the following four tasks:  • Navigate(t).': 2.789309403043309,\n",
       " 'In this subtask, the goal is to move the taxi from its current location to the passenger’s  current location and pick up the passenger.': 1.833008193523215,\n",
       " '• Put.': 0.6063207179087008,\n",
       " 'The goal of this subtask is to move the taxi from the current location to the passenger’s  destination location and drop oﬀ the passenger.': 0.8287163480296528,\n",
       " '• Root.': 0.610612563402263,\n",
       " 'This is the whole taxi task.': 0.6418259851736247,\n",
       " 'Each of these subtasks is deﬁned by a subgoal, and each subtask terminates when the subgoal  is achieved.': 1.6859149434256733,\n",
       " 'After deﬁning these subtasks, we must indicate for each subtask which other subtasks or prim-itive actions it should employ to reach its goal.': 1.8029652750682796,\n",
       " 'For example, the Navigate(t) subtask should use the four primitive actions North, South, East, and West.': 5.775653531018338,\n",
       " 'The Get subtask should use the Navigate subtask and the Pickup primitive action, and so on.': 1.779165040967616,\n",
       " 'All of this information can be summarized by a directed acyclic graph called the task graph, which is shown in Figure 2.': 1.7186890362856029,\n",
       " 'The notation f ormal/actual (e.g., t/source) tells how a formal parameter is to be bound to an actual parameter.': 2.6625048770971516,\n",
       " 'Now suppose that for each of these subtasks, we write a policy (e.g., as a computer program) to achieve the subtask.': 3.8505657432696054,\n",
       " 'We will refer to the policy for a subtask as a “subroutine”, and we can view the parent subroutine as invoking the child subroutine via ordinary subroutine-call-and-return semantics.': 1.902848224736637,\n",
       " 'If we have a policy for each subtask, then this gives us an overall policy for the Taxi MDP.': 1.941474834178697,\n",
       " 'The Root subtask executes its policy by calling subroutines that are policies for the Get and Put subtasks.': 0.8248146703082325,\n",
       " 'The Get policy calls subroutines for the Pickup primitive action and the Navigate(t) subtask.': 1.9005072181037848,\n",
       " 'And so on.': 0.5836909871244635,\n",
       " 'We will call this collection of policies a hierarchical policy.': 0.7896995708154506,\n",
       " 'In a hierarchical policy, each subroutine executes until it enters a terminal state for its subtask.': 1.957081545064378,\n",
       " '3.2 Deﬁnitions  Let us formalize the discussion so far.': 0.6152945766679673,\n",
       " 'The MAXQ decomposition takes a given MDP M and decomposes it into a set of subtasks {M0, M1, .': 2.665626219274288,\n",
       " '.': 26.849785407725303,\n",
       " ', Mn} with the convention that M0 is the root subtask (i.e., solving M0 solves the entire original MDP M ).': 3.7104955130706205,\n",
       " 'Deﬁnition 2 An unparameterized subtask is a three-tuple, hTi, Ai, ˜Rii, deﬁned as follows:  1.': 4.770581349980491,\n",
       " 'Ti(si) is a termination predicate that partitions S into a set of active states, Si and a set of terminal states, Ti.': 3.8002341006632845,\n",
       " 'The policy for subtask Mi can only be executed if the current state s is in Si.': 0.9133827545844713,\n",
       " 'Ai is a set of actions that can be performed to achieve subtask Mi.': 0.7019118220834959,\n",
       " 'We will refer to these actions as the “children” of subtask i.': 0.759266484588373,\n",
       " 'The set of actions Ai may diﬀer from one state to another, so technically, Ai is a function of s. However, we will suppress this dependence in our notation.': 3.8954350370659387,\n",
       " '3.': 0.5961763558330082,\n",
       " '˜Ri(s′|s, a) is the pseudo-reward function, which speciﬁes a pseudo-reward for each transition from a state s ∈ Si to a terminal state s′ ∈ Ti.': 4.055403823644166,\n",
       " 'This pseudo-reward tells how desirable each of the terminal states is for this subtask.': 0.7112758486149044,\n",
       " 'It is typically employed to give goal terminal states a pseudo-reward of 0 and any non-goal terminal states a negative reward.': 0.833008193523215,\n",
       " 'If a subtask has formal parameters, then each possible binding of actual values to the formal parameters speciﬁes a distinct subtask.': 1.7861880608661724,\n",
       " 'We can think of the values of the formal parameters as being part of the “name” of the subtask.': 0.7873585641825985,\n",
       " 'In practice, of course, we implement a parameterized subtask by parameterizing the various components of the task.': 2.683573936792821,\n",
       " 'If b speciﬁes the actual parameter values for task Mi, then we can deﬁne a parameterized termination predicate Ti(s, b) and a parameterized pseudo-reward function ˜Ri(s′|s, a, b).': 6.929379633242294,\n",
       " 'To simplify notation in the rest of the paper, we will usually omit these parameter bindings from our notation.': 1.6215372610222398,\n",
       " '11 Execute primitive action a, and update st+1 to be the resulting state of the environment.': 1.8248146703082326,\n",
       " '12  13 while top(Kt) speciﬁes a terminated subtask do 14  pop(Kt)  15 Kt+1 := Kt is the resulting execution stack.': 2.9192352711666025,\n",
       " 'Deﬁnition 3 A hierarchical policy, π, is a set containing a policy for each of the subtasks in the problem: π = {π0, .': 4.267264923917284,\n",
       " ', πn}.': 1.589543503706594,\n",
       " 'Table 1 gives a pseudo-code description of the procedure for executing a hierarchical policy.': 0.8181818181818181,\n",
       " 'The hierarchical policy is executed using a stack discipline, as in ordinary programming languages.': 1.8014046039797111,\n",
       " 'Let Kt denote the contents of the pushdown stack at time t. When a subroutine is invoked, its name and actual parameters are pushed onto the stack.': 1.6589933671478734,\n",
       " 'When a subroutine terminates, its name and actual parameters are popped oﬀ the stack.': 1.6269996098322277,\n",
       " 'It is sometimes useful to think of the contents of the stack as being an additional part of the state space for the problem.': 0.7538041357783847,\n",
       " 'Hence, a hierarchical policy implicitly deﬁnes a mapping from the current state st and current stack contents Kt to a primitive action a.': 2.0140460397971127,\n",
       " 'This action is executed, and this yields a resulting state st+1 and a resulting stack contents Kt+1.': 1.7978930940304332,\n",
       " 'Because of the added state information in the stack, the hierarchical policy is non-Markovian with respect to the original MDP.': 1.907530238002341,\n",
       " 'This is the value of executing the hierarchical policy beginning in state s and starting at the top level of the hierarchy.': 1.004291845493562,\n",
       " 'The decomposition is based on the following theorem:  Theorem 1 Given a task graph over tasks M0, .': 1.8174014826375342,\n",
       " 'Proof: Consider all of the subroutines that are descendants of task Mi in the task graph.': 0.7709715177526337,\n",
       " 'The set of states Si and the set of actions Ai are obvious.': 0.7093250097541943,\n",
       " 'The interesting part of this theorem is the fact that the expected reward function R(s, a) of the SMDP is the projected value function of the child task Ma.': 3.00585251658213,\n",
       " 'Now let us suppose that the ﬁrst action chosen by πi is a subroutine a.': 0.7050331642606321,\n",
       " 'This subroutine is in-i (s′, N |s, a).': 3.6999609832227858,\n",
       " 'Q.E.D.': 7.587982832618028,\n",
       " 'To obtain a hierarchical decomposition of the projected value function, let us switch to the action-value (or Q) representation.': 2.9001170503316427,\n",
       " 'First, we need to extend the Q notation to handle the task hierarchy.': 1.6589933671478734,\n",
       " 'Let Qπ(i, s, a) be the expected cumulative reward for subtask Mi of performing action a in state s and then following hierarchical policy π until subtask Mi terminates.': 4.268435427233711,\n",
       " 'These equations recursively decompose the projected value function for the root, V π(0, s) into the projected value functions for the individual subtasks, M1, .': 6.030823253999221,\n",
       " ', Mn and the individual completion functions C π(j, s, a) for j = 1, .': 5.916113928989465,\n",
       " 'To make it easier for programmers to design and debug MAXQ decompositions, we have de-veloped a graphical representation that we call the MAXQ graph.': 1.6562621927428796,\n",
       " 'A MAXQ graph for the Taxi domain is shown in Figure 3.': 0.6605540382364417,\n",
       " 'The graph contains two kinds of nodes, Max nodes and Q nodes.': 1.7077643386656263,\n",
       " 'Each primitive Max node i stores the value of V π(i, s).': 2.866952789699571,\n",
       " 'The Q nodes correspond to the actions that are available for each subtask.': 0.6952789699570816,\n",
       " 'Each Q node for parent task i, state s and subtask a stores the value of C π(i, s, a).': 5.042918454935623,\n",
       " 'In addition to storing information, the Max nodes and Q nodes can be viewed as performing parts of the computation described by the decomposition equations.': 1.6855247756535308,\n",
       " 'Speciﬁcally, each Max node i can be viewed as computing the projected value function V π(i, s) for its subtask.': 3.972298088177917,\n",
       " 'For primitive Max nodes, this information is stored in the node.': 1.701131486539212,\n",
       " 'For composite Max nodes, this information is obtained by “asking” the Q node corresponding to πi(s).': 2.803355442840422,\n",
       " 'Each Q node with parent task i and child task a can be viewed as computing the value of Qπ(i, s, a).': 3.8353492001560676,\n",
       " 'It does this by “asking” its child task a for its projected value function V π(a, s) and then adding its completion function C π(i, s, a).': 6.205618415918845,\n",
       " 'As an example, consider the situation shown in Figure 1, which we will denote by s1.': 2.670308232539992,\n",
       " 'Suppose that the passenger is at R and wishes to go to B.': 0.6504096761607492,\n",
       " 'Let the hierarchical policy we are evaluating be an optimal policy denoted by π (we will omit the superscript * to reduce the clutter of the notation).': 2.091689426453375,\n",
       " 'When the passenger is delivered, the agent gets a reward of +20, so the net value is +10.': 2.7393679282091297,\n",
       " 'Figure 4 shows how the MAXQ hierarchy computes this value.': 0.7097151775263364,\n",
       " 'To compute the value V π(Root, s1),  MaxRoot consults its policy and ﬁnds that πRoot(s1) is Get.': 5.004682013265705,\n",
       " 'However, this is just the reward after completing the Get, so it must ask MaxGet to estimate the expected reward of performing the Get itself.': 2.7237612173234496,\n",
       " 'QNavigateForGet knows that after completing the Navigate(R) task, one more action (the Pickup) will be required to complete the Get, so C π(MaxGet, s1, Navigate(R)) = −1.': 9.128365197034725,\n",
       " 'QNorth looks up its completion cost, and ﬁnds that C π(Navigate, s1, North) is 0 (i.e., the Navigate task will be completed after performing the North action).': 6.927428794381584,\n",
       " 'It consults MaxNorth to determine the expected cost of performing the North action itself.': 0.6765509168942645,\n",
       " 'Because MaxNorth is a primitive action, it looks up its expected reward, which is −1.': 2.728833398361296,\n",
       " 'The C value of each Q node is shown to the left of the node.': 0.7612173234490831,\n",
       " 'All other numbers show the values being returned up the graph.': 0.6788919235271167,\n",
       " '• V π(Navigate(R), s1) = −1  • Qπ(Get, s1, Navigate(R)) = −1 + −1  (−1 to perform the Navigate plus −1 to complete the Get.': 8.731174404994146,\n",
       " 'Figure 5: The MAXQ decomposition; r1, .': 1.683964104564963,\n",
       " ', r14 denote the sequence of rewards received from primitive actions at times 1, .': 2.7093250097541945,\n",
       " ', 14.': 1.5872024970737417,\n",
       " 'In general, the MAXQ value function decomposition has the form  V π(0, s) = V π(am, s) + C π(am−1, s, am) + .': 9.31837690206789,\n",
       " '+ C π(a1, s, a2) + C π(0, s, a1),  (12)  where a0, a1, .': 11.021459227467812,\n",
       " ', am is the “path” of Max nodes chosen by the hierarchical policy going from the Root down to a primitive leaf node.': 1.9722980881779164,\n",
       " 'We can summarize the presentation of this section by the following theorem:  Theorem 2 Let π = {πi; i = 0, .': 1.964104564962934,\n",
       " ', n} be a hierarchical policy deﬁned for a given MAXQ graph with subtasks M0, .': 2.846273897776044,\n",
       " ', Mn, and let i = 0 be the root node of the graph.': 2.7713616855247762,\n",
       " 'At each level i, we compute values for C π(i, s, π(s)) (or V π(i, s), if i is primitive) according to the decomposition equations.': 10.127194693718298,\n",
       " 'When i = 0, we obtain the value function for the entire hierarchical policy.': 2.0140460397971127,\n",
       " 'Q. E. D.  It is important to note that this representation theorem does not mention the pseudo-reward function, because the pseudo-reward is used only during learning.': 1.798283261802575,\n",
       " 'That is the subject of the next section.': 0.5891533359344518,\n",
       " 'Of course, for any MDP M , we would like to ﬁnd an optimal policy π∗.': 2.792430745220445,\n",
       " 'However, in the MAXQ method (and in hierarchical reinforcement learning in general), the programmer imposes a hierarchy on the problem.': 3.8302770191182205,\n",
       " 'This hierarchy constrains the space of possible policies so that it may not be possible to represent the optimal policy or its value function.': 1.0257510729613735,\n",
       " '17  In the MAXQ method, the constraints take two forms.': 1.6437768240343347,\n",
       " 'First, within a subtask, only some of the possible primitive actions may be permitted.': 2.7366367538041363,\n",
       " 'For example, in the taxi task, during a Navigate(t), only the North, South, East, and West actions are available—the Pickup and Putdown actions are not allowed.': 7.774873195474054,\n",
       " 'Second, consider a Max node Mj with child nodes {Mj1, .': 2.6898166211470933,\n",
       " ', Mjk}.': 1.5891533359344518,\n",
       " 'The policy learned for Mj must involve executing the learned policies of these child nodes.': 0.8287163480296528,\n",
       " 'When the policy for child node Mji is executed, it will run until it enters a state in Tji.': 1.904799063597347,\n",
       " 'Hence, any policy learned for Mj must pass through some subset of these terminal state sets {Tj1, .': 2.8985563792430744,\n",
       " ', Tjk}.': 1.5891533359344518,\n",
       " 'In the “option” approach, the policy is even further constrained.': 1.81076863051112,\n",
       " 'In this approach, there are only two non-primitive levels in the hierarchy, and the subtasks at the lower level are given complete policies by the programmer.': 2.728053062817011,\n",
       " 'Hence, any learned policy must be constructed by “concatenating” the given lower level policies in some order.': 1.8907530238002344,\n",
       " 'However, these constraints may make it impossible to learn the optimal policy.': 1.792040577448303,\n",
       " 'Deﬁnition 7 A hierarchically optimal policy for MDP M is a policy that achieves the highest cumulative reward among all policies consistent with the given hierarchy.': 1.0269215762777995,\n",
       " 'Parr (1998b) proves that his HAMQ learning algorithm converges with probability 1 to a hier-archically optimal policy.': 1.968396410456496,\n",
       " 'Similarly, given a ﬁxed set of options, Sutton, Precup, and Singh (1998) prove that their SMDP learning algorithm converges to a hierarchically optimal value function.': 5.989465470152165,\n",
       " '(Incidentally, they also show that if the primitive actions are also made available as “trivial” op-tions, then their SMDP method converges to the optimal policy.': 3.514631291455326,\n",
       " 'However, in this case, it is hard to say anything formal about how the options speed the learning process.': 2.6718689036285603,\n",
       " 'They may in fact hinder it (Hauskrecht et al., 1998).)': 3.166601638704643,\n",
       " 'With the MAXQ method, we will seek an even weaker form of optimality: recursive optimality.': 1.709325009754194,\n",
       " 'Deﬁnition 8 A recursively optimal policy for MDP M with MAXQ decomposition {M0, .': 1.813889972688256,\n",
       " ', Mk} is a hierarchical policy π = {π0, .': 2.921966445571596,\n",
       " 'Hence, recursive optimality is a kind of local optimality in which the policy at each node is optimal given the policies of its children.': 1.885680842762388,\n",
       " 'This context-free property makes it easier to share and re-use subtasks.': 0.6184159188451034,\n",
       " 'It will also turn out to be essential for the successful use of state abstraction.': 0.7541943035505267,\n",
       " 'Before we proceed to describe our learning algorithm for recursive optimality, let us see how  recursive optimality diﬀers from hierarchical optimality.': 1.7924307452204449,\n",
       " 'The policy shown in the left diagram is recursively optimal but not hierarchically optimal.': 0.8540772532188842,\n",
       " 'The shaded cells indicate points where the locally-optimal policy is not globally optimal.': 0.7764338665626219,\n",
       " 'North  South  East  It is easy to construct examples of policies that are recursively optimal but not hierarchically optimal.': 0.740148263753414,\n",
       " 'Consider the simple maze problem and its associated MAXQ graph shown in Figures 6.': 0.6714787358564183,\n",
       " 'Suppose a robot starts somewhere in the left room, and it must reach the goal G in the right room.': 1.7085446742099102,\n",
       " 'The robot has three actions, North, South, and East, and these actions are deterministic.': 4.67772142021069,\n",
       " 'The robot receives a reward of −1 for each move.': 0.647288333983613,\n",
       " 'Let us deﬁne two subtasks:  • Exit.': 0.7502926258291065,\n",
       " 'This task terminates when the robot exits the left room.': 0.6523605150214593,\n",
       " 'We can set the pseudo-reward  function ˜R to be 0 for the two terminal states (i.e., the two states indicated by *’s).': 2.942645337495123,\n",
       " '• GotoGoal.': 0.6059305501365587,\n",
       " 'This task terminates when the robot reaches the goal G.  The arrows in Figure 6 show the locally optimal policy within each room.': 0.9102614124073352,\n",
       " 'The arrows on the left seek to exit the left room by the shortest path, because this is what we speciﬁed when we set the pseudo-reward function to 0.': 1.759266484588373,\n",
       " 'The arrows on the right follow the shortest path to the goal, which is ﬁne.': 1.6398751463129146,\n",
       " 'However, the resulting policy is neither hierarchically optimal nor optimal.': 1.8376902067889191,\n",
       " 'There exists a hierarchical policy that would always exit the left room by the upper door.': 0.7990635973468592,\n",
       " 'ﬁx this problem.': 0.6012485368708544,\n",
       " 'The value of the upper starred state under the optimal hierarchical policy is −2 and the value of the lower starred state is −6.': 1.2067889192352714,\n",
       " 'Hence, if we set ˜R to have these values, then the recursively-optimal policy would be hierarchically optimal (and globally optimal).': 3.9586422161529455,\n",
       " 'This basic idea was ﬁrst pointed out by Dean and Lin (1995).': 1.646507998439329,\n",
       " 'They proved that this will converge to a hierarchically optimal policy.': 0.7857978930940305,\n",
       " 'Parr (1998a) proposed an interesting approach that constructs a set of diﬀerent ˜R functions and computes the recursively optimal policy under each of them for each subtask.': 1.9391338275458452,\n",
       " 'His method chooses the ˜R functions in such a way that the hierarchically optimal policy can be approximated to any desired degree.': 0.8283261802575108,\n",
       " 'In practice, we have employed the following simpliﬁed approach to deﬁning ˜R.': 1.6199765899336718,\n",
       " 'For each subtask Mi, we deﬁne two predicates: the termination predicate, Ti, and a goal predicate Gi.': 3.81076863051112,\n",
       " 'The goal predicate deﬁnes a subset of the terminated states that are “goal states”, and these have a pseudo-reward of 0.': 1.8946547015216544,\n",
       " 'For the problems on which we have tested the MAXQ method, this worked very well.': 1.6195864221615295,\n",
       " 'In our experiments with MAXQ, we have found that it is easy to make mistakes in deﬁning Ti and Gi.': 1.6028092079594227,\n",
       " 'If the goal is not deﬁned carefully, it is easy to create a set of subtasks that lead to inﬁnite looping.': 1.687475614514241,\n",
       " 'For example, consider again the problem in Figure 6.': 1.632071790870074,\n",
       " 'This is a very natural deﬁnition, since it is quite similar to the deﬁnition for the left-hand room.': 1.612563402262973,\n",
       " 'However, the resulting locally-optimal policy for this room will attempt to move to the nearest of these three locations: the goal, the upper door, or the lower door.': 3.8829496683573934,\n",
       " 'Now that we have an understanding of recursively optimal policies, we present two learning algorithms.': 1.7678501755754976,\n",
       " 'The ﬁrst one, called MAXQ-0, applies only in the case when the pseudo-reward function ˜R is always zero.': 2.7417089348419816,\n",
       " 'We will ﬁrst prove its convergence properties and then show how it can be extended to give the second algorithm, MAXQ-Q, which works with general pseudo-reward functions.': 2.695669137729224,\n",
       " 'Table 2 gives pseudo-code for MAXQ-0.': 0.6094420600858369,\n",
       " 'To execute an action, MAXQ-0 calls itself recursively.': 1.6574326960593055,\n",
       " 'When the recursive call returns, it updates the value of the completion function for node i.': 1.8092079594225516,\n",
       " 'It uses the count of the number of primitive actions to appropriately discount the value of the resulting state s′.': 0.906359734685915,\n",
       " 'At leaf nodes, MAXQ-0 updates the estimated one-step expected reward, V (i, s).': 4.719469371829887,\n",
       " 'The value αt(i) is a “learning rate” parameter that should be gradually decreased to zero in the limit.': 1.875926648458837,\n",
       " 'There are two things that must be speciﬁed in order to make this algorithm description complete.': 0.685134607881389,\n",
       " 'First, we must specify how to compute Vt(i, s′) in line 12, since it is not stored in the Max node.': 4.777214202106906,\n",
       " 'It is computed by the following modiﬁed versions of the decomposition equations:  20  Table 2: The MAXQ-0 learning algorithm.': 0.8579789309403043,\n",
       " '(13)  (14)  These equations reﬂect two important changes compared with Equations (10) and (11).': 4.823644166991807,\n",
       " 'Second, there are no π superscripts, because the current value function, Vt(i, s) is not based on a ﬁxed hierarchical policy π.': 6.153726102223956,\n",
       " 'Table 3 gives pseudo-code for a recursive function, EvaluateMaxNode, that implements a depth-ﬁrst search.': 2.69683964104565,\n",
       " 'In addition to returning Vt(i, s), EvaluateMaxNode also returns the action at the leaf node that achieves this value.': 3.8314475224346465,\n",
       " 'This information is not needed for MAXQ-0, but it will be useful later when we consider non-hierarchical execution of the learned recursively-optimal policy.': 1.7822863831447522,\n",
       " 'The second thing that must be speciﬁed to complete our deﬁnition of MAXQ-0 is the exploration  policy, πx.': 1.7900897385875927,\n",
       " 'We require that πx be an ordered GLIE policy.': 0.7362465860319938,\n",
       " 'We need this property in order to ensure that MAXQ-0 converges to a uniquely-deﬁned recur-sively optimal policy.': 0.7994537651190012,\n",
       " 'These diﬀerent locally optimal policies will all achieve the same locally optimal value function, but they can give rise to diﬀerent probability transition functions P (s′, N |s, i).': 5.071790870074133,\n",
       " 'And consequently, by induction, it deﬁnes a unique policy for the entire MAXQ graph.': 2.762777994537652,\n",
       " 'Let us call this policy π∗ r .': 0.7830667186890363,\n",
       " 'We will use the r subscript to denote recursively optimal quantities under an ordered greedy policy.': 0.851736246586032,\n",
       " 'Hence, the corresponding value function is V ∗ r denote the corresponding completion function and action-value function.': 1.9726882559500587,\n",
       " 'We now prove that the MAXQ-0 algorithm converges to π∗ r .': 0.6613343737807258,\n",
       " 'Let H be a MAXQ graph deﬁned over subtasks {M0, .': 1.6683573936792822,\n",
       " ', Mk} such that the pseudo-reward function ˜Ri(s′|s, a) is zero for all i, s, a, and s′.': 6.7776043698790485,\n",
       " 'Then with probability 1, algorithm MAXQ-0 converges to π∗  r , the unique recursively optimal policy for M consistent with H and πx.': 2.924307452204448,\n",
       " 'Proof: The proof follows an argument similar to those introduced to prove the convergence of Q learning and SARSA(0) (Bertsekas & Tsitsiklis, 1996; Jaakkola et al., 1994).': 4.881779165040967,\n",
       " 'Let Ft = {r0(x), .': 2.754194303550527,\n",
       " ', rt(x), w0(x), .': 5.75107296137339,\n",
       " ', wt−1(x), α0(x), .': 5.746781115879829,\n",
       " ', αt(x), ∀x} be the entire history of the it-eration.': 3.6898166211470924,\n",
       " 'The notation || · ||ξ denotes a weighted maximum norm  ||A||ξ = max  x  |A(x)| ξ(x)  .': 2.8973858759266484,\n",
       " 'The structure of the proof of Theorem 3 will be inductive, starting at the leaves of the MAXQ graph and working toward the root.': 1.6652360515021456,\n",
       " 'We will employ a diﬀerent time clock at each node i to count the number of update steps performed by MAXQ-0 at that node.': 0.7323449083105735,\n",
       " 'The variable t will always refer to the time clock of the current node i.': 0.6652360515021459,\n",
       " 'To prove the recursive case, consider any composite Max node i with child node j.': 1.7674600078033555,\n",
       " 'By the inductive assumption, MAXQ-0 applied to j will converge to the (unique) recursively optimal value function V ∗ r (j, s) with probability 1.': 5.093640265314085,\n",
       " 'To prove this, we will apply Lemma 1.': 1.6355833008193525,\n",
       " 'We will identify the x in the lemma with a state-action pair (s, a).': 2.672649239172845,\n",
       " 'The vector rt will be the completion-cost table Ct(i, s, a) for all s, a and ﬁxed i after t update steps.': 4.659773702692157,\n",
       " 'The vector r∗ will be the optimal completion-cost C ∗ r (i, s, a) (again, for ﬁxed i).': 5.79126024190402,\n",
       " 'r (a, s), and  To apply the lemma, we must ﬁrst express the C update formula in the form of the update rule in the lemma.': 4.749122122512681,\n",
       " 'V ∗  We now verify the conditions of Lemma 1.': 0.6621147093250097,\n",
       " 'Condition (a) is assumed in the conditions of the theorem with αt(s, a) = αt(i).': 4.850175575497464,\n",
       " 'Condition (b) is satisﬁed because s is sampled from Pt(s′, N |s, a), so the expected value of the  diﬀerence is zero.': 5.8579789309403045,\n",
       " 'Condition (c) follows directly from the assumption that the |Ct(i, s, a)| and |Vt(i, s)| are bounded.': 6.7549746390948116,\n",
       " 'Condition (d) is the condition that U is a weighted max norm pseudo-contraction.': 1.6843542723371048,\n",
       " 'We can derive this by starting with the weighted max norm for Q learning.': 0.6703082325399923,\n",
       " 'Xs′,N  Now we will show how to derive the contraction for the C update operator U .': 1.6137339055793987,\n",
       " 'Our plan is to show ﬁrst how to express the U operator for learning C in terms of the T operator for updating Q values.': 0.7456106125634023,\n",
       " 'Recall from Eqn.': 0.5836909871244635,\n",
       " '(10) that Q(i, s, a) = C(i, s, a)+V (a, s).': 9.838470542333203,\n",
       " 'r + U C(i).': 1.70464299648849,\n",
       " 'Similarly,  Now we can substitute these two formulas into the max norm pseudo-contraction formula for  T , Eqn.': 2.619976589933672,\n",
       " 'Finally, it is easy verify (e), the most important condition.': 3.667577058134998,\n",
       " 'By assumption, the ordered GLIE policies in the child nodes converge with probability 1 to locally optimal policies for the children.': 1.8080374561061259,\n",
       " 'Therefore, |ut| converges to zero with probability 1.': 1.6621147093250097,\n",
       " 'We can trivially construct a sequence θt = |ut| that bounds this convergence, so  |ut(s, a)| ≤ θt ≤ θt(||Ct(s, a)||ξ + 1).': 6.892703862660945,\n",
       " 'We have veriﬁed all of the conditions of Lemma 1, so we can conclude that Ct(i) converges to C ∗ r (i) with probability 1.': 3.853687085446743,\n",
       " 'Algorithm MAXQ-0 can be extended to accelerate learning in the higher nodes of the graph by a technique that we call “all states updating”.': 0.8630511119781507,\n",
       " ', sN , sN +1 = s′.': 2.689426453374951,\n",
       " 'This list is returned when the composite action terminates.': 0.6582130316035896,\n",
       " 'The parent Max node can then process each state in this list as shown above.': 0.777604369879048,\n",
       " 'The parent Max node appends the state lists that it receives from its children and passes them to its parent when it terminates.': 0.7916504096761607,\n",
       " 'All experiments in this paper employ all states updating.': 0.6589933671478736,\n",
       " 'To understand all goals updating, suppose that for each primitive action, there are several composite tasks that could have invoked that primitive action.': 2.7932110807647286,\n",
       " 'Sutton, Precup, and Singh (1998) prove that each of the composite tasks will converge to the optimal Q values under all goals updating.': 3.7740928599297696,\n",
       " 'All goals updating would work in the MAXQ hierarchy for composite tasks all of whose children are primitive actions.': 0.7206398751463129,\n",
       " 'All goals updating cannot provide these samples, so it cannot be applied at these higher levels.': 1.611392898946547,\n",
       " 'Now that we have shown the convergence of MAXQ-0, let us design a learning algorithm for arbitrary pseudo-reward functions, ˜Ri(s).': 3.790089738587593,\n",
       " 'We could just add the pseudo-reward into MAXQ-0, but this has the eﬀect of changing the MDP M to have a diﬀerent reward function.': 1.735076082715568,\n",
       " 'The pseudo-rewards “contaminate” the values of all of the completion functions computed in the hierarchy.': 0.7748731954740539,\n",
       " 'The resulting learned policy will not be recursively optimal for the original MDP.': 0.8037456106125634,\n",
       " 'This problem can be solved by learning two completion functions.': 0.7128365197034725,\n",
       " 'The ﬁrst one, C(i, s, a) is the completion function that we have been discussing so far in this paper.': 4.771751853296918,\n",
       " 'It computes the expected reward for completing task Mi after performing action a in state s and then following the learned policy for Mi.': 1.0218493952399532,\n",
       " 'It is computed without any reference to ˜Ri.': 0.6016387046429965,\n",
       " 'This function will incorporate rewards both from the “real” reward function, R(s′|s, a) and from the pseudo-reward function ˜Ri(s).': 5.087787748731955,\n",
       " 'We will employ two diﬀerent update rules to learn these two completion functions.': 0.6847444401092471,\n",
       " 'The ˜C function will be learned using an update rule similar to the Q learning rule in line 12 of MAXQ-0.': 0.7857978930940305,\n",
       " 'Pseudo-code for the resulting algorithm, MAXQ-Q is shown in Table 4.': 1.645337495122903,\n",
       " 'The key step is at lines 16 and 17.': 0.6012485368708544,\n",
       " 'In line 16, MAXQ-Q ﬁrst updates ˜C using the value of the greedy action, a∗, in the resulting state.': 3.9024580569644947,\n",
       " 'This update includes the pseudo-reward ˜Ri.': 0.6067108856808427,\n",
       " 'Then in line 17, MAXQ-Q updates C using this same greedy action a∗, even if this would not be the greedy action according to the “uncontaminated” value function.': 3.0101443620756925,\n",
       " 'This update, of course, does not include the pseudo-reward function.': 2.691767460007803,\n",
       " 'It is important to note that whereever Vt(a, s) appears in this pseudo-code, it refers to the “uncontaminated” value function of state s when executing the Max node a.': 4.058915333593445,\n",
       " 'This is computed recursively in exactly the same way as in MAXQ-0.': 0.6145142411236832,\n",
       " 'Each Max node i takes the maximum of these Q values and computes either V (i, s) or computes the best action, a∗ using ˜Q.': 3.840421381193913,\n",
       " 'Proof: The argument is identical to, but more tedious than, the proof of Theorem 3.': 2.676160749122123,\n",
       " '5 State Abstraction  There are many reasons to introduce hierarchical reinforcement learning, but perhaps the most important reason is to create opportunities for state abstraction.': 2.037065938353492,\n",
       " 'When we introduced the simple taxi problem in Figure 1, we pointed out that within each subtask, we can ignore certain aspects of the state space.': 2.865782286383145,\n",
       " 'For example, while performing a MaxNavigate(t), the taxi should make the same navigation decisions regardless of whether the passenger is in the taxi.': 3.7296137339055795,\n",
       " 'Speciﬁcally, we will identify ﬁve conditions that permit the “safe” introduction of state abstractions.': 1.834568864611783,\n",
       " 'To establish a starting point, let us compute the number of values that must be stored for the taxi problem without any state abstraction.': 1.9262582910651582,\n",
       " 'The MAXQ representation must have tables for each of the C functions at the internal nodes and the V functions at the leaves.': 0.7019118220834959,\n",
       " 'Second, at the root node, there are two children, which requires 2 × 500 = 1000 values.': 3.8232539992196646,\n",
       " 'Third, at the MaxGet and MaxPut nodes, we have 2 actions each, so each one requires 1000 values, for a total of 2000.': 4.759266484588374,\n",
       " 'Finally, at MaxNavigate(t), we have four actions, but now we must also consider the target parameter t, which can take four possible values.': 5.823644166991808,\n",
       " 'Hence, there are eﬀectively 2000 combinations of states and t values for each action, or 8000 total values that must be represented.': 2.842762387826766,\n",
       " 'In total, therefore, the MAXQ representation requires 14,000 separate quantities to represent the value function.': 2.793601248536871,\n",
       " 'Hence, we can see that without state abstraction, the MAXQ representation requires more than four times the memory of a ﬂat Q table!': 2.2286383144752246,\n",
       " '5.1 Five Conditions that Permit State Abstraction  We now introduce ﬁve conditions that permit the introduction of state abstractions.': 0.9184549356223176,\n",
       " 'For each condition, we then provide some rules for identifying when that condition can be satisﬁed and give examples from the taxi domain.': 1.6699180647678504,\n",
       " 'We begin by introducing some deﬁnitions and notation.': 0.5942255169722981,\n",
       " 'Deﬁnition 10 Let M be a MDP and H be a MAXQ graph deﬁned over M .': 0.6558720249707375,\n",
       " 'Suppose that each state s can be written as a vector of values of a set of state variables.': 0.925087787748732,\n",
       " 'Then H combined with χi is called a state-abstracted MAXQ graph.': 0.6312914553257901,\n",
       " '(When πi is a stationary stochastic policy, this is interpreted to mean that the prob-ability distributions for choosing actions are the same in both states.)': 2.860710105345299,\n",
       " 'One way to achieve this is to construct the exploration policy so that it only uses information from the relevant state variables in deciding what action to perform.': 0.9785407725321889,\n",
       " 'Boltzmann exploration based on the (state-abstracted) Q values, ǫ-greedy exploration, and counter-based exploration based on abstracted states are all abstract exploration policies.': 3.856028092079593,\n",
       " 'Counter-based exploration based on the full state space is not an abstract exploration policy.': 0.905969566913773,\n",
       " 'Now that we have introduced our notation, let us describe and analyze the ﬁve abstraction conditions.': 1.6706984003121343,\n",
       " 'We have identiﬁed three diﬀerent kinds of conditions under which abstractions can be introduced.': 0.6453374951229028,\n",
       " 'The ﬁrst kind involves eliminating irrelevant variables within a subtask of the MAXQ graph.': 0.7229808817791651,\n",
       " 'Hence, this kind of abstraction is most useful at the lower levels of the MAXQ graph.': 1.6582130316035895,\n",
       " 'The second kind of abstraction arises from “funnel” actions.': 0.74131876706984,\n",
       " 'These are macro actions that move the environment from some large number of initial states to a small number of resulting states.': 0.7850175575497464,\n",
       " 'The completion cost of such subtasks can be represented using a number of values proportional to the number of resulting states.': 0.7850175575497464,\n",
       " 'Funnel actions tend to appear higher in the MAXQ graph, so this form of abstraction is most useful near the root of the graph.': 1.7331252438548572,\n",
       " 'The third kind of abstraction arises from the structure of the MAXQ graph itself.': 0.6558720249707375,\n",
       " 'We begin by describing two abstraction conditions of the ﬁrst type.': 0.6632852126414358,\n",
       " 'Then we will present two  conditions of the second type.': 0.6305111197815061,\n",
       " 'And ﬁnally, we describe one condition of the third type.': 1.6351931330472103,\n",
       " '5.1.1 Condition 1: Max Node Irrelevance  The ﬁrst condition arises when a set of state variables is irrelevant to a Max node.': 1.0109246976199766,\n",
       " 'Deﬁnition 12 Let Mi be a Max node in a MAXQ graph H for MDP M .': 0.6968396410456497,\n",
       " 'Lemma 2 Let M be an MDP with full-state MAXQ graph H, and suppose that state variables Yi are irrelevant for Max node i.': 1.8540772532188845,\n",
       " 'Let χi(s) = x be the associated abstraction function that maps s onto the remaining relevant variables Xi.': 1.8895825204838075,\n",
       " 'Let π be any abstract hierarchical policy.': 0.8685134607881388,\n",
       " 'Proof: Deﬁne a new MDP χi(Mi) at node i as follows:  • States: X = {x | χi(s) = x, for some s ∈ S}.': 4.290284822473664,\n",
       " '• Actions: A.': 0.7062036675770582,\n",
       " 'Therefore, it is also a well-deﬁned policy over χi(Mi).': 2.7846273897776044,\n",
       " 'Furthermore, we know that the distribution P π can be factored into separate distributions for Yi and Xi.': 1.6909871244635195,\n",
       " '(20)  Xx′,N  Xs′,N  Xx′,N  Finally, note that equations (18) and (20) are identical except for the expressions for the Q  values.': 7.793211080764729,\n",
       " 'Since the solution to the Bellman equation is unique, we must conclude that  Qπ(i, s, j) = Qπ(i, χ(s), j).': 8.916113928989466,\n",
       " 'where  Q.E.D.': 0.5836909871244635,\n",
       " 'Xx′,N  Of course we are primarily interested in being able to discover and represent the optimal policy at each node i.': 1.838470542333203,\n",
       " 'The following corollary shows that the optimal policy is an abstract policy, and hence, that it can be represented abstractly.': 2.944986344127975,\n",
       " 'Let ρ be an ordering over actions.': 0.6410456496293406,\n",
       " 'Then the optimal ordered policy π∗ ρ at node i is an abstract policy, and its action-value function can be represented abstracted.': 2.071790870074132,\n",
       " 'Proof: Deﬁne the policy ω∗ ρ to be the optimal ordered policy over the abstract MDP χ(M ), and let Q∗(i, x, j) be the corresponding optimal action-value function.': 6.323058915333594,\n",
       " 'Then by the same argument given above, Q∗ is also a solution to the optimal Bellman equation for the original MDP.': 1.683573936792821,\n",
       " 'This means that the policy π∗ ρ deﬁned by π∗ ρ(s) = ω∗(χ(s)) is an optimal ordered policy, and by construction, it is an abstract policy.': 6.2992586812329305,\n",
       " 'However, in practice, this condition is often satisﬁed.': 2.616074912212251,\n",
       " 'For example, let us consider the Navigate(t) subtask.': 2.7245415528677333,\n",
       " 'The source and destination of the passenger are irrelevant to the achievement of this subtask.': 0.6824034334763949,\n",
       " 'Any policy that successfully completes this subtask will have the same value function regardless of the source and destination locations of the passenger.': 0.9656652360515021,\n",
       " '(Any policy that does not complete the subtask will have the same value function also, but all states will have a value of −∞.)': 3.1002731174404996,\n",
       " 'By abstracting away the passenger source and destination, we obtain a huge savings in space.': 1.6433866562621928,\n",
       " 'Instead of requiring 8000 values to represent the C functions for this task, we require only 400 values (4 actions, 25 locations, 4 possible values for t).': 4.967225907140071,\n",
       " 'One rule for noticing cases where this abstraction condition holds is to examine the subgraph rooted at the given Max node i.': 0.7292235661334374,\n",
       " 'By deﬁnition, any abstract hierarchical policy will choose actions based only upon information in Xi.': 1.8423722200546235,\n",
       " 'Therefore, the variables in Yi are irrelevant for Max node i. Q.E.D.': 1.6660163870464302,\n",
       " 'In the Taxi task, the primitive navigation actions, North, South, East, and West only depend on the location of the taxi and not on the location of the passenger.': 5.798283261802576,\n",
       " 'The pseudo-reward function and termination condition for the MaxNavigate(t) node only depend on the location of the taxi (and the parameter t).': 2.8938743659773705,\n",
       " 'Hence, this lemma applies, and the passenger source and destination are irrelevant for the MaxNavigate node.': 2.6831837690206797,\n",
       " '5.1.2 Condition 2: Leaf Irrelevance  The second abstraction condition describes situations under which we can apply state abstractions to leaf nodes of the MAXQ graph.': 0.9543503706593836,\n",
       " 'For leaf nodes, we can obtain a stronger result than Lemma 2 by using a slightly weaker deﬁnition of irrelevance.': 1.6722590714007024,\n",
       " 'Lemma 4 Let M be an MDP with full-state MAXQ graph H, and suppose that state variables Y are irrelevant for leaf node a.': 1.8599297698010147,\n",
       " 'Let χ(s) = x be the associated abstraction function that maps s onto the remaining relevant variables X.': 1.9258681232930155,\n",
       " 'Then we can represent V (a, s) for any state s by an abstracted value function V (a, χ(s)) = V (a, x).': 8.160358954350372,\n",
       " 'Proof: According to the deﬁnition of Leaf Irrelevance, any two states that diﬀer only on the irrelevant state variables have the same value for V (a, s).': 4.008583690987125,\n",
       " 'Hence, we can represent this unique value by V (a, x).': 3.7584861490440886,\n",
       " 'Here are two rules for ﬁnding cases where Leaf Irrelevance applies.': 0.6238782676550917,\n",
       " 'The ﬁrst rule shows that if  the probability distribution factors, then we have Leaf Irrelevance.': 1.6527506827936014,\n",
       " 'Then the variables in Y are irrelevant to the leaf node a.': 0.664845883730004,\n",
       " '32  Proof: Plug in to the deﬁnition of V (a, s) and simplify.': 2.7011314865392126,\n",
       " 'Lemma 6 Let R(s′|s, a) = ra be the reward function for action a in MDP M , which is always equal to a constant ra.': 3.9500585251658222,\n",
       " 'Then the entire state s is irrelevant to the primitive action a.': 0.8029652750682794,\n",
       " 'This does not depend on s, so the entire state is irrelevant to the primitive action a. Q.E.D.': 1.8092079594225519,\n",
       " 'This lemma is satisﬁed by the four leaf nodes North, South, East, and West in the taxi task, because their one-step reward is a constant (−1).': 5.785797893094031,\n",
       " 'Hence, instead of requiring 2000 values to store the V functions, we only need 4 values—one for each action.': 2.7502926258291067,\n",
       " 'Similarly, the expected rewards of the Pickup and Putdown actions each require only 2 values, depending on whether the corresponding actions are legal or illegal.': 2.7655091689426454,\n",
       " 'Hence, together, they require 4 values, instead of 1000 values.': 3.7190792040577456,\n",
       " '5.1.3 Condition 3: Result Distribution Irrelevance  Now we consider a condition that results from “funnel” actions.': 0.8525165821303161,\n",
       " 'Deﬁnition 14 (Result Distribution Irrelevance).': 1.6523605150214593,\n",
       " 'Let χij be the associated abstraction function: χij(s) = x.': 1.9215762777994538,\n",
       " '′  (21)  Xs′,N  Consider any two states s1 and s2, such that χij(s1) = χij(s2) = x.': 6.088177916504097,\n",
       " 'Under Result Distribution Irrelevance, their transition probability distributions are the same.': 1.6359734685914944,\n",
       " 'Hence, the right-hand sides of (21) have the same value, and we can conclude that  C π(i, s1, j) = C π(i, s2, j).': 10.130316035895435,\n",
       " 'Therefore, we can deﬁne an abstract completion function, C π(i, x, j) to represent this quantity.': 5.913382754584473,\n",
       " 'It might appear that this condition would rarely be satisﬁed, and indeed, for inﬁnite horizon discounted problems, this is true.': 3.6468981662114706,\n",
       " 'Consider, for example, the Get subroutine under an optimal policy for the taxi task.': 2.856028092079594,\n",
       " 'Hence, the starting location is irrelevant to the resulting location of the taxi.': 1.6710885680842762,\n",
       " 'In the discounted cumulative reward setting, however, the number of steps N required to complete the Get action will depend very much on the starting location of the taxi.': 2.7857978930940304,\n",
       " 'Consequently, P (s′, N |s, a) is not necessarily the same for any two states s with diﬀerent starting locations even though s′ is always the same.': 4.845883730003902,\n",
       " 'This is much less than the 500 quantities in the unabstracted representation.': 0.6164650799843934,\n",
       " '“Funnel” actions arise in many hierarchical reinforcement learning problems.': 0.8373000390167773,\n",
       " 'For example, abstract actions that move a robot to a doorway or that move a car onto the entrance ramp of a freeway have this property.': 1.682793601248537,\n",
       " 'The Result Distribution Irrelevance condition is applicable in all such situations as long as we are in the undiscounted setting.': 0.630901287553648,\n",
       " '5.1.4 Condition 4: Termination  The fourth condition is closely related to the “funnel’ property.': 0.7842372220054623,\n",
       " 'It applies when a subtask is guaranteed to cause its parent task to terminate in a goal state.': 0.8462738977760438,\n",
       " 'In a sense, the subtask is funneling the environment into the set of states described by the goal predicate of the parent task.': 1.8060866172454155,\n",
       " 'Lemma 8 (Termination).': 1.6547015216543113,\n",
       " 'Let Mi be a task in a MAXQ graph such that for all states s where the goal predicate Gi(s) is true, the pseudo-reward function ˜Ri(s) = 0.': 4.04057744830277,\n",
       " '′  (i.e., if s′ is a possible result state of applying a in s, then s′ is a goal terminal state for task i.)': 4.120171673819742,\n",
       " 'Then for any policy executed at node i, the completion cost C(i, s, a) is zero and does not need  to be explicitly represented.': 4.86110027311744,\n",
       " '34  Proof: By the assumptions in the lemma, with probability 1 the completion cost is zero for any action that results in a goal terminal state.': 1.9754194303550527,\n",
       " 'This is because the termination predicate for Put (i.e., that the passenger is at his or her destination location) implies the goal condition for Root (which is the same).': 3.8197424892703866,\n",
       " 'This means that C(Root, s, Put) is uniformly zero, for all states s where Put is not terminated.': 4.697619976589935,\n",
       " 'It is easy to detect cases where the Termination condition is satisﬁed.': 0.6340226297307843,\n",
       " 'We only need to compare the termination predicate of a subtask with the goal predicate of the parent task.': 0.7651190011705034,\n",
       " 'If the ﬁrst implies the second, then the termination condition is satisﬁed.': 1.6476785017557551,\n",
       " '5.1.5 Condition 5: Shielding  The shielding condition arises from the structure of the MAXQ graph.': 0.738587592664846,\n",
       " 'Lemma 9 (Shielding).': 1.6367538041357783,\n",
       " 'As with the Termination condition, the Shielding condition can be veriﬁed by analyzing the  structure of the MAXQ graph and identifying nodes whose ancestor tasks are terminated.': 1.7159578618806088,\n",
       " 'In the Taxi task, a simple example of this arises in the Put task, which is terminated in all states where the passenger is not in the taxi.': 2.7881388997268832,\n",
       " 'This means that we do not need to represent C(Root, s, Put) in these states.': 3.709715177526336,\n",
       " 'The result is that, when combined with the Termination condition above, we do not need to explcitly represent the completion function for Put at all!': 2.156847444401092,\n",
       " '5.1.6 Dicussion  By applying these ﬁve abstraction conditions, we obtain the following “safe” state abstractions for the Taxi task:  • North, South, East, and West.': 5.022629730784238,\n",
       " 'These terminal nodes require one quantity each, for a total of  four values.': 1.719079204057745,\n",
       " '(Leaf Irrelevance).': 1.6363636363636365,\n",
       " '• Pickup and Putdown each require 2 values (legal and illegal states), for a total of four.': 2.7916504096761603,\n",
       " '(Leaf  Irrelevance.)': 1.6363636363636365,\n",
       " '• QNorth(t), QSouth(t), QEast(t), and QWest(t) each require 100 values (four values for t and  25 locations).': 8.962934061646507,\n",
       " '(Max Node Irrelevance.)': 1.6800624268435427,\n",
       " '• QNavigateForGet requires 4 values (for the four possible source locations).': 1.778774873195474,\n",
       " '(The passenger destination is Max Node Irrelevant for MaxGet, and the taxi starting location is Result Dis-tribution Irrelevant for the Navigate action.)': 2.8408115489660553,\n",
       " '• QPickup requires 100 possible values, 4 possible source locations and 25 possible taxi locations.': 1.7978930940304332,\n",
       " '(Passenger destination is Max Node Irrelevant to MaxGet.)': 1.7198595396020289,\n",
       " '35  • QGet requires 16 possible values (4 source locations, 4 destination locations).': 2.811548966055404,\n",
       " '(Result Distribution Irrelevance.)': 1.644166991806477,\n",
       " '• QNavigateForPut requires only 4 values (for the four possible destination locations).': 1.7822863831447522,\n",
       " '(The passenger source and destination are Max Node Irrelevant to MaxPut; the taxi location is Result Distribution Irrelevant for the Navigate action.)': 1.8544674209910261,\n",
       " '• QPutdown requires 100 possible values (25 taxi locations, 4 possible destination locations).': 2.828716348029653,\n",
       " '(Passenger source is Max Node Irrelevant for MaxPut.)': 1.7163480296527507,\n",
       " '• QPut requires 0 values.': 0.6972298088177917,\n",
       " '(Termination and Shielding.)': 1.6433866562621928,\n",
       " 'This gives a total of 632 distinct values, which is much less than the 3000 values required by ﬂat Q learning.': 1.7904799063597348,\n",
       " 'Hence, we can see that by applying state abstractions, the MAXQ representation can give a much more compact representation of the value function.': 2.942645337495123,\n",
       " 'A key thing to note is that these state abstractions cannot be exploited with the ﬂat representation of the value function.': 0.9145532579008974,\n",
       " 'What prior knowledge is required on the part of a programmer in order to introduce these state abstractions?': 0.18064767850175573,\n",
       " 'It suﬃces to know some general constraints on the one-step reward functions, the one-step transition probabilities, and termination predicates, goal predicates, and pseudo-reward functions within the MAXQ graph.': 4.811939133827546,\n",
       " 'Speciﬁcally, the Max Node Irrelevance and Leaf Irrelevance conditions require simple analysis of the one-step transition function and the reward and pseudo-reward functions.': 1.8341786968396414,\n",
       " 'Opportunities to apply the Result Distribution Irrelevance condition can be found by identifying “funnel” eﬀects that result from the deﬁnitions of the termination conditions for operators.': 0.7553648068669528,\n",
       " 'Similarly, the Shielding and Termination conditions only require analysis of the termination predicates of the various subtasks.': 1.6667967225907143,\n",
       " 'Lemma 10 Let M be an MDP with full-state MAXQ graph H and abstract-state MAXQ graph χ(H) where the abstractions satisfy the ﬁve conditions given above.': 1.7869683964104564,\n",
       " 'Let ρ be an ordering over all actions in the MAXQ graph.': 0.6730394069449863,\n",
       " 'Hence, the heart of this lemma is the ﬁrst claim.': 1.6047600468201328,\n",
       " 'The last two forms of abstraction (Shielding and Termination) do not place any restrictions on abstract policies, so we ignore them in this proof.': 2.7428794381584085,\n",
       " 'The proof is by induction on the levels of the MAXQ graph, starting at the leaves.': 1.6402653140850565,\n",
       " 'As a base case, let us consider a Max node i all of whose children are primitive actions.': 1.7616074912212252,\n",
       " 'In this case, there are no policies executed within the children of the Max node.': 1.6964494732735078,\n",
       " 'Now let us impose the action ordering ρ to compute the optimal ordered policy.': 0.8731954740538431,\n",
       " 'Then the optimal ordered policy must choose a1.': 0.8224736636753804,\n",
       " 'Now in all other states s2 such that χi(s1) = χi(s2), we know that the Q∗ values will be the same.': 3.896995708154506,\n",
       " 'Hence, the same tie will exist between a1 and a2, and hence, the optimal ordered policy must make the same choice in all such states.': 3.8868513460788146,\n",
       " 'Hence, the optimal ordered policy for node i is an abstract policy.': 1.9715177526336325,\n",
       " 'Now let us turn to the recursive case at Max node i.': 0.6820132657042528,\n",
       " 'Make the inductive assumption that the ordered recursively-optimal policy is abstract within all descendant nodes and consider the locally optimal policy at node i.': 1.0296527506827937,\n",
       " 'Similarly, if Y is a set of variables irrelevant to the result distribution of a particular action j, then Lemma 7 tells us the same thing.': 2.77214202106906,\n",
       " 'Hence, by the same ordering argument given above, the ordered optimal policy at node i must be abstract.': 2.889582520483808,\n",
       " 'By induction, this proves the lemma.': 1.5922746781115882,\n",
       " 'We are now ready to prove that MAXQ-Q will converge to this policy.': 0.7292235661334374,\n",
       " 'Let H be an unab-stracted MAXQ graph deﬁned over subtasks {M0, .': 1.6687475614514242,\n",
       " ', Mk} with pseudo-reward functions ˜Ri(s′|s, a).': 3.674990245805697,\n",
       " 'Let χ(H) be a state-abstracted MAXQ graph deﬁned by applying state abstractions χi to each node i of H under the ﬁve conditions given above.': 1.9266484588373003,\n",
       " 'Let π∗ r be the unique recursively-optimal hierarchical policy deﬁned by πx, M , and ˜R.': 2.8361295357003513,\n",
       " 'Proof: Rather than repeating the entire proof for MAXQ-Q, we will only describe what must change under state abstraction.': 1.8431525555989077,\n",
       " 'Since these values are not updated by MAXQ-Q, we can ignore them.': 1.6422161529457666,\n",
       " 'We will now consider the ﬁrst three forms of state abstraction in turn.': 0.7662895044869293,\n",
       " 'We begin by considering primitive leaf nodes.': 0.6457276628950449,\n",
       " 'Let a be a leaf node and let Y be a set of state variables that are Leaf Irrelevant for a.': 0.8396410456496294,\n",
       " 'Let s1 = (x, y1) and s2 = (x, y2) be two states that diﬀer only in their values for Y .': 5.050721810378464,\n",
       " 'When MAXQ-Q visits an abstract state x, it does not “know” the value of y, the part of the state that has been abstracted away.': 3.0429184549356227,\n",
       " 'Nonetheless, it draws a sample according to P (s′|x, y, a), receives a reward R(s′|x, y, a), and updates its estimate of V (a, x) (line 5 of MAXQ-Q).': 12.905579399141631,\n",
       " 'Let Pt(y) be the probability that MAXQ-Q is visiting (x, y) given that the unabstracted part of the state is x.': 3.922356613343738,\n",
       " 'Pt(y)Pt(s  , N |x, y, a)R(s  |x, y, a).': 8.766679672259071,\n",
       " 'Call this value r0(x).': 1.743659773702692,\n",
       " 'This gives  Pt(y)r0(x),  y X  which is equal to r0(x) for any distribution Pt(y).': 5.893874365977371,\n",
       " 'Hence, MAXQ-Q converges under Leaf Irrelevance abstractions.': 1.628950448692938,\n",
       " 'Now let us turn to the two forms of abstraction that apply to internal nodes: Node Irrelevance and Result Distribution Irrelevance.': 0.8181818181818182,\n",
       " 'Consider the SMDP deﬁned at each node i of the abstracted MAXQ graph at time t during MAXQ-Q.': 0.6956691377292236,\n",
       " 'To prove the theorem, we must show that drawing (s′, N ) according to this second distribution is equivalent to drawing (x′, N ) according to the ﬁrst distribution.': 5.828716348029653,\n",
       " 'Because the exploration policy is an abstract policy, Pt(s′, N |s, a) factors in this way.': 4.989465470152166,\n",
       " 'Therefore, MAXQ-Q will converge under Max Node Irrelevance abstractions.': 1.6695278969957084,\n",
       " 'Finally, consider Result Distribution Irrelevance.': 1.6113928989465471,\n",
       " 'Let j be a child of node i, and suppose Yj is a set of state variables that are irrelevant to the result distribution of j.': 1.9129925868123294,\n",
       " 'However, this does not matter, because Result Distribution Irrelevance means that for all possible values of y, Pt(x′, y′, N |x, y, j) is the same.': 8.783847054233318,\n",
       " 'Hence, MAXQ-Q will converge under Result Distribution Irrelevance abstractions.': 1.6340226297307845,\n",
       " 'In each of these three cases, MAXQ-Q will converge to a locally-optimal ordered policy at node i in the MAXQ graph.': 1.8259851736246584,\n",
       " 'By Lemma 10, this can be extended to produce a locally-optimal ordered policy for the unabstracted SMDP at node i.': 1.793991416309013,\n",
       " 'Hence, by induction, MAXQ-Q will converge to  38  the unique ordered recursively optimal policy π∗ exploration policy πx.': 2.9695669137729226,\n",
       " 'Consider the following modiﬁcation of the taxi problem.': 0.647678501755755,\n",
       " 'Suppose that the taxi has a fuel tank and that each time the taxi moves one square, it costs one unit of fuel.': 1.7171283651970348,\n",
       " 'If the taxi runs out of fuel before delivering the passenger to his or her destination, it receives a reward of −20, and the trial ends.': 2.6902067889192356,\n",
       " 'Fortunately, there is a ﬁlling station where the taxi can execute a Fillup action to ﬁll the fuel tank.': 1.6757705813499806,\n",
       " 'MaxRefuel is a child of MaxRoot, and it invokes Navigate(t) (with t bound to the location of the ﬁlling station) to move the taxi to the ﬁlling station.': 3.742099102614125,\n",
       " 'This is unfortunate, because our intuition tells us that the amount of fuel should have no inﬂuence on our decisions inside the Navigate(t) subtask.': 2.7116660163870465,\n",
       " 'Given this intuition, it is natural to try abstracting away the “amount of remaining fuel” within the Navigate(t) subtask.': 2.794771751853297,\n",
       " 'We call this the hierarchical credit assignment problem.': 0.6550916894264533,\n",
       " 'The fundamental issue here is that in the MAXQ decomposition all information about rewards is stored in the leaf nodes of the hierarchy.': 0.6753804135778385,\n",
       " 'We would like to separate out the basic rewards received for navigation (i.e., −1 for each action) from the reward received for exhausting fuel (−20).': 3.814280140460398,\n",
       " 'If we make the reward at the leaves only depend on the location of the taxi, then the Max Node Irrelevance condition will be satisﬁed.': 1.754584471322669,\n",
       " 'One way to do this is to have the programmer manually decompose the reward function and indicate which nodes in the hierarchy will “receive” each reward.': 0.9047990635973469,\n",
       " 'Lines 16 and 17 of the MAXQ-Q algorithm are easily modiﬁed to include R(i, s′|s, a).': 3.7077643386656263,\n",
       " 'P  In most domains, we believe it will be easy for the designer of the hierarchy to decompose the reward function.': 1.7405384315255559,\n",
       " 'It has been straightforward in all of the problems we have studied.': 0.5914943425673039,\n",
       " 'However, an interesting problem for future research is to develop an algorithm that can solve the hierarchical credit assignment problem autonomously.': 1.7042528287163479,\n",
       " '39  6 Non-Hierarchical Execution of the MAXQ Hierarchy  Up to this point in the paper, we have focused exclusively on representing and learning hierarchical policies.': 1.8010144362075695,\n",
       " 'However, often the optimal policy for a MDP is not a strictly hierarchical policy.': 1.947327350760827,\n",
       " 'Kaelbling (1993) ﬁrst introduced the idea of deriving a non-hierarchical policy from the value function of a hierarchical policy.': 2.1201716738197423,\n",
       " 'In this section, we exploit the MAXQ decomposition to generalize her ideas and apply them recursively at all levels of the hierarchy.': 1.6547015216543117,\n",
       " 'The ﬁrst method is based on the dynamic programming algorithm known as policy iteration.': 0.7932110807647288,\n",
       " 'The policy iteration algorithm starts with an initial policy π0.': 0.8849005072181038,\n",
       " 'It then repeats the following two steps until the policy converges.': 0.7686305111197815,\n",
       " 'In the policy evaluation step, it computes the value function V πk of the current policy πk.': 2.027701911822083,\n",
       " '′  (22)  a  Xs′  Howard (1960) proved that if πk is not an optimal policy, then πk+1 is guaranteed to be an im-provement.': 3.9192352711666008,\n",
       " 'Note that in order to apply this method, we need to know the transition probability distribution P (s′|s, a) and the reward function R(s′|s, a).': 5.948888021849395,\n",
       " 'If we know P (s′|s, a) and R(s′|s, a), we can use the MAXQ representation of the value function to perform one step of policy iteration.': 6.079984393289114,\n",
       " 'We start with a hierarchical policy π and represent its value function using the MAXQ hierarchy (e.g., π could have been learned via MAXQ-Q).': 3.197815060476006,\n",
       " 'Then, we can perform one step of policy improvement by applying Equation (22) using V π(0, s′) (computed by the MAXQ hierarchy) to compute V π(s′).': 7.257120561841592,\n",
       " 's′ P (s′|s, a)[R(s′|s, a) + γV π(0, s)], where V π(0, s) is the value Corollary 3 Let πg(s) = argmaxa function computed by the MAXQ hierarchy.': 11.450643776824032,\n",
       " 'Nonetheless, one step of policy improvement can give very signiﬁcant improvements.': 1.760046820132657,\n",
       " 'This approach to non-hierarchical execution ignores the internal structure of the MAXQ graph.': 0.6675770581349981,\n",
       " 'In eﬀect, the MAXQ hierarchy is just viewed as a kind of function approximator for representing V π—any other representation would give the same one-step improved policy πg.': 1.8735856418259849,\n",
       " 'The second approach to non-hierarchical execution borrows an idea from Q learning.': 0.6948888021849395,\n",
       " 'This gives us the same one-step greedy policy as we computed above using one-step lookahead.': 0.7881388997268826,\n",
       " 'With the MAXQ decomposition, we can perform these policy improvement steps at all levels of the hierarchy.': 1.7737026921576278,\n",
       " 'We have already deﬁned the function that we need.': 0.6855247756535311,\n",
       " 'In addition, EvaluateMaxNode returns the primitive action a at the end of this best path.': 1.70074131876707,\n",
       " 'This action a would  40  Table 5: The procedure for executing the one-step greedy policy.': 0.8841201716738197,\n",
       " 'The pseudo-code is shown in Table 5.': 0.6067108856808427,\n",
       " 'The following theorem shows that this can give a better policy than the original, hierarchical policy.': 1.9344518142801406,\n",
       " 'Let V hg(0, s) be the value computed by ExecuteHGPolicy, and let πhg∗ be the resulting policy.': 3.901677721420211,\n",
       " 'Deﬁne V hg∗ to be the value function of πhg∗.': 0.7479516191962544,\n",
       " 'Then for all states s, it is the case that  V π(s) ≤ V hg(0, s) ≤ V hg∗  (s).': 5.8915333593445185,\n",
       " '(23)  Proof: (sketch) The left inequality in Equation (23) is satisﬁed by construction by line 7 of EvaluateMaxNode.': 3.8174014826375355,\n",
       " 'Hence, V hg(0, s) must be at least as large as V π(0, s).': 5.8447132266874755,\n",
       " 'Hence, V hg(0, s) is an underestimate of the actual value of πhg∗.': 3.7335154116269997,\n",
       " 'Note that this theorem only works in one direction.': 0.630901287553648,\n",
       " 'It says that if we can ﬁnd a state where V hg(0, s) > V π(s), then the greedy policy, πhg∗, will be strictly better than π.': 7.143581740148263,\n",
       " 'Hence, unlike the policy improvement theorem of Howard, we do not have a guarantee that if π is suboptimal, then the hierarchically greedy policy is a strict improvement.': 3.9703472493172063,\n",
       " '41  In contrast, if we perform one-step policy improvement as discussed at the start of this section, Corollary 3 guarantees that we will improve the policy.': 2.8856808427623877,\n",
       " 'So we can see that in general, neither of these two methods for non-hierarchical execution dominates the other.': 1.6652360515021463,\n",
       " 'Nonetheless, the ﬁrst method only operates at the level of individual primitive actions, so it is not able to produce very large improvements in the policy.': 2.843932891143191,\n",
       " 'In contrast, the hierarchical greedy method can obtain very large improvements in the policy by changing which actions (i.e., subroutines) are chosen near the root of the hierarchy.': 3.9481076863051117,\n",
       " 'Hence, in general, hierarchical greedy execution is probably the better method.': 2.7065938353492003,\n",
       " '(Of course, the value functions of both methods could be computed, and the one with the better estimated value could be executed.)': 3.871634802965276,\n",
       " 'Sutton, Singh, Precup and Ravindran (1999) have simultaneously developed a closely-related method for non-hierarchical execution of macros.': 3.6878657822863827,\n",
       " 'Their method is equivalent to ExecuteHGPolicy for the special case where the MAXQ hierarchy has only one level of subtasks.': 0.6999609832227858,\n",
       " 'The interesting aspect of ExecuteHGPolicy is that it permits greedy improvements at all levels of the tree to inﬂuence which action is chosen.': 0.6749902458056964,\n",
       " 'Some care must be taken in applying Theorem 5 to a MAXQ hierarchy whose C values have been learned via MAXQ-Q.': 0.7331252438548577,\n",
       " 'Being an online algorithm, MAXQ-Q will not have correctly learned the values of all states at all nodes of the MAXQ graph.': 1.7729223566133439,\n",
       " 'For example, in the taxi problem, the value of C(Put, s, QPutdown) will not have been learned very well except at the four special locations.': 5.792040577448303,\n",
       " 'During exploration, both children of Put will be tried in such states.': 1.6547015216543115,\n",
       " 'The PutDown will usually fail, whereas the Navigate will eventually succeed (perhaps after lengthy exploration) and take the taxi to the destination location.': 2.7054233320327734,\n",
       " 'Hence, if we train the MAXQ representation using hierarchical execution (as in MAXQ-Q), and then switch to hierarchically-greedy execution, the results will be quite bad.': 4.765119001170504,\n",
       " 'In particular, we need to introduce hierarchically-greedy execution early enough so that the exploration policy is still actively exploring.': 1.7729223566133436,\n",
       " '(In theory, a GLIE exploration policy never ceases to explore, but in practice, we want to ﬁnd a good policy quickly, not just asymptotically).': 5.9204057744830285,\n",
       " 'Of course an alternative would be to use hierarchically-greedy execution from the very beginning of learning.': 0.6890362856028093,\n",
       " 'However, remember that the higher nodes in the MAXQ hierarchy need to obtain samples of P (s′, N |s, a) for each child action a.': 4.829496683573937,\n",
       " 'If the hierarchical greedy execution interrupts child a before it has reached a terminal state, then these samples cannot be obtained.': 1.815450643776824,\n",
       " 'Hence, it is important to begin with purely hierarchical execution during training, and make a transition to greedy execution at some point.': 2.722980881779165,\n",
       " 'We start with L set very large, so that execution is completely hierarchical—when a child action is invoked, we are committed to execute that action until it terminates.': 2.784237222005462,\n",
       " 'However, gradually, we reduce L until it becomes 1, at which point we have hierarchical greedy execution.': 3.7245415528677333,\n",
       " 'We time this so that it reaches 1 at about the same time our Boltzmann exploration cools to a temperature of 0.1 (which is where exploration eﬀectively has halted).': 1.7323449083105735,\n",
       " 'As the experimental results will show, this generally gives excellent results with very little added exploration cost.': 1.6480686695278972,\n",
       " 'In this section, we describe these experiments and present the results.': 1.6035895435037064,\n",
       " '7.1 The Fickle Taxi Task  Our ﬁrst experiments were performed on a modiﬁed version of the taxi task.': 0.7249317206398751,\n",
       " 'This version incor-porates two changes to the task described in Section 3.1.': 0.6539211861100274,\n",
       " 'The purpose of this change is to create a situation where the optimal policy is not a hierarchical policy so that the eﬀectiveness of non-hierarchical execution can be measured.': 0.9812719469371829,\n",
       " 'These conﬁgurations are controlled by many parameters.': 0.5996878657822864,\n",
       " 'For Boltzmann exploration, we established an initial temperature and then a cooling rate.': 1.6273897776043698,\n",
       " 'The following parameters were chosen.': 0.6137339055793992,\n",
       " 'For ﬂat Q learning: initial Q values of 0.123, learning rate 0.25, and Boltzmann exploration with an initial temperature of 50 and a cooling rate of 0.9879.': 2.900507218103785,\n",
       " '(We use initial values that end in .123 as a “signature” to aid debugging.)': 1.7854077253218887,\n",
       " 'reached 1.': 0.6211470932500975,\n",
       " 'So after 50 trials, execution was completely greedy.': 1.626219274287944,\n",
       " 'Figure 7 shows the averaged results of 100 training trials.': 0.6156847444401092,\n",
       " 'The ﬁrst thing to note is that all forms of MAXQ learning have better initial performance than ﬂat Q learning.': 0.7678501755754975,\n",
       " 'This is because of the constraints introduced by the MAXQ hierarchy.': 0.6160749122122513,\n",
       " 'For example, while the agent is executing a Navigate subtask, it will never attempt to pickup or putdown the passenger.': 2.7034724931720637,\n",
       " 'Similarly, it will never attempt to putdown the passenger until it has ﬁrst picked up the passenger (and vice versa).': 2.689426453374951,\n",
       " 'longer to converge, so that the Flat Q curve crosses the MAXQ/no abstraction curve.': 1.621927428794382,\n",
       " 'This shows that without state abstraction, the cost of learning the huge number of parameters in the MAXQ representation is not really worth the beneﬁts.': 1.8630511119781508,\n",
       " 'The third thing to notice is that with state abstractions, MAXQ-Q converges very quickly to a hierarchically optimal policy.': 1.9364026531408505,\n",
       " 'This can be seen more clearly in Figure 8, which focuses on the range of reward values in the neighborhood of the optimal policy.': 1.8661724541552869,\n",
       " 'The last thing to notice is that with greedy execution, the MAXQ policy is also able to attain optimal performance.': 1.837300039016777,\n",
       " 'Despite this drop in performance, greedy MAXQ-Q recovers rapidly and reaches hierarchically optimal performance faster than purely-hierarchical MAXQ-Q learning.': 1.7499024580569644,\n",
       " 'Hence, there is no added cost—in terms of exploration—for introducing greedy execution.': 1.6355833008193525,\n",
       " 'This ﬁgure also shows two horizontal lines indicating optimal performance and hierarchically optimal performance in this domain.': 0.7643386656262193,\n",
       " 'To make this ﬁgure more readable, we have applied a 100-step moving average to the data points.': 1.6035895435037064,\n",
       " '7.2 Kaelbling’s HDG Method  The second task that we will consider is a simple maze task introduced by Leslie Kaelbling (1993) and shown in Figure 10.': 1.792820912992587,\n",
       " 'There is a small cost for each move, and the agent must maximize the undiscounted sum of these costs.': 1.6765509168942645,\n",
       " 'Because the goal state can be in any of 100 diﬀerent locations, there are actually 100 diﬀerent MDPs.': 1.7873585641825989,\n",
       " 'So it computes a value function for this MDP.': 0.7393679282091299,\n",
       " 'Each circled state is a landmark state, and the heavy lines show the boundaries of the Voronoi cells.': 1.8556379243074523,\n",
       " 'In each episode, a start state and a goal state are chosen at random.': 1.8817791650409679,\n",
       " 'In this ﬁgure, the start state is shown by the shaded hexagon, and the goal state is shown by the shaded square.': 2.8930940304330868,\n",
       " 'Note that while terms (t1) and (t3) can be exact estimates, term (t2) is computed using the landmark subtasks as subroutines.': 4.813109637143972,\n",
       " 'This means that the corresponding path must pass through the intermediate landmark states rather than going directly to the goal landmark.': 0.7604369879047991,\n",
       " 'Hence, term (t2) is typically an overestimate of the required distance.': 2.644947327350761,\n",
       " '(Also note that (t3) is the same for all choices of the intermediate landmarks, so it does not need to be explicitly included in the computation.)': 3.712446351931331,\n",
       " 'For example, in Figure 9, term (t1) is the cost of reaching the landmark in row 7, column 4, which is 4.': 5.735076082715568,\n",
       " 'Term (t2) is the cost of getting from row 7, column 4 to the landmark at row 1 column 4 (by going from one landmark to another).': 3.851736246586033,\n",
       " 'In this case, the best landmark-to-landmark path is from row 7, column 1 to row 5 column 6, and then to row 1 column 4.': 3.7499024580569644,\n",
       " 'Hence, term (t2) is 12.': 2.646507998439329,\n",
       " 'Term (t3) is the cost of getting from row 1 column 4 to the goal, which is 2.': 2.7655091689426454,\n",
       " 'The sum of these is 4 + 12 + 2 = 18.': 0.7861880608661724,\n",
       " 'For comparison, the optimal path has length 10.': 1.6558720249707373,\n",
       " 'Figure 10 shows a MAXQ approach to solving this problem.': 0.6328521264143582,\n",
       " 'The overall task Root, takes one  argument g, which speciﬁes the goal cell.': 2.7151775263363254,\n",
       " 'There are three subtasks:  • GotoGoalLmk, go to the landmark nearest to the goal location.': 1.7795552087397581,\n",
       " 'The termination for the predicate is true if the agent reaches the landmark nearest to the goal.': 0.703862660944206,\n",
       " 'The goal predicate is the same as the termination predicate.': 0.6609442060085837,\n",
       " 'The goal predicate for this subtask is true only for condition (a).': 1.7530238002341008,\n",
       " 'The MAXQ decomposition is essentially the same as Kaelbling’s method, but somewhat redun-dant.': 1.643776824034335,\n",
       " 'mark gl nearest the goal.': 0.6316816230979322,\n",
       " '• C(GotoGoalLmk(gl), s, M axGotoLmk(l)) the cost of getting from landmark l to the land• C(Root, s, GotoGoalLmk(gl)) the cost of getting to the goal location after reaching gl.': 9.977760436987904,\n",
       " 'When the agent is inside the goal Voronoi cell, then again HDG and MAXQ store essentially the same information.': 1.6730394069449863,\n",
       " 'Let us compare the amount of memory required by ﬂat Q learning, HDG, and MAXQ.': 2.6933281310963713,\n",
       " 'There are 100 locations, 4 possible actions, and 100 possible goal states, so ﬂat Q learning must store 40,000 values.': 3.9430355052672654,\n",
       " 'This gives a total of 2,028 values that must be stored.': 0.6894264533749512,\n",
       " 'To compute quantity (t2), HDG must store, for each landmark, information on the shortest path to every other landmark.': 4.732735076082717,\n",
       " 'There are 12 landmarks.': 0.5973468591494343,\n",
       " 'Consider the landmark at row 6, column 1.': 1.6625048770971518,\n",
       " 'It has 5 neighboring landmarks which constitute the ﬁve macro actions that the agent can perform to move to another landmark.': 0.7062036675770582,\n",
       " 'The nearest landmark to the goal cell could be any of the other 11 landmarks, so this gives a total of 55 Q values that must be stored.': 1.773312524385486,\n",
       " 'Similar computations for all 12 landmarks give a total of 506 values that must be stored.': 0.7054233320327741,\n",
       " 'This requires 3,536 values.': 0.6535310183378853,\n",
       " 'Hence, the grand total for HDG is 6,070, which is a huge savings over ﬂat Q learning.': 2.6683573936792824,\n",
       " 'Now let’s consider the MAXQ hierarchy with and without state abstractions.': 0.7959422551697231,\n",
       " '• V (a, s): This is the expected reward of each primitive action in each state.': 2.971517752633633,\n",
       " 'There are 100 states and 4 primitive actions, so this requires 400 values.': 1.7936012485368709,\n",
       " 'However, because the reward is constant (−1), we can apply Leaf Irrelevance to store only a single value.': 3.7694108466640657,\n",
       " '• C(GotoLmk(l), s, a), where a is one of the four primitive actions.': 5.811548966055404,\n",
       " 'This requires the same amount of space as (t1) in Kaelbling’s representation—indeed, combined with V (a, a), this represents exactly the same information as (t1).': 6.782286383144753,\n",
       " 'It requires 2,028 values.': 0.6535310183378853,\n",
       " 'No state abstractions can be applied.': 0.7296137339055794,\n",
       " 'This is exactly the same as Kaelbling’s quantity (t2), which requires 506 values.': 2.7222005462348813,\n",
       " 'However, if the primitive actions are stochastic—as they were in Kaelbling’s original paper—then we must store this value for each possible terminal state of each GotoLmk action.': 1.9847834568864613,\n",
       " '• C(GotoGoal, s, a): This is the cost of completing the GotoGoal task after making one of the primitive actions a.': 3.861880608661725,\n",
       " 'This is the same as quantity (t3) in the HDG representation, and it requires the same amoount of space: 3,536 values.': 2.796332422941865,\n",
       " '• C(Root, s, GotoGoalLmk): This is the cost of reaching the goal once we have reached the landmark nearest the goal.': 3.8361295357003513,\n",
       " 'MAXQ must represent this for all combinations of goal landmarks and goals.': 0.6761607491221225,\n",
       " 'This requires 100 values.': 0.6570425282871635,\n",
       " 'Note that these values are the same as the values of C(GotoGoal(g), s, a) + V (a, s) for each of the primitive actions.': 6.943035505267265,\n",
       " 'This means that the MAXQ representation stores this information twice, whereas the HDG representation only stores it once (as term (t3)).': 3.740928599297699,\n",
       " '• C(Root, s, GotoGoal).': 3.6562621927428793,\n",
       " 'This is the cost of completing the Root task after we have executed the GotoGoal task.': 0.6874756145142411,\n",
       " 'If the primitive action are deterministic, this is always zero, because GotoGoal will have reached the goal.': 2.7257120561841592,\n",
       " 'Hence, we can apply the Termination condition and not store any values at all.': 1.6866952789699574,\n",
       " 'However, if the primitive actions are stochastic, then we must store this value for each possible state that borders the Voronoi cell that contains the goal.': 2.9535700351150993,\n",
       " 'This requires 96 diﬀerent values.': 0.6621147093250098,\n",
       " 'Again, in Kaelbling’s HDG representation of the value function, she is ignoring the probability that GotoGoal will terminate in a non-goal state.': 2.920795942255169,\n",
       " 'Because MAXQ is an exact representation of the value function, it does not ignore this possibility.': 1.758486149044089,\n",
       " 'If we (incorrectly) apply the Termination condition in this case, the MAXQ representation becomes a function approximation.': 2.783066718689036,\n",
       " 'In the stochastic case, without state abstractions, the MAXQ representation requires 12,760 values.': 2.838080374561062,\n",
       " 'With safe state abstractions, it requires 12,361 values.': 1.7971127584861493,\n",
       " 'With the approximations employed by Kaelbling (or equivalently, if the primitive actions are deterministic), the MAXQ representation with state abstractions requires 6,171 values.': 3.936792820912993,\n",
       " 'These numbers are summarized in Table 6.': 0.5969566913772922,\n",
       " 'The MAXQ formulation guarantees that the value function of the hierarchical policy will be represented exactly.': 0.9227467811158798,\n",
       " 'The assumptions will introduce approximations into the value function representation.': 0.7600468201326571,\n",
       " 'This might be useful as a general design methodology for building application-speciﬁc hierarchical representations.': 0.6492391728443231,\n",
       " 'Our long-term goal is to develop such methods so that each new application does not require inventing a new set of techniques.': 0.6788919235271167,\n",
       " 'Instead, oﬀ-the-shelf tools (e.g., based on MAXQ) could be specialized by imposing assumptions and state abstractions to produce more eﬃcient special-purpose systems.': 3.801014436207569,\n",
       " 'One of the most important contributions of the HDG method was that it introduced a form of non-hierarchical execution.': 0.6874756145142411,\n",
       " 'As soon as the agent crosses from one Voronoi cell into another, the current subtask is “interrupted”, and the agent recomputes the “current target landmark”.': 2.9278189621537267,\n",
       " 'The eﬀect of this is that (until it reaches the goal Voronoi cell), the agent is always aiming for a landmark outside of its current Voronoi cell.': 2.7510729613733904,\n",
       " 'Hence, although the agent “aims for” a sequence of landmark states, it typically does not visit many of these states on its way to the goal.': 2.86110027311744,\n",
       " 'The states just provide a convenient set of intermediate targets.': 0.6589933671478736,\n",
       " 'The same eﬀect is obtained by hierarchical greedy execution of the MAXQ graph (which was directly inspired by the HDG method).': 1.7717518532969179,\n",
       " 'Note that by storing the N L (nearest landmark) function, Kaelbing’s HDG method can detect very eﬃciently when the current subtask should be interrupted.': 2.8564182598517363,\n",
       " 'This technique only works for navigation problems in a space with a distance metric.': 0.6086617245415529,\n",
       " 'In contrast, ExecuteHGPolicy performs a kind of “polling”, where it checks after each primitive action whether it should interrupt the current subroutine and invoke a new one.': 2.8193523214982443,\n",
       " 'An important goal for future research on MAXQ is to ﬁnd a general purpose mechanism for avoiding unnecessary “polling”— that is, a mechanism that can discover eﬃciently-evaluable interrupt conditions.': 1.7534139680062428,\n",
       " 'Figure 11 shows the results of our experiments with HDG using the MAXQ-Q learning al-gorithm.': 0.6902067889192353,\n",
       " 'The ﬁgure conﬁrms the observations made in our experiments with the Fickle Taxi task.': 0.6500195083886071,\n",
       " 'With-out state abstractions, MAXQ-Q converges much more slowly than ﬂat Q learning.': 1.815450643776824,\n",
       " 'With state abstractions, it converges roughly three times as fast.': 1.7514631291455327,\n",
       " 'Figure 12 shows a close-up view of Figure 11 that allows us to compare the diﬀerences in the ﬁnal levels of performance of the methods.': 0.6519703472493172,\n",
       " 'Notice however, that the best performance that can be obtained by hierarchical greedy execution of the best recursively-optimal policy cannot match optimal perfor-mance.': 1.8860710105345302,\n",
       " '(Average of 100 runs.)': 3.271946937182989,\n",
       " 'about one fewer primitive action.': 0.6944986344127975,\n",
       " 'Finally notice that as in the taxi domain, there was no added exploration cost for shifting to greedy execution.': 1.6792820912992588,\n",
       " 'The hierarchy is executed using a procedure-call-and-return discipline, and it provides a partial policy for the task.': 1.8084276238782675,\n",
       " 'The programmer puts “choice” states at any point where he/she does not know what action should be performed.': 0.7936012485368709,\n",
       " 'Given this partial policy, Parr’s goal is to ﬁnd the best policy for making choices in the choice states.': 1.986344127975029,\n",
       " 'A key observation is that it is only necessary to learn this value function at choice states hs, mi.': 1.8006242684354272,\n",
       " 'Parr’s algorithm does not learn a decomposition of the value function.': 0.7959422551697231,\n",
       " 'Instead, it “ﬂattens” the hierarchy to create a new Markov decision problem over the choice states hs, mi.': 2.77214202106906,\n",
       " 'Hence, it is hierarchical primarily in the sense that the programmer structures the prior knowledge hierarchically.': 1.6589933671478736,\n",
       " 'An advantage of this is that Parr’s method can ﬁnd the optimal hierarchical policy subject to constraints provided by the programmer.': 0.8759266484588373,\n",
       " 'A disadvantage is that the method cannot be executed “non-hierarchically” to produce a better policy.': 0.8384705423332033,\n",
       " 'Parr illustrated his work using the maze shown in Figure 13.': 0.6242684354272338,\n",
       " 'In each trial, the agent starts in the top left corner, and it must move to any state in the bottom right corner room.': 2.806086617245416,\n",
       " 'The agent has the usual four primitive actions, North, South, East, and West.': 4.688646117830667,\n",
       " 'If an action would collide with a wall or an obstacle, it has no eﬀect.': 1.6547015216543113,\n",
       " 'The maze is structured as a series of “rooms”, each containing a 12-by-12 block of states (and various obstacles).': 2.7678501755754974,\n",
       " 'Some rooms are parts of “hallways”, because they contain walls on two opposite sides, and they are open on the other two sides.': 2.721810378462739,\n",
       " 'Other rooms are “intersections”, where two or more hallways meet.': 1.6894264533749515,\n",
       " 'The loop terminates when the agent reaches a goal state.': 0.7857978930940305,\n",
       " 'MRoot will only invoke a particular machine if there is a hallway in the speciﬁed direction.': 0.612173234490831,\n",
       " 'Hence, in the start state, it will only consider MGo(South)  2The author thanks Ron Parr for providing the details of the HAM for this task.': 3.8025751072961373,\n",
       " '52  and MGo(East).': 1.628170113148654,\n",
       " 'The MGo(d) machine begins executing when the agent is in an intersection.': 1.668747561451424,\n",
       " 'It does this by ﬁrst invoking a ExitIntersection(d) machine.': 1.648849005072181,\n",
       " 'When that machine returns, it then invokes a MExitHallway(d) ma-chine.': 2.6398751463129146,\n",
       " 'When that machine returns, MGo also returns.': 1.6152945766679672,\n",
       " 'The MExitIntersection and MExitHallway machines are identical except for their termination con-ditions.': 0.6090518923136949,\n",
       " 'Both machines consist of a loop with one choice state that chooses among four possible sub-routines.': 0.7725321888412018,\n",
       " 'To simplify their description, suppose that MGo(East) has chosen MExitIntersection(East).': 3.6866952789699576,\n",
       " 'Then the four possible subroutines are MSniﬀ(East, N orth), MSniﬀ(East, South), MBack(East, N orth), and MBack(East, South).': 11.797112758486147,\n",
       " 'The MSniﬀ(d, p) machine always moves in direction d until it encounters a wall (either part of an obstacle or part of the walls of the maze).': 3.7346859149434257,\n",
       " 'Then it moves in perpendicular direction p until it reaches the end of the wall.': 0.6269996098322279,\n",
       " 'But the actions are carried out in any case, and then the MBack machine returns.': 1.6449473273507609,\n",
       " 'The MSniﬀ and MBack machines also terminate if they reach the end of a hall or the end of an  intersection.': 0.6305111197815061,\n",
       " 'These ﬁnite-state controllers deﬁne a highly constrained partial policy.': 0.7315645727662895,\n",
       " 'The MBack, MSniﬀ, and MGo machines contain no choice states at all.': 2.642606320717909,\n",
       " 'Figure 14 shows a MAXQ graph that encodes a similar set of constraints on the policy.': 0.7854077253218884,\n",
       " 'The  subtasks are deﬁned as follows:  • Root.': 0.7136168552477565,\n",
       " 'This is exactly the same as the MRoot machine.': 0.5938353492001561,\n",
       " 'It must choose a direction d and invoke Go.': 0.6371439719079204,\n",
       " 'It terminates when the agent enters a terminal state.': 0.7518532969176746,\n",
       " 'This is also its goal condition (of course).': 1.7050331642606322,\n",
       " '• Go(d, r).': 2.683573936792821,\n",
       " 'The parameter r is bound to the current 12-by-12 “room” in which the agent is located.': 0.7514631291455326,\n",
       " 'Go terminates when the agent enters the room at the end of the hallway in direction d or when it leaves the desired hallway (e.g., in the wrong direction).': 2.721810378462739,\n",
       " 'The goal condition for Go is satisﬁed only if the agent reaches the desired intersection.': 0.6863051111978151,\n",
       " '• ExitInter(d, r).': 2.680062426843543,\n",
       " 'This terminates when the agent has exited room r. The goal condition is that  the agent exit room r in direction d.  • ExitHall(d, r).': 2.8673429574717133,\n",
       " 'This terminates when the agent has exited the current hall (into some intersec-tion).': 1.6777214202106907,\n",
       " 'The goal condition is that the agent has entered the desired intersection in direction d.  • Sniﬀ(d, r).': 2.7826765509168947,\n",
       " 'This encodes a subtask that is equivalent to the MSniﬀ machine.': 0.6453374951229028,\n",
       " 'However, Sniﬀ must have two child subtasks, ToWall and FollowWall that were simply internal states of MSniﬀ.': 2.7268825595005852,\n",
       " 'In particular, it can have one state for when it is moving forward and another state for when it is following a wall sideways.': 1.8802184939523996,\n",
       " '• ToWall(d).': 1.648849005072181,\n",
       " '• FollowWall(d, p).': 2.657042528287164,\n",
       " 'This is equivalent to the other part of MSniﬀ.': 0.5907140070230199,\n",
       " 'It moves in direction p until the wall in direction d ends (or until it is stuck in a corner with walls in both directions d and p).': 1.683183769020679,\n",
       " 'The goal condition is the same as the termination condition.': 2.0390167772142025,\n",
       " '• Back(d, p, x, y).': 4.696839641045649,\n",
       " 'This attempts to encode the same information as the MBack machine, but this is a case where the MAXQ hierarchy cannot capture the same information.': 1.646117830667187,\n",
       " 'MBack simply executes a sequence of 6 primitive actions (one step back, ﬁve stes in direction p).': 2.7740928599297696,\n",
       " 'But to do this, MBack must have 6 internal states, which MAXQ does not allow.': 2.678501755754975,\n",
       " 'Back terminates if it achieves this subgoal or if it runs into walls that prevent it from achieving the subgoal.': 0.6102223956301209,\n",
       " '• BackOne(d, x, y).': 3.6870854467421,\n",
       " 'This moves the agent one step backwards (in the direction opposite to d. It needs the starting x and y position in order to tell when it has succeeded.': 1.242294186500195,\n",
       " 'It terminates if it has moved at least one unit in direction d or if there is a wall in this direction.': 0.6554818571985954,\n",
       " 'Its goal condition is the same as its termination condition.': 0.6796722590714007,\n",
       " '• PerpThree(p, x, y).': 3.695278969957082,\n",
       " 'This moves the agent three steps in the direction p. It needs the starting x and y positions in order to tell when it has succeeded.': 0.6984003121342177,\n",
       " 'It terminates when it has moved at least three units in the direction p or if there is a wall in that direction.': 0.6465079984393289,\n",
       " '• Move(d).': 1.6605540382364417,\n",
       " 'This is a “parameterized primitive” action.': 0.7502926258291065,\n",
       " 'It executes one primitive move in  direction d and terminates immediately.': 0.6781115879828327,\n",
       " 'From this, we can see that there are three major diﬀerences between the MAXQ representation and the HAM representation.': 1.6398751463129146,\n",
       " 'First, a HAM ﬁnite-state controller can contain internal states.': 1.6426063207179087,\n",
       " 'To convert them into a MAXQ subtask graph, we must make a separate subtask for each internal state in the HAM.': 1.8833398361295361,\n",
       " 'Third, it is more diﬃcult to formulate the termination conditions for MAXQ subtasks than for HAM machines.': 1.647288333983613,\n",
       " 'However, this is important for the MAXQ method, because in MAXQ, each subtask learns its own value function and policy—independent of its parent tasks.': 3.836909871244635,\n",
       " 'Hence, if the agent is in the middle of executing a ToWall action when it leaves an intersection, the ToWall subroutine terminates because the ExitInter subroutine has terminated.': 2.7128365197034725,\n",
       " 'If this satisﬁes the goal condition of ExitInter, then it is also considered to satisfy the goal condition of ToWall.': 1.724151385095591,\n",
       " 'There are essentially no opportunities for state abstraction in this task, because there are no ir-relevant features of the state.': 1.8911431915723762,\n",
       " 'There are some opportunities to apply the Shielding and Termination properties, however.': 1.6129535700351152,\n",
       " 'In particular, ExitHall(d) is guaranteed to cause its parent task, MaxGo(d) to terminate, so it does not require any stored C values.': 5.788138899726882,\n",
       " 'Nonetheless, even after applying the state elimination conditions, the MAXQ representation for this task requires much more space than a ﬂat representation.': 2.8408115489660553,\n",
       " 'An exact computation is diﬃcult, but after applying MAXQ-Q learning, the MAXQ representation required 52,043 values, whereas ﬂat Q learning requires fewer than 16,704 values.': 3.8759266484588375,\n",
       " 'Parr states that his method requires only 4,300 values.': 0.7245415528677331,\n",
       " 'To test the relative eﬀectiveness of the MAXQ representation, we compare MAXQ-Q learning with ﬂat Q learning.': 1.742099102614124,\n",
       " 'Hence, we experimented with both ǫ-greedy exploration and counter-based exploration.': 1.6227077643386658,\n",
       " 'The ǫ-greedy exploration policy is an ordered, abstract GLIE policy in which a random action is chosen with probability ǫ, and ǫ is gradually decreased over time.': 2.9957081545064383,\n",
       " 'Then it switches to greedy execution.': 0.6207569254779556,\n",
       " 'Hence, it is not a genuine GLIE policy.': 1.7167381974248928,\n",
       " 'Parr employed counter-based exploration policies in his experiments with this task.': 0.6706984003121342,\n",
       " 'Figure 15 plots the results.': 0.5946156847444402,\n",
       " 'We can see that MAXQ-Q learning converges about 10 times faster than Flat Q learning.': 0.7467811158798283,\n",
       " 'We do not know whether MAXQ-Q has converged to a recursively optimal policy.': 0.7885290674990246,\n",
       " 'HAMQ learning should converge to a policy equal to or slightly better than our hand-coded policy.': 0.9289894654701522,\n",
       " 'This experiment demonstrates that the MAXQ representation can capture most—but not all— of the prior knowledge that can be represented by the HAMQ hierarchy.': 0.6433866562621928,\n",
       " 'It also shows that the MAXQ representation requires much more care in the design of the goal conditions for the subtasks.': 0.7300039016777214,\n",
       " 'All of these tasks can be easily and naturally placed into the MAXQ framework—indeed, all of them ﬁt more easily than the Parr and Russell maze task.': 1.6371439719079204,\n",
       " 'MAXQ is able to exactly duplicate Singh’s work and his decomposition of the value function— while using exactly the same amount of space to represent the value function.': 0.9032383925087788,\n",
       " 'In the Feudal-Q task, MAXQ is able to give better performance than Feudal-Q learning.': 1.706984003121342,\n",
       " 'make decisions based on the sum of its completion function, C(i, s, j), and the costs estimated by its descendants, V (j, s).': 8.866562621927427,\n",
       " 'Of course, MAXQ also supports non-hierarchical execution, which is not possible for Feudal-Q, because it does not learn a value function decomposition.': 3.8228638314475223,\n",
       " '8 Discussion: Design Tradeoﬀs in Hierarchical Reinforcement  Learning  At the start of this paper, we discussed four issues concerning the design of hierarchical reinforce-ment learning architectures.': 1.9305501365587205,\n",
       " 'In this section, we want to highlight a tradeoﬀ between two of those issues: the method for deﬁning subtasks and the use of state abstraction.': 1.8817791650409679,\n",
       " 'MAXQ deﬁnes subtasks using a termination predicate Ti and a pseudo-reward function ˜R.': 0.7483417869683964,\n",
       " 'There are at least two drawbacks of this method.': 0.6355833008193523,\n",
       " 'Second, it leads us to seek a recursively optimal policy rather than a hierarchically optimal policy.': 1.9953179867342956,\n",
       " 'Recursively optimal policies may be much worse than hierarchically optimal ones, so we may be giving up substantial performance.': 1.7655091689426454,\n",
       " 'However, in return for these two drawbacks, MAXQ obtains a very important beneﬁt: the policies and value functions for subtasks become context-free.': 2.8220834959032386,\n",
       " 'In other words, they do not depend on their parent tasks or the larger context in which they are invoked.': 1.608661724541553,\n",
       " 'To understand this point, consider again the MDP shown in Figure 6.': 1.6188060866172453,\n",
       " 'It is clear that the optimal policy for exiting the left-hand room (the Exit subtask) depends on the location of the goal.': 1.9364026531408505,\n",
       " 'Furthermore, this policy does not depend on the location of the goal.': 1.7760436987904797,\n",
       " 'Hence, we can apply Max node irrelevance to solve the Exit subtask using only the location of the robot and ignore the location of the goal.': 1.8025751072961376,\n",
       " 'This example shows that we obtain the beneﬁts of subtask reuse and state abstraction because we deﬁne the subtask using a termination predicate and a pseudo-reward function.': 1.0070230198985564,\n",
       " 'The termination predicate and pseudo-reward function provide a barrier that prevents “communication” of value information between the Exit subtask and its context.': 0.9266484588373001,\n",
       " 'Compare this to Parr’s HAM method.': 0.6262192742879439,\n",
       " 'The HAMQ algorithm ﬁnds the best policy consistent with the hierarchy.': 0.7740928599297698,\n",
       " 'To achieve this, it must permit information to propagate “into” the Exit subtask (i.e., the Exit ﬁnite-state controller) from its environment.': 3.82988685134608,\n",
       " 'To represent these diﬀerent values, the Exit subtask must know the location of the goal.': 1.8033554428404217,\n",
       " 'In short, to achieve a hierarchically optimal policy within the Exit subtask, we must (in general) represent its value function using the entire state space.': 4.239563012095201,\n",
       " 'We can see, therefore, that there is a direct tradeoﬀ between achieving hierarchical optimality and achieving recursive optimality.': 2.671478735856419,\n",
       " 'Methods for hierarchical optimality have more freedom in deﬁning subtasks (e.g., using complete policies, as in the option approach, or using partial policies, as in the HAM approach).': 5.842762387826766,\n",
       " 'But they cannot employ state abstractions within subtasks, and in general, they cannot reuse the solution of one subtask in multiple contexts.': 2.853687085446742,\n",
       " 'Methods for recursive optimality, on the other hand, must deﬁne subtasks using some method (such as pseudo-reward functions) that isolates the subtask from its context.': 3.8470542333203275,\n",
       " 'But in return, they can apply state abstraction and the learned policy can be reused in many contexts (where it will be more or less optimal).': 2.9859539602028873,\n",
       " 'It is interesting that the iterative method described by Dean and Lin (1995) can be viewed as a method for moving along this tradeoﬀ.': 1.6968396410456497,\n",
       " 'In the Dean and Lin method, the programmer makes an initial guess for the values of the terminal states of each subtask (i.e., the doorways in Figure 6).': 3.862270776433866,\n",
       " 'Based on this initial guess, the locally optimal policies for the subtasks are computed.': 1.7198595396020289,\n",
       " 'Then the locally optimal policy for the parent task is computed—while holding the subtask policies ﬁxed (i.e., treating them as options).': 2.9527896995708156,\n",
       " 'At this point, their algorithm has computed the recursively optimal solution to the original problem, given the initial guesses.': 2.7405384315255557,\n",
       " 'Instead of solving the various subproblems sequentially via an oﬄine algorithm, we could use the MAXQ-Q learning algorithm.': 1.7179087007413187,\n",
       " 'But the method of Dean and Lin does not stop here.': 0.6090518923136949,\n",
       " 'Instead, it computes new values of the terminal states of each subtask based on the learned value function for the entire problem.': 1.9504486929379634,\n",
       " 'This allows it to update its “guesses” for the values of the terminal states.': 0.7865782286383145,\n",
       " 'The entire solution process can now be repeated.': 0.5973468591494343,\n",
       " 'To obtain a new recursively optimal solution, based on the new guesses.': 1.6851346078813891,\n",
       " 'They prove that if this process is iterated indeﬁnitely, it will converge to the recursively optimal policy (provided, of course, that no state abstractions are used within the subtasks).': 5.026141240733516,\n",
       " 'This suggests an extension to MAXQ-Q learning that adapts the ˜R values online.': 0.703862660944206,\n",
       " 'Each time a subtask terminates, we could update the ˜R function based on the computed value of the terminated state.': 1.9676160749122125,\n",
       " 'However, this will only work if ˜R(j, s′) is represented using the full state s′.': 3.8903628560280916,\n",
       " 'Such an algorithm could be expected to converge to the best hierarchical policy consistent with the given state abstractions.': 0.9824424502536091,\n",
       " 'These progressive reﬁnements of the state space could be guided by monitoring the degree to which the values of ˜V (i, s′) vary for a single abstract state x′.': 3.007413187670698,\n",
       " 'If they have a large variance, this means that the state abstractions are failing to make important distinctions in the values of the states, and they should be reﬁned.': 2.8544674209910257,\n",
       " 'Both of these kinds of adaptive algorithms will take longer to converge than the basic MAXQ method described in this paper.': 0.6535310183378853,\n",
       " 'An important goal for future research is to ﬁnd methods for diagnosing and repairing errors (or sub-optimalities) in the initial hierarchy so that ultimately the optimal policy is discovered.': 1.9192352711666016,\n",
       " '9 Concluding Remarks  This paper has introduced a new representation for the value function in hierarchical reinforcement learning—the MAXQ value function decomposition.': 1.0105345298478345,\n",
       " 'This representation supports subtask sharing and re-use, because the overall value function is de-composed into value functions for individual subtasks.': 1.9270386266094417,\n",
       " 'The paper introduced a learning algorithm, MAXQ-Q learning, and proved that it converges with probability 1 to a recursively optimal policy.': 3.020288724151385,\n",
       " 'This increases the opportunities for subtask sharing and state abstraction.': 0.7865782286383145,\n",
       " 'Finally, the paper has argued that there is a tradeoﬀ governing the design of hierarchical rein-forcement learning methods.': 1.7292235661334376,\n",
       " 'At one end of the design spectrum are “context free” methods such as MAXQ.': 0.7167381974248928,\n",
       " 'They provide good support for state abstraction and subtask sharing but they can only learn recursively optimal policies.': 0.8907530238002341,\n",
       " 'At the other end of the spectrum are “context-sensitive” meth-ods such as HAMQ, the options framework, and the early work of Dean and Lin.': 2.6839641045649634,\n",
       " 'I also thank the anonymous reviewers of previous drafts of this paper for their suggestions and careful reading, which have improved the paper immeasurably.': 1.6367538041357783,\n",
       " 'References  Belmont, MA.': 1.5836909871244635,\n",
       " 'Bellman, R. E. (1957).': 2.628170113148654,\n",
       " 'Dynamic Programming.': 0.5883730003901678,\n",
       " 'Princeton University Press.': 0.5836909871244635,\n",
       " 'Bertsekas, D. P., & Tsitsiklis, J. N. (1996).': 4.6457276628950455,\n",
       " 'Neuro-Dynamic Programming.': 0.5868123293015998,\n",
       " 'Athena Scientiﬁc,  Boutilier, C., Shoham, Y., & Wellman, M. (1997).': 7.640655481857199,\n",
       " 'Economic principles of multi-agent systems  (Editorial).': 1.630901287553648,\n",
       " 'Artiﬁcial Intelligence, 94 (1–2), 1–6.': 3.628560280920796,\n",
       " 'Boutilier, C., Dean, T., & Hanks, S. (1999).': 6.647678501755755,\n",
       " 'Decision theoretic planning: Structural assumptions  and computational leverage.': 0.6566523605150215,\n",
       " 'Journal of Artiﬁcial Intelligence Research, To appear.': 1.5891533359344518,\n",
       " 'Currie, K., & Tate, A.': 3.596176355833008,\n",
       " '(1991).': 1.62738977760437,\n",
       " 'O-plan: The open planning architecture.': 0.6511900117050332,\n",
       " 'Artiﬁcial Intelligence,  52 (1), 49–86.': 3.66406554818572,\n",
       " 'Dayan, P., & Hinton, G. (1993).': 4.6468981662114714,\n",
       " 'Feudal reinforcement learning.': 0.6578228638314475,\n",
       " 'In Advances in Neural Information  Processing Systems, 5, pp.': 2.609051892313695,\n",
       " '271–278.': 0.5840811548966056,\n",
       " 'Morgan Kaufmann, San Francisco, CA.': 2.5836909871244638,\n",
       " 'Dean, T., & Lin, S.-H. (1995).': 4.641435817401484,\n",
       " 'Decomposition techniques for planning in stochastic domains.': 0.6137339055793991,\n",
       " 'Tech.': 2.918454935622318,\n",
       " 'rep. CS-95-10, Department of Computer Science, Brown University, Providence, Rhode Island.': 4.586422161529458,\n",
       " 'Dietterich, T. G. (1998).': 2.6359734685914944,\n",
       " 'The MAXQ method for hierarchical reinforcement learning.': 0.7284432305891534,\n",
       " 'In Fifteenth  International Conference on Machine Learning.': 0.6531408505657433,\n",
       " 'Fikes, R. E., Hart, P. E., & Nilsson, N. J.': 5.634412797502926,\n",
       " '(1972).': 1.62738977760437,\n",
       " 'Learning and executing generalized robot plans.': 0.6613343737807257,\n",
       " 'Artiﬁcial Intelligence, 3, 251–288.': 2.597346859149434,\n",
       " 'Hauskrecht, M., Meuleau, N., Boutilier, C., Kaelbling, L., & Dean, T. (1998).': 10.650409676160749,\n",
       " 'Hierarchical solution of Markov decision processes using macro-actions.': 0.6578228638314475,\n",
       " 'rep., Brown University, Department of Computer Science, Providence, RI.': 4.586422161529458,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryML = heapq.nlargest(10, sentence_scores, key = sentence_scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'Nonetheless, it draws a sample according to P (s′|x, y, a), receives a reward R(s′|x, y, a), and updates its estimate of V (a, x) (line 5 of MAXQ-Q).',\n",
       " 'Then the four possible subroutines are MSniﬀ(East, N orth), MSniﬀ(East, South), MBack(East, N orth), and MBack(East, South).',\n",
       " 's′ P (s′|s, a)[R(s′|s, a) + γV π(0, s)], where V π(0, s) is the value Corollary 3 Let πg(s) = argmaxa function computed by the MAXQ hierarchy.',\n",
       " '+ C π(a1, s, a2) + C π(0, s, a1),  (12)  where a0, a1, .',\n",
       " 'Hauskrecht, M., Meuleau, N., Boutilier, C., Kaelbling, L., & Dean, T. (1998).',\n",
       " 'Hence, the right-hand sides of (21) have the same value, and we can conclude that  C π(i, s1, j) = C π(i, s2, j).',\n",
       " 'At each level i, we compute values for C π(i, s, π(s)) (or V π(i, s), if i is primitive) according to the decomposition equations.',\n",
       " 'For example, suppose that the two best actions at state s are a1 and a2, that Q(s, a1) = Q(s, a2), and that ω(a1, a2).',\n",
       " '• C(GotoGoalLmk(gl), s, M axGotoLmk(l)) the cost of getting from landmark l to the land• C(Root, s, GotoGoalLmk(gl)) the cost of getting to the goal location after reaching gl.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
