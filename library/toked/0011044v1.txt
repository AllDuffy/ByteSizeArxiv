when comparing inductive logic programming and attribute value learning techniques there is a trade off between expressive power and efficiency . inductive logic programming techniques are typically more expressive but also less efficient . therefore the data sets handled by current inductive logic programming systems are small according to general standards within the data mining community . the main source of inefficiency lies in the assumption that several examples may be related to each other so they can not be handled independently . within the learning from interpretations framework for inductive logic programming this assumption is unnecessary which allows to scale up existing ilp algorithms . in this paper we explain this learning setting in the context of relational databases . we relate the setting to propositional data mining and to the classical ilp setting and show that learning from interpretations corresponds to learning from multiple relations and thus extends the expressiveness of propositional learning while maintaining its efficiency to a large extent . as a case study we present two alternative implementations of the ilp system tilde tilde classic which loads all data in main memory and tilde lds which loads the examples one by one . we experimentally compare the implementations showing tilde lds can handle large data sets and indeed scales up linearly in the number of examples .
#####
keywords inductive logic programming machine learning data mining . ams classiﬁcation primary i. . secondary i. . . a version of this report has appeared in data mining and knowledge discovery introduction there is a general trade oﬀ in computer science between expressive power and eﬃciency . theorem proving in ﬁrst order logic is less eﬃcient but more expressive than theorem proving in propositional logic . it is therefore no surprise that ﬁrst order induction techniques are less eﬃcient than propositional or attribute value learning techniques . on the other hand inductive logic programming is able to solve induction problems beyond the scope of attribute value learning cf. the computational requirements of inductive logic programming systems are higher than those of propositional learners due to the following reasons ﬁrst the space of clauses considered by inductive logic programming systems typically is much larger than that of propositional learners and can even be inﬁnite . second testing whether a clause covers an example is more complex than in attribute value learners . in attribute value learners an example corresponds to a single tuple in a relational database whereas in inductive logic programming one example may correspond to multiple tuples of multiple relations . therefore the coverage test in inductive logic programming needs a database system to solve complex queries or even a theorem prover . third and this is related to the second point in attribute value learning testing whether an example is covered is done locally i.e. independently of the other examples . therefore even if the data set is huge a speciﬁc coverage test can be performed eﬃciently . this contrasts with the large majority of inductive logic programming systems such as foil or progol in which coverage is tested globally i.e. to test the coverage of one example the whole ensemble of examples and background theory needs to be considered . global coverage tests are much more expensive than local ones . moreover systems using global coverage tests are hard to scale up . due to the fact that one single coverage test typically takes more than constant time in the size of the database the complexity of induction systems exploiting global coverage tests will grow more than linearly in the number of examples . in a more recent setting for inductive logic programming called learning from interpretations it is assumed that each example is a small database and local coverage tests are performed . algorithms using local coverage tests are typically linear in the number of examples . furthermore as each example can be loaded independently of the other ones there is no need to use a database system even when the whole data set can not be loaded into main memory . within the setting of learning from interpretations we investigate the issue of scaling up inductive logic programming . more speciﬁcally we present two alternative implementations of the tilde system tildeclassic which loads all data in main memory and tildelds which loads the examples one by one . the latter is inspired by the work by mehta et al. who propose a level wise algorithm that needs one pass through the data per level of the tree it builds . furthermore we experimentally compare the algorithms on large data sets involving examples . the experiments clearly show that inductive logic programming systems can be scaled up to satisfy the standards imposed by the data mining community . at the same time this provides evidence in favor of local coverage tests in inductive logic programming . this article is organized as follows . in section we introduce the learning from interpretations setting and relate it to the relational database context . in section we introduce ﬁrst order logical decision trees and discuss the ilp system tilde e.g. testing the coverage of member may depend on member . which induces such trees . section shows how many propositional techniques can be upgraded to the learning from interpretations setting and discusses why this is much harder for the classical ilp setting . section reports on experiments with tilde through which we empirically validate our claims section discusses some related work and in section we conclude . the learning setting we ﬁrst introduce the problem speciﬁcation in a logical context then discuss it in the context of relational databases and ﬁnally relate it to the standard inductive logic programming setting . we assume familiarity with prolog or datalog and relational databases . a word on our notation in logical formulae we will adopt the prolog convention that names starting with a capital denote variables and names starting with a lowercase character denote constants . problem speciﬁcation in our framework each example is a set of facts . these facts encode the speciﬁc properties of the examples in a database . furthermore each example is classiﬁed into one of a ﬁnite set of possible classes . one may also specify background knowledge in the form of a prolog program . more formally the problem speciﬁcation is given a set of classes c a set of classiﬁed examples e and a background theory b h e b c and c c h e b c this setting is known in inductive logic programming under the label learning from interpretations . notice that within this setting one always learns ﬁrst order deﬁnitions of propositional predicates . an implicit assumption is that the class of an example depends on that example only not on any other examples . this is a reasonable assumption for many classiﬁcation problems though not for all it precludes e.g. recursive concept deﬁnitions . example figure shows a set of pictures each of which is labelled or . the task is to classify new pictures into one of these classes by looking at the objects in the pictures . we call this kind of problems bongard problems after mikhail bongard who used similar problems for pattern recognition tests . assuming we only consider the shape conﬁguration and relative position of objects the pictures in figure can be represented as follows figure bongard problems picture picture etc. background knowledge might be provided to the learner e.g. the following deﬁ nitions could be in the background doubletriangle triangle triangle o o. polygon triangle . polygon square . when considering a particular example in conjunction with the background knowledge it is possible to deduce additional facts in the example . for instance in picture the facts doubletriangle and polygon hold . the format of a hypothesis in this setting will be illustrated later . learning from multiple relations the learning from interpretations setting as introduced before can easily be related to learning from multiple relations in a relational database . typically each predicate will correspond to one relation in the relational database . each fact in an interpretation is a tuple in the database and an interpretation corresponds to a part of the database . background knowledge can be expressed by means of views as well as extensional tables . example for the bongard example the following database contains a description of the ﬁrst two pictures in figure contains circle object o o triangle points object o o o object direction o o o up up down inside inner o o outer o o the background knowledge can be deﬁned using views as follows picture object o o o o o define view doubletriangle as select c.object c.object from contains c c where c.object c.object and c.picture c.picture and c.object in triangle and c.object in triangle define view polygon as select object from triangle union select object from square in this example the background knowledge is in a sense redundant it is computed from the other relations . this is not necessarily the case . the following example illustrates this . it is also a more realistic example of an application where mining multiple relations is useful . example assume that one has a relational database describing molecules . the molecules themselves are described by listing the atoms and bonds that occur in them as well as some properties of the molecule as a whole . mendelev s periodic table of elements is a good example of background knowledge about this domain . the following tables illustrate what such a chemical database could look like mendelev number . symbol h he li be b c. atomic weight . electrons in outer layer . molecules contains formula ho co co ch choh . name water carbon dioxide carbon monoxide methane methanol . class inorganic inorganic inorganic organic organic . molecule ho ho ho co co. atom id ho ho ho co co. atoms atom id element ho ho ho co. h o h o. bonds atom id ho ho co co. atom id ho ho co co. type single single double double . a possible classiﬁcation problem here is to classify unseen molecules into organic and inorganic molecules based on their chemical structure . notice that this representation of examples and background knowledge upgrades the typical attribute value learning representation in two respects . first in attribute value learning an example corresponds to a single tuple for a single relation . our representation allows for multiple tuples in multiple relations . second it also allows for using background knowledge . by joining all the relations in a database into one huge relation one can of course eliminate the need for learning from multiple relations . the above example should make clear that in many cases this is not an option . the information in mendelev s table for instance would be duplicated many times . moreover unless a multiple instance learner is used all the atoms a molecule consists of together with their properties have to be stored in one tuple so that an indeﬁnite number of attributes is needed see for a more detailed discussion . while mining such a database is not feasible using propositional techniques it is feasible using learning from interpretations . we proceed to show how a relational database can be converted into a suitable format . conversion from relational database to interpretations converting a relational database to a set of interpretations can be done easily and in a semi automated way as follows decide which relations are background knowledge . let db be the original database without the background relations . choose an attribute in a relation that uniquely identiﬁes the examples . for each value i of that attribute s set of all tuples in db containing that value repeat s s set of all tuples in db referred to by a foreign key in s until s does not change anymore si s the tuples in s are here assumed to be labelled with the name of the relation they are part of . a tuple of a relation r can trivially be converted to a fact r. by doing this conversion for all si each si becomes a set of facts describing an individual example i. the extensional background relations can be converted in the same manner into one set of facts that forms the background knowledge . background relations deﬁned by views can be converted to equivalent prolog programs . the only parts in this conversion process that are hard to automate are the selection of the background knowledge and the conversion of view deﬁnitions to prolog programs . also the user must indicate which attribute should be chosen as an example identiﬁer as this depends on the learning task . example in the chemical database we choose as example identiﬁer the molecular formula . the background knowledge consists of the table mendelev . in order to build a description of ho one ﬁrst collects the tuples containing ho these are present in molecules and contains . these tuples contain references to atom id s ho i i so the tuples containing those symbols are also collected . these again refer to the elements h and o which are foreign keys for the mendelev relation . since this relation is in the background no further tuples are collected . converting the tuples to facts we get the following description of ho some variations of this algorithm can be considered . for instance when the example identiﬁer has no meaning except that it identiﬁes the example this attribute can be left out from the example description . the key notion in this conversion process is localization of information . it is assumed that for each example only a relatively small part of the database is relevant and that this part can be localized and extracted . from now on we will refer to this assumption as the locality assumption . the standard ilp setting we now brieﬂy discuss the standard ilp setting and how it diﬀers from our setting . for a more thorough discussion of diﬀerent ilp settings and the relationships among them we refer to . the standard ilp setting is usually formulated as follows given a set of positive examples e and a set of negative examples e and a background theory b find a hypothesis h such that e e h b e and e e h b e note that in this setting an example e is a fact that is to be explained by h b while in the learning from interpretations setting a property of the example is to be explained by h b e. thus the latter setting explicitates the separation between example speciﬁc information and general background information . the problem speciﬁcation as given above is natural for the standard ilp setting where one could for instance give the following examples for the predicate member member . member . member . member . member . member . and expect the ilp system to come up with the following deﬁnition member . member member . note that the class of an example now depends on the class of other examples e.g. the class of member depends on the class of member which is a diﬀerent example . because of this property it is in general not possible to ﬁnd a small subset of the database that is relevant for a single example i.e. local coverage tests can not be used . results from computational learning theory conﬁrm that learning hypotheses in this setting generally is intractable . since in learning from interpretations the class of an example is assumed to be independent of other examples this setting is less powerful than the standard ilp setting . with this loss of power comes a gain in eﬃciency through local coverage tests . the interesting point is that the full power of standard ilp is not used for most practical applications and learning from interpretations usually turns out to be suﬃcient for practical applications see e.g. the proceedings of the ilp workshops and conferences of the last few years . tilde induction of first order logical deci sion trees in this section we discuss one speciﬁc ilp system that learns from interpretations called tilde . this system will be used to illustrate the topics discussed in the following sections . we ﬁrst introduce the hypothesis representation formalism used by tilde then discuss an algorithm for the induction of hypotheses in this formalism . first order logical decision trees we will use ﬁrst order logical decision trees for representing hypotheses . these are an upgrade of the well known propositional decision trees to ﬁrst order learning . a ﬁrst order logical decision tree is a binary decision tree in which the nodes of the tree contain a conjunction of literals diﬀerent nodes may share variables under the following restriction a variable that is introduced in a node must not occur in the right branch of that node . the need for this restriction follows from the semantics of the tree . a variable x that is introduced in a node is quantiﬁed existentially within the conjunction of that node . the right subtree is only relevant when the conjunction fails in which case further reference to x is meaningless . an example of such a tree is shown in figure . triangle inside figure a ﬁrst order logical decision tree that allows to discriminate the two classes for the bongard problem shown in figure . first order logical decision trees can be converted to normal logic programs and to prolog programs . in the latter case the prolog program represents a ﬁrst order decision list i.e. an ordered set of rules where a rule is only relevant if none of the rules before it succeed . each clause in such a prolog program ends with a cut . we refer to for more information on the relationship between ﬁrst order decision trees ﬁrst order decision lists and logic programs . the prolog program equivalent to the tree in figure is class triangle inside . class triangle . class . figure shows how to use foldts for classiﬁcation . we use the following notation a tree t is either a leaf with class c in which case we write t leaf or it is an internal node with conjunction conj left branch left and right branch right in which case we write t inode . because an example e is a prolog program a test in a node corresponds to checking whether a query c succeeds in e b. note that it is not suﬃcient to use for c the conjunction conj in the node itself . since conj may share variables with nodes higher in the tree c consists of several conjunctions that occur in the path from the root to the current node . more speciﬁcally c is of the form q conj where q is the conjunction of all the conditions that occur in those nodes on the path from the root to this node where the left branch was chosen . we call q the associated query of the node . when an example is sorted to the left q is updated by adding conj to it . when sorting an example to the right q need not be updated a failed test never introduces new variables . e.g. if in figure an example is sorted down the tree in the node containing inside the correct test is triangle inside it is not correct to test inside on its own . the tilde system first order logical decision trees can be induced in very much the same manner as propositional decision trees . the generic algorithm for this is usually referred to the prolog program entails class instead of c in order to ensure that the cuts have the intended meaning this is a merely syntactical diﬀerence with the original task formulation . procedure classify returns class q true n root while n leaf do let n inode if q conj succeeds in e b then q q conj n lef t else n right return c figure classiﬁcation of an example using an foldt procedure buildtree qb element of ρ with highest gain if qb is not good e.g. does not yield any gain at all then t leaf else conj qb q e e buildtree buildtree t inode procedure tilde buildtree figure algorithm for ﬁrst order logical decision tree induction as tdidt top down induction of decision trees . examples of systems using this approach are c. and cart . the algorithm we use for inducing ﬁrst order decision trees is shown in figure . the tilde system is an implementation of this algorithm that is based on c. . it uses the same heuristics the same post pruning algorithm etc. the main point where our algorithm diﬀers from c. is in the computation of the set of tests to be considered at a node . c. only considers tests comparing an attribute with a value . tilde on the other hand generates possible tests by means of a user deﬁned reﬁnement operator . roughly this operator speciﬁes given the associated query of a node which literals or conjunctions can be added to the query . more speciﬁcally the reﬁnement operator is a reﬁnement operator under θ subsumption . such an operator ρ maps clauses onto sets of clauses such that for any clause c and c ρ c θ subsumes c. a clause c θ subsumes another clause c if and only if there exists a variable substitution θ such that cθ c. the operator could for instance add literals to the clause or unify several variables in it . the use of such reﬁnement operators is standard practice in ilp . in order to reﬁne a node with associated query q tilde computes ρ and chooses the query qb ρ that results in the best split . the best split is the one that maximizes a certain quality criterion in the case of tilde this is by default the information gain ratio as deﬁned by quinlan . the conjunction put in the node consists of qb q i.e. the literals that have been added to q in order to produce qb . example consider the tree in figure . assuming that the root node has already been ﬁlled in with the test triangle how does tilde process the left child of it this child has as associated query triangle . tilde now generates ρ . according to the language bias speciﬁed by the user a possible result could be ρ assuming the best of these reﬁnements is qb triangle inside the conjunction put in the node is qb q inside . language bias while propositional systems usually have a ﬁxed language bias most ilp systems make use of a language bias that has been provided by the user . the language bias speciﬁes what kind of hypotheses are allowed in the case of tilde what kind of literals or conjunctions of literals can be put in the nodes of the tree . this bias follows from the reﬁnement operator so it is suﬃcient to specify the latter . the speciﬁc reﬁnement operator that is to be used is deﬁned by the user in a progollike manner . a set of facts of the form rmode is provided indicating which conjunctions can be added to a query the maximal number of times the conjunction can be added and the modes and types of its variables . to illustrate this we return to the example of the bongard problems . a suitable reﬁnement operator deﬁnition in this case would be rmode . rmode . rmode . rmode . rmode . rmode . rmode . the mode of an argument is indicated by a or sign before a variable . stands for input the variable should already occur in the associated query of the node where the test is put . stands for output the variable has to be one that does not occur yet . means that the argument can be both input and output i.e. the variable can be a new one or an already existing one . note that the names of the variables in the rmode facts are formal names when the literal is added to a clause actual variable names are substituted for them . also note that a literal can have multiple modes e.g. the above facts specify that at least one of the two arguments of inside has to be input . this rmode deﬁnition tells tilde that a test in a node may consist of checking whether an object that has already been referred to has a certain shape checking whether there exists an object with a certain shape in the picture testing the conﬁguration of a certain object and so on . at most literals of a certain type can occur on any path from root to leaf . the decision tree shown in figure conforms to this speciﬁcation . when tilde in the root node only the tests triangle square and builds this tree circle are considered because each other test requires some variable to occur in the associated query of the node . the left child node of the root has as associated query triangle which contains one variable x hence the tests that are considered for this node are triangle square circle triangle square circle inside inside points points assuming that inside yields the best split this literal is put in the node . in addition to rmodes so called lookahead speciﬁcations can be provided . these allow tilde to perform several successive reﬁnement steps at once . this alleviates the well known problem in ilp that a reﬁnement may not yield any gain but may introduce new variables that are crucial for classiﬁcation . by performing successive reﬁnement steps at once tilde can look ahead in the reﬁnement lattice and discover such situations . for instance lookahead speciﬁes that whenever the literal triangle is considered as possible addition to the current associated query additional reﬁnement by adding points should be tried in the same reﬁnement step . thus both triangle and triangle points would be considered as possible addition . this is useful because normally tilde can construct the test triangle points only by ﬁrst putting triangle in the node then putting points in its left child node . but if triangle already occurs in the associated query then triangle can not yield any gain and hence would never be selected and this would prevent points from being added as well . this lookahead method is very similar to lookahead methods that have been proposed for propositional decision tree learners . while for propositional systems the advantage of lookahead is generally considered to be marginal it is much greater in ilp because of the occurrence of variables . we ﬁnally mention that tilde handles numerical data by means of a discretization algorithm that is based on fayyad and irani s and dougherty et al. s work but extends it to ﬁrst order logic . the algorithm accepts input of the form discretize with var a variable occurring in query . it runs query in all the examples collecting all instantiations of var that can be found and ﬁnally generates discretization thresholds based on this set of instantiations . since this discretization procedure is not crucial to this paper we refer to for more details . input format a data set is presented to tilde in the form of a set of interpretations . each interpretation consists of a number of prolog facts surrounded by a begin and end line . the background knowledge is simply a prolog program . examples of this will be shown in section . applications of tilde although the above discussion of tilde takes the viewpoint of induction of classiﬁers the use of ﬁrst order logical decision trees is not limited to classiﬁcation . numerical predictions can be made by storing numbers instead of classes in the leaves such trees are usually called regression trees . another task that is important for data mining is clustering . induction of cluster hierarchies can also be done using a tdidt approach as is explained in . it should be clear therefore that the techniques that will be described later in this text should not be seen as speciﬁc for the classiﬁcation context . they have a much broader application domain . upgrading propositional kdd techniques for tilde in this section we discuss how existing propositional kdd techniques can be upgraded to ﬁrst order learning in our setting . the tilde system will serve as a case study here . indeed all of the techniques proposed below have been implemented in tilde . we stress however that the methodology of upgrading kdd techniques is not speciﬁc for tilde nor for induction of decision trees . it can also be used for rule induction discovery of association rules and other kinds of discovery . systems such as claudien icl and warmr are illustrations of this . both learn from interpretations and upgrade propositional techniques . icl learns ﬁrst order rule sets upgrading the techniques used in cn and warmr learns a ﬁrst order equivalent of association rules . warmr has been designed speciﬁcally for large databases and employs an eﬃcient algorithm that is an upgrade of apriori . diﬀerent implementations of tilde we discuss two diﬀerent implementations of tilde one is a straightforward implementation following closely the tdidt algorithm . the other is a more sophisticated implementation that aims speciﬁcally at handling large data sets it is for each reﬁnement qi counter and counter are class distributions i.e. arrays mapping classes onto their frequencies for each class c counter counter for each example e if qi succeeds in e then increase counter by else increase counter by si weighted average class entropy qb that qi for which si is minimal highest gain figure computation of the best test qb in tildeclassic . based on work by mehta et al. and as such is our ﬁrst example of how propositional techniques can be upgraded . . . a straightforward implementation tildeclassic the original tilde implementation which we will refer to as tildeclassic is based on the algorithm shown in figure . this is the most straightforward way of implementing tdidt . noteworthy characteristics are that the tree is built depth ﬁrst and that the best test is chosen by enumerating the possible tests and for each test computing its quality as is shown in figure . this algorithm should be seen as a detailed description of line in figure . note that with this implementation it is crucial that fetching an example from the database in order to query it is done as eﬃciently as possible because this operation is inside the innermost loop . for this reason tildeclassic loads all data into main memory when it starts up . localization is then achieved by using the module system of the prolog engine in which tilde runs . each example is loaded into a diﬀerent module and accessing an example is done by changing the currently active module which is a very cheap operation . one could also load all the examples into one module no example selection is necessary then and all data can always be accessed directly . the disadvantage is that the relevant data needs to be looked up in a large set of data so that a good indexing scheme is necessary in order to make this approach eﬃcient . we will return to this in the section on experiments . we point out that when examples are loaded into diﬀerent modules tildeclassic partially exploits the locality assumption . it does not exploit this assumption at all when all the examples are loaded into one module . . . a more sophisticated implementation tildelds mehta et al. proposed an alternative implementation of tdidt that is oriented towards mining large databases . with their approach the database is accessed less intensively which results in an important eﬃciency gain . we have adopted this approach for an alternative implementation of tilde which we call tildelds . the alternative algorithm is shown in figure . it diﬀers from tildeclassic in that the tree is now built breadth ﬁrst and examples are loaded into main memory procedure tildelds s while s φ do add one level to the tree for each example e that is not covered by a leaf node load e n the node in s that covers e q associated query for each reﬁnement qi of q if qi succeeds in e then increase counter by else increase counter by for each node n s remove n from s qb best test if qb is not good then n leaf else q associated query conj qb q n inode add left and right to s function best test returns query q associated query for each reﬁnement qi of q cdl counter cdr counter si weighted average class entropy qb that qi for which si is minimal return qb figure the tildelds algorithm one at a time . the algorithm works level wise . each iteration through the while loop will expand one level of the decision tree . s contains all nodes at the current level of the decision tree . to expand this level the algorithm considers all nodes n in s. for each node and for each reﬁnement in that node a separate counter is kept . the algorithms makes one pass through the data during which for each example that belongs to a non leaf node n it tests all reﬁnements for n on the example and updates the corresponding counters . note that while for tildeclassic the example loop was inside the reﬁnement loop the opposite is true now . this minimizes the number of times a new example must be loaded which is an expensive operation . in the current implementation each example needs to be loaded at most once per level of the tree hence the total number of passes through the data ﬁle is equal to the depth of the tree which is the same as was obtained for propositional learning algorithms . the disadvantage of this algorithm is that a four dimensional array of counters needs to be stored instead of a two dimensional one because diﬀerent counters are kept for each node and for each reﬁnement . care has been taken to implement tildelds in such a way that the size of the data set that can be handled is not restricted by internal memory . whenever information needs to be stored the size of which depends on the size of the data set this information is stored on disk . when processing a certain level of the tree the space complexity of tildelds therefore contains a component o with n the number of nodes on that level and r the number of reﬁnements of those nodes but is constant in the number of examples . this contrasts with tildeclassic where space complexity contains a component o with m the number of examples . while memory now restricts the number of reﬁnements that can be considered in each node and the maximal size of the tree this restriction is unimportant in practice as the number of reﬁnements and the tree size are usually much smaller than the upper bounds imposed by the available memory . therefore tildelds typically consumes less memory than tildeclassic and may be preferable even when the whole data set can be loaded into main memory . sampling while the above implementation is one step towards handling large data sets there will always be data sets that are too large to handle . an approach that is often taken by data mining systems when there are too many examples is to select a sample from the data and learn from that sample . such techniques are incorporated in e.g. c. and cart . in the standard ilp context there are some diﬃculties with sampling which can be ascribed to the lack of a locality assumption . when one example contains information that is relevant for another example either both examples have to be included together in the sample or none of them should . otherwise one obtains a sample in which some examples have an incomplete description . it is even possible that no good sample can be drawn because all the examples are related to one another . to the best of our knowledge sampling has received little attention inside ilp as is also noted by f urnkranz and srinivasan . if the locality assumption can be made such sampling problems do not occur . picking individual examples from the population in a random fashion independently from one another is suﬃcient to create a good sample . automatic sampling has not been included in the current tilde implementations . we do not give this high priority because tilde learns from a ﬂat ﬁle of data which is produced by extracting information from a database and putting related information together . sampling should be done at the level of the extraction of information not by tilde itself . it is rather ineﬃcient to convert the whole database into a ﬂat ﬁle and then use only a part of that ﬁle instead of only converting the part of the database that will be used . we do not present experiments with sampling as the eﬀect of sampling in data mining is out of the scope of this paper instead we refer to the already existing studies on this subject . the results of all queries for each example are stored in this manner so that when the best query is chosen after one pass through the data these results can be retrieved from the auxiliary ﬁle avoiding a second pass through the data . internal validation internal validation means that a part of the learning set is kept apart for validation purposes and the rest is used as the training set for building the hypothesis . such a methodology is often followed for tuning parameters of a system or for pruning . similar to sampling partitioning the learning set is easy if the locality assumption holds otherwise it may be hard hence learning from interpretations makes it easier to incorporate validation based techniques in an ilp system . scalability de raedt and dˇzeroski have shown that in the learning from interpretations setting learning ﬁrst order clausal theories is tractable . more speciﬁcally given ﬁxed bounds on the maximal length of clauses and the maximal arity of literals such theories are polynomial sample polynomial time pac learnable . this positive result is related directly to the learning from interpretations setting . quinlan has shown that induction of decision trees has time complexity o where a is the number of attributes of each example n is the number of examples and n is the number of nodes in the tree . since tilde uses basically the same algorithm as quinlan it inherits the linearity in the number of examples and in the number of nodes . the main diﬀerence between tilde and c. as we already noted is the generation of tests in a node . the number of tests to be considered in a node depends on the reﬁnement operator . there is no theoretical bound on this as it is possible to deﬁne reﬁnement operators that cause an inﬁnite branching factor . in practice useful reﬁnement operators always generate a ﬁnite number of reﬁnements but even then this number may not be bounded the number of reﬁnements typically increases with the length of the associated query of the node . also the time for performing one single test on a single example depends on the complexity of that test . thus we can say that induction of ﬁrst order decision trees has time complexity o with t the average number of tests performed in each node and c the average time complexity of performing one test for one example if those averages exist . if one is willing to accept an upper bound on the complexity of the theory that is to be learned and deﬁnes a ﬁnite reﬁnement operator both the complexity of performing a single test on a single example and the number of tests are bounded and the averages do exist . our main conclusion from this is that the time complexity of tilde is linear in the number of examples . this is a stronger claim than can be made for the standard ilp setting . the time complexity also depends on the global complexity of the theory and the branching factor of the reﬁnement operator which are domaindependent parameters . experiments in this experimental section we try to validate our claims about time complexity empirically and explore some inﬂuences on scalability . more speciﬁcally we want to validate the claim that when the localization assumption is exploited induction time is linear in the number of examples study the inﬂuence of localization on induction time investigate how the induction time varies with the size of the data set in more practical situations before discussing the experiments themselves we describe the data sets that we have used . description of the data sets . . robocup data set this is a data set containing data about soccer games played by software agents training for the robocup competition . it contains examples and is mb large . each example consists of a description of the state of the soccer terrain as observed by one speciﬁc player on a single moment . this description includes the identity of the player the positions of all players and of the ball the time at which the example was recorded the action the player performed and the time at which this action was executed . figure shows one example . while this data set would allow rather complicated theories to be constructed for our experiments the language bias was very simple and consisted of a propositional language . this use of the data set reﬂects the learning tasks considered up till now by the people who are using it see . this does not inﬂuence the validity of our results for relational languages because the propositions are deﬁned by the background knowledge and their truth values are computed at runtime so the query that is really executed is relational . for instance the proposition have ball indicating whether some player of the team has the ball in its possession is computed from the position of the player and of the ball . . . poker data sets the poker data sets are artiﬁcially created data sets where each example is a description of a hand of ﬁve cards together with a name for the hand . the aim is to learn deﬁnitions for several poker concepts from a set of examples . the classes that are considered here are nothing pair two pairs three of a kind full house and four of a kind . this is of course a simpliﬁcation of the real poker domain where more classes exist and it is necessary to distinguish between e.g. a pair of queens and a pair of kings but this simpliﬁed version suﬃces to illustrate the relevant topics and keeps learning times suﬃciently low to allow for reasonably extensive experiments . figure illustrates how one example in the poker domain can be represented . we have created the data sets for this domain using a program that randomly generates examples for this domain . the advantage of this approach is its ﬂexibility it is easy to create multiple training sets of increasing size as well as an independent test set . an interesting property of this data set is that some classes e.g. four of a kind are very rare hence a large data set is needed to learn these classes . . . mutagenesis data set the mutagenesis dataset is a classic benchmark in inductive logic programming . the set that has been used most often in the literature consists begin . player . player . player . player . player . player . player . player . player . player . player . player . player . player . player . player . player . player . player . player . player . player . ball . mynumber . rctime . turn . actiontime . end . figure the prolog representation of one example in the robocup data set . a fact such as player means that player of the other team was last seen at position at time . a position of means that that player has never been observed by the player that has generated this model . the action performed currently by this player is turn it is turning towards the ball . begin . card . card . card . card . card . pair . end . figure an example from the poker data set . begin . pos . atom . atom . atom . atom . atom . atom . atom . atom . bond . bond . bond . bond . bond . bond . bond . bond . bond . bond . bond . bond . end . figure the prolog representation of one example in the mutagenesis data set . the atom facts enumerate the atoms in the molecule . for each atom its element type and partial charge . the bond facts enumerate all the bonds between the atoms . pos denotes that the molecule belongs to the positive class . of examples . each example describes a molecule . some of these molecules are mutagenic others are not . the task is to predict the mutagenicity of a molecule from its description . the data set is a typical ilp data set in that the example descriptions are highly structured and there is background knowledge about the domain . several levels of background knowledge have been studied in the literature for our experiments we have always used the simplest background knowledge i.e. only structural information about the molecules are available . figure shows a part of the description of one molecule . materials and settings all experiments were performed with the two implementations of tilde we discussed tildeclassic and tildelds . these programs are implemented in prolog and run under the masterprolog engine . the hardware we used is a sun ultra at mhz running the solaris system . both tildeclassic and tildelds oﬀer the possibility to precompile the data ﬁle . we exploited this feature for all our experiments . for tildelds this raises the problem that in order to load one example at a time a diﬀerent object ﬁle has to be created for each example . this can be rather impractical . for this reason several examples are usually compiled into one object ﬁle a parameter called granularity controls how many examples can be included in one object ﬁle . object ﬁles are then loaded one by one by tildelds which means that g examples at a time are loaded into main memory . because of this the granularity parameter can aﬀect the eﬃciency of tildelds . this is investigated in our experiments . by default a value of was used for g. experiment time complexity . . aim of the experiment as mentioned before induction of trees with tildelds should in principle have a time complexity that is linear in the number of examples . with our ﬁrst experiment we empirically test whether our implementation indeed exhibits this property . we also compare it with other approaches where the locality assumption is exploited less or not at all . we distinguish the following approaches loading all data at once in main memory without exploiting the locality as sumption loading all data at once in main memory exploiting the locality assumption this is what tildeclassic does loading examples one at a time in main memory this is what tildelds does to the best of our knowledge all ilp systems that do not learn from interpretations follow the ﬁrst approach . we can easily simulate this approach with tildeclassic by specifying all information about the examples as background knowledge . for the background knowledge no locality assumption can be made since all background knowledge is potentially relevant for each example . the performance of a prolog system that works with a large database is improved signiﬁcantly if indexes are built for the predicates . on the other hand adding indexes for predicates creates some overhead with respect to the internal space that is needed and a lot of overhead for the compiler . the masterprolog system by default indexes all predicates but this indexing can be switched oﬀ . we have performed experiments for the standard ilp approach both with and without indexing . . . methodology since the aim of this experiment is to determine the inﬂuence of the number of examples on time complexity we want to control as much as possible other factors that might also have an inﬂuence . we have seen in section . that these other factors include the number of nodes n the average number of tests per node t and the average complexity of performing one test on one single example c. c depends on both the complexity of the queries themselves and on the example sizes . when varying the number of examples for our experiments we want to keep these factors constant . this means that ﬁrst of all the reﬁnement operator should be the same for all the experiments . this is automatically the case if the user does not change the reﬁnement operator speciﬁcation between consecutive experiments . the other factors can be kept constant by ensuring that the same tree is built in each experiment and that the average complexity of the examples does not change . in order to achieve this we adopt the following methodology . we create from a small data set larger data sets by including each single example several times . by ensuring that all the examples occur an equal number of times in the resulting data set the class distribution average complexity of testing a query on an example etc. are all kept constant . in other words all variation due to the inﬂuence of individual examples is removed . because the class distribution stays the same the test that is chosen in each node also stays the same . this is necessary to ensure that the same tree is grown but not suﬃcient the stopping criterion needs to be adapted as well so that a node that can not be split further for the small data set is not split when using the larger data set either . in order to achieve this the minimal number of examples that have to be covered by each leaf is increased proportionally to the size of the data set . by following this methodology the mentioned unwanted inﬂuences are ﬁltered out of the results . . . materials we used the mutagenesis data set for this experiment . other materials are as described in section . . . . setup of the experiment four diﬀerent versions of tilde are compared tildeclassic without locality assumption without indexing tildeclassic without locality assumption with indexing tildeclassic with locality assumption tildelds the ﬁrst three versions are actually the same version of tilde as far as the implementation of the learning algorithm is concerned but diﬀer in the way the data are represented and in the way the underlying prolog system handles them . each tilde version was ﬁrst run on the original data set then on data sets that contain each original example n times with n ranging from to . table summarizes some properties of the data sets that were obtained in this fashion . for each run on each data set we have recorded the following the time needed for the induction process itself the time needed to compile the data . the diﬀerent systems compile the data in diﬀerent ways . as compilation of the data need only be done once even if afterwards several runs of the induction system are done compilation time table properties of the example sets multiplication factor examples facts size . table scaling properties of tildelds in terms of the number of examples multiplication factor time induction compilation . may seem less relevant . still it is important to see how the compilation scales up since it is not really useful to have an induction method that scales linearly if it needs a preprocessing step that scales super linearly . . . discussion of the results tables and give an overview of the time each tilde version needed to induce a tree for each set as well as the time it took to compile the data into the correct format . the results are shown graphically in figure . note that both the number of examples and time are indicated on a logarithmic scale . care must be taken when interpreting these graphs a straight line does not indicate a linear relationship between the variables . indeed if log y n log x then y xn . this means the slope of the line should be in order to have a linear relationship while indicates a quadratic relationship and so on . in order to make it easier to recognize a linear relationship the function y x has been drawn on the graphs as a reference . note that only tildelds scales up well to large data sets . the other versions of tilde had problems loading or compiling the data from a multiplication factor of or on . the graphs and tables show that induction time is linear in the number of examples for tildelds for tildeclassic with locality and for tildeclassic without locality but with indexing . for tildeclassic without locality or indexing the induction time increases quadratically with the number of examples . this is not unexpected as in this setting the time needed to run a test on one single example increases with the size of the dataset . table scaling properties of tildeclassic in terms of the number of examples multiplication factor time induction compilation . prolog engine failed to load the data table scaling properties of tilde without locality assumption with indexing in terms of number of examples table scaling properties of tilde without locality assumption without indexing in terms of number of examples multiplication factor time induction compilation . prolog engine failed to load the data multiplication factor time induction compilation . prolog engine failed to load the data prolog compiler failed to compile the data lds classic no locality indexing no locality no indexing y x multiplication factor lds classic no locality indexing no locality no indexing y x s d n o c e s u p c s d n o c e s u p c e m i t n o i t a l i p m o c e multiplication factor figure scaling properties of tildelds in terms of number of examples with respect to compilation times we note that all are linear in the size of the data set except tildeclassic without locality and with indexing . this is in correspondence with the fact that building an index for the predicates in a deductive database is an expensive operation super linear in the size of the database . furthermore the experiments conﬁrm that tildeclassic with locality scales as well as tildelds with respect to time complexity but for large data sets runs into problems because it can not load all the data . observing that without indexing induction time increases quadratically and with indexing compilation time increases quadratically we conclude that the locality assumption is indeed crucial to our linearity results and that loading only a few examples at a time in main memory makes it possible to handle much larger data sets . experiment the eﬀect of localization . . aim of the experiment in the previous experiment we studied the eﬀect of the number of examples on time complexity and observed that this eﬀect is diﬀerent according to whether the locality assumption is made . in this experiment we do not just distinguish between localized and not localized but consider gradual changes in localization and thus try to quantify the eﬀect of localization on the induction time . . . methodology we can test the inﬂuence of localization on the eﬃciency of tildelds by varying the granularity parameter g in tildelds . g is the number of examples that are loaded into main memory at the same time . localization of information is stronger when g is smaller . the eﬀect of g was tested by running tildelds successively on the same data set under the same circumstances but with diﬀerent values for g. in these experiments g ranged from to . for each value of g both compilation and induction were performed ten times the reported times are the means of these ten runs . . . materials we have used three data sets a robocup data set with examples a poker data set containing examples and the mutagenesis data set with a multiplication factor of . the data sets were chosen to contain a suﬃcient number of examples to make it possible to let g vary over a relatively broad range but not more . other materials are as described in section . . . . discussion of the results induction times and compilation times are plotted versus granularity in figure . it can be seen from these plots that induction time increases approximately linearly with granularity . for very small granularities too the induction time can increase . we suspect that this eﬀect can be attributed to an overhead of disk access . a similar eﬀect is seen when we look at the compilation times these decrease when the granularity increases but asymptotically approach a constant . this again suggests an overhead caused by compiling many small ﬁles instead of one large ﬁle . the fact that the observed eﬀect is smallest for mutagenesis where individual examples are larger increases the plausibility of this explanation . this experiment clearly shows that the performance of tildelds strongly depends on g and that a reasonably small value for g is preferable . it thus conﬁrms the hypothesis that localization of information is advantageous with respect to time complexity . experiment practical scaling properties . . aim of the experiment with this experiment we want to measure how well tildelds scales up in practice without controlling any inﬂuences . this means that the tree that is induced is not guaranteed to be the same one or have the same size and that a natural variation is allowed with respect to the complexity of the examples as well as the complexity of the queries . this experiment is thus meant to mimic the situations that arise in practice . since diﬀerent trees may be grown on diﬀerent data sets the quality of these trees may diﬀer . we investigate this as well . . . methodology the methodology we follow is to choose some domain and then create data sets with diﬀerent sizes for this domain . tildelds is then run on each data set and for each run the induction time is recorded as well as the quality of the tree . . . materials data sets from two domains were used robocup and poker . these domains were chosen because large data sets were available for them . for each domain several data sets of increasing size were created . whereas induction times have been measured on both data sets predictive accuracy has been measured only for the poker data set . this was done using a separate test set of examples which was the same for all the hypotheses . for the robocup data set interpretability of the hypotheses by domain experts is the main evaluation criterion . the robocup experiments have been run on a sun sparcstation at mhz for the poker experiments a sun ultra at mhz was used . . . discussion of the results table shows the consumed cpu times in function of the number of examples as well as the predictive accuracy . these ﬁgures are plotted in figure . note that the cpu time graph is again plotted on a double logarithmic scale . with respect to accuracy the poker hypotheses show the expected behavior when more data are available the hypotheses can predict very rare classes which results in higher accuracy . the graphs further show that in the poker domain tildelds scales up linearly even though more accurate theories are found for larger data sets . in the robocup domain the induced hypotheses were the same for all runs except the examples run . in this single case the hypothesis was more simple and according to the domain expert less informative than for the other runs . this poker mutagenesis robocup granularity poker mutagenesis robocup granularity poker mutagenesis robocup s d n o c e s u p c s d n o c e s u p c s d n o c e s u p c s d n o c e s u p c s d n o c e s u p c e m t i induction compilation examples figure consumed cpu time for tildelds in the robocup domain plotted against the number of examples suggests that in this domain a relatively small set of examples suﬃces to learn from . it is harder to see how tildelds scales up for the robocup data . since the same tree is returned in all runs except the examples run one would expect the induction times to grow linearly . however the observed curve does not seem linear although it does not show a clear tendency to be super linear either . because large variations in induction time were observed we performed these runs times the estimated mean induction times are reported together with their standard errors . the standard errors alone can not explain the observed deviations nor can variations in example complexity . a possible explanation is the fact that the prolog engine performs a number of tasks that are not controlled by tilde such as garbage collection . in speciﬁc cases the prolog engine may perform many garbage collections before expanding its memory space and the time needed for these garbage collections is included in the measured cpu times . the masterprolog engine is known to sometimes exhibit such behavior . in order to sort this out tildelds would have to be reimplemented in a lowerlevel language than prolog where one has full control over all computations that occur . such a reimplementation is planned . due to the domain dependent character of these complexity results one should be careful when generalizing them it seems safe to conclude however that the linear scaling property has at least a reasonable chance of occurring in practice . related work our work is closely related to eﬀorts in the propositional learning ﬁeld to increase the capability of machine learning systems to handle large databases . it has been inﬂuenced more speciﬁcally by a tutorial on data mining by usama fayyad in which the work of mehta and others was mentioned . they were the ﬁrst to propose the level wise tree building algorithm we adopted and to implement it in the sliq and sprint systems . the main diﬀerence with our approach is that sliq and sprint learn from one single relation while tildelds can learn from multiple relations . related work inside ilp includes the rdt db system which presents the ﬁrst approach to coupling an ilp system with a relational database management system . being an ilp system rdt db also learns from multiple relations . the approach followed is that a logical test that is to be performed is converted into an sql query and sent to an external relational database management system . this approach is essentially diﬀerent from ours in that it exploits as much as possible the power of the rdbms to eﬃciently evaluate queries . also there is no need for preprocessing the data . disadvantages are that for each query an external database is accessed which is relatively slow and that it is less ﬂexible with respect to background knowledge . furthermore to obtain good performance complex modiﬁcations to the rdbms system are needed . preliminary experiments with coupling claudien and tilde to an oracle rdbms conﬁrmed these claims and caused us to abandon such an approach . we also mention the kepler system a data mining tool that provides a framework for applying a broad range of data mining systems to data sets this includes ilp systems . kepler was deliberately designed to be very open and systems using the learning from interpretations setting can be plugged into it as easily as other systems . at this moment few systems use the learning from interpretations setting . of these the research described in is most closely related to the work described in this paper in the sense that there too an eﬀort was made to adapt the system for large databases . the focus of that text is not on the advantages of learning from interpretations in general however but on the power of ﬁrst order association rules . more loosely related work inside ilp would include all eﬀorts to make ilp systems more eﬃcient . since most of this work concerns ilp systems that work in the classical ilp setting the ways in which this is done usually diﬀer substantially from what we describe in this paper . for instance the well known ilp system progol has recently been extended with caching and other eﬃciency improvements . other directions are the use of sampling techniques and stochastic methods such as proposed by e.g. srinivasan and sebag . finally the tilde system is related to other systems that induce ﬁrst order decision trees such as the struct system and the regression tree learner srt . conclusions we have argued and demonstrated empirically that the use of ilp is not limited to small databases as is often assumed . mining databases of a hundred megabytes was shown to be feasible and this does not seem to be a limit . the positive results that have been obtained are due mainly to the use of the learning from interpretations setting which is more scalable than the classical ilp setting and makes the link with propositional learning more clear . this means that a lot of results obtained for propositional learning can be extrapolated to learning from interpretations . we have discussed a number of such upgrades using the tildelds system as an illustration . the possibility to upgrade the work by mehta et al. has turned out to be crucial for handling large data sets . it is not clear how the same technique could be incorporated in a system using the classical ilp setting . although we obtained speciﬁc results only for a speciﬁc kind of data mining the results are generalizable not only to other approaches within the classiﬁcation context but also to other inductive tasks within the learning from interpretations setting such as clustering regression and induction of association rules . acknowledgements nico jacobs and hendrik blockeel are supported by the flemish institute for the promotion of scientiﬁc and technological research in the industry . luc de raedt is supported by the fund for scientiﬁc research flanders . this work is also part of the european community esprit project no . inductive logic programming . the authors thank luc dehaspe kurt driessens h el ene legras and jan ramon for proofreading this text as well as the anonymous reviewers and saˇso dˇzeroski for their very valuable comments on an earlier draft . references r. agrawal h. mannila r. srikant h. toivonen and a.i. verkamo . fast discovery of association rules . in u. fayyad g. piatetsky shapiro p. smyth and r. uthurusamy editors advances in knowledge discovery and data mining pages . the mit press . h. blockeel and l. de raedt . lookahead and discretization in ilp . in proceedings of the seventh international workshop on inductive logic programming volume of lecture notes in artiﬁcial intelligence pages . springerverlag . h. blockeel and l. de raedt . top down induction of ﬁrst order logical decision trees . artiﬁcial intelligence june . h. blockeel l. de raedt and j. ramon . top down induction of clustering trees . in proceedings of the th international conference on machine learning pages . http www.cs.kuleuven.ac.be ml ps ml . ps . m. bongard . pattern recognition . spartan books . i. bratko . prolog programming for artiﬁcial intelligence . addison wesley wokingham england . nd edition . i. bratko and s. muggleton . applications of inductive logic programming . communications of the acm . l. breiman j.h. friedman r.a. olshen and c.j. stone . classiﬁcation and regression trees . wadsworth belmont . w.w. cohen . pac learning recursive logic programs negative results . journal of artiﬁcial intelligence research . w.w. cohen and d. page . polynomial learnability and inductive logic programming methods and results . new generation computing . j. cussens . part of speech tagging using progol . in proceedings of the seventh international workshop on inductive logic programming lecture notes in artiﬁcial intelligence pages . springer verlag . l. de raedt editor . advances in inductive logic programming volume of frontiers in artiﬁcial intelligence and applications . ios press . l. de raedt . logical settings for concept learning . artiﬁcial intelligence . l. de raedt . attribute value learning versus inductive logic programming the missing links . in d. page editor proceedings of the eighth international conference on inductive logic programming volume of lecture notes in artiﬁcial intelligence pages . springer verlag . l. de raedt h. blockeel l. dehaspe and w. van laer . three companions for ﬁrst order data mining . in s. dˇzeroski and n. lavraˇc editors inductive logic programming for knowledge discovery in databases lecture notes in artiﬁcial intelligence . springer verlag . to appear . l. de raedt and l. dehaspe . clausal discovery . machine learning . l. de raedt and s. dˇzeroski . first order jk clausal theories are pac learnable . artiﬁcial intelligence . l. de raedt and w. van laer . inductive constraint logic . in klaus p. jantke takeshi shinohara and thomas zeugmann editors proceedings of the sixth international workshop on algorithmic learning theory volume of lecture notes in artiﬁcial intelligence pages . springer verlag . l. dehaspe and l. de raedt . mining association rules in multiple relations . in proceedings of the seventh international workshop on inductive logic programming volume of lecture notes in artiﬁcial intelligence pages berlin . springer verlag . t. g. dietterich r. h. lathrop and t. lozano p erez . solving the multipleinstance problem with axis parallel rectangles . artiﬁcial intelligence . j. dougherty r. kohavi and m. sahami . supervised and unsupervised discretization of continuous features . in a. prieditis and s. russell editors proceedings of the twelfth international conference on machine learning . morgan kaufmann . s. dˇzeroski s. muggleton and s. russell . pac learnability of determinate logic programs . in proceedings of the th acm workshop on computational learning theory pages . r. elmasri and s. b. navathe . fundamentals of database systems . ben jamin cummings nd edition . u.m. fayyad and k.b. irani . multi interval discretization of continuous valued attributes for classiﬁcation learning . in proceedings of the thirteenth international joint conference on artiﬁcial intelligence pages san mateo ca . morgan kaufmann . j. f urnkranz . dimensionality reduction in ilp a call to arms . in l. de raedt and s. muggleton editors proceedings of the ijcai workshop on frontiers of ilp . http www.cs.kuleuven.ac.be lucdr filp.html . j. f urnkranz . noise tolerant windowing . in m. e. pollack editor proceedings of the th international joint conference on artiﬁcial intelligence pages . morgan kaufmann . n. jacobs k. driessens and l. de raedt . using ilp systems for veriﬁcation and validation of multi agent systems . in proceedings of the eighth international conference on inductive logic programming volume pages . springer verlag . h. kitano m. veloso h. matsubara m. tambe s. coradeschi i. noda p. stone e. osawa and m. asada . the robocup synthetic agent challenge . in proceedings of the th international joint conference on artiﬁcial intelligence pages . morgan kaufmann . stefan kramer . in proceedings of the thirteenth national conference on artiﬁcial intelligence pages cambridge menlo park . aaai press mit press . structural regression trees . n. lavraˇc and s. dˇzeroski editors . proceedings of the seventh international workshop on inductive logic programming volume of lecture notes in artiﬁcial intelligence . springer verlag . m. mehta r. agrawal and j. rissanen . sliq a fast scalable classiﬁer for data mining . in proceedings of the fifth international conference on extending database technology . k. morik and p. brockhausen . a multistrategy approach to relational discovery i n databases . machine learning . s. muggleton . optimal layered learning a pac approach to incremental samin proceedings of the th conference on algorithmic learning theory . pling . ohmsma tokyo japan . invited paper . s. muggleton . inverse entailment and progol . new generation computing special issue on inductive logic programming . s. muggleton editor . proceedings of the sixth international workshop on inductive logic programming volume of lecture notes in artiﬁcial intelligence . springer verlag . s. muggleton and l. de raedt . inductive logic programming theory and methods . journal of logic programming . d. page editor . proceedings of the eighth international conference on inductive logic programming volume of lecture notes in artiﬁcial intelligence . springer verlag . g. plotkin . a note on inductive generalization . in b. meltzer and d. michie editors machine intelligence volume pages . edinburgh university press . j. ross quinlan . c. programs for machine learning . morgan kaufmann series in machine learning . morgan kaufmann . j.r. quinlan . induction of decision trees . machine learning . j.r. quinlan . learning logical deﬁnitions from relations . machine learning . j.r. quinlan . foil a midterm report . in p. brazdil editor proceedings of the th european conference on machine learning lecture notes in artiﬁcial intelligence . springer verlag . j.c. shafer r. agrawal and m. mehta . sprint a scalable parallel classiﬁer for data mining . in proceedings of the th international conference on very large databases . a. srinivasan . a study of two sampling methods for analysing large datasets with ilp . data mining and knowledge discovery . a. srinivasan s.h. muggleton m.j.e. sternberg and r.d. king . theories for mutagenicity a study in ﬁrst order and feature based induction . artiﬁcial intelligence . w. van laer l. de raedt and s. dˇzeroski . on multi class problems and discretization in inductive logic programming . in zbigniew w. ras and andrzej skowron editors proceedings of the tenth international symposium on methodologies for intelligent systems volume of lecture notes in artiﬁcial intelligence pages . springer verlag . l. watanabe and l. rendell . learning structural decision trees from examples . in proceedings of the th international joint conference on artiﬁcial intelligence pages . s. wrobel d. wettschereck e. sommer and w. emde . extensibility in data in proceedings of the second international conference on mining systems . knowledge discovery and data mining . aaai press .