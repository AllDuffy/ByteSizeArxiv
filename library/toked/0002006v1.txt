we construct new algorithms from scratch which use the fourth order cumulant of stochastic variables for the cost function . the multiplicative updating rule here constructed is natural from the homogeneous nature of the lie group and has numerous merits for the rigorous treatment of the dynamics . as one consequence the second order convergence is shown . for the cost function functions invariant under the componentwise scaling are choosen . by identifying points which can be transformed to each other by the scaling we assume that the dynamics is in a coset space . in our method a point can move toward any direction in this coset . thus no prewhitening is required .
#####
introduction suppose that n dimensional stochastic variables are observed . the independent component analysis pursues a map x y where each component of y becomes mutually independent . in this letter we restrict ourselves to the linear independent component analysis . there we want to ﬁnd a linear transformation c x y cx which minimizes some cost function that measures the independence . hereafter we denote by the upper subscript the transposition and by the complex conjugate . akuzawa islab.brain.riken.go.jp there can be many candidates for the cost function . for example the kullback leibler inforin this case the problem is translated to the mation is a good measure for the independence . minimization of dyipi ln pi where pi is the probability density function of the i th component . it is obvious that we must evaluate pi s to ﬁnd the optimal solution . a robust estimation of the probability density functions is not an easy task and if it is possible it may be computationally expensive . n i p r to make use of the cumulant of an alternative idea is the kurtosis which we will adopt in this letter . the fourth order cumulant vanishes for the normal distribution . so this cost function is robust under the gaussian random noises . we will construct algorithms where a matrix which speciﬁes the linear transformation is updated by the left multiplication of a matrix d e. this expression implies that d belongs to gl which ensures the conservation of the rank . the speciﬁcation of d by the coordinate has many advantages since it has a compatibility with the homogeneous nature of the lie group . the fourth order or there are variations for the form of the cost function . we will show our deﬁnitions in the following two sections which are choosen to possess invariance under componentwise scaling . this invariance is crucial for a rigorous treatment of the convergence properties . moreover this invariance allows us to identify points in gl which is transformed to each other by the scaling . then we can legitimately restrict the dynamics to a coset space which is introduced by this identiﬁcation . under these settings we determine by using the newton method for the second order expansion of the cost function with respect to . it is assumed that the diagonal elements of are zeros which does not impose any restrictions . that is a point can move toward any direction in this coset by a left multiplication of e. thus it is not necesarry for our method to prewhiten the data . it is also not required that the optimal solution is the maximum or the minimum of the cost function . indeed the sole requirement is that the optimal point is a saddle point of the cost function since our method is in principle the newton method . these are great advantages of our method . our strategy is as follows . as an initial condition we set c. for t we introduce an n n matrix t and denote ct as ct e tct . next we evaluate the cost function at ct by using the expansion around ct with respect to the elements of t up to the second order . then t is choosen as a saddle point of this second order expansion . we iteratively follow these procedures until we obtain a satisfactory solution . this letter is organized as follows . in section the main part of our algorithm is constructed where the cost function is essentially identical to the sum of kurtoses . we adopt the square of the kurtoses for the cost function in section . explicit expressions for the optimal are obtained both in sections and . section is a short section where we show how each updating step is combined to obtain the optimal c. in section the convergence property of our algorithm is discussed . section contains conclusions and discussions . multiplicative update algorithm . expansion of the cost function let us start by deﬁning the cost function f fi xi fi e i. e where fi s are the fourth order moments of components divided by the square of their variances in this letter we denote by e the expectation of a. obviously the cost function f coincides with the sum of kurtoses of all the components up to the constant . we set d e and expand f in terms of the elements of . for example expansions term by term are evaluated as follows e e e ip iqe o xp xp xp q xp q e e e ip iqe o. hereafter we denote by o polynomials of matrix elements of which does not contain terms with degrees less than k. for brevity s sake we introduce the following notations pi σ i e y k e y k r σ i e y k k i yp k i ypyq k pq u κi . i and using the quantities deﬁned above we can show that the cost function is expanded as fi r κi r u u ii ii ii r κi κi ii ii r ii o r ii ii o ii o ii by straightforward calculations . next we evaluate partial derivatives of the cost function by the matrix elements of . partially diﬀerentiating we get an expression f kl k r o lk lk klk r r kk r lk lk kk r lk r kk where k is an n n matrix deﬁned by kpq κqr pq . we want to decide for which the partial derivative by kl of the cost function vanish on condition that ii for i n. we neglect o terms in the cost function . thus the right hand side of is regarded as a polynomial of of at most ﬁrst order and it is always possible in principle to determine which satiﬁes the above condition . it is at the same time not easy to describe the problem in a form which is valid for arbitrary n. in the following subsection we will introduce a transparent and uniﬁed method for handling the partial derivatives of f. we leave this subsection by introducing n n matrices and for later convenience . v u κiu q k r. expression by tensor product and determination of the expression is quite complicated and not convenient for our purpose determine where all the partial derivatives vanish . fortunately by mapping the relations between elements of n n matrices to those of n n matrices we can handle the problem transparently . some preparations are needed . first let us introduce a map cs mat f n an a a. a. an n an a cs where f is an unspeciﬁed ﬁeld . we also introduce two useful operators t and p. the intertwiner t is an n n matrix deﬁned by cs t cs for a mat . the projection operator p p diag pk for k n i i n pk otherwise is used to extract the diagonal elements of a matrix from its image by cs . on this setting we can rewrite as f kl cs in q t t cs n v cs mi p p p cs l n where in is the n n unit matrix and n mi v v v. v v. we make use of the following fact for x mat see appendix a for the proof of . then becomes t t x in . f kl l n w cs l n where and n w in q q in v t p p p mi t. now let us determine . remember that we are going along the spirit of the newton method . thus we want to ﬁnd which satisﬁes the conditions f kl o for k l n k l kk for k n. the conditions make the problem rather complicated one . fortunately by using p we can combine the conditions and into a matrix equation w p h i cs cs . immediately it follows that cs h w p cs . i thus we have obtained which specify a saddle point of the expansion of f up to the second order . note that quantities in the right hand side of are easily estimated ones from the observed data . so an updating is determined by without any ambiguities . case ii square of kurtosis obviously points where kurtosis vanishes do not play any special role for the cost function f in section . the optimal solution however contains components with zero kurtoses when the number of the sources is less than that of the observation channels . thus in this section we treat with a slightly diﬀerent cost function which is the sum f f i xi of the square of the kurtoses f i e i e. as in the last section we want to know the saddle point d e of the expansion of f i in terms of up to the second order . we do not describe details of the calculations in this section which is carried out almost in the same way as in section . first the expansion of f i is evaluated as f i ii κi r ii r ii ii ii o. r ii next we introduce n n matrices k s and q deﬁned respectively by kpq r pq κq v s diag qpq . qpq . and we also rewrite q in by q in order to avoid confusions now we proceed to the expression by using the tensor product . we can show that the gradients of the cost function have the following expression f kl l n w cs l n o where in q q in w p p mi v t p p t. this is a completely analogous expression to . thus the coordinate of the saddle point of the second order expansion is determined by cs h w p cs . i in many cases obtained through the two cost functions in section and section are almost the same results . as implied at the beginning of this section the main diﬀerence between these two lies in the points where the kurtosis of one of the components vanishes . these point indeed constitue saddle points of the cost function f while it is impossible to capture them by the algorithm in section . thus we must choose an appropriate method for individual problems having this diﬀernce in mind . iteration of updating now we have obtained the updating rules . it is not necessary to tune the learning rate . apparently and look complicated . they are however easily implemented by the numerical tools starting from c ci like matlab . for positive i is determined by the left multiplication by e i where is determined by setting y ci x i.e the source codes will be available from our web site . ct e t e t e t e c. if becomes saﬁciently small we can stop the iteration and exit the process . second order convergence first we will take over the notations in section . the following discussion is however valid for the algorithm in section if we substitute the quantities f w and so on by their boldface counterparts . let us start this section by introducing some additional notations . we set g gl n and k gl n. g g k g g f f. ex ct g ft f. we also deﬁne the coset space k g by introducing the equivalence relation to g. that is k g. our method is understood as an orthodox adaptation of the newton method to this coset space k g. note that the cost function f def f on g satisﬁes the relation so f is naturally considered as a function on k g. that is the reason of our choice for the cost function . thus the second order convergence immediately follows if the the correction to the error with respect to the coordinating resulting from the multiplicative nature is properly evaluated . at time t a point g on k g is speciﬁed by the coordinate x m such that where m is the set of n n matrices whose diagonal elements are zeros . actually this statement itself is not a thing of course for which the proof will be given elsewhere . deﬁne ft the representation of the cost function at t by here we introduce an n matrix p by drawing out the i n th raws from the unit n n matrix where i n n. we will denote by h the hessian h kl ft k l note that if we set the hessian is written as ht t w p c ex ct suppose that at some neighborhood of the optimal solution g h is lipschitz continuous for some t h h l x x h p ht p. where a is the norm of a matrix a as the euclidian space we set and a tr . β h. there exists a positive real number r for which h β for g b r x t x t is satisﬁed . then it is known that for all g b g min def g x t x t βl x t x t x t x t are fulﬁlled . thus the second order convergence in this norm is shown . unfortunately this norm is not invariant and is unnatural . but it suﬃces in practice . x t x t discussions . nonholonomy our method is related to the nonholonomic method by amari chen and chichocki . in essence our method is a newton approach to the same problem the optimization without prewhitening . let us set ez exey for x y gl . then it is obvious that z does not necessarily belongs to m even if x y m. this may be explained by using the concept of nonholonomy . the degree of freedom in each step however equals the dimension of the space k g in our setting . the nonholonomic nature emerges when we go back to g gl again . there exist several studies which deal with cosets like k g or the right coset g k when k is a maximal compact subgroup of g. unfortunately what we are studying is the case where k is not a maximal compact subgroup of g. so for example it is necessary to show whether the coordinate is justiﬁed or not . as mentioned above further studies including this justiﬁcation will appear elsewhere . global convergence we should carefully treat ﬁrst few steps since this method also has a somewhat undesirable global convergent property inherent in the newton method . fortunately enough there exfor example the nonholonomic gradient ist methods which can handle the earlier stage . method may be applicable . another posiibility is to construct a nonholonomic ﬁxed point algorithm which uses the kernel method . these methods are suitable for capturing the optimal point which contains components with zero kurtoses . there we must of course use the method in section . if it is not necessary to worry about these zero kurtosis components there is little diﬀerence between the two methods described in section and section . conclusions we have constructed a new algorithm for ﬁnding a optimal point in a matrix space where we have introduced a new multiplicative updating method . the algorithm is in essence the newton method on a coset . so it converges quite rapidly and it can capture the saddle point . since it does not require prewhitening it is not necessary to worry about the error resulting from the prewhitening . indeed it is possible to increase the kurtosis slightly for data preprocessed by the fastica .