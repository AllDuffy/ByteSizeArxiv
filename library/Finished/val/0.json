{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\valTokenized\\0103003v1",
    "article": [
        "introduction a reinforcement learning agent must learn a mapping from a stream of observations of the world to a stream of actions . in completely observable domains it is sufficient to look only at the last observation so the agent can learn a memoryless mapping from observations to actions . in general however the agent s actions may have to depend on the history of previous observations . previous work there have been many approaches to learning to behave in partially observable domains . they fall roughly into three classes optimal memoryless finite memory and model based . able domains memoryless policies can actually perform fairly well . basic reinforcement learning techniques such as q learning often perform poorly in partially observable domains due to a very strong markov assumption . littman showed that finding the optimal memoryless policy is np hard . however loch and singh e\ufb00ectively demonstrated that techniques such as sarsa that are more oriented toward optimizing total reward rather than in addibellman residual often perform very well . tion jaakkola jordan and singh have developed an algorithm for finding stochastic memoryless policies which can perform significantly better than deterministic ones . one class of finite memory methods are the finitehorizon memory methods which can choose actions based on a finite window of previous observations . for many problems this can be quite e\ufb00ective . more generally we may use a finite size memory which can possibly be infinite horizon . wiering and schmidhuber proposed such an approach learning a policy that is a finite sequence of memoryless policies . another class of approaches assumes complete knowledge of the underlying process modeled as a partially observable markov decision process . given a model it is possible to attempt optimal solution or to search for approximations in a variety of ways . these methods can in principle be coupled with techniques such as variations of the baum welch algorithm for learning the model to yield model based reinforcement learning systems . the first strategy is to search for the best possiin many partially observble memoryless policy . stigmergy in this paper we pursue an approach based on stigmergy . the term is defined in the ox observation x action a state free rl memory figure the architecture of a stigmergic policy . ford english dictionary as the process by which the results of an insect s activity act as a stimulus to further activity and is used in the mobile robotics literature to describe activity in which an agent s changes to the world a\ufb00ect its future behavior usually in a useful way . one form of stigmergy is the use of external memory devices . we are all familiar with practices such as making grocery lists tying a string around a finger or putting a book by the door at home so you will remember to take it to work . in each case an agent needs to remember something about the past and does so by modifying its external perceptions in such a way that a memoryless policy will perform well . we can apply this approach to the general problem of learning to behave in partially observable environments . figure shows the architectural idea . we think of the agent as having two components one is a set of memory bits the other is a reinforcementlearning agent . the reinforcement learning agent has as input the observation that comes from the environment augmented by the memory bits . its output consists of the original actions in the environment augmented by actions that change the state of the memory . if there are sufficient memory bits then the optimal memoryless policy for the internal agent will cause the entire agent to behave optimally in its partially observable domain . consider for instance the load unload problem represented in figure . in this problem the agent is a cart that must drive from an unload location to a load location and then back to unload . this problem is a simple mdp with a one bit hidden variable that makes it non markov . it can be solved using a one bit external memory we set the bit when we make the unload observation and we go right as long as it is set to this value and we do not make the load observation . when we do make the load observation we clear the r start unload load figure the state transition diagram of the loadunload problem aliased states are grouped by dashed boxes . bit and we go left as long as it stays at value until we reach state getting a reward . there are two alternatives for designing an architecture with external memory either we augment the action space with actions that change the content of one of the memory bits changing the state of the memory may require multiple steps . or we compose the action space with the set of all possible values for the memory . in this case changing the external memory is an instantaneous action that can be done at each time step in parallel with a primitive action and hence we can reproduce the optimal policy of the load unload problem without taking additional steps . complexity considerations usually lead us to take the first option . it introduces a bias since we have to lose at least one time step each time we want to change the content of the memory . however it can be fixed in most algorithms by not discounting memory setting actions . the external memory architecture has been pursued in the context of classifier systems and in the context of reinforcement learning by littman and by mart \u0131n . littman s work was model based it assumed that the model was completely known and did a branch and bound search in policy space . mart \u0131n worked in the model free reinforcement learning domain his algorithms were very successful at finding good policies for very complex domains including some simulated visual search and block stacking tasks . however he made a number of strong assumptions and restrictions task domains are strictly goal oriented it is assumed that there is a deterministic policy that achieves the goal within some specified number of steps from every initial state and there is no desire for optimality in path length . this work we were inspired by the success of mart \u0131n s algorithm on a set of difficult problems but concerned about its restrictions and a number of details of the algorithm that seemed relatively ad hoc . at the same time baird and moore s work on vaps a general method for gradient descent in reinforcement learning appealed to us on theoretical grounds . this paper is the result of attempting to apply vaps algorithms to stigmergic policies and understanding how in this process we it relates to mart \u0131n s algorithm . have derived a much simpler version of vaps for the case of highly non markovian domains we calculate the same gradient as vaps but with much less computational e\ufb00ort . in the next section we present the relevant learning algorithms . then we describe a set of experimental domains and discuss the relative performance of the algorithms . algorithms we begin by describing the most familiar of the algorithms sarsa . we then describe the vaps algorithm in some detail followed by our simplified version . sarsa sarsa is an on policy temporal di\ufb00erence control learning algorithm . given an experience in the world characterized by starting state x action a reward r resulting state x and next action a the update rule for sarsa is q q \u03b1 . it di\ufb00ers from the classical q learning algorithm in that rather than using the maximum q value from the resulting state as an estimate of that state s value it uses the q value of the resulting state and the action that was actually chosen in that state . thus the values learned are sensitive to the policy being executed . in truly markov domains q learning is usually the algorithm of choice policy sensitivity is often seen as a liability because it makes issues of exploration more complicated . however in non markov domains policy sensitivity is actually an asset . because observations do not uniquely correspond to underlying states the value of a policy depends on the distribution of underlying states given a particular observation . but this distribution generally depends on the policy . so the value of a state given a policy can only be evaluated while executing that policy . in fact q learning can be shown to fail to converge on very simple non markov domains . note that when sarsa is used in a non markovian environment the symbols x and x in equation represent observations which usually can correspond to several states . the sarsa algorithm can be augmented with an eligibility trace to yield the sarsa algorithm with the parameter \u03bb set to sarsa is just sarsa . with \u03bb set to it is a pure monte carlo method in which at the end of every trial each state action pair is adjusted toward the cumulative reward received on this trial after the state action pair occurred . pure monte carlo algorithms make no attempt at satisfying bellman equations relating the values of subsequent states in partially observable domains it is often impossible to satisfy the bellman equation making monte carlo a reasonable choice . sarsa describes a useful class of algorithms then with appropriate choice of \u03bb depending on the problem . thus sarsa with a large value of \u03bb seems like the most appropriate of the conventional reinforcement learning algorithms for solving partially observable problems . vaps baird and moore have derived from first principles a class of stochastic gradient descent algorithms for reinforcement learning . at the most abstract level we seek to minimize some measure of the expected cost of our policy we can describe this high level criterion as b pr\u03b5 xt x s st where st is the set of all possible experience sequences that terminate at time t. that is s hx u r ... xt ut rt ... xt ut rt i where xt ut and rt are the observation action and reward at step t of the sequence and xt is an observation associated with a terminal state . the loss incurred by a sequence s is \u03b5 . we restrict our attention to time separable loss functions which can be written as t xt \u03b5 e for all s st where e is an instantaneous error function associated with each sequence prefix s hx u r ... xt ut rti and trunc representing the sequence s truncated after time t. for instance an error measure closely related to q learning is the squared bellman residual eql pr xx . the sarsa version of the algorithm uses the following error measure esarsa pr xx . xu pr note that we average over all possible actions ut according to their probability of being chosen by the policy instead of picking the one that maximizes q values as in eql . baird and moore also consider a kind of policy search which is analogous to reinforce epolicy b \u03b3trt where b is any constant . this immediate error is summed over all time t leading to a summation of all discounted immediate rewards \u03b3trt . in order to obtain the good properties of both criteria they construct a final criterion that is a linear combination of the previous two e esarsa \u03b2epolicy . this criterion combines value and policy search and is hence called vaps . we will refer to it as vaps for di\ufb00erent values of \u03b2 . baird and moore show that the gradient of the global error with respect to weight k can be written as wk b xt xs st pr where st is the set of all experience prefixes of length t. technically it is necessary that pr for all . in this work we use the boltzmann law for picking actions which guarantees this property . it is possible to perform stochastic gradient descent of the error b by repeating several trials of interaction with the process . each experimental trial of length t provides one sample of s st for each t t. of course these samples are not independent but it does not matter since we are summing them and not multiplying them . we are thus using stochastic approximation to estimate the expectation over s st in the above equation . during each trial the weights are kept constant and the approximate gradients of the error at each time t wk t xj wk e e ln pr are accumulated . weights are updated at the end of each trial using the sum of these immediate gradients . an incremental implementation of the algorithm can be obtained by using at every step t the following update rules tk t ln pr wk wk \u03b1 wk e etk t where st represents the experience prefix hx u r. xt ut rti i.e. the history at time t. note that the exploration trace tk t is independent of the immediate error e used . it only depends on the way the output pr varies with the weights wk i.e. on the representation chosen for the policy . the gradient of the immediate error e with respect to the weight wk is easy to calculate . for instance in the case of the sarsa variant of the algorithm we have wk esarsa pr pr xx xu wk \u03b3 q wk q. wk t xj wk e e ln pr once more we descend this gradient by stochastic approximation the averaging over xt and ut is replaced by a sampling of these quantities . however since these variables appear twice in the equation and they are not just added we have to sample both xt and ut independently two times in order to avoid any bias in the estimation of the gradient . it is not realistic to satisfy this requirement in a truly on line situation since the only way to get a new observation is by actually performing the action . note that for the case \u03b2 we do not need the second sample so the vaps algorithm is e\ufb00ective in the on line case . wk epolicy in the case of policy search we have for all wk . this may seem strange but for policy search the important thing is the state occupations which enter into the weight updates through the trace . vaps in this section we explore a special case of vaps in which the q values are stored in a look up table . that is there is one weight wk q for each stateaction pair . note that it is not necessary to use the vaps sequence based gradient in a look up table implementation of ql or sarsa as long as it is confined to a markovian environment . however it makes sense to use it in the context of pomdps . under this hypothesis the exploration trace tk t associated with each parameter q will be written tx u t. we will also focus on a very popular rule for randomly selecting actions as a function of their q value namely the boltzmann law p r eq c u eq c p where c is a temperature parameter . under this rule we get ln pr q pr c c if x x if x x and u u if x x and u u. the exploration trace tx u t takes a very simple form tx u t c c n t n t x u n t x u e x pr where n t x u is the number of times that action u has been executed in state x at time t n t x is the number of times that state x has been visited at time t and e represents the expected number of times we should have performed action u in state x knowing our exploration policy and our previous history . as a result of equation vaps using epolicy as immediate error look up tables and boltzmann exploration reduces to a very simple algorithm . at each time step where the current trial does not complete we just increment the counter n t x u of the current state action pair . when the trial completes this trace is used to update all the q values as described above . it is interesting to try to understand the properties and implications of this simple rule . first a direct consequence is that when something surprising happens the algorithm adjusts the unlikely actions more than the likely ones . in other words this simple procedure is very intuitive since it assigns credit to state action pairs proportional to the deviation from the expected behaviour . note that sarsa is not capable of such a discrimination . this di\ufb00erence in behaviour is illustrated in the simulation results . a second interesting property is that the q value updates tend to as the length of the trial tends to infinity . this also makes sense since the longer the trial the less the final information received is relevant in evaluating each particular action . alternatively we could say that when too many actions have been performed there is no reason to attribute the final result more to one of them than to others . finally unlike with baird and moore s version of the boltzmann law the sum of the updates to the q values on every step is . this makes it more likely that the weights will stay bounded . experiments in this case and if we add the hypothesis that the problem is an achievement task i.e. the reward is always except when we reach an absorbing goal state note that baird and moore use an unusual version of the boltzmann law with ex in place of ex in both the numerator and the denominator . we have found that it complicates the mathematics and worsens the performance so we will use the standard boltzmann law throughout . domains we have experimented with sarsa and vaps on five simple problems . two are illustrative problems previously used in the reinforcement learning literature two others are instances of load unload with di\ufb00erent parameters and the fifth is a variant of loadunload designed by us in an attempt to demonstrate a situation in which vaps might outperform sarsa . the five problems are r start r correct incorrect load h t g n e l l a i r t e g a r e v a unload original load unload problem vaps sarsa figure the state transition diagram of the loadunload problem with two loading locations aliased states are grouped by dashed boxes . number of iterations figure learning curves for vaps and sarsa on the load unload problem . baird and moore s problem designed to illus trate the behavior of vaps mccallum s state maze which has only observations . the load unload problem as described above in which there are three locations a five location load unload problem and a variant of the load unload problem where a second loading location has been added and the agent is punished instead of rewarded if it gets loaded at the wrong location . the state space is shown in figure states contained in a box are observationally indistinguishable to the agent . the idea here is that there is a single action that if chosen ruins the agent s long term prospects . if this action is chosen due to exploration then sarsa will punish all of the action choices along the chain but vaps will punish only that action . all these domains have a single starting state except mccallum s problem where the starting state is chosen uniformly at random . algorithmic details for each problem we ran two algorithms vaps and sarsa . the optimal policy for baird s problem is memoryless so the algorithms were applied directly in that case . for the other problems we augmented the input space with an additional memory bit and added two actions one for setting the bit and one for clearing it . the q functions were represented in a table with one weight for each observation action pair . the learning rate is determined by a parameter \u03b1 the actual learning rate has an added factor that decays to over time \u03b1 \u03b1 n where n is trial number . the temperature was also decayed in an ad hoc way from cmax down to cmin with an increment of \u03b4c cmin cmax in order to guarantee convergence of on each trial . sarsa in mdps it is necessary to decay the temperature in a way that is dependent on the q values themselves in the pomdp setting it is much less clear in any case we what the correct decay strategy is . have found that the empirical performance of the algorithm is not particularly sensitive to the temperature . the parameter b in the immidiate error epolicy of vaps was always set to . experimental protocol each learning algorithm was executed for k runs each run consisted of n trials which began at the start state and executed until a terminal state was reached or m steps were taken . if the run was terminated at m steps it was given a terminal reward of m was chosen in each case to be times the length of the optimal solution . at the beginning of each run the weights were randomly reinitialized to small values . results it was easy to make both algorithms work well on the first three problems baird s mccallum s and small load unload . the algorithms typically converged in fewer than runs to an optimal policy . one thing to note here is that our version of vaps using the true boltzmann exploration distribution rather load unload problem with two loading locations sarsa vaps h t g n e l l a i r t e g a r e v a number of iterations figure learning curves for vaps and sarsa on the load unload problem with two loading locations . than the one described by baird and moore seems to perform significantly better than the original according to results in their paper . things were somewhat more complex with the last two problems . we experimented with parameters over a broad range and determined the following vaps requires a value of \u03b2 equal or very nearly equal to these problems are highly nonmarkovian so the bellman error is not at all useful as a criterion . di\ufb00erence . vaps consistently converges to a nearoptimal policy but sarsa does not . the idea is that sometimes even when the policy is pretty good the agent is going to pick up the wrong load due to exploration and get punished for it . sarsa will punish all the state action pairs equally vaps will punish the bad state action pair more due to the di\ufb00erent principle of credit assignment . conclusions as mart \u0131n and littman showed small pomdps can be solved e\ufb00ectively using stigmergic policies . learning reactive policies in highly non markovian domains is not yet well understood . we have seen that the vaps algorithm somewhat modified can solve a collection of small pomdps and that although sarsa performs well on some pomdps it is possible to construct cases on which it fails . in a generalization of this work we applied the vaps algorithm to the problem of learning general finite state controllers for pomdps . acknowledgments this work was supported in part by darpa rome labs planning initiative grant f. for similar reasons \u03bb is best for sarsa ."
    ],
    "abstract": [
        "in order for an agent to perform well in partially observable domains it is usually necessary for actions to depend on the history of observations . in this paper we explore a approach in which the agent s actions include the ability to set and clear bits in an external memory and the external memory is included as part of the input to the agent . in this case we need to learn a reactive policy in a highly non markovian domain . we explore two algorithms sarsa which has had empirical success in partially observable domains and vaps a new algorithm due to baird and moore with convergence guarantees in partially observable domains . we compare the performance of these two algorithms on benchmark problems ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.6692913385826772
    ]
}