{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\valTokenized\\0211007v1",
    "article": [
        "introduction in kernel machines such as support vector machines objects are represented as a kernel matrix where n objects are represented as an n n positive semidefinite matrix . essentially the entry of the kernel matrix describes the similarity between i th and j th objects . due to positive semidefiniteness the objects can be embedded as n points in an euclidean feature space such that the inner product between two points equals to the corresponding entry of kernel matrix . this property enables us to apply diverse learning methods without explicitly constructing a feature space . biological data such as amino acid sequences gene expression arrays and phylogenetic profiles are derived from expensive experiments . typically initial experimental measurements are so noisy that they can not be given to learning machines directly . since high quality data are created by extensive work of human experts it is often the case that good data are available only for a subset of samples . when a kernel matrix is derived from such incomplete data we have to leave the entries for unavailable samples as missing . we call such a matrix an incomplete matrix . our aim is to estimate the missing entries but it is obviously impossible without additional information . so we make use of a parametric model of admissible matrices and estimate missing entries by fitting the model to existing entries . in this scheme it is important to define a parametric model appropriately . for example graepel used the set of all positive definite matrices as a model . although this model worked well when only a few entries are missing this model is too general for our cases where whole columns and rows are missing . thus we need another information source for constructing a parametric model . fortunately in biological data it is common that one object is described by two or more representations . for example genes are represented by gene networks and gene expression arrays at the same time . also a bacterium is represented by several marker sequences . in this paper we assume that a complete matrix is available from another information source and a parametric model is created by giving perturbations to the matrix . we call the complete matrix a base matrix . when creating a parametic model of admissible matrices from a base matrix one typical way is to define the parametric model as all spectral variants of the base matrix which have the same eigenvectors but di\ufb00erent eigenvalues . when several base matrices are available the weighted sum of these matrices would be a good parametric model as well . in order to fit a parametric model the distance between two matrices has to be determined . a common way is to define the euclidean distance between matrices and make use of the euclidean geometry . recently vert and kanehisa tackled with the incomplete matrix approximation problem by means of kernel cca . also cristianini et al. proposed a similarity measure called alignment which is basically the cosine between two matrices . in contrast that their methods are based on the euclidean geometry this paper will follow an alternative way we will define the kullback leibler divergence between two kernel matrices and make use of the riemannian information geometry . the kl divergence is derived by relating a kernel matrix to a covariance matrix of gaussian distribution . the primal advantage is that the kl divergence allows us to use the em algorithm to approximate an incomplete kernel matrix . the e and m steps are formulated as convex programming problems and moreover they can be solved analytically when spectral variants are used as a parametric model . we performed bacteria clustering experiments using two marker sequences s and gyrb . we derived the incomplete and base kernel matrices from gyrb and s respectively . as a result even when of columns rows are missing the clustering performance of the completed matrix was better than that of the base matrix which illustrates the e\ufb00ectiveness of our approach in real world problems . this paper is organized as follows sec . introduces the information geometry to the space of positive definite matrices . based on geometric concepts the em algorithm for matrix approximation is presented in sec . where detailed computations are deferred in sec . in sec . the matrix approximation problem is formulated as statistical inference and the equivalence between the em and em algorithms is shown . then the bacteria clustering experiment is described in sec . after seeking for possible extensions in sec . we conclude the paper in sec . information geometry of positive definite ma trices we first explain how to introduce the information geometry in the space of positive definite matrices . only necessary parts of the theory will be presented here so refer to for details . let us define the set of all d d positive definite matrices as p. the first step is to relate a d d positive definite matrix p p to the gaussian distribution with mean and covariance matrix p p d p exp . it is well known that the gaussian distribution belongs to the exponential family . the canonical form of an exponential family distribution is written as p exp where r is the vector of sufficient statistics \u03b8 is the natural parameter and \u03c8 is the normalization factor . when is rewritten in the canonical form we have the sufficient statistics as r x. x d xx . xd xd and the natural parameter as \u03b8 . dd . d d where ij denotes the entry of matrix m. the natural parameter \u03b8 provides a coordinate system to specify a positive definite matrix p which is called the \u03b8 coordinate system . on the other hand there is an alternative representation for the exponential family . let us define the mean of ri as \u03b7i for example when ri xsxt \u03b7i xsxtpdx pst . z this new set of parameters \u03b7i provides another coorninate system called \u03b7coordinate system \u03b7 . let us consider the following curve \u03b8 connecting two points \u03b8 and \u03b8 linearly in \u03b8 coordinates \u03b8 t \u03b8 . when written is the matrix form this reads p t p. this curve is regarded as a straight line from the exponential viewpoint and is called an exponential geodesic or e geodesic . in particular each coordinate curve \u03b8i t \u03b8j cj is an e geodesic . when the e geodesic between any two points in a manifold s p is included in s the manifold s is said to be e \ufb02at . on the other hand the mixture geodesic or m geodesic is defined as in the matrix form this reads \u03b7 t \u03b7 . p t p. when the m geodesic between any two points in s is included in s the manifold s is said to be m \ufb02at . in information geometry the distance between probability distributions is defined as the kullback leibler divergence kl p log z p q dx . by relating a positive definite matrix to the covariance matrix of gaussian we have the kullback leibler divergence for two matrices p q kl tr log det q log det p d. with respect to a manifold s p and a point p p the projection from p to s is defined as the point in s closest to p. since the kl divergence is asymmetric there are two kinds of projection e projection q argminq s kl . m projection q argminq s kl . it is proved that the m projection to an e \ufb02at submanifold is unique and eprojection to an m \ufb02at manifold is unique . this uniqueness property means that the corresponding optimization problem is convex and so the global optimal solution is easily obtained by any reasonable method . approximating an incomplete kernel matrix in this section we describe the em algorithm to approximate an incomplete kernel matrix . let x. x\u2113 x be the set of samples in interest . in supervised learning cases this set includes both training and test sets thus we are considering the transductive setting . let us assume that the data is available for the first n samples and unavailable for the remaining m \u2113 n samples . denote by ki an n n kernel matrix which is derived from the data for the first n samples . then an incomplete kernel matrix is described as d ki dvh d vh dhh where dvh is an n m matrix and dhh is an m m symmetric matrix . since d has missing entries it can not be presented as a point in p. instead all the possible kernel matrices form a manifold d where d means that d is positive definite . we call it the data manifold as in the conventional em algorithm . it is easy to verify that d is an m \ufb02at manifold hence the e projection to d is unique . next let us define the parametric model to approximate d. here the model is derived as the spectral variants of kb which is an \u2113 \u2113 base kernel matrix derived from another information source . let us decompose kb as \u2113 kb \u03bbiviv i i x where \u03bbi and vi is the i th eigenvalue and eigenvector respectively . define mi viv i then all the spectral variants are represented as m we call it the model manifold . for notational simplicity we choose a di\ufb00erent parametrization of m m where bj \u03b2j . it is easily seen that the manifold m is e \ufb02at and m \ufb02at at the same time . such a manifold is called dually \ufb02at . \u2113 j x \u2113 j x e m m e e m psfrag replacements d m figure information geometric picture of the em algorithm . the data manifold d corresponds to the set of all completed matrices whereas the model manifold m corresponds to the set of all spectral variants of a base matrix . the nearest points are found by gradually minimizing the kl divergence by repeating e and m projections . our approximation problem is formulated as finding the nearest points in two manifolds find d d and m m to minimize kl . in geometric terms this problem is to find the nearest points between e \ufb02at and m \ufb02at manifolds . it is well known that such a problem is solved by an alternating procedure called the em algorithm . the em algorithm gradually minimizes the kl divergence by repeating e step and m step alternately . in the e step the following optimization problem is solved with fixing m find d d that minimizes kl . this is rewritten as follows find dvh and dhh that minimize le tr log det d subject to the constraint that d. notice that this constraint is not needed because log det d log \u00b5i \u2113 i x where \u00b5i is the i th eigenvalue of d. here log det d is undefined when one of eigenvalues is negative and log det d decreases to as an eigenvalue get closer to . so at the optimal solution d is necessarily positive definite because the kl divergence is infinite otherwise . as indicated by information geometry this is a convex problem which can readily be solved by any reasonable optimizer . moreover the solution is obtained in a closed form let us partition m as the solution of is described as m svv svh s vh shh . dvh ki svhs hh hh svhkisvhs hh s dhh s hh . the derivation of and will be described in sec . . . in the m step the following optimization problem is solved with fixing d find m m that minimizes kl . this is rewritten as follows find b \u211c\u2113 that minimizes lm bj tr log det \u2113 j x \u2113 j x \u2113 subject to the constraint that j bjmj . notice that this constraint can be ignored as well . when \u2113 j are defined as the closed form solution p of is obtained as bi tr i. \u2113 . the derivation of will be described in sec . . . computing projections this section presents the derivation of e and m projections in detail . e projection first we will show the derivation of e projection and . the log determinant of a partitioned matrix is rewritten as le tr log det ki log det. when we partition m as it turns out that le tr tr tr log det ki log det. the saddle point equation with respect to dhh is obtained as le dhh shh because respect to dhh we have c log det c c for any symmetric matrix c. solving with dhh s hh d vhk i dvh . substituting into we have le tr tr tr log det ki log det shh . now the saddle point equation with respect to dvh is obtained as le dvh svh k i dvhshh . solving this equation we have the solution for dvh . by substituting into we have the solution for dhh . m projection next we will show the derivation of m projection . the m projection is obtained as the solution b to minimize lm bj tr log det. \u2113 j x since log det q q q the saddle point equations are described as tr tr i. \u2113 . \u2113 j x c j x remembering that mj vjv j we have bjmj \u2113 j x bj vjv j. \u2113 j x since the left hand side of is tr tr bi bi bj \u2113 j x the solution of is analytically obtained as bi tr i. \u2113 . we have shown that the m projection is obtained analytically when the model manifold corresponds to spectral variants of a matrix . however it is not always the case . for example consider we have c base matrices n. nc and the model manifold is constructed as harmonic mixture of them m. c j x this is an e \ufb02at manifold so the optimization problem is convex but the analytical solvability depends on geometric properties of base matrices c i. we will brie\ufb02y discuss this issue in the appendix . relation to the em algorithm in statistical inference with missing data the em algorithm is commonly used . by posing the matrix approximation problem as statistical inference the em algorithm can be applied and as shown later it eventually leads to the same procedure . in a sense it is misleading to relate matrix approximation to statistical concepts such as random variables observations and so on . nevertheless it would be meaningful to rewrite our method in terms of statistical concepts for establishing connections to other literature . let v and h be the n and m dimensional visible and hidden variables . from observed data the covariance matrix of v is known as eo ki where eo denotes the expectation with respect to observed data . however we do not know the covariances dvh eo and dhh eo . our purpose is to obtain the maximum likelihood estimate of parameter b of the following gaussian model p d m exp v h m v h where m is described as . in the course of maximum likelihood estimation we have to estimate the observed covariances dvh and dhh in an appropriate way . the em algorithm consists of the following two steps . e step fix b and update dvh and dhh by conditional expectation . m step fix d and update b by maximum likelihood estimation . it is shown that the likelihood of observed data increases monotonically by repeating these two steps . the m step maximizes the likelihood which is easily seen to be equivalent to minimizing the kl divergence . so the m step is equivalent to the m step . however the equivalence between e step and e step is not obvious in fact we do not have observed data in any sense . however we assumed them as a matter of form for relating em and em . because the former is based on conditional expectation and the latter minimizes the kl divergence . in the e step the covariance matrices are computed from the conditional distribution described as p m s hh exp shh where s matrices are derived as . taking expectation with this distribution we have eb vv svhs hh eb s vhvv svhs hh s hh . hh s then the covariance matrices are estimated as dvh eoeb kisvhs hh hh s dhh eoeb s vhki svhs hh . since these solutions are equivalent to and respectively the e step is shown to be equivalent to the e step in this case . refer to amari for general discussion of the equivalence between em and em algorithms . bacteria classification experiment in this section we perform unsupervised classification experiments for bacteria based on two marker sequences s and gyrb . basically we would like to identify the genus of a bacterium by means of extracted entities from the cell . it is known that several specific proteins and rnas can be used for genus identification . among them we especially focus on s rrna and gyrase subunit b protein . s rrna is an essential constituent in all living organisms and the existence of many conserved regions in the rrna genes allows the alignment of their sequences derived from distantly related organisms while their variable regions are useful for the distinction of closely related organisms . gyrb is a type ii dna topoisomerase which is an enzyme that controls and modifies the topological states of dna supercoils . this protein is known to be well preserved over evolutional history among bacterial organisms thus is supposed to be a better identifier than the traditional s rrna . notice that s is represented as a nucleotide sequence with symbols and gyrb is an amino acid sequence with symbols . since gyrb has been found to be useful more recently than s gyrb sequences are available only for a limited number of bacteria . thus it is considered that gyrb is more expensive than s. our dataset has bacteria of three genera each of which has both s and gyrb sequences . for simplicity let us call these genera as class respectively . for s and gyrb we computed the second order count kernel which is the dot product of bimer counts . each kernel matrix is normalized such that the norm of each sample in the feature space becomes one . the kernel matrices of gyrb and s can be seen in fig . and respectively . for reference we show an ideal matrix as fig . which indicates the true classes . in our senario for a considerble number of bacteria gyrb sequences are not available as in fig . we will complete the missing entries by the em algorithm with the spectral variants of s matrix . when the em algorithm converges we end up with two matrices the completed matrix on data manifold d and the estimated matrix on model manifold m. these two matrices are in general not the same because the two manifolds may not have intersection . in order to evaluate the quality of completed and estimated matrices kmeans clustering is performed in the feature space of each kernel . in evaluating the partition we use the adjusted rand index . let u. uc be the obtained clusters and t. ts be the ground truth clusters . let nij be the number of samples which belongs to both ui and tj . also let ni . and n.j be the number of samples in ui and tj respectively . ari is defined as i j nij j ni . i p n.j n.j j p i ni . j n n.j. n p ni . i p the attractive point of ari is that it can measure the di\ufb00erence of two partitions even when the number of clusters is di\ufb00erent . when the two partitions are exactly the same ari is and the expected value of ari over random partitions is . p p p the clustering experiment is performed by randomly removing samples from gyrb data . the ratio of missing samples is changed from to . the aris of completed and estimated matrices averaged over trials are shown in fig . and respectively . comparing the two matrices the estimated matrix performed significantly worse than the complete matrix . it is because the completed matrix maintains existing entries unchanged and so the class information in gyrb matrix is well preserved . we especially focus on the comparison between the completed matrix and s matrix because there is no point in performing the em algorithm when s matrix works better than the completed matrix . according to the plot the ari of completed matrix was larger than s matrix up to missing ratio . it implies that the matrix completion is meaningful even in quite hard situations sample loss implies loss in entries . this result encourages us to apply the em algorithm to other data such as gene networks . possible extension as we related the em algorithm to maximum likelihood inference in sec . it is straightforward to generalize it to the maximum a posteriori inference ideal gyrb s gyrb completed matrix estimated matrix figure an example of kernel matrix completion . see the text for details . x e d n i d n a r d e t s u d a j. x e d n i d n a r d e t s u d a j. missing samples figure clustering performance of the completed matrix . the solid curve shows the averaged ari of the completed matrix and the error bar describes the standard deviation . the upper and lower \ufb02at lines show the aris of the complete gyrb and s kernel matrices respectively . missing samples figure clustering performance of the estimated matrix . the solid curve shows the averaged ari of the estimated matrix and the error bar describes the standard deviation . or more generally the bayes inference . for example we are going to modify the em algorithm to obtain the map estimate . the map estimation amounts to minimizing the kl divergence penalized by a prior kl log \u03c0 d d m m where \u03c0 is a prior distribution for m. since the additional term log \u03c0 depends only on the model m only the m step is changed so as to minimize the above objective function with respect to m. let us give a simple example of map estimation in the spectral variants case . in bayesian inference it is common to take a conjugate prior so that the posterior distribution remains as a member of the exponential family . since the model parameter b is related to a covariance matrix we choose the gamma distribution which works as a conjugate prior for the variance of gaussian distribution . the prior distribution is defined independently for each bj as \u03c0 \u03b3\u03b1\u03bd exp bj \u03b1 log bj where \u03bd and \u03b1 denote hyperparameters by which the mean and the variance are specified by e \u03b1\u03bd and v \u03b1\u03bd . the m step for map estimation is to minimize l map m lm log \u03c0 \u2113 j x which leads to the equation tr tr i. \u2113 . \u03bd bi \u03b1 c j x in the spectral variants case the left hand side is reduced to \u03bd bi thus we obtain the map solution in a closed form as bi \u03bd tr \u03b1 i. \u2113 . conclusion in this paper we introduced the information geometry in the space of kernel matrices and applied the em algorithm in matrix approximation . the main difference to other euclidean methods is that we use the kl divergence . in general we can not determine which distance is better because it is highly data dependent . however our method has a great utility because it can be implemented only with algebraic computation and we do not need any specialized optimizer such as semidefinite programmming unlike . one of our contribution is that we related matrix approximation to statistical inference in sec . thus in future works it would be interesting to involve advanced methods in statistical inference such as generalized em and variational bayes . also we are looking forward to apply our method to diverse kinds of real data which are not limited to bioinformatics . a analytical solvability of the m step in this appendix we discuss the solvability of the m step . the left hand side of is the m coordinate of the submanifold m while bj denote the e coordinate of m. the e coordinate and m coordinate are connected by the legendre transform . in the mother manifold p the legendre transform is easily obtained as the inverse of the matrix . in the submanifold m of p however it is difficult to obtain the legendre transform in general . the difficulty is caused by the di\ufb00erence of geodesics defined in m and p. when the geodesic defined by a coordinate system of a submanifold s p coincides the geodesic defined by the corresponding global coordinate system of p the in our case m is autoparallel for the esubmanifold is called autoparallel . coordinate but it is not always autoparallel for the m coordinate . when the submanifold is autoparallel for the both coordinate systems the submanifold is called doubly autoparallel . let us consider when a submanifold becomes doubly autoparallel . to begin with let us define the product between two d d symmetric matrices x y sym x y. the algebra equipped with the usual matrix sum and the product is called the jordan algebra of the vector space of sym . the following theorem provides the necessary and sufficient condition for doubly autoparallel submanifold . theorem . assume the identity matrix i is an element of the submanifold m then m is doubly autoparallel if and only if the tangent space of m is a jordan subalgebra of sym . when a submanifold m p is determined as m is doubly autoparallel if the following holds for all i j ni nj span . ohara has shown that if and only if m is doubly autoparallel the mprojection can be solved analytically that is the optimal solution is obtained by one newton step . for example in the spectral variants case ni viv i and ni nj span . thus the m projection is obtained analytically in this case . acknowledgement the authors gratefully acknowledge that the bacterial gyrb amino acid sequences are o\ufb00ered by courtesy of identification and classification of bacteria database team of marine biotechnology institute kamaishi japan . the authors would like to thank t. kin y. nishimori t. tsuchiya and j. p. vert for fruitful discussions ."
    ],
    "abstract": [
        "in biological data it is often the case that observed data are available only for a subset of samples . when a kernel matrix is derived from such data we have to leave the entries for unavailable samples as missing . in this paper we make use of a parametric model of kernel matrices and estimate missing entries by fitting the model to existing entries . the parametric model is created as a set of spectral variants of a complete kernel matrix derived from another information source . for model fitting we adopt the em algorithm based on the information geometry of positive definite matrices . we will report promising results on bacteria clustering experiments using two marker sequences s and gyrb ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.8373983739837398
    ]
}