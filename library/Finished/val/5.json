{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\valTokenized\\0311042v1",
    "article": [
        "v o n g l. s c v s c v i x r a supported by an nsf mathematical sciences postdoctoral research fellowship . introduction . attribute efficient learning a central goal in machine learning is to design efficient e\ufb00ective algorithms for learning from small amounts of data . an obstacle to achieving this goal is that learning problems are often characterized by an abundance of irrelevant information . in many learning problems each data point is naturally viewed as a high dimensional vector of attribute values as a motivating example in a natural language domain a data point representing a text document may be a vector of word frequencies over a lexicon of words . a newly encountered word in a corpus may typically have a simple definition which uses only a dozen or so words from the entire lexicon . one would like to be able to learn the meaning of such a word using a number of examples which is closer to a dozen than to . towards this end an important goal in machine learning theory is to design attribute efficient algorithms for learning various classes of boolean functions . a class of boolean functions over n variables x. xn is said to be attribute efficiently learnable if there is a poly time algorithm which can learn any function f c using a number of examples which is polynomial in the size of the function f to be learned rather than in n. thus an attribute efficient learning algorithm for say the class of boolean conjunctions must be able to learn any boolean conjunction of k literals over x. xn using poly examples since k log n bits are required to specify such a conjunction . c. decision lists a longstanding open problem in machine learning posed first by blum in and again by valiant in is to determine whether or not there exist attribute efficient algorithms for learning decision lists . a decision list is essentially a nested if then else statement . attribute efficient learning of decision lists is of both theoretical and practical interest . blum s motivation for considering the problem came from the infinite attribute model in this model there are infinitely many attributes but the concept to be learned depends on only a small number of them and each example consists of a finite list of active attributes . blum et al. showed that for a wide range of concept classes attribute efficient learnability in the standard n attribute model is equivalent to learnability in the infinite attribute model . since simple classes such as disjunctions and conjunctions are attribute efficiently learnable this motivated blum to ask whether the richer class of decision lists is thus learnable as well . several researchers have subsequently considered this problem see e.g. we summarize some of this previous work in section . . from an applied perspective valiant relates the problem of learning decision lists attribute efficiently to the question how can human beings learn from small amounts of data in the presence of irrelevant information he points out that since decision lists play an important role in various models of cognition a first step in understanding this phenomenon would be to identify efficient algorithms which learn decision lists from few examples . due to the lack of progress in developing such algorithms for decision lists valiant suggests that models of cognition should perhaps focus on \ufb02atter classes of functions such as projective dnf . additional motivation comes from the fact that decision lists have such a simple algorithm in the pac model . parity functions another outstanding challenge in machine learning is to determine whether there exist attribute efficient algorithms for learning parity functions . the parity function on a set of valued variables xik modulo . as with the class of decision lists a simple pac xi . xik is equal to xi learning algorithm is known for the class of parity functions but no attribute efficient pac learning algorithm is known . learning parity functions plays an important rule in fourier learning methods and is closely related to decoding random linear codes . both a. blum and y. mansour cite attribute efficient learning of parity functions as an important open problem . our results decision lists we give the first learning algorithm for decision lists that is subexponential in both sample complexity and running time . our results demonstrate for the first time that it is possible to simultaneously avoid the worst case in both sample complexity and running time and thus suggest that it may indeed be possible to learn decision lists attribute efficiently . our main learning result for decision lists is theorem there is an algorithm for learning decision lists over decision list of length k has mistake bound o log n and runs in time n o. n which when learning a the value f of the unknown target function f. after each prediction the learner is given the true value of f and can update its hypothesis before the next trial begins . the mistake bound of a learning algorithm on a target concept c is measured by the worst case number of mistakes that the algorithm makes over all sequences of examples and the mistake bound of a learning algorithm on a concept class c is the worst case mistake bound across all functions f c. the running time of a learning algorithm a for a concept class c is defined as the product of the mistake bound of a on c times the maximum running time required by a to evaluate its hypothesis and update its hypothesis in any trial . a parity function of length k is defined by a set of variables s or . given any x such that we say that p is a polynomial threshold function of degree d and weight w for f. \u2113 \u2113b this polynomial has degree h and has weight at most h. summing these polynomial representations for f. fk h as in we see that the resulting polynomial threshold function given by h has degree h and weight at most k h f \u2113b \u2113b \u2113hbh . k h h. \u2113h specializing to the case h k we obtain corollary let l be a decision list of length k. then l is computed by a polynomial threshold function of degree k and weight k. we close this section by observing that an intermediate result of can be used to give an alternate proof of corollary with slightly weaker parameters see appendix a. inner approximator in this section we construct low degree low weight polynomials which approximate the modified decision lists from the previous subsection . moreover the polynomials we construct are exactly correct on inputs which fall o\ufb00 the end theorem let f bh be a modified decision list of length h. then there is a degree h log h polynomial p such that for every input x h we have p p lower bounds for decision lists here we observe that our construction from theorem is essentially optimal in terms of the tradeo\ufb00 it achieves between polynomial threshold function degree and weight . in beigel constructs an oracle separating pp from pnp . at the heart of his construction is a proof that any low degree polynomial threshold function for a particular decision list called the the oddmaxbitn function must have large weights definition the oddmaxbitn function on input x x. xn threshold function regardless of degree must have weight \u03c9 . for parity functions one challenge is to learn parity functions on k \u03b8 variables in polynomial time using a sublinear number of examples . another challenge is to improve the sample complexity of learning size k parities from our current bound of o. we thank les valiant for his observation that claim can be reinterpreted in terms of polynomial threshold functions . we thank jean kwon for suggesting the chebychev polynomial . acknowledgements"
    ],
    "abstract": [
        "we make progress on two important problems regarding attribute efficient learnability . first we give an algorithm for learning decision lists of length k over n variables using ^ tilde k ^ log n examples and time n ^ tilde k ^ . this is the first algorithm for learning decision lists that has both subexponential sample complexity and subexponential running time in the relevant parameters . our approach establishes a relationship between attribute efficient learning and polynomial threshold functions and is based on a new construction of low degree low weight polynomial threshold functions for decision lists . for a wide range of parameters our construction matches a lower bound due to beigel for the oddmaxbit predicate and gives an essentially optimal tradeoff between polynomial threshold function degree and weight . second we give an algorithm for learning an unknown parity function on k out of n variables using o n ^ examples in time polynomial in n. for k o this yields a polynomial time algorithm with sample complexity o. this is the first polynomial time algorithm for learning parity on a superconstant number of variables with sublinear sample complexity ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.42487046632124353
    ]
}