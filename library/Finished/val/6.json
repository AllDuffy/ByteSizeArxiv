{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\valTokenized\\0312004v1",
    "article": [
        "keywords naive bayes k nearest neighbor text classification spam filtering introduction proposed by sahami et al in the year naive bayes have been used very successfully for spam filtering within the last few months . although filter systems based on naive bayes achieve a very high overall accuracy of about in this paper we want to try to improve these results by combining naive bayes with simple k nearest neighbor searches . as shown in the accuracy of naive bayes can be increased successively by increasing the dimension of the document vectors until the dimension reaches about . we now show that with the approach presented in this paper the dimension of document vectors can be reduced down to features . at the same time we achieve an even higher classification accuracy than without the combination of naive bayes and a k nearest neighbor search . the k nearest neighbor search is also a very simple method to classify documents and was found to show very good performance on text categorization tasks . it is a lazy learning method which means that it does not need a learning phase . the only thing which has to be done is to index the documents of the training set and convert them into a document vector representation . when classifying a new document the similarity of its document vector to each document vector in the training set has to be computed . then we determine the categories of the k nearest neighbors and choose the category which occurs most frequently . as a measure for the similarity of two documents we use the widely used euclidean distance and the angle between two documents . the paper is organized as follows . in the next section we describe our simple approach to combine naive bayes with a k nearest neighbor search . in the third section we describe our experiments and the way we tokenize emails and in the last section we present the results of our experiments . combining naive bayes with knn let p rnb and p rnb be the probability computed by using naive bayes that an email m is spam legitimate . of course it follows that p rnb p rnb for the case of spam filtering . furthermore let u v rv be two document vectors and v the number of features or dimension of the vectors . the euclidean distance of u and v is computed as follows e u v. p the probabilities p rknn and p rknn that an email m is classified as spam legitimate using the k nearest neighbor classification are computed as follows . first we are looking for the k nearest neighbors of m. here m denotes the document vector of an email m. then we determine the categories of the emails in the neighborhood . let ns be the number of emails assigned to the category spam and ng be the number of emails assigned to the category legitimate . now the probabilities p rknn and p rknn can be computed as follows p rknn p rknn ng k ns k ns k the next step of the algorithm is to combine the probabilities of the naive bayes classification and the k nearest neighbor search . we compute a score \u03b4g for the category legitimate and \u03b4s for the category spam as follows \u03b4g \u03b1p rnb \u03b2p rknn \u03b1 \u03b2 \u03b4s \u03b1p rnb \u03b2p rknn \u03b1 \u03b2 theorem \u03b4g \u03b4s proof \u03b1 \u03b2 \u03b1 \u03b2 \u03b1 \u03b2 \u03b1 \u03b2 \u03b1 \u03b2 in our experiments we choose \u03b1 \u03b2 and compute the score \u03b4g for every new document . we assign the document to the category legitimate if \u03b4g . otherwise to the category spam . using angles instead of euclidean distances in this paper we do not only use the euclidean distance as a measure of similarity between two documents but we also perform k nearest neighbor searches by computing the angle between two documents . the angles between two vectors u and v can be computing by the following equation sim u v u v i uivi p i u i qp i v i qp experiments the email corpus consists of emails . of these emails are assigned to the category spam the remaining emails are assigned to the category legitimate . we run our experiments with three di\ufb00erent training and test sets . the first pair of training and test set is created by splitting the corpus at a ratio of . the second pair is at a ratio of and the third one at a ratio of . for each pair we reduce the dimension by using information gain to and features . thus we get models which we test with di\ufb00erent values for parameter k of the k nearest neighbor search . to tokenize each email we take the header and body convert them to lower case remove all html tags and extract all words w and numbers with at least two characters . we ignore uuencoded lines . since spam is often written in html we append the word html three times at the end of the email for each occurring html tag which occurs . in previous experiments we have achieved good results with this technique . legitimate legitimate k v. v. spam spam k v. v. v. v. v. v. figure these tables show the classification accuracy which is achieved by combining naive bayes with a knn search which uses the euclidean distance as the similarity measure . the table at the top shows the accuracy that legitimate emails were classified correctly . the table at the bottom shows the accuracy that spam was classified correctly . to classify an email of the test set with naive bayes we use rainbow a toolkit to perform statistical text classification . with the options lex white and lex pipecommand rainbow can be told to use our tokenizer instead of the build in tokenizer . to compute the k nearest neighbors of an email we first convert all emails of the training set into their document vector representation and compute the euclidean distance or the angle between each of these vectors and the document vector of the email which we want to classify . for simplicity the attributes of the vectors contain the number of occurrences of the represented feature . we do not apply and special weighting algorithm . we combine the results of the naive bayes and k nearest neighbor classification as described in the previous section and choose the category with the highest \u03b4 score . results the results of our experiments are shown in figure for which we have used the euclidean distance as the measure of similarity and figure for which we have used the angle between two document vectors as the measure of similarity . in both figures the table at the top shows the accuracy that legitimate emails were classified correctly whereas the accuracy that spam was classified correctly is shown in the table at the bottom . the maximum of each column is bold . the first row which legitimate legitimate k v. v. spam spam k v. v. v. v. v. v. figure these tables show the classification accuracy which is achieved by combining naive bayes with a knn search which uses the angle between two documents as the similarity measure . the table at the top shows the accuracy that legitimate emails were classified correctly . the table at the bottom shows the accuracy that spam was classified correctly . is labeled as contains the results of a naive bayes classification without a k nearest neighbor search . in most cases the combination of naive bayes with a k nearest neighbor search performs better than a classification without using a k nearest neighbor search . both similarity measures achieve similar results . in practice it is quite important to prevent wrong classification of legitimate emails because this is much more expensive than the wrong classification of spam . a legitimate email which is incorrectly assigned to the category spam is called false positive . our results show that the number of false positives can be reduced dramatically by our new classification approach . for the case of correctly classified legitimate emails we achieve in most experiments a classification accuracy better than for k. also for k and v the classification accuracy of legitimate emails is higher than the accuracy for v without a k nearest neighbor search . therefore the dimension or number of features can be reduced down to which results in lower computational cost for the learning and classification task . nevertheless for both similarity measures the best classification accuracy of legitimate emails is still achieved for a large number of features here for v and k. the accuracy that is achieved is . for the euclidean distance and . by using the angle as similarity measure . in contrast to legitimate emails for which the best accuracy is obtained for k the best accuracy for spam is obtained for k. as previously seen for legitimate emails also for spam this accuracy is higher than the accuracy which is achieved for the classification without our new approach for a high number of features . therefore when using our approach to reduce the computational cost for learning and classification it should be the best to use only features with a value for k either equal to one or to two . the number of false positives is reduced most for a value of one whereas for a value of two both the accuracy for legitimate emails and spam is better than the accuracy of a classification without our approach for high dimensional document vectors ."
    ],
    "abstract": [
        "using naive bayes for email classification has become very popular within the last few months . they are quite easy to implement and very efficient . in this paper we want to present empirical results of email classification using a combination of naive bayes and k nearest neighbor searches . using this technique we show that the accuracy of a bayes filter can be improved slightly for a high number of features and significantly for a small number of features ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.7037037037037037
    ]
}