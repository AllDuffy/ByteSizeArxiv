{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\valTokenized\\0211003v1",
    "article": [
        "introduction the markov blanket bayesian classifier algorithm has been recently proposed in a paper by this author as an alternative to other structures based on bayesian networks for classification tasks . in that paper the algorithm was described some preliminary experiments using it were discussed . this paper presents an extended description of the mbbc algorithm along with the results of a more comprehensive set of experiments . by way of providing context for this research bayesian networks and bayesian classifiers are discussed in this section and the k algorithm for is described in section . learning of bayesian networks inductive inductive several algorithms have been developed over the past decade for learning of bayesian networks . russell and norvig identify four classes of problem according to whether or not the structure is known and whether all variables are observed or some are hidden . in this paper we are concerned with problems where the structure is unknown and all variables are observed and the resulting bayesian network is applied to classification problems . although general bayesian network structures may be used for classification tasks as will be described in section . this may be computationally inefficient since the classification node is not explicitly identified and not all of the structure may be relevant for classification since parts of the structure outside of the classification node s markov blanket . accordingly several simplified bayesian structures intended specifically for classification tasks have been proposed these include na\u00efve bayes treeaugmented na\u00efve bayes and bayesian network augmented na\u00efve bayes which are illustrated in figure and discussed below in section . . however in all of these structures it is assumed that the classification variable thereby excluding structures where the classification variable is causally dependent on another variable . mbbc does not make that assumption . in addition mbbc is more expressive than those other structures as it is able to full markov blanket around a represent classification node in a general bayesian network . the root node the is . bayesian networks bayesian networks graphically represent the joint probability distribution of a set of random variables . a bayesian network is composed of a qualitative portion and a quantitative portion . the structure bs is a directed acyclic graph where the nodes correspond to domain variables x xn and the arcs between nodes represent direct dependencies between the variables . likewise the absence of an arc between two nodes x and x represents that x is independent of x given its parents in bs . following the notation of cooper and herskovits the set of parents of a node xi in bs is denote pi . the structure is annotated with a set of conditional probabilities containing a term p for each possible value xi of xi and each possible instantiation pi of pi . bayesian networks were originally constructed manually through consideration of causal dependencies in a system . however several algorithms have been proposed in the last decade for inductive learning of bayesian networks following on from a seminal paper by cooper and herskovits . their k algorithm for bayesian network induction is overviewed in section as the mbbc algorithm is based on it . classifiers based on bayesian networks figure schematically illustrates the structure of the bayesian classifiers considered in this paper . the simplest form of bayesian classifier known as na\u00efve bayes was shown by langley et al. to be competitive with quinlan s popular c. decision tree classifier . na\u00efve bayes is so called because it makes unrealistic following the assumptions . all other variables are conditionally independent of often two each other given the classification variable . all other variables are directly dependent on the classification variable when represented as a bayesian network a na\u00efve bayes classifier has a simple structure whereby there is an arc from the classification node to each other node and there are no arcs between other nodes as in figure . x c x c x c x c x x x x x x x x na\u00efve bayes x x x x x x tan x x x c x c x x x x x x ban x x x x x x x x x c x c x x general bn figure illustration of na\u00efve bayes tan ban and general bn structures several researchers have examined ways of achieving better performance than na\u00efve bayes by relaxing these assumptions . friedman et al. analyze tree augmented na\u00efve bayes which allows arcs between the children of the classification node xc as in figure thereby relaxing the first assumption above . in their approach each node has xc and at most one other node as a parent so that the nodes excluding xc form a tree structure . they use a minimum description length metric rather than the bayesian metric used in this paper . to find arcs between the nodes they use an algorithm first proposed by chow and liu for learning treestructured bayesian networks . langley and sage consider an alternative approach called selective na\u00efve bayes in which a subset of attributes is used to construct a na\u00efve bayes classifier . by doing this they relax the second of the two assumptions listed above . kohavi and john improve on this by using a wrapper approach to searching for a subset of features over which the performance of na\u00efve bayes is optimised . in figure cheng and greiner evaluate the performance of two other network structures . the first is bayesian network augmented na\u00efve bayes in which all other nodes are direct children of the classification node but a complete bayesian network is constructed between the child nodes as in figure rather than just a tree . the second is the general bayesian network in which a full fledged bayesian network as shown is used for classification . after constructing the bayesian network they delete all nodes outside the markov blanket prior to using the network for classification . they use an efficient network construction technique based on condition independence tests . they achieve good results with the ban and gbn algorithms compared with na\u00efve bayes and tan particularly when a wrapper is used to fine tune a threshold parameter setting . baesens et al. take a similar approach to bayesian network classification in a more recent paper they construct a general bayesian network using a markov chain monte carlo approach and then delete all nodes outside the classification node s markov blanket . induction of bayesian networks in the work described here the framework developed by cooper and herskovits for induction of bayesian networks from data is used . this section summarizes their approach to induction of bayesian network structures and probabilities and outlines how classification may be performed in this framework . determining network structure c h s framework is built on in determining which of two bayesian network structures is more likely . if d is a database of cases z is the set of variables represented by d and bsi and bsj are two belief network structures containing exactly those variables that are in z then they aim to calculate p p. however p p p p p p p p therefore the problem of calculating p reduces to that of calculating p. c h s equation for calculating p is based on four assumptions that they identify . variables are discrete and all are observed . cases occur independently given a belief network model . there are no cases that have variables with missing . determining network probabilities values . the density function f is uniform we are therefore indifferent regarding the prior probabilities to place on a network structure bs let z be a set of n discrete variables where a variable xi in z has ri possible value assignments . let d be a database of m cases where each case contains a value assignment for each variable in z. let bs denote a network structure containing just the variables in z. each variable xi in bs has a set of parents represented as a list of variables pi . let wij denote the jth unique instantiation of pi relative to d. suppose there are qi such unique instantiations of pi . let nijk be defined as the number of cases in d in which variable xi has the value vik and pi is instantiated as wij . let nij be defined as ir n ij n ijk then given the assumptions outlined above k bpdbp s r. if g from the node z that maximizes g i pi g no arc is added . c h present a simple formula for calculating conditional probabilities after the network structure has been found . let qijk denote the conditional probability that a variable xi in bs has the value vik for some k from to ri given that the parents of xi represented by pi are instantiated as wij . then qijk p termed a network conditional probability . let x denote the four assumptions of section . . then given the database d the bayesian network structure bs and the assumptions x the expected value of qijk is given by is e nijk nij ri . using a bayesian network for classification as pointed out by friedman and goldszmidt inductive learning of general bayesian networks is unsupervised in the sense that no distinction is made between the classification node and other nodes the objective is to generate a network that best describes the data . of course this does not preclude their use for classification tasks . a bayesian network may be used for classification as follows . firstly assume that the value of the classification node xc is unknown and the values of all other nodes are known . then for every possible instantiation of xc calculate the joint probability of that instantiation of all variables in the network given the database d. c h s formula for calculating the joint probability of a particular instantiation of all n variables is n i the by normalizing joint probabilities of all possible instantiations of xc an estimate of the relative probability of each is found . the vector of class probabilities may be multiplied by a misclassification cost matrix if available . set of n i resulting construction of mbbc as was mentioned in the introduction construction of a full bayesian network for the purposes of classification may be computationally inefficient as the whole structure may not be to classification . specifically classification is unaffected by parts of the classification node s markov blanket . as described by pearl the markov blanket of a node x is the union of x s direct parents x s direct children and all direct parents of x s direct children . the markov blanket of x the structure lie outside relevant that is one of its markov boundaries meaning that x is unaffected by nodes outside the markov blanket . our approach seeks to directly construct an approximate markov blanket around the classification node . the algorithm involves three steps as illustrated in figure . in the first step every node xi z is tested relative to xc to determine whether it should be considered to be a parent or a child of xc as follows . if xi is added as a parent of xc the overall probability of the network will change by a factor dp that is calculated as alternatively if xi is added as a child of xc the overall probability of the network will change by a factor dc given by dp g g dc g g accordingly by testing whether dp dc we can add xi to either the set of xc s parent nodes zp or its child nodes zc . however if max no arc is added xi is added to the set of nodes zn that are not directly connected to xc . at the end of this first step having performed this calculation for each node in turn a set of direct parents and direct children of xc have been identified as shown in figures and . as originally described this procedure was sensitive to the node ordering since pc changes as parent nodes are added . in the current implementation this sensitivity to node ordering has been removed by initially calculating max for each node assuming it has no parents and then sorting nodes in descending order of this quantity . this heuristic seeks to ensure that the nodes that most affect the structure are considered first . the second and third steps are concerned with completing the markov blanket by finding the direct parents of xc s direct children . in the second step parents are added to the nodes xi zc from a set of candidates comprising zp zn as shown in figure . this is done using the k algorithm as was described in section . . in general this requires less computational effort than using k to construct the entire network structure as the nodes in this case have into mutually exclusive sets of been partitioned children and candidate parents . furthermore the partitioning means that mbbc does not require k s node ordering . in the third step dependencies between the nodes in zc are found . since children of xc may be parents of other children of xc such dependencies fall within the markov blanket of xc . this step is performed by constructing a tree of arcs between the nodes in zc as illustrated in figure . this is similar to what is done in the tan algorithm except that it handles nodes having different sets of parents . naturally this step is an approximation as it can discover at most one additional parent for each node within the group . the procedure followed in the third step is as follows a candidate list of arcs is constructed from all permutations of two nodes in zc for each pair the quantity g is x x x x x x x x zp zp zc zc x x x x xc xc x x zn zn x x xc xc x x zn zn x x zp zp zc zc x x x x x x x x x x x x x x x x x x x start start step step step result step result x x x x x x x x x x x x x x x x xc xc x x x xc xc xc x x x x x xc xc xc xc xc x x x x x x x x x x x x x x x x x x x x x step step step step final result final result figure stages in construction of mbbc figure stages in construction of mbbc calculated and the pair is deleted from the list if this is less than g the list is sorted in order of decreasing g working sequentially through the list for each pair an arc is added from b to a unless a is already an ancestor of b and all other pairs beginning with a are removed from the list . having constructed the mbbc structure conditional probabilities are found using the approach of c h already described in section . and the network can then be used for classification as was described in section . . experimental results . methodology framework . accordingly this section describes an experimental evaluation of the mbbc algorithm . it is compared with compared with the na\u00efve bayes tan and gbn algorithms all of which were implemented using c h s inductive is learning implemented as step of the mbbc algorithm as described in section . likewise the gbn algorithm is actually c h s k. since k requires a node ordering the ordering of variables in the original datasets was used except that the classification node was placed first so that it could be included as a parent of any other node . for all algorithms estimated using conditional probabilities were equation . tan the algorithms have been tested on datasets from uci machine learning repository . since mbbc and k require discrete variables and can not accommodate missing values datasets were selected that had these characteristics and that were not very small . the datasets are listed in table . dataset chess wisconsin breast cancer diagnosis led dna splice junction gene sequences lymphography nursery spect heart diagnosis tictactoe endgame i a table datasets used number of instances and number of attributes in each . accuracy comparisons standard accuracy comparisons were carried out for the four algorithms on all of the datasets . each dataset was randomly divided into for training and for testing and the accuracy of each algorithm on the testing data was measured . misclassification costs were assumed equal so that the class predicted in all cases was simply the most probable one . for all except the two datasets with the fewest instances this procedure was the spect and times . for lymphography datasets the procedure was repeated times to reduce variability . repeated prediction accuracy results and standard deviations are reported in table . following usual conventions for each dataset the algorithm with best accuracy is highlighted in boldface . where two algorithms have statistically indistinguishable performance and they outperform they are both highlighted in bold . for example k and mbbc are both best on the dna splice dataset and all four are equally good on the breast cancer dataset . the other algorithms tan k na\u00efve mbbc chess . wbcd . led . dna . lymph . nursery . spect . ttt table average standard deviation of percentage accuracy for all algorithms and datasets looking at the table it can be seen that mbbc appears competitive with na\u00efve bayes tan and k for classification tasks . it is best in of the datasets . its performance is a little worse than tan on the lymphography dataset . roc analysis although accuracy estimation values such as those in table are very widely used in the machine learning community for comparison of classifiers provost et al. have argued that accuracy estimation is not the most appropriate metric when cost and class distributions are not specified precisely . as an alternative they propose the technique of receiver operating characteristic analysis which is taken from signal detection theory . in the machine learning context a roc graph is a plot of false true positives . a deterministic positives against classifier produces a single point in roc space but a probabilistic classifier such as those considered in this paper produces a curve as the threshold probability over which a positive classification is accepted is varied from to . as stated by provost and fawcett the benefit of roc curves is that they illustrate the behaviour of a classifier without regard to class distribution or error cost . figure shows roc curves for the experiments . there is one graph for each of the eight datasets and each graph has four curves one for each classifier . s e v i t i s o p e u r t s e v i t i s o p e u r t s e v i t i s o p e u r t s e v i t i s o p e u r t roc curves for chess data roc curves for breast cancer data false positives r o c c u r v e s f o r l e d d a t a false positives roc curves for dna splice data f a l s e p o s i t i v e s r o c c u r v e s f o r l y m p h o g r a p h y d a t a false positives r o c c u r v e s f o r n u r s e r y d a t a f a l s e p o s i t i v e s r o c c u r v e s f o r s p e c t d a t a f a l s e p o s i t i v e s roc curves for tictactoe data f a l s e p o s i t i v e s false positives figure roc curves for all datasets figure roc curves for all datasets s e v i t i s o p e u r t s e v i t i s o p e u r t s e v i t i s o p e u r t s e v i t i s o p e u r t na\u00efve tan k mbbc n a\u00ef v e t a n k m b b c n a\u00efve t a n k m b b c n a\u00efve t a n k m b b c na\u00efve tan k mbbc na\u00efve tan k mbbc n a\u00efve k m b b c t a n n a\u00efve t a n k m b b c each individual curve is an average of curves one for each of the analyses described above in section . . roc graphs are best suited to two class problems which all but one of the datasets are . for the nursery dataset the roc curve is for the prediction of the priority class . on a roc graph the point represents the strategy of never returning a positive classification no matter how probable whereas the point represents the strategy of always returning a positive classification . the ideal point is the top left corner at . in comparing the curves corresponding to two classifiers one is said to dominate the other if all points on it are to the upper left or equal to corresponding points on the other curve . is there looking at figure reasonable correspondence with the results of table . mbbc clearly dominates the other bayesian classifiers for the chess and tictactoe datasets . for the breast cancer and led datasets there is no real difference in performance between any of the classifiers based on their roc curves . for the nursery dataset mbbc and tan jointly dominate the others . similarly for the dna splice dataset mbbc and k both dominate the others as reflected in the results of table . for the spect and lymphography datasets results are more ambiguous and do not correlate well with those of table . in the lymphography case na\u00efve bayes appears to dominate although tan had the highest accuracy in table . the graph for the spect dataset is similarly interesting as it shows that k is worse than the other algorithms including na\u00efve bayes even though in table its accuracy estimation is on par with tan and mbbc and better than that of na\u00efve bayes . these two cases illustrate how roc graphs by allowing a broader comparison of classifiers than that available from a single value metric such as accuracy estimation may reveal different trends in performance . in general it is to be expected that mbbc should perform as well as the na\u00efve bayes and tan algorithms as it is strictly more expressive than these if the most appropriate representation is a na\u00efve bayes or tan structure mbbc should be able to find it . it is that on balance mbbc interesting outperforms k even though k as a general bayesian network algorithm should be as expressive than mbbc . the reason proposed for this are k requires a node ordering but mbbc does not so if the given node ordering is not the most appropriate k will discover a sub optimal structure to note rather than searching for an optimal general bn structure as k does mbbc seeks to optimise the markov blanket of nodes that affect classification and thus may be able to discover more subtle dependencies in the data that are specifically relevant to classification . speed comparisons it is intended that the mbbc algorithm be fast as well as accurate . to evaluate this its speed in constructing a network has been compared with that of the tan and k algorithms . naturally na\u00efve bayes could not be included in these comparisons as its structure is fixed . as a measure of the relative speed of the algorithms the number of calls to g function was counted for each algorithm for each dataset . figure shows the results with the number of attributes and tan k mbbc s l l a c n o i t c n u f g. nursery wbcd tictactoe lymph . spect led chess dna dataset figure count of calls to g function for tan k and mbbc instances in each dataset listed in brackets . in cases where the number of variables is small all algorithms are relatively fast and there is not much difference between them . in cases with larger numbers of variables it can be seen that mbbc s performance scales well outperforming k and sometimes even outperforming tan . conclusions in the mbbc . this paper has presented a description of a bayesian network structure called a markov blanket bayesian classifier for classification tasks and a method for this method an constructing approximate markov blanket is constructed around the classification node . the method has been implemented using the bayesian framework for network induction of cooper and herskovits although it could be based equally well on any metric for bayesian networks that has the property of locality whereby networks are scored in terms of their local structure . a noteworthy feature of bayesian classifiers in general is their ability to accommodate noisy data conflicting training examples decrease the likelihood of a hypothesis rather than eliminating it completely . key features of the mbbc algorithm are in the first step of constructing the mbbc all nodes are classified as either parents of the classification node children of it or unconnected to it . this contrasts with na\u00efve bayes tan and ban structures where all nodes are children of the classification node . it also contrasts with snb where a wrapper based approach is taken to find which nodes are connected to the classification node . in the second and third steps of constructing the mbbc the only arcs added are to children of the classification node so that an approximate markov blanket is constructed . this contrasts with gbn structures in which arcs may be added outside of the markov blanket but are not considered when using the gbn for classification . classification around node the unlike k the mbbc algorithm does not require an ordering on the nodes . this paper has also reported on the results of a comprehensive set of experimental comparisons of mbbc with other bayesian classifiers . these experiments indicate that mbbc is competitive in terms of speed and accuracy with these other classifiers . as was discussed in section . mbbc is strictly more expressive than the na\u00efve bayes and tan algorithms so it is to be expected that it should perform at least as well as well as them . that section also proposed some explanations as to why mbbc can out perform k. in the future it is hoped to research whether the mbbc approach could be improved by using a different scoring metric such as mdl or conditional independence testing . it is also planned to extend the algorithm to support missing variables and to support dynamic discretization of continuous variables while constructing the network the structure of the mbbc should facilitate this as all variables are associated with the classification node . this could build on previous research by wu or friedman and goldszmidt . all of the algorithms described in this paper have been implemented in common lisp and are available for download . for details please refer to the author s web page http www.it.nuigalway.ie m_madden . acknowledgement the research described this paper has been in supported by nui galway s millennium research programme . references . baesens b. egmont petersen m. castelo r. vanthienen j learning bayesian network classifiers for credit scoring using markov chain monte carlo search proc . international congress on pattern recognition . blake c.l. merz c.j. uci repository of machine learning databases http www.ics.uci.edu mlearn mlrepository.html . of california at irvine department of information and computer science . university . cheng j. bell d.a. liu w. learning belief networks from data an information theory based approach . proc . acm cikm . cheng j. greiner r. comparing bayesian network classifiers . proc . uai . cheng j. greiner r. learning bayesian belief network classifiers algorithms and system . proc . th canadian conference on artificial intelligence . chow c.k. liu c.n. approximating discrete probability distributions with dependence trees . ieee transactions on information theory vol . cooper g.f. herskovits e. a bayesian method for the induction of probabilistic networks from data . machine learning vol . kluwer academic publishers boston . friedman n. goldszmidt m. building classifiers using bayesian networks . proc . aaai vol . friedman n. goldszmidt m. discretizing continuous attributes while learning bayesian networks . proc . icml . friedman n. geiger d. goldszmidt m. bayesian network classifiers . machine learning vol . kluwer academic publishers boston . heckerman d geiger d. chickering d.m. learning bayesian networks the combination of knowledge and statistical data . technical report msr tr microsoft corporation redmond . heckerman d a tutorial on learning with bayesian networks . technical report msr tr microsoft corporation redmond . kohavi r. john . g. wrappers for feature subset selection . artificial intelligence journal vol . no . langley p. iba w. thompson k. an analysis of bayesian classifiers . proc . aaai . pearl j. probabilistic reasoning . langley p. sage . s. induction of selective bayesian classifiers . proc . uai . madden m.g. a new bayesian network structure for classification tasks . proc . aics . in intelligent systems networks of plausible inference . morgan kaufmann san francisco . provost f. fawcett t. analysis and performance visualization comparison under imprecise class and cost distributions . proc . kdd . classifier of . provost f. fawcett t. and kohavi r. the case against accuracy estimation for comparing induction algorithms . proc . imlc . quinlan j.r. c. programs for machine learning . morgan kaufmann san francisco . ramoni m. sebastiani p. learning bayesian networks from incomplete databases . kmi technical report kmi tr . russell s. norvig p. artificial intelligence a modern approach . prentice hall new jersey . wu x. a bayesian discretizer for realvalued attributres . the computer journal vol . no . ."
    ],
    "abstract": [
        "the markov blanket bayesian classifier is a recently proposed algorithm for construction of probabilistic classifiers . this paper presents an empirical comparison of the mbbc algorithm with three other bayesian classifiers naive bayes tree augmented naive bayes and a general bayesian network . all of these are implemented using the k framework of cooper and herskovits . the classifiers are compared in terms of their performance and speed on a range of standard benchmark data sets . it is concluded that mbbc is competitive in terms of speed and accuracy with the other algorithms considered ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.7395833333333334
    ]
}