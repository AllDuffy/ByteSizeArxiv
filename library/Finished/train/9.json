{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0507062v1",
    "article": [
        "keywords expert advice online algorithms partial observations adaptive adversary bandit problems fpl introduction expert advice stands for an active research area which studies online algorithms . in each time step t. the master algorithm henceforth called master for brevity is required to commit to a decision which results in some cost . the master has access to a class of experts each of which suggests a decision at each time step . the goal is to design master algorithms such that the cumulative regret with respect to any expert is guaranteed to be small . this work was supported by jsps st century coe program c. j. poland bounds on the regret are typically proven in the worst case i.e. without any statistical assumption on the process assigning the experts costs . in particular this might be an adaptive adversary which aims at maximizing the master s regret and also knows the master s internal algorithm . this implies that the master must randomize in order to protect against this danger . in the recent past a growing number of di\ufb00erent but related online problems have been considered . prediction of a binary sequence with expert advice has been popular since the work of littlestone and warmuth in the early s. freund and schapire removed the structural assumption on the decision space and gave a very general algorithm called hedge which in each time step randomly picks one expert and follows its recommendation . we will refer to this setup as the online decision problem . auer et al. considered the first partial observation case namely the bandit setup where in each time step the master algorithm only learns its own cost i.e. the cost of the selected expert . all these and many other papers are based on weighted forecasting algorithms . a di\ufb00erent approach follow the perturbed leader was pioneered as early as by hannan and rediscovered recently by kalai and vempala . compared to weighted forecasters fpl has two main advantages and one major drawback . first it applies to the online decision problem and admits a much more elegant analysis for adaptive learning rate . even infinite expert classes do not cause much complication . adaptive learning rate is necessary unless the total number of time steps to be played is known in advance . as a second advantage fpl also admits efficient treatment of cases where the expert class is potentially huge but has a linear structure . we will refer to such problems as geometric online optimization . an example is the online shortest path problem on a graph where the set of admissible paths experts is exponential in the number of vertices but the cost of each path is just the sum of the costs of the vertices . fpl s main drawback is that its general analysis only applies against an oblivious adversary that is an adversary that has to decide on all cost vectors before the game starts as opposed to an adaptive one that before each time step t just needs to commit to the current cost vector . for the full information game one can show that a regret bound against oblivious adversary implies the same bound against an adaptive one . the intuition is that fpl s current decision at time t does not depend on its past decisions . therefore the adversary may well decide on the current cost vector before knowing fpl s previous decisions . this argument does not apply in partial observation cases as there fpl s behavior does depend on its past decisions . as a consequence authors started to explicitly distinguish between oblivious and adaptive adversary sometimes restricting to the fpl analysis for adaptive bandits former sometimes obtaining bounds of lower quality for the latter . e.g. mcmahan and blum suggest a workaround proving sublinear regret bounds against an adaptive bandit however of worse order . this is not satisfactory since in case of the bandit online decision problem for a suitable weighted forecaster even a o bound against adaptive adversary is known . log t instead of t in this work we remove fpl s major drawback . we give a simple argument which shows that also in case of partial observation a bound for fpl against an oblivious adversary implies the same bound for adaptive adversary . this will allow in particular to prove a o bound for the bandit online decision problem . this bound is shown for the common construction where only the observations of designated exploration rounds are used . as this master algorithm is label efficient the bound is essentially sharp . in contrast using all informations will enable us to prove a stronger o bound . this matches the best bound known so far for the adversarial bandit problem which is sharp within log n. the downside of this algorithm is that either the sampling probabilities have to be given by an oracle or they have to be approximated with to sufficient accuracy using o samples . the case of an infinite expert class is brie\ufb02y discussed in section . fpl oblivious adaptive n is a sequence of cost vectors . there are n assume that c c. experts . that is ci t is expert i s cost at time t and the costs are bounded . in the full observation game at time t the master would know the past cumulative costs c t t c t s cs . however our focus are partial observations where this is not the case . hence assume that there are estimates \u02c6ct for the cost vectors ct. then at time t fpl n the components of which are independently samples a perturbation vector qt exponentially distributed that is p e p qt \u03b7t fpl arg min n i \u02c6ci t qi t \u03b7t where qi t d. exp independently . n o denote the expert fpl chooses at time t by it fpl . then an adaptive adversary is a function a n the complete game between fpl and a is specified by ct a and it fpl for t. the estimated cost vector \u02c6ct is revealed to fpl after time t and specified by a mechanism outside this game which is defined later . n ii . it n. t t j. poland after the game has proceeded for a number of time steps t we want to evaluate fpl s performance . actually the expected performance is the right quantity to if we are rather interested in high probability bounds on the actual peraddress . formance then they are easily obtained by observing that the di\ufb00erence of actual to expected performance is a martingale with bounded di\ufb00erences . thus high probability bounds follow by azuma s inequality as we will demonstrate in proposition . how can we compute fpl s expected costs ecfpl t the key observation is that on the cost vectors generated by fpl and a and with the given estimated costs \u02c6ct fpl s expected costs at time t are the same as another algorithm fpl s expected costs . fpl is defined by t e t cit p t \u02c6ci t qi \u03b7t i g g fpl arg min n where q is a single fixed vector with independently exponentially distributed com ponents . since we have to be careful to take expectations w.r.t. the appropriate randomness we explicitely refer to the randomness in the notation by writing e.g. t eqtcfpl ecfpl have the same distribution . then the following statement trivially holds as qt and q g o n t proposition at each time t t we have eqtcfpl t eq c fpl t. g g this means that in order to analyze fpl we may now proceed by considering the expected costs of fpl instead . we can use the standard analysis based on the tools fpl is executed on a sequence of by kalai and vempala which requires that cost vectors that is fixed and not known in advance . actually in contrast to the full observation game analysis the bandit analysis will never require the true cost vectors to be revealed but rather the estimated cost vectors . for the cost vectors generated by a in response to fpl the prerequisite for fpl as a virtual or hypothetic algorithm which is not actually executed . therefore it does not make any decisions or cause any response from the adversary . just for the sake of analysis we pretend that it runs and evaluate the expected cost it incurs which is the same as fpl . fpl is satisfied just consider g g g since our key argument and the way it is used in the analysis appears quite subtle at the first glance we encourage the reader to thoroughly verify each of the subsequent formal steps . the standard strategy against adversarial ban dits the first algorithm we consider bandit fpl is specified in figure and proceeds as follows . at time t it decides if to perform an exploration or an exploitation fpl analysis for adaptive bandits for t. t for all i set \u02c6ci sample rt independently s.t. p \u03b3t t fpl according to . n t from uniformly play decision i b if rt then set \u02c6ci b t and observe cost ci b t t t t n ci b t \u03b3t t figure the algorithm bfpl . the exploration rate \u03b3t and the learning rate \u03b7t will be specified in theorem . t is uniformly sampled from step according to some exploration probability \u03b3t . this is realized by sampling rt costs c c. bfpl satisfies the regret bound n log n t and s at all other places . then the process of selecting a minimum can be written as q. scalar product mini \u03b7t \u03b7t p p \u03b7t \u03b7t log n and \u03b7t t n concludes the proof of the theorem . finally shows and proposition for each t costs of bfpl are bounded with probability at least and \u03b4 the actual j. poland \u03b4 by t log \u03b4 . cbfpl t ecbfpl t q proof . again we use the explicit notation from the proof of the previous theorem . it is easy to see that the sequence of random variables xt ert ut qt t t t \u03bb t using all observations the algorithm bfpl considered so far does only uses a \u03b3 fraction of all the input . it is thus a label efficient decision maker . one possible way to specify a label efficient problem setup is to require that the master usually does not observe anything and it incurs maximal cost if it decides to observe something . since just before we upper bounded the costs in case of exploration by it is immediate that the same analysis and hence also theorem transfer to the label efficient case . prove that there is a label efficient prediction problem such that any forecaster incurs a regret proportional to t. hence the bound in theorem is essentially sharp for bfpl . of course the usual bandit setup does not require the master to make use of only a tiny fraction of all information available . for weighted forecasters it is very easy to produce an unbiased cost estimate if each round s inputs are used . it turns out that then regret bound proportional to t can be obtained . unfortunately this is di\ufb00erent for fpl as here the sampling probabilities are not explicitely available . in the following we will first discuss the computationally infeasible case assuming that we know the sampling probabilities . after that we show how to approximate them by a monte carlo simulation to sufficient accuracy . surprisingly it is possible to work with the plain fpl algorithm from without exploration . we just have to use the correct estimated cost vectors t i ci t p and \u03c0i p. moreover for x r let x max . d\u00b5 q ifpl t x g g r pi d\u00b5d\u00b5 e qj \u03b7t pi e e max j i. j. poland for t. set \u02c6ci t for i sample rt for i i t qi t t log wi \u02c6ci according to the weights \u03b7t wi i t \u03c4 i wi p figure the algorithm bfpl for infinite expert class . the entering times \u03c4 i the exploration rate \u03b3t and the learning rate \u03b7t will be specified in theorem . p infinite expert classes here we sketch a variant of bfpl taken from with guaranteed worst case performance against a bandit with countably infinitely many arms . so we consider the following setup the adversary subsequently generates cost vectors ct and at each time t we have to select one index or expert i and incur its cost ci t. we learn only the cost of the selected expert . p i as a prerequisite we need that each of the infinitely many experts is associated with a prior weight wi such that . since in order to obtain a cost estimate \u02c6c the observed cost is divided by the weight of the sampled expert we have to be careful not to admit too small weights . we need to keep control of the maximum possible expected cost since otherwise the step from fpl to ifpl would be problematic . one possibility to do so is defining an entering time \u03c4 i for each expert . prior to \u03c4 i the expert is not active and can not be chosen . we choose \u03c4 i with \u03b1 to be defined later . then it is not hard to see that the minimum weight \u03b1 . letting wi t of any active expert at time t is lower bounded min \u03b2 with \u03b2 to be defined later the maximum the exploration rate be \u03b3t t unbiasedly estimated cost is at most t\u03b1 \u03b2 . for the step from fpl to ifpl to go \u03b2 . then both steps from bfpl to fpl through we thus may choose \u03b7t t \u03b2 . on the \u03b2 and from fpl to ifpl each cause a regret of at most other hand causes a regret of at most t \u03b1 \u03b2 . in order to minimize these bounds \u03b7t \u03b1 simultaneously we choose \u03b2 l t t t \u03b2 t wi p t \u03c4 i \u03b1 m \u03b1 . in order to guarantee that the step from ifpl to some fixed expert holds we have to correctly assign estimated costs to inactive experts . for example if an expert enters the game and previously has been assigned no estimated cost at all then a bound w.r.t. this expert may be difficult to obtain . we therefore assign maximum possible estimated costs to all inactive experts . then one can show that evaluating the expected costs the step ifpl to some fixed reference expert holds almost without modification . clearly the reference expert s estimated costs now exceed its true costs \u03b1 \u03b2 \u03c4 i t t\u03b1 \u03b2 which is easily shown to be upper bounded by \u03b1 . by at most wi p fpl analysis for adaptive bandits this gives another additive bound to the regret in terms of the weight of the reference expert there is not multiplicative factor of wi any more . this is an artifact of the design of the algorithm and proof technique and does not mean that the new variant performs better than the old one . actually since \u03b1 the bound is now o as opposed to o order in t gets large while a small \u03b1 has the opposite e\ufb00ect . before . choosing a large \u03b1 results in a small t term but the \u03b1 wi t the complete algorithm is specified in figure . the following statement which improves on the bounds given in is an example where we select \u03b1 . theorem consider a bandit problem with countably many arms experts each expert i having a prior weight wi such that the weights sum up to at most . then the above described bfpl variant with entering times \u03c4 i exploration rate \u03b3t t and learning rate \u03b7t t l satisfies the regret bound m ecbfpl t ci t o wi t log wi wi for all t any adaptive assignment of the cost vectors and any reference expert i. the formal proof is omitted . it follows the outline of theorem using the arguments discussed above . many of the arguments including the step from ifpl to the reference expert are formally carried out in . discussion the main statement of this paper is the following if we have a regret minimization algorithm with a bound guaranteed against an oblivious adversary and if the algorithm chooses the current action expert by some independent random sampling based on past cumulative scores then the same bound also holds against an adaptive adversary . this is true both for full and partial observations . we have used this argument for showing bounds for fpl in the adversarial bandit problem . the strategy to use only feedback from exploration rounds which is common for fpl achieves a regret bound of o. as the algorithm is label efficient this bound is sharp . using all observations allows to push the regret down to o. then however the sampling probabilities have to be approximated . in the same way it is possible to use our argument for the general geometric online optimization problem also resulting in a o regret bound against adaptive adversary . an interesting open problem is the following under j. poland which conditions and how is it possible to use all observations in the geometric online optimization problem hopefully arriving at a o bound we conclude with a note on regret against an adaptive adversary . we considered the external regret w.r.t. the best action strategy expert from a pool . there are two directions from here . one is to go to di\ufb00erent regret definitions such as internal regret . the other one is to change the reference and compare to the hypothetical performance of the best strategy in this way accepting a stronger type of dependency of the future costs from the currently selected action . it is one of the major open problems to propose refined algorithms and prove better bounds in this model ."
    ],
    "abstract": [
        "a main problem of follow the perturbed leader strategies for online decision problems is that regret bounds are typically proven against oblivious adversary . in partial observation cases it was not clear how to obtain performance guarantees against adaptive adversary without worsening the bounds . we propose a conceptually simple argument to resolve this problem . using this a regret bound of o t ^ for fpl in the adversarial multi armed bandit problem is shown . this bound holds for the common fpl variant using only the observations from designated exploration rounds . using all observations allows for the stronger bound of o t ^ matching the best bound known so far for adversarial bandits . surprisingly this variant does not even need explicit exploration it is self stabilizing . however the sampling probabilities have to be either externally provided or approximated to sufficient accuracy using o t ^ log t samples in each step ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.5668789808917197
    ]
}