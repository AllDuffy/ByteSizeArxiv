{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0512059v2",
    "article": [
        ". in this paper we develop banach space methods to construct a prediction algorithm with a regret term of o where p. we consider the the object s label he is shown the actual label yn u v u. let us say that a banach space of real valued functions f on x is a proper banach functional space on x if for each x x the evaluation functional kx f f is continuous . we will assume that f f cf sup x x k kxkf where is the dual banach space . f the following theorem will be proved in . theorem let be a proper banach functional space such that f \u01eb n n x. there exists a prediction algorithm producing \u00b5n y y for some p that are guaranteed to satisfy . any n. and any \u03b4 s for all x and that hn x for all x we have \u03b4 exp \u01ebn y riskp riskp n n n n n n x n n x n n x n n x yn \u01eb d y cs p n p \u01eb d ks p y k riskp y cs p d k ks p y n p \u01eb riskp y cs p d ks p y k n p \u01eb . t. the first inequality follows from the convexity of the function t inequalities and follow from hoe\ufb00ding s martingale inequality . either of and holds with probability at least \u03b4 . finally \u03b4 therefore both will hold with probability at least inequality follows from . our goal follows from the inequality between the extreme terms of if we substitute \u01eb y ln n \u03b4 r. for a fixed \u03b4 the regret term of grows as n p. for a discussion of related results in statistical learning theory see . filtering of random processes r sequentially suppose we are interested in the value of a signal \u03b8 observed at moments tn n n n. n where n is a large positive integer let \u03b8n \u03b8 . the problem is that our observations of \u03b8n are imperfect and in fact we see yn \u03b8n \u03ben where each noise random variable \u03ben has zero expectation given the past . we assume that \u03b8 belongs to w s p and that \u03b8n yn y. let us use the \u00b5n from theorem as estimates of the true values \u03b8n . the elementary equality and \u03ben are ran where \u03b8n \u03b8 s chosen at will after observing \u03ben and taking into account all other information that becomes available before and including time n n. a clean formalization of this intuitive picture seems to require the game theoretic probability of . in the case where \u03b8 is generated from a di\ufb00usion process it will almost surely belong to w and so the regret term in and can be made o for an arbitrarily small \u01eb . the kalman filter which is stochastically optimal gives a somewhat better regret o. corollary however does not depend on the very specific assumptions of the kalman filter we do not require the linearity gaussianity or even stochasticity of the model the assumption about instead we have the noise \u03ben is minimal . the assumption that all \u03b8n and yn are chosen from . until the end of this section we will discuss in more detail the standard vi . the signal is now mod governed by the stochastic di\ufb00erential stochastic approach to the problem of filtering see also and for a continuous time version eled as a random process \u03b8t t equation d\u03b8t dt bdbt where bt is the standard brownian motion and as before is observed at points tn n n \u03b8n \u03b8 . the observed sequence is yn \u03b8n \u03c3\u03ben where \u03c3 is a positive constant and \u03ben are standard gaussian random variables independent between themselves and of the initial position \u03b8 and the brownian motion bt . in some important respects this is a simplification of the usual filtering problems e.g. we consider scalar rather than vector \u03b8t and yn . earlier we discussed the possibility of positive contributions of competitive on line results such as theorem to the problem of filtering and now we will brie\ufb02y explore the connection in the opposite direction limitations on competitive on line prediction following from the known optimality properties of the kalman filter . according to there is a prediction algorithm ocompetitive with w s for any \u01eb . it remains an open problem to show that the rate n s \u01eb can not be improved but the following considerations make it likely in the case s. a suppose the prediction rule d r is generated randomly as the and trajectory of the stochastic process with \u03b8 a b c. the positive constant c is chosen small as compared to y so that d is unlikely to take values approaching y or y. it is clear that the observations yn are generated independently from the normal distribution n with mean d and variance \u03c3 if yn falls outside it is truncated to y sign yn . the variance \u03c3 is assumed to be small enough for the probability y to be close to for each n. according to the standard properties of the kalman filter the variance \u03b3n of the best estimate of \u03b8n n given y. yn satisfies the recurrent equation yn yn \u03b3n \u03b3n c n \u03b3 n \u03c3 \u03b3n . c c c\u03c3n n c\u03c3 n it is clear that \u03b3n is an increasing sequence tending as n to to a limit equal and that it will move significantly towards this limit already during the first n rounds . by hoe\ufb00ding s inequality the excess of the total loss of the stochastically best algorithm over the total loss of d will be of order n and so the excess of its average loss will be of order n. since the sample paths of di\ufb00usion processes almost surely belong to w s for all s we can see that no prediction algorithm can be o competitive with w \u01eb . therefore if we disregard the . figure the growth of the kalman filter s error \u03b3n n. n for c \u03c3 and n the final value \u03b3n is approximately n. epsilons our algorithm achieves the optimal rate of decay in n of the regret term for s. a similar argument might have also worked in the case s had we known an analogue of the kalman filter result for the fractional brownian motion where b is replaced with a stochastic process b h defined in the same way except that the variance of each increment b h is . unfortunately we know of no such result although a step in this direction is made in . t b t t t more geometry of banach spaces in the proof of theorem we will need not only clarkson s modulus of convexity but a whole range of di\ufb00erent moduli of convexity and smoothness . in our description we will often follow for information about other moduli and further references see . we will only consider banach spaces of dimension at least . moduli of convexity and smoothness a natural modification of clarkson s modulus of convexity was proposed by gurary \u03b4 u inf u v su ku vku \u01eb inf t k tu v. ku it is clear that \u03b4u \u03b4 u \u03b4u and it was shown recently that this relation can not be improved . the standard modulus of smoothness was proposed by lindenstrauss \u03c1u sup u v su u \u03c4 v k ku u k \u03c4 v ku \u03c4 . lindenstrauss also established a simple but very useful relation of conjugacy between \u03b4 and \u03c1 \u03c1u sup \u01eb \u01eb\u03c4 \u03b4u we can see that \u03c1u is the fenchel transform of \u03b4u . the following inequality will be the basis of the proof of theorem in the satisfies the condition of theorem . by next section . suppose a pbfs f we obtain for the dual space \u03c1f sup \u01eb assuming \u03c4 to f f \u01eb\u03c4 p p \u03c4 q q where q p. the banach space u is called uniformly convex if \u03b4u for all \u01eb and it is called uniformly smooth if \u03c1u . all uniformly convex and all uniformly smooth banach spaces u are re\ufb02exive . as \u03c4 if v is a hilbert space the parallelogram identity immediately gives and u v k k v u k v v k u k v k v k v k \u03b4v \u03c1v p \u03c4 \u01eb \u03c4 . n ordlander proved that the unit balls in hilbert spaces are most convex and smooth if u is a banach space and v is a hilbert space p \u03b4u \u03b4v \u03c1u \u03c1v \u03c4 p. p the original definitions and of the moduli of convexity and smoothness look very di\ufb00erent and bana s proposed a definition of modulus of smoothness similar to \u03c1 u sup u v su ku vku \u03c4 u v \u03c4 . u c a d b f e o figure relation between \u03c1 and \u03c1 . the di\ufb00erence \u03c1 \u03b4u measures the degree to which u is deformed . what we will need in this paper is the modification of in the direction of u \u03c1 u sup u v su ku vku \u03c4 sup t \u03c4 . since the standard results about moduli of convexity and smoothness are about the definitions and we first need to establish connections between and . the first of these results appears in . lemma for all \u03c4 \u03c1 u \u03c1 u \u03c1u \u03c4 \u03c1 u. proof let c \u03c1 u be such that for some u v su satisfying u k v ku \u03c4 u v c u it is clear that c can be chosen as close to \u03c1 u v u k. since u v u v \u03c4 v ku \u03c4 c c u as we wish . set u v su we have ku u k u \u03c4 v k \u03c1u \u03c4 v ku c which can be rewritten as \u03c4 \u03c1u c. c letting c by e.g. proposition . e. on p. \u03c1 u completes the proof the modulus of smoothness is continuous corollary for all \u03c4 \u03c1 u \u03c1u . proof let \u03c4 . following proof of lemma we obtain \u03c1 u sup u v su ku vku \u03c4 u k ku k u v ku sup u v su ku vku \u03c4 u v ku k u k u v ku k ku v \u03c4 . we can now easily deduce from and the fact that \u03c1u is a non decreasing function \u03c1 u \u03c1 u \u03c1 u \u03c1u \u03c4 \u03c1 u \u03c1u . lemma for all \u03c4 proof suppose \u03c1 \u03c4 and u c. let u v su and t be such that u k v ku \u03c1 u \u03c1 u. tu v k ku . since c. without loss of generality we assume t u v u u t t ku t t k u t tu v t k we have \u03c1 u c. u ku t t t c t t c direct sums of uniformly smooth spaces if u and u are two banach spaces their weighted direct sum u defined to be the cartesian product u and multiplication by scalar defined by u is u with the operations of addition c we will equip it with the norm u u k q uk uk ku u a k a k where a and a are positive constants . the operation of weighted tion them explicitly in our notation for u direct sum provides a means of merging di\ufb00erent banach spaces which plays an important role in our proof technique . the euclidean definition of the norm in the direct sum suggests that the sum will be as smooth as the components this intuition is formalized in the following lemma . lemma if u and u are banach spaces and f r \u03c4 \u03c1u f \u03c1u proof we will follow the proof of proposition in which is based on the following weak form of the parallelogram identity valid for all banach spaces . f \u03c4 u v k k u u k v u k u k u k u k u v k k u v ku it is clear that implies v u u v u k v u k k k let u and v be arbitrary norm one vectors in u applying to and we obtain u k u \u03c1u . u ku u k u k u k v k k k k u. u \u03c4 vk k u u k \u03c4 vk u uk k u uk k \u03c4 u vk k \u03c1u u \u03c4 vku k uku k and u \u03c4 vk k u u k \u03c4 vk u uk u k uk k \u03c4 u vk u k \u03c1u \u03c4 vku k uku k. multiplying by a and by a and summing now gives u \u03c4 v u u u \u03c4 v u u \u03c4 j x aj k ujk uj \u03c1uj \u03c4 vjkuj k ujkuj k. to estimate the sum over j notice that when vjkuj k ujkuj k \u03c1uj \u03c4 vjkuj k ujkuj k \u03c1uj vjkuj k ujkuj k when vjkuj k ujkuj k \u03c1uj \u03c4 vjkuj k ujkuj k l\u03c1uj vjkuj k ujkuj k l\u03c1 \u03c4 for all pos\u03c3 see proposition on p. and the remark after its . using the cauchy schwarz inequality the sum can be bounded above as follows j x aj k ujk uj \u03c1uj \u03c4 vjkuj k ujkuj k j x aj k vj kuj \u03c1uj max ujkuj k l vjkuj k j x aj k vjk uj f aj j x k aj \u03c1uj j x uj ujk k l uj vj k k uj ujk l uj vjk k p l f the last line assuming \u03c4 . now we have all we need to deduce the conclusion of the lemma when \u03c4 u \u03c4 v u u u \u03c4 v \u03c4 u \u03c4 v u u u u u \u03c4 v u u l f \u03c4 l f f l f p l f p t the second . it remains to compare the resulting inequality with the definition of the modulus of convexity and remember that l . . p p convexity and smoothness for sobolev spaces it was shown by clarkson that for p \u03b4lp p. a quick inspection of the standard proofs shows that the underlying measurable space \u03c9 and measure \u00b5 of lp lp can be essentially arbitrary although this generality is usually not emphasized . it is easy to see that the modulus of convexity of each sobolev space w s p s and p also satisfies \u03b4w s p p. indeed with each f such that w s p we can associate a function f x x r f f f f x f s y for x x for x x coincides with the lebesgue measure on the measurable the measure on x m subsets of x and with the measure whose density is with respect to the lebesgue measure on the measurable subsets of x. the x x y bound can now be deduced from clarkson s result as follows \u03b4w s p inf f g sw s p kf gkw s p \u01eb inf f g x r f g lp f g k klp \u01eb w s p f g f g lp inf u v lp ku vklp \u01eb u v lp \u03b4lp p. since for t t p we have and p p \u03b4w s p p p. the sobolev spaces indeed satisfy the condition therefore as we said in of theorem . proof of theorem in this section we partly follow the proof of theorem in . the bbk algorithm let u be a banach space . we say that a function \u03c6 for every fixed x fn \u00b5i \u03c6 \u00b5i xi \u03c6 \u00b5 xn yi n i x u \u03c6 \u00b5i xi u banach space balanced k algorithm parameter forecast continuous \u03c6 x u with u a banach space is continuous in \u00b5 . for n. x. read xn y y define fn of fn fn as \u00b5n y y read yn . end for . the validity of this description depends on the existence of \u00b5 isfying supy fn not have roots \u00b5 fn fn does if when the equation fn fn for all \u00b5 y \u00b5 fn fn and hence supy fn fn for all \u00b5 setting \u00b5 y leads to by the convexity of in y if fn fn fn and hence supy fn will sometimes be called the feature mapping . the parameter \u03c6 of the bbk algorithm theorem let \u03c6 be a forecast continuous mapping from for some constants q y y such that with parameter \u03c6 outputs \u00b5n \u03c6 k \u03c4 y y ku . suppose \u03c1u q. the bbk algorithm x to a y c\u03c6 q n \u03c6 always holds for all n. n x u proof set our goal is to prove for n this follows from sn \u03c6 u y c\u03c6 q. n n x sn y c\u03c6 y c\u03c6 q which in turn follows from aq a q. it remains to prove that which in turn follows from the condition sn y c\u03c6 q implies sn . without loss of generality we assume that fn fn fix n. we will assume that sn y c\u03c6 q fn y c\u03c6 q and arrive at a contradiction . by the definition of \u03c1 sn fn \u03c1 u y \u03c6 k k fn . since fn y c\u03c6 by corollary and lemma this implies sn fn a y \u03c6 k k fn q. as the right hand side is a monotonically increasing function of fn in combination with the last inequality gives y c\u03c6 q y c\u03c6 q a q q i.e. q n q qn . it remains to rewrite the last inequality as n q q n q q and notice that by the mean value theorem the left hand side of equals q q for some \u03b8 as q we have the required contradiction . the feature mapping for the proof of theorem in the proof of theorem we will need two feature mappings from f f \u03c6 \u03c6 \u03c6 to the weighted direct sum u r later . by lemma and \u03c1u help of theorem we obtain for the bbk algorithm with parameter \u03c6 with the weights a and a to be chosen a\u03c4 q where a. q. with the f \u00b5n \u03c6 n n x and n n x n n x a n r a u \u03c6 y c\u03c6 q n n x d \u00b5n kxn \u03c6 d kxn \u03c6 y c\u03c6 q d k kf n x n a for each function d yn n x proof proper . f u the proof is based on the inequality n n x n n x n n x. using this inequality and with a y and a we obtain for the \u00b5n y y output by the bbk algorithm with \u03c6 as parameter define fn of fn fn as \u00b5n y y i am grateful to glenn shafer for a series of useful discussions . this work was partially supported by mrc and the royal society ."
    ],
    "abstract": [
        "we consider the problem of on line prediction competitive with a benchmark class of continuous but highly irregular prediction rules . it is known that if the benchmark class is a reproducing kernel hilbert space there exists a prediction algorithm whose average loss over the first n examples does not exceed the average loss of any prediction rule in the class plus a regret term of o n ^ . the elements of some natural benchmark classes however are so irregular that these classes are not hilbert spaces . in this paper we develop banach space methods to construct a prediction algorithm with a regret term of o n ^ where p is in infty and p reflects the degree to which the benchmark class fails to be a hilbert space ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.5
    ]
}