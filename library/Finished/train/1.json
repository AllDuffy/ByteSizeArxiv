{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0502016v1",
    "article": [
        "p keywords statistical learning theory learning in the limit regularized least squares regression rkhs introduction y where x y in regression learning problems we are given data i . . n in x is a bounded subset of r. we assume this data is chois a bounded subset of rn and sen iid according to an unknown probability distribution \u00b5 . we say that x is a position and y is a label . these data points may be for example images of people s faces in pixel space with a person s age as the corresponding label or auto regressive time series data . the output of a r. even though we only know n learning algorithm is a decision function f data points from distribution \u00b5 we hope to construct f which will be able to generalize to unobserved points in the distribution . this means we would like f to predict the value of y for any given value of x. since we want our function f to fit the x data accurately and also have this generalization ability we refer to vapnik s structural risk minimization principle . in srm we limit our choice of func of finite capacity . tions f so they are chosen from a class x f otherwise we can not hope to choose a function f which has generalization ability be a ball we would overfit the data . one convenient way to implement srm is to let f within a reproducing kernel hilbert space k kh . in this form we have an ivanov regularization problem one can show that the solution is always the minimizer of a corresponding tikhonov regularization problem . algorithms for classification and regression solve this tikhonov regularization problem so that the decision function is given by fmin where with norm h fmin argmin l where l f v \u03bb k f k h. n n xi f k h iff h h kh is finite . thus minimizing over f l is called the regularized risk functional . note that we define our rkhs so that is equivalent to minimizing over f is a preall functions f. the first term in l is called the empirical risk and v for each x. we denote the expectation value of the label y for position x x and we denote the marginal distribution along the x axis as \u00b5 . this is bution \u00b5 y as e for the case when n regularization term vanishes provided fe and \u03bb simultaneous convergence n fe is defined by we show convergence of fmin to a function fe as the i.e. we need to find conditions on the fe . here the function so that fmin h fe argmin actual risk where actual risk f e x d\u00b5 . in other words fe is the minimizer of the actual risk . since we are using the least squares loss function this minimizer is simply the expectation of y for each x fe e. we assume that we have chosen a rkhs which is large enough to contain fe . in other words . this is not an exceedingly strong assumption in fact many popular kernels can produce rkhs of arbitrarily high vc kh fe k dimension . although fe may not be in for most h smooth processes which have bounded noise as long as we implement a sufficiently powerful rkhs . for every case fe will be in h x y y f b where f for the fixed n case we may express label y for position x as the random variable is a deterministic function assumed to be in and b is random noise with some probability distribution with b and b h x. we denote the vector of noise values as b independent if x b i . . n. in order to force the noise to vanish we will assume the noise is gen because \u03c6 is a real function the operator \u03c6 provided by the spectral theorem is also self adjoint . in addition for each f h we have a measure \u03bdf a on spec such spec \u03c6d\u03bdf a. the measure \u03bdf a is concentrated on that that h r part of the spectrum spec along which f has a nonzero component . in particular if ker then f is an eigenvector of a with eigenvalue and \u03bdf a is a \u03b4 measure f concentrated on . if on the contrary f kl \u03c6 . \u03c6 z value \u03b7 . the bias term vanishes as n so there must exist an n and \u03bbn so that for n n \u03bbn is sufficiently small so that the bias term is bounded by \u03b7 . thus we consider the bias term bounded by \u03b7 in the limit as n since this term does not depend on the data the bound clearly holds with probability . now we must choose \u03b5n so that the variance term is bounded by \u03b7 . using the bound in we choose \u03b5n \u03b7\u03bbn . the corresponding probability pn is then given by fzn \u03bbn k h pn m c\u03ba m \u03bbn n \u03bbn \u03b5 n. n \u03bb n \u03b7 we need pn to vanish as n this is satisfied if n \u03bb there must exist an n such that for n n we have n to use the algorithmic stability theorem . thus theorem is proved . as n. also we need this in order n m \u03b5 n proof of theorem this section contains the proof of theorem . first some notation . the positions x . . xn will be considered fixed throughout this section . def p rn h f the evaluation operator p evaluates a function f at each position xi in the data set . note that p loses information about a function f by evaluating it at only n points . that is ker p is a nontrivial subspace of of the n operator p is given by p i cikxi . one can show that p is a n. the p bounded operator with kl i l k j p operator p p is automatically positive and self adjoint . we will later use the spectral theorem on the bounded self adjoint operator p p. the adjoint p rn p p k gij h max h p k we start the proof of theorem with the following lemma . lemma . the following characterizations of f are equivalent . f satisfies f f for i . . n and g kh kh k h f g k. f satisfies that satisfy g f. f f wxi where wxi g i\u2113 kx\u2113 n xi n x\u2113 . f satisfies f f for i . . n and ker p we have h. h. proof . we will show . first we show that the function described in . is unique . from the reproducing property we know that f has nonzero components along each of the kxi s for which f is a hilbert space we can always decompose f into a component fk within the span of the kxi s and a component f orthogonal to each kxi . now h. thus if f kh . since f k fkk fkk h h h k k k k k f of the kxi s is determined by the value of f. so functions f that satisfy both n i \u03b1ikxi for the fixed values of \u03b1i i . . n. and can be written f fk in particular the \u03b1i s must satisfy p n xi n xi \u03b1igij \u03b1ikxi f. thus the function described in . is unique . it is straightforward to see that the function described in . is exactly the function described in . evaluating the right side of at xj we obtain f f g j\u2113 g\u2113i f. n n xj x\u2113 moreover the function described in . lies entirely within the span of the kxi s. there . fore it obeys and and we have . because . is satisfied . we just need to show . ker p. for any h h h h \u2113 \u2113 \u2113 i . . n . . n by the reproducing property h because f is a linear combination of the wxi s thus holds . . . n because the wxi s are each a linear combination of the kx\u2113 s. here is automatic so we need to check . h take arbitrary g then from assumption and thus with g f f for i . . n. ker p. f h h. f g f g h f k k now g k k h k k g g g f k k. f f k h f h k f h f k f k h h k k h with equality only if g f. k k thus we have . so lemma . is proved . back to the proof of theorem . the functional lzt \u03bbt in the main algorithm expressed in terms of p becomes lzt \u03bbt p f t \u03bbtk f k h n \u2113 the minimizer of lzt \u03bbt must satisfy \u03b3 . in other words the first variational derivative of lzt \u03bbt is at its minimizer . recalling that p f p f this minimization problem becomes \u03b3 lzt \u03bbt p f \u03b3p h p f b p f \u03b3p h t n \u03b3 n \u2113 \u03bbth n p b t \u03bbtf . h p f t b \u2113 \u03bbtk f \u03b3h h k \u03b3 this must be true for any function h so implying n p p f n p p f n p b t \u03bbtf f f \u03bbtf n p b t. it follows that f k f kh k \u03bbtf kh n t k p klk b k\u2113 . in order to show stability we bound the two terms on the right of and construct . that is we need to bound the norms above . to these bounds so they vanish as t accomplish this we will use the spectral theorem on the bounded self adjoint operator p p. to bound the first term in equation recall that the operator obtained from the function \u03c6t t of the self adjoint operator p p is self adjoint . also since p p is a positive operator the spectrum spec of the operator p p is concentrated on r n \u03bbtf h h. n k zspec \u03bbt n z \u03bbt d\u03bdf p p n z \u03bbt \u03bb r where \u03c6 kl \u03c6 . thus \u03c6 z k z n p kl n \u03bbt n n t \u03bbt k n t k p klk b k\u2113 b k\u2113 t \u03bbt bmax a.s. as long as we design \u03bbt so that t \u03bbt t then we have the desired convergence of this term to . we are done with the second term of equation . theorem is proven . conclusion we have proved stability for the regularized least squares regression algorithm for the sense in which inverse problems are examined . we have shown stability for this algorithm in two cases the case when the number of data points n is a constant and the case where n. it is important that our algorithm is stable in this sense because we do not want any inherent error in the algorithm s output . neither a small amount of noise in the data nor a small amount of regularization should drastically in\ufb02uence the algorithm s output . we hope that the reader will gain more from our result than the knowledge that regularized least squares regression is stable in the inverse operator sense . we have found the particular methods introduced in the proofs of theorem and theorem useful for various learning problems especially those which require the convexity of learning functionals or convergence of learning algorithms . namely we demonstrate two methods for showing that the minimizers of two learning functionals are close use of the spectral theorem and the technique of lemma . which can both be generally applied to other learning algorithms . the author would like to express infinite gratitude to ingrid daubechies . acknowledgements"
    ],
    "abstract": [
        "we discuss stability for a class of learning algorithms with respect to noisy labels . the algorithms we consider are for regression and they involve the minimization of regularized risk functionals such as l n sum_i ^ lambda f _ h ^ . we shall call the algorithm stable if when y_i is a noisy version of f for some function f in h the output of the algorithm converges to f as the regularization term and noise simultaneously vanish . we consider two flavors of this problem one where a data set of n points remains fixed and the other where n infinity . for the case where n infinity we give conditions for convergence to f_e as lambda . for the fixed n case we describe the limiting non noisy non regularized function f and give conditions for convergence . in the process we develop a set of tools for dealing with functionals such as l which are applicable to many other problems in learning theory ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.48520710059171596
    ]
}