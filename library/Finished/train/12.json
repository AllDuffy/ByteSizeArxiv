{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0511058v2",
    "article": [
        "algorithm using e.g. the aggregating algorithm . so far in this subsection we have only discussed the asymptotic notion of universal consistency although it is clear that one needs universality in a stronger to be sense . in practical problems it is not enough for the benchmark class universal we also want as many prediction rules d as possible to belong to f or at least to be well approximated by the elements of d kf k to be as small as possible . the sobolev spaces on m discussed in are not only universal rkhs but also include all functions that are smooth in a fairly weak sense . however the hilbert space methods have their limitations it is not clear e.g. how to apply them to functions that are as smooth as typical trajectories of the brownian motion . these larger benchmark classes seem to require banach space methods see . we also want f f implications for the statistical theory of re gression so far we have not made any stochastic assumptions about the way the examples are produced . in this section we derive simple implications from theorem for the statistical learning framework assuming that the examples are drawn independently from some probability distribution on x y y. similar implications can be derived from the results of and some other papers the corollary stated in this section however has somewhat better constants . is defined as x and let h n n. be the prediction rules obtained by for all x averaging from some on line prediction algorithm guaranteeing . for any probability distribution p on x y y any n. and any \u03b4 be such that d f f riskp riskp c f y ln r \u03b4 y n q with probability at least \u03b4 . proof for a suitable choice of \u01eb we will have riskp riskp n n x n n x n n x n n n n n \u01eb c f \u01eb riskp c d f \u01eb n x riskp y n q d k kf y \u01eb \u03b4 . the inequalities and always hold the with probability at least t and the second from first follows from the convexity of the function t theorem . by hoe\ufb00ding s martingale inequality to make the probability of their \u03b4 it suffices to find \u01eb from the equation e \u01eb e \u01eb n y n taking values in yn intuitively the statistical assumption that the examples are produced independently from the same distribution is strong enough for the requirement of continuity to be super\ufb02uous as cover mentioned in his discussion of stone s paper it holds automatically with high probability . examples of rkhs and reproducing kernels the usefulness of the results stated in the previous two sections depends on the availability of suitable rkhs . in this section i will only give simplest examples for numerous other examples see e.g. and . the sobolev spaces the sobolev norm defined by f kh of an absolutely continuous function f k f h k k z dt dt . z the sobolev space h on is the set of absolutely continuous f r satisfying kh k kh . it is easy to see that h is an rkhs . in fact h is only one of a range of sobolev spaces see e.g. for the definition of the full range denoted w s p there we are interested in the case equipped with the norm f k r is s p and \u03c9 with the elements of w extended to by continuity . the space h is the least smooth among the sobolev spaces h s if we ignore the slightly less natural case of a fractional s. all of h s are universal rkhs but h is a proper superset of all other h s and so is the most universal sobolev space of this type . it is easy to see that neither of the two addends in can be omitted if the first addend is omitted the square root of the right hand side of ceases to be a norm and if the second addend is omitted the function space ceases to be an rkhs . we can however partially omit the first addend replacing with the fermi sobolev norm f k kfs defined by f k fs k f dt dt z z r. the fermi sobolev space on for absolutely continuous functions f is the set of absolutely continuous f equipped with the norm universal . k kfs . it is clear that it is still an rkhs and it is still of course the underlying set z of an rkhs does not have to be a comkh of an absolutely pact topological space we can define the sobolev norm continuous function f r r by essentially the same formula r satisfying kfs f k f k f k h k z z dt dt and define the sobolev space h on r as the set of absolutely continuous f r r satisfying f to apply theorems to these rkhs we need to know the value of cf for kh . k them later in this section we will see that cf ch coth . for the sobolev space h for the fermi sobolev space on and cf cfs . cf ch . for the sobolev space h. remark the term sobolev space usually serves as the name for a topological vector space all these spaces are normable but di\ufb00erent norms are not considered to lead to di\ufb00erent sobolev spaces as long as the topology does not change . the norms given by and are the most standard ones . it is easy to see that the norm leads to the same topology as \u03c0 r f kh o. kh follows from the standard inequality between the l and l f kfs follows from wirtinger s inequality which imk \u03c0 for every function f on such that f is in l f \u03c0 f for the statement and a proof of wirtinger s inequality see r r we are often interested in the case where the objects xn are vectors in a euclidean space rm if their components are bounded we can scale them so m. in any case we can take the mth tensor power of one of that xn the three rkhs we have just defined as our benchmark class . we will see later that cf for the mth tensor power is the mth power of the cf for the original rkhs . the mth tensor power of the sobolev and fermi sobolev spaces on are universal on m and the mth tensor power of h is universal on rm . f theorem requires separable rkhs the separability of sobolev spaces h s for integer s is proved in e.g. theorem . reproducing kernels an equivalent language for talking about rkhs is provided by the notion of a reproducing kernel this subsection defines reproducing kernels and summarizes some of their properties . for a detailed discussion see e.g. or . be an rkhs on z. by the riesz fischer theorem for each z let z f there exists a function kz f such that f kz f h if f. f kzkf is the norm cf of the evaluation funck the next lemma asserts that f. tional f lemma let be an rkhs on z. for each z z proof fix z z. we are required to prove kzkf cf. k sup f kf kf f kzkf . k f the inequality follows from where f k kf where f kz f. the inequality f kzif k h f kf k kzkf k kzkf follows from kz kzif kz kzkf kzkf kzkf k k k kzkf is assumed to be non zero . h f kzkf and k k the reproducing kernel of is the function k z r defined by f k kz kz h. the origin of this name is the reproducing property . if there is a simple internal characterization of reproducing kernels of rkhs . first it is easy to check that the function k as we defined it is symmetric k k z and positive definite m m i x j x \u03b1i\u03b1jk m. rm z m. r there . such that k is the reproducing kernel of f f we can see that the notions of a reproducing kernel of rkhs and of a symmetric positive definite function on z have the same content and we will sometimes say kernel on z to mean a symmetric positive definite function on z. kernels in this sense are the main source of rkhs in learning theory cf. every kernel on x is a valid parameter for our prediction algorithms to apply theorems we can use the equivalent definition of cf cf ck sup x x k p k being the reproducing kernel of . f it was convenient to start from rkhs in stating the theorems of prediction algorithms two of which are explicitly described in constructive representation of rkhs via their reproducing kernels . but our use the more norm vs. the reproducing kernel in rkhs finding the norm given the reproducing kernel and vice versa are often nontrivial problems for specific rkhs . the most popular methods appear to be the following . as we saw in the proof of lemma kz kzkf is the function at which k f sup f kf kf and that this optimization problem is attained . solving this optimization problem we can find the kernel k given the norm f kf . for application of this method to the fermi sobolev space on see appendix c. kzkf k f k one can use expansions into fourier series of functions in a given rkhs . for examples see e.g. . . or for the fermi sobolev space on . if z is a euclidean space and the reproducing kernel k only depends z an explicit formula on the di\ufb00erence z for the reproducing kernel can sometimes be obtained by applying the fourier transform to both sides of . the reproducing kernel of the sobolev space h as given in with a reference to is . k cosh min cosh min . sinh this implies marti s result that c k sup t cosh t cosh cosh cosh sinh coth as stated above . the reproducing kernel of the fermi sobolev space on was found in it is given by k kk kk k t min min t t where kl bl l are scaled bernoulli polynomials bl . so for the fermi sobolev space on we have c k max t t. the reproducing kernel of the sobolev space h is k exp . from the last equation we can see it is the general fact that the reproducing kernel of the m fold product of rkhs can be obtained as the m fold product of the reproducing kernels of the components . for example the reproducing kernel of the mth power of h is k m cosh min cosh min . i y we can see that cf m cf cf m m for the mth power of the sobolev space h of the fermi sobolev space on and of the sobolev space h respectively . an extensive list of rkhs together with their reproducing kernels is given in . . some comparisons the first paper about competitive on line regression is for a brief review . our results are especially close to of the work done in the s see those of and . there are two main proof techniques in the existing theory of competitive on line regression various generalizations of gradient descent and the bayes type aggregating algorithm . in this subsection we will only discuss the former some information about the latter will be given in . comparison between our results and the known ones is somewhat complicated by the fact that most of the existing literature only deals with the euclidean spaces rm . typically when loss bounds do not depend on m they can be carried over to hilbert spaces and so to some rkhs . to understand what xnk such known results say in the case of rkhs the upper bound on the size of the objects has to be replaced by cf and the upper bound on the size of the weight vector has to be interpreted as an upper bound on d w k k k with such replacements theorem iv . on p. of cesa bianchi et al. kf . k becomes n n x inf d kdkf y x. y inf d kdkf y x y n n x where \u00b5n are their algorithm s predictions . this result is of the same type as but kf is bounded by y x because of such a bound the corresponding prediction algorithm is not guaranteed to be universally consistent . d k auer et al. make the upper bound on . implies that for their algorithm d k kf more general their theorem n n x v u u t n n x n n x c f u cf u c f u n n x v u u t where u is a known upper bound on remarkably similar to and . kf and y is assumed to be . this is this type of results was extended by zinkevich to a general d k class of convex loss functions . d k the main di\ufb00erences of these results from our theorems are that their leading constants are somewhat worse and that they assume a known upper kf . the last circumstance might appear especially serious since it bound on prevents universal consistency even when the hilbert space used is a universal rkhs . however there is a simple way to achieve universal consistency the aggregating algorithm or a similar procedure may be used on top of the existing algorithm . this was noticed by auer et al. although they did not develop this idea further . the remaining minor component in achieving universal consistency is using a universal function class as the benchmark class . it is interesting that cesabianchi et al. used an almost universal function class in their pioneering paper . a very interesting early paper about on line regression competitive with function spaces is it however assumes that the benchmark class contains a perfect prediction rule and its results are very di\ufb00erent from ours . a major advantage of the methods based on gradient descent is their simplicity and computational efficiency . the technique of defensive forecasting which we emphasize in this paper appears closer to gradient descent than to the bayes type algorithms . there has been a mutually beneficial exchange of ideas between the gradient descent and bayes type approaches and combining gradient descent and defensive forecasting might turn out even more productive . results such as corollary can be obtained by a routine application of well known results in competitive on line learning but they might not be easy to obtain by the traditional methods of statistical learning theory . the closest results of this kind in statistical learning theory that i am aware of are theorem c of and corollary . of . these results however use balls in rkhs as benchmark classes and therefore do not guarantee even universal consistency . corollary can be strengthened by using the results of instead of those of . proof of theorem this section is essentially a simplified version of introducing a third player skeptic who is allowed to bet at the odds implied by predictor s moves . of . first we modify the protocol of forecasting game i players reality predictor skeptic protocol for n. x. reality announces xn predictor announces \u00b5n r. skeptic announces sn reality announces yn . in this protocol the prediction \u00b5n is interpreted as the price predictor charges for a ticket paying yn sn is the number of tickets skeptic decides to buy . the protocol describes not only the players moves but also the changes in skeptic s capital k can be an arbitrary real number . protocols of this type are studied extensively in . for any continuous strategy for skeptic there exists a strategy for predictor that does not allow skeptic s capital to grow regardless of reality s moves . to state this observation in its strongest form we make skeptic announce his strategy for each round before predictor s move on that round rather than announce his full strategy at the beginning of the game . therefore we consider the following perfect information game kn its initial value forecasting game ii players reality predictor skeptic protocol for n. x. reality announces xn skeptic announces continuous sn r predictor announces \u00b5n reality announces yn kn sn . r. lemma predictor has a strategy in forecasting game ii that ensures \u00b5n for all n. and k k k. proof predictor s goal is achieved by the following strategy if the function sn takes value on the interval choose \u00b5n if sn is always positive on take \u00b5n y if sn is always negative on take \u00b5n y. algorithm of large numbers we say that a kernel k on x is forecast continuous if the function y y for all fixed x. n i x. is continuous in \u00b5 sn k \u00b5i k \u00b5 k \u00b5n . yn yn \u00b5n k \u00b5n k \u00b5n yn n i x x n n x n n n i x x n k n x n n x n n n i x x n k n x n \u03c6 y \u00b5 n n h y \u00b5 n \u03c6 k k h n x resolution this subsection makes the next step in our proof of theorem . our goal is to prove the following result . theorem let \u00b5n output by the aln with parameter k always satisfy n n x d. f y cf k d kf n for all n and all functions d proof using with \u03c6 being the feature mapping x obtain x kx f we n n x d kxn d kxn d h n if kxn d kf k f y cf k d kf n d k kf v u u t n x for any d. f theorem can be interpreted as asserting that the aln has a good reso lution when is a universal rkhs for details see . f mixing feature mappings in the proof of theorem we will mix the feature mapping \u03c6 \u00b5 and the feature mapping \u03c6 kx used in the proof of theorem . this can be done using the following corollary of theorem . corollary let \u03c6j x to hilbert spaces hj j be forecast continuous hj and let a a be two positive y y output by the aln with a suitable kernel for all fixed \u00b5 x and x of sn as \u00b5n x n n x n h n x always holds for all n. \u03c6 \u03c6 k k h proof following the k algorithm predictor ensures that skeptic will never increase his capital with the strategy n i x sn k which implies kn k sn n n x n n n i x x n n n i x x n k n x n k k h n n x \u03c6 \u03c6 k k h corollary let \u03c6j mappings from x hj j be forecast continuous hj and let aj j be positive y y output by the k algorithm with a n n x \u03c6j hj aj n n x for all n and for both j and j. a k \u03c6 k h a k \u03c6 k h proof being forecast continuous the kernel k defined in the proof of corollary is a fortiori k admissible . applying the k algorithm to it and using we obtain hj \u00b5n \u03c6 aj \u03c6j n n x n n x yn n n h n x n x aj k \u03c6j k hj . j x merging \u03c6 \u00b5 and \u03c6 kx by corollary we obtain \u00b5n \u03c6 n n x n n x n v a u u t n x r a\u00b5 n ak a q n v u u t n x ac f ay and using n n x d n n n x \u03c6 n x a kf v u u t n n x d k a\u00b5 d k kf f n ak n v u u t n x q a k d kf ac f ay for each function d. f proof proper using and with a a and a we obtain for the \u00b5n output by the k algorithm with the merged kernel as parameter n n x n n x n n x \u00b5n yn kf n x a d k \u00b5n \u00b5n . d yn n yn v u u t n x the inequality between the extreme terms of this chain is quadratic in n v u u t n x solving it we obtain n v u u t n x n d kf k a which is equivalent to when a y. c f ay q d k kf a bayes type competitive on line regression and proof of theorem the first result in the bayes style competitive on line regression appears to be consists of the linear functions d the following if the benchmark class f on x rm whose complexity is measured by the l norm k \u03b8 x i h m i \u03b8 i of \u03b8 s components \u03b8i and if a is a positive constant some on line \u03b8 k prediction algorithm ensures pp \u00b5n . in particular if are bounded by a constant rm yn a f y ln det d k k n x on x where k is n gram matrix with the elements ki j k i j. n for all n and all prediction rules d in a separable rkhs the n n x f i k a s reproducing kernel . a disadvantage of the bound is that for a fixed a the term k i cikzi where k c. ck . see p can have order of magnitude n indeed if ln det i k a k if i j otherwise yn a d f y n ln k k c f a y c f n a d ad . n n x yn y cf d n. we can see an analogue of the familiar term y cf k c d f k y ln n f k o d kf n. the expression in can be interpreted as the price that we pay for not knowing n in advance . d kf and k kummer s u function in the proof of theorem we will need an approximation to kummer s u function u e ztta b a dt \u03b3 z from p. a concise statement of this result is given in p. . . the approximation is given by the formula \u03ba \u03bazb e\u03ba z u ei\u03be o r o \u03be \u03ba where r \u03ba b a i\u03be \u03ba ln z \u03ba z and it is assumed that \u03be and for some constant \u03b4 . we are only interested in the case z a b can be rewritten as which also implies \u03ba . since ln z \u03b4 \u03ba r i\u03be \u03ba ln z \u03ba z \u03bai\u03c0 and therefore arg \u03be and we can see that we have already used this fact in choosing the expression among the expressions given in p. \u03c0 we . . using and the fact that deduce from \u211c \u03ba \u03be ln u \u03ba ln \u03ba b ln z ln \u03ba z i\u03be o \u03ba r \u03ba ln \u03ba b ln z ln \u03ba z z \u03ba ln z o b ln z \u03ba ln \u03ba z b z a z b z ln z a ln b ln \u03ba \u03ba z z b z z ln \u03ba r \u03ba z \u03ba o ln a \u03ba b r z a ln z a b z r \u03ba . o by stirling s formula ln \u03b3 a a ln a ln o a which for b gives ln ln z ln a b z a b z b a ln a b b z z b z z ln a ln \u03c0 o a r a ln z b z b z a a ln a b b ln b z b a b z z a ln a ln \u03c0 o z a a r. proof of theorem to get rid of the parameter a in the first inequality of we will merge the aa predictions corresponding to all possible a w.r. to the probability measure here and in what follows we let c stand for cf and \u01eb for a constant on in to be chosen later . taking \u03b7 and \u03b2 e \u03b7 making use of lemmas and of and letting d kf we obtain the following bound for the excess loss of the merged of stand for predictions over d s predictions over the first n rounds d k log\u03b2 \u03b2ad y n lnq z \u03b7y n q \u03b7 ln e \u03b7ad n c a q z c a y ln e ad y ln z \u01ebc\u01eb e ad a c z n \u01eb an da . substituting ct for a transforms this to y ln \u01eb d t n \u01eb tn dt which by and can be written as z \u01eb cd y cd y y ln \u01eb\u03b3 n y ln \u01eb y u \u01eb \u01eb ln cd y n cd y y ln n y cd y cd n \u01eb y ln \u01eb n cd y cd y n \u01eby ln y ln \u03c0 y o n r. n remember that the validity of the approximation requires the condition . in the most interesting case \u03ba we can take r to get the best z bound corresponding to the accuracy of o unfortunately such a bound would be rather clumsy and the following transformations are performed to a much worse accuracy o. to make sure that holds it is now sufficient to assume that cd y \u03b4n \u03b4 for some constant \u03b4 we will first assume that this condition holds and at the end of the proof will get rid of it . the fourth addend from the end of can be bounded above as follows y ln \u01eb n cd y y ln y cd y cd y cd y n cd y n cd y n cd y cd n \u01eb . this allows us to bound from above by cd y y cd cd y \u01eb y ln where h r be the rkhs corresponding to the kernel on x r defined by k let f ch the positive definiteness of k follows from bochner s theorem . representation xix . and polya s theorem example in shows that cf c. r is the triangular function h max t reality s strategy is xn n and yn y with sign opposite to sign . this will make sure that the loss of the on line prediction algorithm over the first n rounds is at least y n. let fn f be the function defined by fn ch n. it fnkf . set k d \u03b1 f yn c fn n n x one of the conditions of the theorem ensures that \u03b1 values in . the loss of d over the first n rounds is d k y n y n y n y cd n which completes the proof . the algorithms in this short section we extract the prediction strategies achieving and from our proof of theorems and . replacing in the kernel k by the merged kernel \u00b5\u00b5 y kx kx h if we obtain sn n i x \u00b5\u00b5i y k \u00b5 y k \u00b5 this immediately leads to the following explicit description for the on line prediction algorithm we used in the proof of theorem . an algorithm achieving parameter the reproducing kernel k of f for n. x. read xn define sn by for all \u00b5 define \u00b5n as any root \u00b5 of sn read yn . end for . to obtain an algorithm achieving it suffices to replace by sn n i x \u00b5\u00b5i y k. acknowledgments i am grateful to nicol o cesa bianchi alex smola and especially olivier bousquet for useful comments . this work was partially supported by mrc and the royal society ."
    ],
    "abstract": [
        "we consider the problem of on line prediction of real valued labels assumed bounded in absolute value by a known constant of new objects from known labeled objects . the prediction algorithm s performance is measured by the squared deviation of the predictions from the actual labels . no stochastic assumptions are made about the way the labels and objects are generated . instead we are given a benchmark class of prediction rules some of which are hoped to produce good predictions . we show that for a wide range of infinite dimensional benchmark classes one can construct a prediction algorithm whose cumulative loss over the first n examples does not exceed the cumulative loss of any prediction rule in the class plus o the main differences from the known results are that we do not impose any upper bound on the norm of the considered prediction rules and that we achieve an optimal leading term in the excess loss of our algorithm . if the benchmark class is universal this provides an on line non stochastic analogue of universally consistent prediction in non parametric statistics . we use two proof techniques one is based on the aggregating algorithm and the other on the recently developed method of defensive forecasting ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.4834123222748815
    ]
}