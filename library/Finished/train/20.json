{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0606077v1",
    "article": [
        "contents introduction notation and definitions dominance with decreasing coefficients preservation of the predictive ability under summation with an arbitrary measure miscellaneous outlook and conclusion sequence prediction local absolute continuity non stationary measures average expected criteria absolute kl divergence mixtures of measures . this work was supported by the swiss nsf grant . keywords introduction in of letters from some finite alphabet let a sequence xt t be generated by some probability measure \u00b5 . having observed the first n letters x ... xn we want to predict what is the probability of the next letter being x for each x. this task is motivated by numerous applications from weather forecasting and stock market prediction to data compression . x x if the measure \u00b5 is known completely then the best forecasts one can make for the st outcome of a sequence x ... xn is \u00b5 conditional probabilities of x x given x ... xn . on the other hand it is immediately apparent that if nothing is known about the distribution \u00b5 generating the sequence then no prediction is possible since for any predictor there is a measure on which it errs on every step . thus one has to restrict the attention to some class of measures . laplace was perhaps the first to address the question of sequence prediction his motivation being as follows suppose that we know that the sun has risen every day for years what is the probability that it will rise tomorrow he suggested to assume that the probability that the sun rises is the same every day and the trials are independent of each other . thus laplace considered the task of sequence prediction when the true generating measure belongs to the family of bernoulli i.i.d. measures with binary alphabet . the predicting measure but what if the measure generating the sequence is not stationary a di\ufb00erent assumption one can make is that the measure \u00b5 generating the sequence is computable . solomono\ufb00 suggested a measure \u03be for predicting any computable probability measure . the key observation here is that the class of all computable probability measures is countable let us denote it by i in . a bayesian predictor \u03be for a countable class of measures i in is constructed as follows \u03be i wi\u03bdi for any measurable set a where the weights wi are positive and sum to one . the best predictor for a measure \u00b5 is the measure \u00b5 p it is not necessary for prediction that the weights sum to one . in and wi k where k stands for the prefix kolmogorov complexity and so the weights do not sum to . further itself . the bayesian predictor simply takes the weighted average of the predictors for all measures in the class for countable classes this is possible . it was shown by solomono\ufb00 that \u03be conditional probabilities converge to \u00b5 conditional probabilities almost surely for any computable measure \u00b5 . in fact this is a special case of a more general result of blackwell and dubins which states that if a measure \u00b5 is absolutely continuous with respect to a measure \u03c1 then \u03c1 converges to \u00b5 in total variation \u00b5 almost surely . convergence in total variation means prediction in a very strong sense convergence of conditional probabilities of arbitrary events or prediction with arbitrary fast growing horizon . since for \u03be we have \u03be wi\u03bdi for every measurable set a and for every \u03bdi each \u03bdi is absolutely continuous with respect to \u03be . thus the problem of sequence prediction for certain classes of measures was often addressed in the literature . although the mentioned classes of measures are sufficiently interesting it is often hard to decide in applications with which assumptions does a problem at hand comply not to mention such practical issues as that a predicting measure for all computable measures is necessarily non computable itself . moreover to be able to generalize the solutions of the sequence prediction problem to such problems as active learning where outcomes of a sequence may depend on actions of the predictor one has to understand better under which conditions the problem of sequence prediction is solvable . in particular in active learning the stationarity assumption does not seem to be applicable although say the markov assumption is often applicable and is extensively studied . thus we formulate the following general questions which we start to address in the present work general motivating questions . for which classes of measures is sequence prediction possible under which conditions does a measure \u03c1 predict a measure \u00b5 as we have seen these questions have many facets and in particular there are many criteria of prediction to be considered such as almost sure convergence of conditional probabilities convergence in average etc. extensive as the literature on sequence prediction is these questions in their full generality have not received much attention . one line of research which exhibits this kind of generality consists in extending the result of blackwell and dubins mentioned above which states that if \u00b5 is absolutely continuous with respect to \u03c1 then \u03c1 predicts \u00b5 in total variation distance . and a in a question of whether given a class of measures prior \u03bb over this class of measures the conditional probabilities w.r.t. \u03bb converge to the true \u00b5 probabilities of a bayesian mixture of the class c for \u03bb almost any measure \u00b5 in . this question can be considered solved since the authors provide necessary and sufficient w.r.t. \u03bb under which conditions on the measure given by the mixture of the class c c c the \u03bd and \u03be are only semi measures . prediction is possible . the major di\ufb00erence from the general questions we posed above is that we do not wish to assume that we have a measure on our class of measures . for large classes of measures it may not be intuitive which measure over it is natural rather the question is whether a natural measure which can be used for prediction exists . in we have \u03be to address the general questions posed we start with the following observation . as it was mentioned for a bayesian mixture \u03be of a countable class of measures \u03bdi i wi\u03bdi for any i and any measurable set a where wi is a constant . this condition is stronger than the assumption of absolute continuity and is sufficient for prediction in a very strong sense . since we are willing to be satisfied with prediction in a weaker sense let us make a weaker assumption say that a measure \u03c1 dominates a measure \u00b5 with coefficients cn if \u03c1 cn\u00b5 for all x ... xn . x n is absolutely continuous w.r.t. \u03c1 the first concrete question we pose is under what conditions on cn does imply that \u03c1 predicts \u00b5 observe that if \u03c1 for any x ... xn then any measure \u00b5 is locally absolutely continuous with respect to \u03c1 and moreover for any measure \u00b5 some constants cn can be found that satisfy . for example if \u03c1 is bernoulli i.i.d. measure with parameter and \u00b5 is any other measure then is satisfied with cn n. thus we know that c then \u03c1 predicts \u00b5 in a very strong sense whereas exponentially decreasing if cn cn are not enough for prediction . perhaps somewhat surprisingly we will show that dominance with any subexponentially decreasing coefficients is sufficient for prediction in a weak sense of convergence of expected averages . dominance with any polynomially decreasing coefficients and also with coefficients decreasing is sufficient for prediction on average example as cn exp k n k n pk log \u00b5 xt \u00b5 xt \u03c1 x t e log n n xt yt \u00b5 n n eet log log c n x t x t \u00b5 xt \u03c1 log c n. then \u03c1 predicts \u00b5 in average kl divergence dn \u00b5 a.s. lute distance an xn \u00b5 a.s. and in average abso in particular the condition on the coefficients is satisfied for polynomially decreasing coefficients or for cn exp . proof . again the second statement follows from the first one and lemma so that we only have to prove the statement about kl divergence . introduce the symbol en for \u00b5 expectation over xn conditional on x n. consider n t lt. observe that dn enln so that dn form a martingale di\ufb00erence sequence . let also mn n \u00b5 a.s. which implies dn \u00b5 a.s. p ln note that n log log c n. n \u00b5 \u03c1 n thus to show that ln goes to we need to bound it from below . it is easy to see that n ln is bounded from below by a constant since \u03c1 is a \u00b5\u00b5 martingale whose expectation is and so it converges to a finite limit \u00b5 a.s. by doob s submartingale convergence theorem see e.g. next we will show that mn \u00b5 a.s. we have mn log log \u00b5 \u03c1 \u00b5 \u03c1 log \u00b5 \u03c1 \u00b5 \u03c1 . en log en log \u00b5 \u03c1 en log \u00b5 \u03c1 let f be some function monotonically increasing to infinity such that e.g. choose f logn and exploit f and . xn n \u03bbn if \u03bbn otherwise f and \u03bb n \u03bbn \u03bb n. introduce also m n log \u00b5 \u03c1 en log \u00b5 \u03c1 m n and m n. observe that m n and the averages m m n mn n is a martingale di\ufb00erence sequence . hence to establish the convergence m we can use the martingale strong law of large numbers which states that for a martingale di\ufb00erence sequence \u03bbn if e then \u03bbn and a.s. indeed for m n the first condition is trivially satisfied and the second follows from the fact that m n n n n e\u03bb logc n f and . p n furthermore we have m n log \u00b5 \u03c1 en log \u00b5 \u03c1 . as it was mentioned before log \u00b5 log \u00b5 \u03c1 to a finite number . hence \u03c1 converges \u00b5 a.s. either to infinity or is non zero only a finite number of times and so its average goes to zero . to see that en en log \u00b5 \u03c1 xxn x xxn x log \u00b5 \u03c1 we write \u00b5 \u03c1 \u00b5 \u03c1 log \u00b5 xn \u03c1 xn log \u00b5 x n x n \u00b5 log \u00b5 log and note that the first term in brackets is bounded from below and so for the sum in x n brackets to be less than has to go to f the second term log\u00b5 \u01eb infinitely often \u00b5 a.s. for all x n yet an \u01eb and dn \u00b5 \u03c1 proof . let \u00b5 be concentrated on the sequence ... and let \u03c1 for all n except for a subsequence of steps n nk k in on which \u03c1 independently of each other . it is easy to see that choosing nk sparse enough we can make \u03c1 decrease arbitrary slowly yet \u00b5 for all k. \u03c1 thus for the first question whether dominance with some coefficients decreasing to zero is sufficient for prediction we have the following table of questions and answers where in fact positive answers for an are implied by positive answers for dn and vice versa for the negative answers e dn dn dn e an an an however if we take into account the conditions on the coefficients we see some open problems left and di\ufb00erent answers for dn and an may be obtained . following is the table of conditions on dominance coefficients and answers to the questions whether these conditions are sufficient for prediction . dn e an logc n o logc n n c we know form proposition that the condition cn can not be improved thus the open problem left is to find whether logc sufficient for prediction in dn or at least in an . an dn e dn cn an n p c for convergence in dn n o is another open problem is to find whether any conditions on dominance coefficients are necessary for prediction so far we only have some sufficient conditions . on the one hand the obtained results suggest that some form of dominance with decreasing coefficients may be necessary for prediction at least in the sense of convergence of averages . on the other hand the condition is uniform over all sequences which probably is not necessary for prediction . as for prediction in the sense of almost sure convergence perhaps more subtle behavior of the ratio \u00b5 \u03c1 should be analyzed since dominance with decreasing coefficients is not sufficient for prediction in this sense . preservation of the predictive ability under summation with an arbitrary measure now we turn to the question whether given a measure \u03c1 that predicts a measure \u00b5 in \u03b5 \u03c1 \u03b5\u03c7 for some \u03b5 also predicts some sense the contaminated measure . x t \u00b5 xt \u03c1 xt e dn n n e log e log \u00b5 \u03c1 \u03c1 \u03c1 where the first term tends to by assumption and the second term is bounded from above by . since the sum is bounded from below by we obtain the statement of the proposition . n log next we consider some negative results . an example of measures \u00b5 \u03c1 and \u03c7 such that \u03c1 predicts \u00b5 in absolute distance but does not can be constructed similarly to the example from . the idea is to take a measure \u03c7 that predicts \u00b5 much better than \u03c1 on almost all steps but on some steps gives grossly wrong probabilities . proposition there exist measures \u00b5 \u03c1 and \u03c7 such that \u03c1 predicts \u00b5 in absolute distance but does not predict \u00b5 in absolute distance . proof . let \u00b5 be concentrated on the sequence ... and let \u03c1 n n with probabilities independent on di\ufb00erent trials . clearly \u03c1 predicts \u00b5 in both absolute distance and kl divergence . let \u03c7 for all n except on the sequence n nk k in on which \u03c7 nk nk k. this implies that \u03c7 nk and \u03c7 \u03c7 nk nk . it is now easy to see that does not predict \u00b5 neither in absolute distance nor in kl divergence . indeed for n nk for some k we have k k n \u03c1 \u03c7 \u03c1 \u03c7 n n n. for the average absolute distance the negative result also holds proposition there exist such measures \u00b5 \u03c1 and \u03c7 that \u03c1 predicts \u00b5 in average absolute distance but does not predict \u00b5 in average absolute distance . proof . let \u00b5 be bernoulli distribution and let \u03c1 for all n except for some sequence nk k in on which \u03c1 . choose nk sparse enough for \u03c1 to predict \u00b5 in the average absolute distance . let \u03c7 be bernoulli . observe that \u03c7 assigns non zero probabilities to all finite sequences whereas \u00b5 a.s. from some n on \u03c1 . hence \u03c7 and so does not predict \u00b5 . thus for the question of whether predictive ability is preserved when an arbitrary measure is added to the predictive measure we have the following table of answers . e dn dn dn e an an an as it can be seen there is one open question whether this property is preserved under almost sure convergence of the average kl divergence . it can be inferred from the example in proposition that contaminating a predicting measure \u03c1 with a measure \u03c7 spoils \u03c1 if \u03c7 is better than \u03c1 on almost every step . it thus can be conjectured that adding a measure can only spoil a predictor on sparse steps not a\ufb00ecting the average . conjecture suppose that a measure \u03c1 predicts a measure \u00b5 in absolute distance . then for any measure \u03c7 the measure predicts \u00b5 in average absolute distance . as far as kl divergence is concerned we expect even a stronger conjecture to be true since limited kl divergence does not allow a predicting measure to be zero on any step . conjecture suppose that a measure \u03c1 predicts a measure \u00b5 in average kl divergence . then for any measure \u03c7 the measure predicts \u00b5 in average kl divergence . miscellaneous x o n x k in section we special cases of dominance with decreasing coefficients . have shown that laplace s measure \u03c1l for dominates any bernoulli i.i.d. measure with linearly decreasing coefficients . it can also be shown that a generalization of \u03c1l to a measure \u03c1k l for predicting any measure with memory k for a given k dominates any such measure with polynomially decreasing coefficients . the measure \u03c1r from for predicting any stationary measure n was constructed as a sum of \u03c1k l. l with positive weights \u03c1r by construction \u03c1r dominates any finite memory measure with polynomially deit is interesting to find whether \u03c1r dominates every stationary measure with some subexponentially decreasing coefficients . clearly this is a special case of the general open question whether some form of dominance with decreasing coefficients is necessary for prediction . k wk\u03c1k p special questions of summation of a predictor with arbitrary measures . although we know that adding a measure may spoil a predicting measure it may be that carefully selecting which groups of measures to sum we can save all their predicting properties . one of the interesting cases is zvonkin levin universal semi computable measure \u03be wi\u03bdi xi in in is the class of all lower semi computable semi measures and where i wi . since \u03be wi\u03bdi for any measurable set a it predicts all \u03bdi in the sense of convergence in total variation in kl divergence and absolute distance . the question is what else does it predict in fact \u03be is not a measure but only a semi measure but a semi measure is sufficient for making predictions and it will not a\ufb00ect our arguments further . which other measures laplace s measure \u03c1l is computable and hence is present in the sum . we know that \u03c1l predicts any bernoulli i.i.d. measure so we can ask whether \u03be being a sum of \u03c1l and some other measures still predicts all bernoulli measures . the predictor \u03c1r from is computable and predicts all stationary measures in average kl divergence . thus \u03be is a sum of \u03c1r and some other measure and we can ask whether it still predicts all stationary measures . conjecture for i.i.d. measure \u03bd the measure \u03be as defined in predicts \u03bd in absolute distance . for every stationary measure \u03bd the measure \u03be predicts \u03bd in average kl divergence . every proof idea . for the first question consider any bernoulli i.i.d. measure \u03bd . from conjecture and from the fact that \u03c1l predicts \u03bd in absolute distance we conclude that \u03be predicts \u03bd in average absolute distance . since \u03bd is bernoulli i.i.d. measure that is probabilities on all steps are equal and independent of each other from any measure \u03b8 that predicts \u03bd in average absolute distance we can make a measure n \u03b8 which predicts \u03bd in absolute distance as follows \u03b8 n p if \u03bd is a stationary measure then we known from proposition and that \u03c1r is computable that \u03be predicts \u03bd in expected average kl divergence . conjecture would also imply that \u03be predicts \u03bd in average kl divergence and average absolute distance . other measures of divergence . the last question we discuss is criteria of prediction other than introduced in section . apart form the measures of divergence of probability measures that we considered we mention also the following squared distance sn x x \u00b5 \u03c1 hellinger distance hn x x \u00b5 q q \u03c1 p the average squared distance sn and the average hellinger distance hn are introduced analogously to an and dn . it is easy to check that all negative results obtained hold with respect to sn and hn as well . positive results for sn and hn follow from corresponding positive results for kl divergence dn and inequalities sn dn dn see e.g. expected absolute convergence and hn and expected kl convergence edn ean may also be considered . outlook and conclusion in the present work we formulated and started to address the question for which classes of measures sequence prediction is possible . towards this aim we defined the notion of dominance with decreasing coefficients and found some forms of it which are sufficient for prediction . we have also addressed the question which forms of predictive ability are preserved under contamination of a predictor by an arbitrary measure . besides some more concrete open problems posed in the corresponding sections a program for answering the general questions formulated can be outlined as follows we would like to find some conditions on dominance with decreasing coefficients which are necessary and sufficient for prediction for those notions of prediction ability for which this is not possible more subtle behavior of the ratio \u00b5 \u03c1 should be analyzed to obtain conditions both necessary and sufficient for prediction . this should give rise to an abstract characterization of classes of measures for which a measure satisfying such conditions for all measures in the class exists that is to a description of classes of measures for which prediction is possible . it is expected that such characterization will naturally lead to a construction of a predictor as well perhaps in form of a bayesian integral . the latter conjecture also encourages studying the question of contamination of a predictor with arbitrary measures . the next step will be to extend this approach to the task of active learning ."
    ],
    "abstract": [
        "suppose we are given two probability measures on the set of one way infinite finite alphabet sequences and consider the question when one of the measures predicts the other that is when conditional probabilities converge when one of the measures is chosen to generate the sequence . this question may be considered a refinement of the problem of sequence prediction in its most general formulation for a given class of probability measures does there exist a measure which predicts all of the measures in the class to address this problem we find some conditions on local absolute continuity which are sufficient for prediction and which generalize several different notions which are known to be sufficient for prediction . we also formulate some open questions to outline a direction for finding the conditions on classes of measures for which prediction is possible ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.6267605633802817
    ]
}