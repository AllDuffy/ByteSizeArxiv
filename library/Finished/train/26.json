{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0607110v1",
    "article": [
        "introduction ensembles of classifiers are a popular way to build a strong classifier by leveraging simple decision rules weak classifiers . many ensemble architectures have been proposed such as neural networks decision trees adaboost bagged classifiers random forests trees holding a boosted classifier at each node boosted decision trees ... one drawback of ensemble methods is that they are often dispendious about the computational cost of the resulting classifier . for example adaboost bagging random forests all multiply the runtime complexity by a factor approximately proportional to the training time.this is not acceptable in applications involving large amounts of data and requiring low complexity method such as video analysis and data mining . many approaches have been proposed to deal with such situations . the cascade architecture i.e. a degenerate decision tree has become very popular and has been intensely studies . however cascades are mostly appropriate to detect rare exemplars of interest amongst a huge majority of uninteresting ones . decision trees on the other hand are better adapted to the case of balanced target classes . this advantage comes from their greater facility to decompose the input space into more manageable and useful subsets . in addition their run time complexity is approximately proportional to the logarithm of the training time . counterbalancing these advantages is the fact that decision trees tend to overfit the training data . there exist many proposed methods to improve overfitting for example pruning and smoothing but the main recognized cause remains data elements are passed to one only of the descendant of each node whether during training or at run time . at run time one proposed solution is to pass examples along more than one child node . slightly expanded version of my ecml submission a b c d e f g construction of a matryoshka tree index of path s figure left construction of a matryoshka decision trees . here each tree has just two nodes but other numbers are possible . right notation use to specify a path in a decision tree . during training it has been proposed to pass down to all descendants the exemplars that lay within a fixed distance of the separating surface . our approach to avoid the hard split at each node is to consider that the examples have a certain probability not necessarily always or of being passed to any descendant of the tree . that is we study probabilistic decision trees but pursue a different analysis from these last works . first we show that probabilistic decision trees are eminently tractable within the framework of boosting . we bound the expected misclassification error as a function of the number of nodes in the tree in section . this bound is very high when the probabilistic weak classifiers are very weak . moreover we present arguments that suggest that any bound using the same probabilistic weak learner hypothesis will necessarily be high . however we also note that the bound achieved with stronger weak classifiers is much better . in an attempt to strengthen our weak classifiers we explore the possibility of assembling decision trees consisting of decision trees the inner and the outer trees being built by the same algorithm . this is not the first time this idea is suggested but we believe we are the first to show the theoretical benefits of doing so . continuing on the idea of embedding decision trees one into another we propose to assemble trees of trees of ... of trees of probabilistic weak classifiers . this is similar to matryoshka dolls with the difference that each tree contains more than one tree rather than a single other doll . figure left illustrates this concept . our main contribution is to prove a greatly improved bound reached by trees with exactly two nodes each node being a tree with two nodes and so on until the last nesting level which holds two probabilistic weak classifiers . another merit of our study is that it proposes a methodology that is essentially parameterless . the user only needs to provide a probabilistic weak learner and a stopping criterion such as the number of nodes or the error on the training dataset . if a stopwatch is available during training then we propose ways of using it . the freedom of parameters results partly from applying a principle of greedy error minimization . before presenting our study on decision trees we define in section the probabilistic weak learners that are the basis of this work . we then present in section the analogy between deterministic decision trees and boosting has been studied in . this metaphor is to say that if the time complexity of the learner and classifier are known or measurable then this information can be used to greedily reduce the training error . slightly expanded version of my ecml submission the probabilistic equivalent of adaboost that will serve as reference for the rest of the article . after presenting our main theory in sections and we discuss further the findings of this study and open directions for future research . probabilistic weak learner we consider learning algorithms s with d. d return a. probabilistic classifier or oracle written h. for any input x h is identified with a bernoulli random variable with parameter q. that given a training dataset a adaboost for probabilistic weak learners we now adapt adaboost to probabilistic rather than deterministic weak learners . like the original adaboost we consider classifiers of the form h \u03b1t x ht t xt but here ht is a bernoulli random variable so that h is itself a random variable . like in adaboost we consider domainpartitioned weights we have constants \u03b1t and \u03b1t such that \u03b1t x \u03b1t if ht is observed to be and that \u03b1t x \u03b1t otherwise . we proceed as in adaboost increasing the number t of weak classifier and not changing a weak classifier once it has been trained . each random classifier ht is this prior is pessimistic since q has expectation smaller than but the edge \u03b5 being unknown using another prior would not be less hazardous . slightly expanded version of my ecml submission obtained by running the weak learner on the training data set . with weights dt n chosen to emphasize misclassified examples . the weight update rule is n dt dt q e \u03b1t yn q t n n dt e\u03b1t yn normalizes where zt the weights so they sum to one . p with a deterministic weak classifier one would have q are unknown . we address this issue in sec . and assume for now that the q. boosting property we now give an upper bound for the expected misclassification error of h and show how to choose the weights \u03b1t . this derivation parallels that of the expected training error is e d e n xn d e e hyn . where is the indicator function being if the bracketed expression is true and zero otherwise . since h may take at most t possible values depending on the outputs st t t of the t classifiers ht one has e e hyn \u02c6q e \u03b1t st yn \u02c6q e \u03b1t st yn dt zt dt \u02c6q e \u03b1t st yn dt zt dt dt zt dt n xn t xs ... st yt xs ... st t yt t xs ... st. etc. yt dt d t yt zt . summing over all samples one gets the familiar expression the rest goes as with adaboost each zt is minimized by setting w t w t w t w t and \u03b1t \u03b1t log log e t zt . yt slightly expanded version of my ecml submission t w t w t w. for p it must be made clear that in practice during training the users only have estimates n so that they reduce an estimate of the bound of the expected error . of q during training . estimation of q in order to compute dt for the next the user has to estimate the q is biased towards the map and ml differ in that the map estimator of q and s the last edge followed to reach s e \u03b1s yn . are in this expression zsa normalizing constants and q is the parameter of the bernoulli random variable hs . like in sec . we use estimates \u02c6q in place of the true values . n n ds q e \u03b1sayn for a p ds zs . bound on the expected error we now bound the error of the boosting tree algorithm and specify the weights \u03b1s and the choice of the trained node at each step . using again the exponential error inequality loss e hy eq . the expected misclassification error for a training example xn is upper bounded by e e hy p e hlyn xl leaf h where p is the probability of an input x reaching the leaf l. more generally assuming independence of the outputs of classifiers at each node the probability that x reaches a node s is p q q. q q yr s where the product is taken for all nodes between the root and s. the error bound is thus e e hy xl leaf h ys l xl leaf h ys l q e hlyn q e s\u03b1syn ds d s zs xl leaf h ys l dl zs xl leaf h ys l slightly expanded version of my ecml submission summing over all examples xn n and replacing in eq . yields the bound n e zs xl leaf h ys l like above each zs is minimized by setting \u03b1s log w s s s w s s s where w ab s d s q a b xn yn b and is the negation operator . for these values of \u03b1s each zs is takes the value zs w s s w s s. q this bound can also be found in slightly different contexts in our previous work and in our unpublished manuscript . in the present paper we additionally study how this bound evolves with the size of the tree . expected error bound as a function of the tree size we now describe the evolution of the bound when the tree is grown by a greedy bound reducing algorithm . probabilistic weak learner hypothesis . as previously we may show that zs zs we now proceed recursively . after training and incorporating t nodes the exs l zs . at this point the tree has t pected error bound is c leaves so that one leaf l at least has an error not less than c. after q p training a probabilistic weak classifier at l the new error bound is \u03b5 \u03c1 owing to the l leaf h c c zs zs zs ys l ys l ys l c zs ys l c t since c we have the general relation c t yt t \u03c1 t t b f t \u03c1 \u03b3 where b is the beta function and \u03b3 is the gamma function . the rightmost term is the asymptotic approximation for large t it is coherent with the bound of d. this bound is interesting in more than one respect slightly expanded version of my ecml submission bound of tree adaboost and matryoshka bound for tree of trees bound for trees of trees d n u o b r o r r e d e t c e p x e. bound of tree bound of adaboost bound of matryoshka . r o r r e d e t c e p x e n o d n u o b. number of weak classifiers e size of subtrees size of subtrees figure left bound of boosted decision tree eq . of probabilistic adaboost eq . and of matryoshka . from . right top to bottom \u03c1 bound of boosted tree of simple trees given by eq . i.e. \u03b5 it appears that it can not be very much improved in the following sense consider a probabilistic learner with error \u03b5 independently of the weights d with which it is trained . this learner verifies the probabilistic weak learner hypothesis . now for both the probabilistic adaboost and for a decision tree h is a binomial random variable with parameters and the number of weak parameters traversed by x. this second parameter is t for adaboost and log for the decision tree . it is clear then that the decision tree requires exponentially more weak classifiers than the probabilistic adaboost . this bound is especially bad for very weak classifiers . the full curves in figure left plot the bound f for \u03c1 and . for comparison the expected error bound of adaboost \u03c1t plotted alongside is much lower especially for \u03c1 . this bound calls the attention of designers of decision trees tempted to pass all the training dataset along all branches if the weak classifier is very weak the number of needed weak classifiers may grow very much . with stronger classifiers the boosting tree algorithm may be more practical . matryoshka decision trees based on the conclusion of the previous section that stronger classifiers yield better boosted decision trees we now address the question of obtaining sufficiently strong classifiers . the first step in this direction is to explore the idea of putting a boosted tree at each node . we will see that there is an advantage in doing so . it will then be natural in section . to build trees of trees of trees of ... of weak classifiers that is a matryoshka of decision trees . bound for a tree of trees in this section we study the error bounds obtainable by a decision tree built using the method of section but where the nodes are themselves trees built according to that slightly expanded version of my ecml submission same method . we place ourselves in the situation of having the resource to train a fixed number t of weak classifiers and our objective is to minimize the bound on the expected error . in this context it is natural to study the bound obtainable by assembling t subtrees of fixed size t t t. by eq . the error bound for the sub trees is f and that of the outer tree is f t t f. figure right plots this bound plotted against t. the curves show that for t and t t the bound is the same as f i.e. that of a not nested decision tree . more interestingly for intermediate values of t the bound of eq . is always lower than f. in particular the minimum is always near t t. given these encouraging results we are naturally tempted to substitute the sub s.t. sub trees of sub sub trees of size t t. the same idea can also be applied to the outer tree . trees byt t t for some t t. bound for a tree of trees ... of trees of weak classifiers more generally we are tempted to determine the bounds reachable by trees of trees of ... of trees of weak classifiers . for some l and t t. . tl s.t. tt . tl t the bound is easily shown to be f tl f. l may not be easy . finding analytically the optimal combination of ti but guided by the observation that for l the optimal choice seems to be near t t t we naturally consider the case t t. tl t l. in this case the bound is i f l f t l. f t l \u03c1 t. the black graph in figure plots this value against the nesting level l with the original bound f for comparison . this figure clearly shows that deeper nesting levels improve the bound . in fact eq . continues to decrease for l log t i.e. when the trees each have less than two nodes . this effect is due to the fact that f is defined for any positive real t. since the number of nodes is in an integer there are no practical repercussions . however these curves clearly indicate that smaller sub trees yield better bounds . this suggests building the smallest possible trees with just two nodes each node a tree with two nodes etc until the last level consisting of trees with two weak classifiers . bound for matryoshka we now derive the expected error bound for the matryoshka tree having exactly two nodes at all nesting levels having precisely two nodes . we thus need to assume that t l is a power of two . slightly expanded version of my ecml submission bound on error of nested trees vs. nesting level t ^ bound on error of nested trees vs. nesting level t ^ d n u o b r o r r e. d n u o b r o r r e. nesting level nesting level figure black curve bound on error of matryoshka decision trees at various levels of nesting . the sub tree sizes are t. light colored curves near the black curve are for trees w integer number of nodes . the topmost line marks the error bound of the decision tree . at left t and t at right . these curves are for \u03c1 . . i.e. \u03b5 we call m f f the bound for this tree . recalling from eq . that f \u03c1 \u03c1 writes m as a polynomial of degree t. \u03c1 figure left shows the graph of m in dashed lines . this figure shows that the matryoshka tree has a much stronger boosting ability than the plain boosting tree and this is the main result of this paper . building a matryoshka the algorithm for the matryoshka would thus be train a two leaf tree and collect the leaves into a single node . train a two leaf sub tree on one of the branches collect its leaves in a single node . collect the leaves once more etc. if all weak classifiers have the same edge \u03b5 then this approach is the most appropriate . in practice the classifiers will not have the same edge and a greedy with respect to number of nodes or physical training time bound decreasing approach could be considered . each time a classifier is added to the tree we will consider each subtree containing that node starting from the top . for each sub tree we compare the instantaneous bound decrease rate of the sub tree at t csimple c t with that of a tree having such a sub tree at each node cmatryoshka t f t t c. if the later is smaller then the leaves of the sub tree are collected into a single node . we now give the detail of computing eq . using the relation \u03c8 where \u03c8 is the digamma function \u03c8 b \u03c8 x b x log here we consider the decrease rate per added node but the decrease rate per unit of training time could be used too . slightly expanded version of my ecml submission one gets f t f \u03c8 \u03c8 and f \u03c1 f. t the first line above then gives t f t t c t c \u03b3 \u03c8 c where \u03b3 \u03c8 . is euler s constant . one can check that for t csimple cmatryoshka and that if c f i.e. if the bound is tight then csimple cmatryoshka for all t. discussion and conclusions we have developed in this paper a theory of probabilistic boosting aimed at decision trees . we proposed a boosting tree algorithm and a theoretically superior matryoshka decision tree algorithm . these algorithms are essentially parameter free owing to the principle of choosing whichever training action most reduces the expected training error bound and to a judicious choice of possible training actions . we showed bounds on the expected training error of the algorithms one of them discouraging the other encouraging . the bounds for simple trees and for trees of trees are coherent with our early experiments . future developments include an analysis of the effect of approximating the node branching probabilities q during training and experimental evaluation of the matryoshka . on a more general level we believe that the high bound for boosting trees indicates that the probabilistic weak learner hypothesis is inadequate . this hypothesis directly adapted from the theory of boosting does not take into account the fact that real world classifiers usually have a lower training error on smaller training sets . our intuition is thus that the entropy of the training weights d should be taken into account in future work ."
    ],
    "abstract": [
        "we present a theory of boosting probabilistic classifiers . we place ourselves in the situation of a user who only provides a stopping parameter and a probabilistic weak learner classifier and compare three types of boosting algorithms probabilistic adaboost decision tree and tree of trees of ... of trees which we call matryoshka . nested tree embedded tree and recursive tree are also appropriate names for this algorithm which is one of our contributions . our other contribution is the theoretical analysis of the algorithms in which we give training error bounds . this analysis suggests that the matryoshka leverages probabilistic weak classifiers more efficiently than simple decision trees ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.6181818181818182
    ]
}