{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0609093v1",
    "article": [
        "i \u03c0i \u03b3i \u01eb for all i \u01eb for all i. kl \u01eb for all i i \u01eball for all i i. then letting p denote the \u03c0 mixture of the pi s and q the \u03b3 mixture of the qi s for any \u01eb \u01eb we have kl \u01eb \u01eb \u01eb \u01eb \u01eb . i more precisely our goal is to apply this proposition with parameters \u01eb k\u01ebwts \u01eb \u01ebwts \u01eb \u01ebminwt \u01eb \u01ebvars n \u03c3 i min \u01eb \u01eb wts . \u01eb min \u01eball n max \u00b5 \u03c3 \u03c3 min max to satisfy the conditions of the proposition we must upper bound bound \u03c0i for all i upper bound kl for all i. we now do this . \u03c0i \u03c0i for all i lower \u01ebminwt and upper bound upper bounding \u03c0i \u01ebwts lower bounding \u03c0i . in it is also shown that \u03c0i we get \u03c0i \u03c0i \u03c0i . a straightforward argument given in shows that assuming k\u01ebwts . xi for all i such that \u03c0i . consider some particular \u00b5i j \u02c6\u03c3i \u01ebminwt \u01ebmeans \u00b5max by the definition of \u00b5i \u01ebmeans \u01ebvars . let p and q be the one dimensional gaussians with assuming that \u01ebwts \u01ebminwt . fix an i such that \u03c0i j and \u03c3i \u02c6\u00b5i j so we have j \u00b5i j we have that j upper bounding kl xi and fix any j \u01ebwts kl \u01ebvars \u03c3 min \u01eb means \u01ebvars . min each xi is the product of n such gaussians . since kl divergence is additive for product distributions we have the following bound for each i such that \u03c0i \u01ebminwt kl n \u01ebvars \u03c3 min \u01eb means \u01ebvars min . upper bounding kl bounded it follows from fact and proposition that we have . using the fact that both xi and xi are kl n max \u00b5 \u03c3 \u03c3 min max . proposition now gives us kl n \u01ebvars \u03c3 \u01eb means \u01ebvars k\u01ebminwt n max \u00b5 \u03c3 \u03c3 min max r min where r k\u01eb ln \u01eb \u01eb \u01eb \u01eb k \u01eb x the first of these two terms is at most k of these terms is at most k\u01eb min wts \u01eb wts ln \u01eb \u01eb k\u01ebwts wts \u01eb k\u01ebwts . using the fact that ln x x for wts . using the fact that \u01ebwts the second wts . so r is at most k\u01eb wts and the theorem is proved . getting a list of distributions one of which is kl close to the target in this section we show that combining the conversion procedure from the previous subsection with the results of section lets us obtain the following theorem let z be any unknown mixture of k o axis aligned gaussians over rn . there is an algorithm with the following property for any \u01eb \u03b4 given samples from z the algorithm runs in poly \u03b4 outputs a list of poly many mixtures of gaussians with the following properties log time and with probability . for any m \u00b5max such that m poly every distribution z in the list satisfies exp z polyn for all x m m n. some distribution z in the list satisfies kl \u01eb . note that theorem guarantees that z has bounded mass only on the range n whereas the support of z goes beyond this range . this issue is addressed in the proof of theorem where we put together theorem and the maximum likelihood procedure . proof of theorem we will use a specialization of theorem in which we have di\ufb00erent parameters for the di\ufb00erent roles that \u01eb plays theorem let z be a mixture of k o axis aligned gaussians x. xk over rn de\u03c3i \u03c0i scribed by parameters \u02c6\u03c3i \u01ebvars for all i j such that \u03c0i \u01ebminwt . the algorithm runs in time poly . \u01ebwts \u01ebmeans \u01ebvars \u01ebminwt log where \u01eb min \u03c0i \u02c6\u03c0i \u02c6\u00b5i j \u00b5i j \u01ebwts for all i and \u01ebmeans and \u01ebvars for all i j such that \u03c0i \u01ebminwt . we now pass each of these candidate parameter settings through theorem . by theorem for any m poly all the m m n. resulting distributions will satisfy exp is at most \u01eb . thus namely n \u03c3 \u01eb so at least one of the resulting distributions z satisfies kl min z \u01ebvars \u03c3 min z n where m \u00b5max is any poly . \u03b4 satisfies kl \u01eb . proposition let p and q be any mixtures of n dimensional gaussians . let pm denote the m truncated version of p. for some m poly we have \u01eb \u01eb kl pm kl q z \u01eb . this proposition implies that kl zm now run the ml algorithm with m poly log on this list of hypothesis distributions using zm as the target distribution . note that running the algorithm with zm as the target distribution lets us assert that all hypothesis distributions have pdfs bounded above and below on the support of the target distribution as is required by theorem . by \u03b4 the ml algorithm outputs a hypothesis zml such that theorem with probability at least kl . by proposition we have zml \u01eb \u01eb \u01eb zml kl z kl z which implies that kl \u01eb . the running time of the overall algorithm is o extensions to other distributions in this paper we have shown how to pac learn mixtures of any constant number of distributions each of which is an n dimensional gaussian product distribution . this expands upon the work by feldman et al. which worked for discrete distributions in place of gaussians . it should be clear from our work that in fact many nice univariate distributions can be handled similarly . also it should be noted that the n coordinates need not come from the same family of distributions for example our methods would handle mixtures where some attributes had discrete distributions and the remainder had gaussian distributions . what level of niceness do our methods require for a parameterized family of univariate distributions on r first and foremost it should be amenable to the method of moments from statistics . by this it is meant that it should be possible to solve for the parameters of the distribution given a constant number of the moments . distributions in this category include gamma distributions chi square distributions beta distributions exponential more generally weibull distributions and more . as a trivial example the unknown parameter of an exponential distribution is simply its mean . as a slightly more involved example given a beta distribution with on these unknown parameters \u03b1 and \u03b2 \u03b2 \u03b1 e e var \u03b2 e var . so long as the univariate distribution family can be determined by a constant number of moments our basic strategy of running wam multiple times to determine moment estimates and then taking the cross products of these lists can be employed . there are only two more concerns that need to be addressed for a given parameterized family of distributions . first one needs an analogue of proposition showing that products of independent random variables from the distribution family are efficiently samplable . this immediately holds for any distribution with bounded support it will also typically hold for reasonable probability distributions that have pdfs with rapidly decaying tails . second one needs an analogue of theorem . this requires that it should be possible to convert accurate candidate parameter values into a kl close actual distribution . it seems that this will typically be possible so long as the distributions in the family are not highly concentrated at any particular point . the conversion procedure should also have the property that the distributions it output have pdfs that are bounded below above by at most exponentially small large values at least on polynomially sized domains . this again seems to be a mild constraint satisfiable for reasonable distributions with rapidly decaying tails . in summary we believe that for most parameterized distribution families d of interest performing a small amount of technical work should be sufficient to show that our methods can learn mixtures of products of d s. we leave the problem of checking these conditions for distribution families of interest as an avenue for future research ."
    ],
    "abstract": [
        "we propose and analyze a new vantage point for the learning of mixtures of gaussians namely the pac style model of learning probability distributions introduced by kearns et al. here the task is to construct a hypothesis mixture of gaussians that is statistically indistinguishable from the actual mixture generating the data specifically the kl divergence should be at most epsilon . in this scenario we give a poly time algorithm that learns the class of mixtures of any constant number of axis aligned gaussians in n dimensional euclidean space . our algorithm makes no assumptions about the separation between the means of the gaussians nor does it have any dependence on the minimum mixing weight . this is in contrast to learning results known in the clustering model where such assumptions are unavoidable . our algorithm relies on the method of moments and a subalgorithm developed in previous work by the authors for a discrete mixture learning problem ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.42138364779874216
    ]
}