{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0507044v1",
    "article": [
        "ave with probability at least k p \u2113t k \u03b4t i e ki q \u2113 foe t t ki \u2113i \u03b7t \u03c4 i t xt xt t xt \u02c6bt \u03b3t\u03b7t \u02c6b t \u03b3tbt t v u u t \u02c6bt v u u t xt t xt b. t consequently in expectation we have e\u2113 foe t t ki \u2113i \u03b7t \u03c4 i t \u02c6bt \u03b3t\u03b7t \u02c6b t xt xt t xt \u03b3tbt v u u t xt t xt proof . this follows by summing up all excess terms in the above lemmas . recall that we only need to take expectations on both sides of the assertions of lemmas in order to obtain the second bound on the expectation . corollary . assume the conditions of theorem and choose \u03b7t t \u03b3t t. then and bt bt bt t bt t \u03c4 i \u03c4 i \u03c4 i \u03c4 i \u2113 \u2113i t o foe t \u2113i t o \u2113i t o e\u2113 foe t foe e\u2113 t foe \u2113i t o t \u2113 wi kit kit kit kit foe is asymptotically optimal w.r.t. each expert i.e. for all i ln wi . moreover in both cases bounded and \u02c6b\u03c4 i \u03b1b\u03c4 i \u03b3\u03c4 i wmin \u03c4 i \u03b1 \u03b1\u03b2 \u03b1 wi \u02c6bt \u03c4 i xt . then and follow from \u03b1 \u03b2 and and follow from \u03b1 \u03b2 . the asymptotic optimality finally follows from the borel cantelli lemma since according to and \u03b1 \u2113foe t mini \u2113i t t p ct ln t t for an appropriate c. and \u03b7t t as mentioned in the first paragraph of this section it is possible to avoid lemma thus arriving at better bounds . e.g. in choosing \u03c4 i \u03b3t t kit can be shown . a regret bound of o of course also a corresponding high probability bound like holds . likewise \u03b3t t for a similar statement as we may set \u03c4 i bt t and \u03b7t t wi kit generally in this is possible at the cost of increasing c way any regret bound o. where \u03b5 arriving at a regret bound of o c kit \u03b5 set t for t. invoke foe and play its decision for bt basic time steps set t t bt fig . the algorithm foe where bt is specified in corollary . g reactive environments and a universal master algorithm regret can become a quite subtle notion if we start considering reactive environments i.e. care for future consequences of a decision . an extreme case is the heaven hell example we have two experts one always playing the other one always playing . if we always follow the first expert we stay in heaven and get no loss in each step . as soon as we curse only once we get into hell and receive maximum loss in all subsequent time steps . clearly any algorithm without prior knowledge must fail in this situation . one way to get around this problem is taking into account the actual game we are playing . for instance after cursing once also the praying expert goes to hell together with us and subsequently has maximum loss . \u2113i hence were are interested in a regret defined as e\u2113 t t as in the previous section . so what is missing this becomes clear in the following example . consider the repeated prisoner s dilemma against the tit for tat strategy . if we use two strategies as experts namely always cooperate and always defect then it is clear that always cooperating will have the best long term reward . however standard expert advice or bandit master algorithm will not discover this since it compares only the losses in one step which are always lower for the defecting expert . to put it di\ufb00erently minimizing short term regret is not at all a good idea here . e.g. always defecting has no regret while for always cooperating the regret grows linearly . but this is only the case for short term regret i.e. if we restrict to time intervals of length one . single games and receives loss \u2113i we therefore give the control to a selected expert for periods of increasing length . precisely we introduce a new time scale t at which we have single games with losses \u2113 t. the master s time scale t does not coincide with t. instead at each t the master gives control to the selected expert i \u2113i t. assume that the game has bounded instantaneous losses \u2113i . then the master algorithm s instantaneous losses in the prisoner s dilemma two players both decide independently if thy are cooperating or defecting . if both play c they get both a small loss if both play d they get a large loss . however if one plays c and one d the cooperating player gets a very large loss and the defecting player no loss at all . thus defecting is a dominant strategy . tit for tat plays c in the first move and afterwards the opponent s respective preceding move . t bt t t t t p are bounded by bt . we denote the algorithm which is completely specified foe . then the following assertion is an easy consequence of the in fig . by previous results . g corollary . assume losses \u2113i then for all experts i and all t. choose \u03b3t t t g \u03b7t t foe plays a repeated game with bounded instantaneous . and \u03c4 i bt t w.p. t and foe \u2113 t g foe e \u2113 t g \u2113i t o \u2113i t o wi ki t ki t ki t ki t. since accesa bianchi et al. have shown a lower regret bound of oc kit even if the bounds in particular this is almost tight except for the additive c term . it is wi and t. wi seem not practical maybe foe would learn sufficiently quickly in practice anyway we believe that this is not so in most cases the design of foe is too much tailored towards worst case environments foe is too defensive . assume that we have a good and a bad expert and foe learns this fact after some time . then it still would spend a relatively huge fraction of \u03b3t t to exploring the bad expert . such defensive behavior seems only acceptable if we are already starting with a class of good experts . acknowledgment . this work was supported by snf grant . and jsps st century coe program c."
    ],
    "abstract": [
        "this paper shows how universal learning can be achieved with expert advice . to this aim we specify an experts algorithm with the following characteristics it uses only feedback from the actions actually chosen it can be applied with countably infinite expert classes and it copes with losses that may grow in time appropriately slowly . we prove loss bounds against an adaptive adversary . from this we obtain a master algorithm for reactive experts problems which means that the master s actions may influence the behavior of the adversary . our algorithm can significantly outperform standard experts algorithms on such problems . finally we combine it with a universal expert class . the resulting universal learner performs in a certain sense almost as well as any computable strategy for any online decision problem . we also specify the convergence speed which is very slow ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.3082191780821918
    ]
}