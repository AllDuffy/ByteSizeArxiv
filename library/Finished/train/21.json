{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0606093v1",
    "article": [
        "the standard goal for predictor in learning theory is to incur a small loss for a given loss function measuring the discrepancy between the predictions and the actual outcomes . competitive on line learning concentrates on a relative version of this goal the predictor is to perform almost as well as the best strategies in a given benchmark class of prediction strategies . such predictions can be interpreted as decisions made by a small decision maker . predictions or probability forecasts considered in the foundations of probability are statements rather than decisions the loss function is replaced by a procedure for testing the forecasts . the two main approaches to the foundations of probability are measure theoretic and game theoretic the former is now dominant in mathematical probability theory but the latter appears to be better adapted for uses in learning theory discussed in this paper . an important achievement of kolmogorov s school of the foundations of probability was construction of a universal testing procedure and realization that there exists a forecasting strategy that produces ideal forecasts . levin s ideal forecasting strategy however is not computable . its more practical versions can be obtained from the results of game theoretic probability theory . for a wide class of forecasting protocols it can be shown that for any computable game theoretic law of probability there exists a computable forecasting strategy that produces ideal forecasts as far as this law of probability is concerned . choosing suitable laws of probability we can ensure that the forecasts agree with reality in requisite ways . probability forecasts that are known to agree with reality can be used for making good decisions the most straightforward procedure is to select decisions that are optimal under the forecasts . this gives inter alia a powerful tool for competitive online learning i will describe its use for designing prediction algorithms that satisfy the property of universal consistency and its more practical versions . in conclusion of the paper i will discuss some limitations of competitive on line learning and possible directions of further research . contents introduction universal consistency defensive forecasting calibration and resolution implications for probability theory defensive forecasting for decision making asymptotic theory defensive forecasting for decision making loss bounds conclusion introduction this paper is based on my invited talk at the th annual conference on learning theory . in recent years colt invited talks have tended to aim at establishing connections between the traditional concerns of the learning community and the work done by other communities . following this tradition i will argue that some ideas from the foundations of probability can be fruitfully applied in competitive on line learning . in this paper i will use the following informal taxonomy of predictions d predictions are mere decisions . they can never be true or false but can be good or bad . their quality is typically evaluated with a loss function . s predictions are statements about reality . they can be tested and if found inadequate rejected as false . f predictions are intermediate between d predictions and s predictions . they are successful if they match the frequencies of various observed events . traditionally learning theory in general and competitive on line learning in particular consider d predictions . i will start in section from a simple asymptotic result about d predictions there exists a universally consistent on line prediction algorithm . section is devoted to s prediction and section to f prediction . we will see that s prediction is more fundamental than and can serve as a tool for f prediction . section explains how f prediction is relevant for d prediction . in section i will prove the result of section about universal consistency as well as its non asymptotic version . universal consistency in all prediction protocols in this paper every player can see the other players moves made so far . the most basic one is prediction protocol for n. x. reality announces xn predictor announces \u03b3n y. reality announces yn \u03b3 . end for . at the beginning of each round n predictor is given some data xn relevant to predicting the following observation yn xn may contain information about n and the previous observations yn yn . the data is taken from the data space x and the observations from the observation space y. the predictions \u03b3n are taken from the prediction space \u03b3 and a prediction s quality in light of r. this the actual observation is measured by a loss function \u03bb x is how we formalize d predictions . the prediction protocol will sometimes be referred to as the prediction game . y \u03b3 we will always assume that the data space x the prediction space \u03b3 and the observation space y are non empty topological spaces and that the loss function \u03bb is continuous . moreover we are mainly interested in the case where x \u03b3 and y are locally compact metric spaces the prime examples being euclidean spaces and their open and closed subsets . traditionally only loss functions \u03bb \u03bb that do not depend on x are considered in learning theory and this case appears to be most useful and interesting . the reader might prefer to concentrate on this case . predictor s total loss over the first n rounds is n n \u03bb . as usual in competitive on line prediction predictor competes with a wide range of prediction rules d x total loss of such a prediction rule is goal is to achieve \u03b3 . the n n \u03bb and so predictor s n n x p n n x \u03bb \u03bb for all n. and as many prediction rules d as possible . predictor s strategies in the prediction protocol will be called on line predic tion algorithms . remark some common prediction games are not about prediction at all as this word is usually understood . for example in cover s game of sequential investment with k stocks y k \u03b3 is a test of randomness if it is lower semicontinuous and p for all p where p p p p tp . z\u03c9 the intuition behind this definition is that if we first choose a test t then observe \u03c9 and then find that t is very large for the observed \u03c9 we are entitled to reject the hypothesis that \u03c9 was generated from p. the following fundamental result is due to levin although our proof is slightly di\ufb00erent . lemma let \u03c9 be a metric compact . for any test of randomness t there exists a probability measure p such that \u03c9 \u03c9 t. i\u00b5 where i\u00b5 before proving this result let us recall some useful facts about the probability measures on the metric compact \u03c9 . the banach space of all continuous functions on \u03c9 with the usual pointwise addition and scalar action and the sup norm will be denoted c. by one of the riesz representation theorems the mapping \u00b5 between the set of all finite borel measures \u00b5 on \u03c9 with the total variation norm r and the dual space c to c with the standard dual norm . we will identify the finite borel measures \u00b5 on \u03c9 with the corresponding i\u00b5 we will be interested however in a di\ufb00erent topology on c the weakest topology for which all evaluation functionals \u00b5 c are continuous . this topology is known as the weak topology and is known as the topology of weak convergence the topology inherited by . the point mass \u03b4\u03c9 \u03c9 \u03c9 is defined to be the probability . the simple example of a sequence measure concentrated at \u03c9 \u03b4\u03c9 \u03c9 \u03c9 for all n shows of point masses \u03b4\u03c9n such that \u03c9n a convex closed subset of c. c. this makes and \u03c9n \u00b5 f c \u03c9 as n p p it is not difficult to check that \u03b4\u03c9 holds in one but does not hold in the other . that the topology of weak convergence is di\ufb00erent from the dual norm topology \u03b4\u03c9n remains a closed subset of c in the weak topology . by the banach alaoglu theorem is compact in the topology of weak convergence . in the rest of this paper are always equipped with the topology of weak convergence . p p p since \u03c9 is a metric compact is also metrizable . p proof of lemma if t takes value p q set p redefine it as t min . for all \u03c6 tq . z\u03c9 the function \u03c6 is linear in its first argument q and lower semicontinuous in its second argument p. ky fan s minimax theorem is a compact convex subset shows that there exists p such that p p q p \u03c6 sup p p \u03c6 . therefore q p tq z\u03c9 and we can see that t never exceeds . this proof used the following topological lemma . r is a non negative lower semicontinuous lemma suppose f x function defined on the product of two metric compacts x and y. if q is a y f q is also lower probability measure on y the function x semicontinuous . r x y f as n proof the product x y is also a metric compact . according to hahn s theorem there exists a nondecreasing sequence of continuous functions fn such that y. since each fn is unifor all fn y fnq are continuformly continuous the functions ous and by the monotone convergence theorem they converge to r y f q is lower y f q. therefore again by hahn s theorem semicontinuous . r r x lemma says that for any test of randomness t there is a probability forecast p such that t never detects any disagreement between p and the outcome \u03c9 whatever \u03c9 might be . gacs defines a uniform test of randomness as a test of randomness that is lower semicomputable . he proves that there exists a universal uniform test of randomness . if t for a fixed universal test t \u03c9 is said to be random with respect to p. applied to the universal test lemma says that there exists a neutral probability measure p such that every \u03c9 is random with respect to p. gacs shows that under his definition there are no neutral measures that are computable even in the weak sense of upper or lower semicomputability even for \u03c9 the compactified set of natural numbers . levin s original definition of a uniform test of randomness involved some extra conditions which somewhat mitigate the problem of non computability . testing predictions in game theoretic probability there is an obvious mismatch between the dynamic prediction protocol of section and the one step probability forecasting setting of the previous subsection . if we still want to fit the former into the latter perhaps we will have to take the infinite sequence of data and observations x y x y. as \u03c9 and so take y. to find a probability measure satisfying a useful property such \u03c9 is the predicted probability that with probability one where pn pn in game theoretic probability theory the binary strong law of large numbers k is stated as follows skeptic has a strategy that when started with never risks bankruptcy and makes skeptic infinitely rich when is violated . we prove many such game theoretic laws of probability in all of them exhibit strategies for skeptic that make him rich when some property of agreement between the forecasts and the actual observations is violated . when forecaster plays the strategy of defensive forecasting against such a strategy for skeptic the property of agreement is guaranteed to be satisfied no matter how reality plays . in the next section we will apply the procedure of defensive forecasting to a law of large numbers found by kolmogorov in . calibration and resolution in this section we will see how the idea of defensive forecasting can be used for producing f predictions . it is interesting that the pioneering work in this direction by foster and vohra was completely independent of levin s idea . the following is our basic probability forecasting protocol . probability forecasting protocol for n. reality announces xn forecaster announces pn p reality announces yn y. x. end for . forecaster s prediction pn is a probability measure on y that intuitively describes his beliefs about the likely values of yn . forecaster s strategy in this protocol will be called a probability forecasting strategy . asymptotic theory of calibration and resolution the following is a simple asymptotic result about the possibility to ensure calibration and resolution . theorem suppose x and y are locally compact metric spaces . there is a probability forecasting strategy that guarantees and y y. n prove but not obvious . on the other hand their continuity is obvious . k \u03c9 \u03c9 k without a finite subcover . if xn are chosen from only finitely many p p. a s ness and convexity conditions they exist by the following lemma . lemma let x be a paracompact y be a non empty compact convex subset r be a continuous function such of a topological vector space and f x y that f is convex in y continuous approximate choice function g x y for each x x. for any \u01eb there exists a y such that f \u01eb . x x f inf y y proof each bx y are open sets in x and y respectively and y has a neighborhood ax y x bx y such that ax y and sup ax y bx y f inf ax y bx y f \u01eb . for each x y of sets ax constitute an open cover of x such that x choose a finite subcover of the cover y and let ax be the intersection of all ax y in this subcover . the ax y bx y ax y y x x on line prediction algorithm achieving and \u03b4 \u03bb \u03bb n n x f n c\u03bb \u03b4 for any randomized prediction rule d x n ln r q \u03b4 with probability at least gn and dn are independent random variables distributed as \u03b3n and d p respectively . f f. if the above results are non vacuous only when \u03bbd is an element of the function space is a sobolev space this condition follows from d being in the sobolev space and the smoothness of \u03bb . for example moser proved in the following result concerning composition in sobolev spaces . let \u03c9 be a smooth bounded domain in rk and m be an integer number satisfying m k. if h m. cm then \u03c6 h m and \u03c6 two special cases of calibration cum resolution in the chain we applied the law of large numbers twice in the third and fifth equalities . it is easy to see however that in fact the fifth equality depends only on resolution and the third equality although it depends on calibration cum resolution involves a known function f. we will say that the fifth equality depends on general resolution whereas the third equality depends on specific calibration cum resolution . this limited character of the required calibrationcum resolution becomes important for obtaining good bounds on the predictive performance in the following subsections we will construct prediction algorithms that satisfy the properties of specific calibration cum resolution and general resolution and merge them into one algorithm we will start from the last step . synthesis of prediction algorithms the following corollary of lemma will allow us to construct prediction algorithms that achieve two goals simultaneously . corollary let y be a metric compact and \u03c6n j x hj n. j be functions taking values in hilbert spaces hj and such that \u03c6n j is continuous in for all n and both j. let a and a be two positive constants . there is a probability forecasting strategy that guarantees p y n n x \u03c8n j hj n aj a k \u03c8n k h a k \u03c8n k h n x for all n and for both j and j where \u03c8n j \u03c6n j \u03c6n j p. y z proof define the weighted direct sum product h h equipped with the inner product h of h and h as the cartesian g g h ih h ajh gj g jihj . j x now we can define \u03c6 x y p h by \u03c6n . it is clear that \u03c6n is continuous in for all n. applying the strategy of lemma to it and using we obtain aj \u03c8n j n n x hj n n x n n x n \u03c8n \u03c8n \u03c8n h \u03c8n h k k n x n h n x suppose x \u03b3 y are metric compacts and on x functions satisfying y. let gn x p n aj k \u03c8n j k hj . n x j x is a forecast continuous rkhs \u03b3 be a sequence of approximate choice f \u03bb inf \u03b3 \u03b3 \u03bb n x x p p. corollary will be applied to a a and to the mappings \u03c8n \u03bb \u03c8n kx y where kx y is the evaluation functional at for kx y with respect to p. it is easy to see that kx p f \u03bb and kx p is the mean of \u03c8n \u03c8n c\u03bb \u03c8n cf. k kf kr k specific calibration cum resolution corollary immediately implies lemma the probability forecasting strategy of corollary based on and guarantees \u03bb \u03bb \u03bb c c f n. proof this follows from \u03bb \u03bb q n n x \u03bb c c f general resolution i the following lemma is proven similarly to lemma . lemma the probability forecasting strategy of corollary based on and guarantees \u03bb \u03bb \u03bb c c \u03bbdkf f k n. q n x n n n x. n n x proof this follows from \u03bb \u03bb n \u03bbd \u03bbd n x kxn pn if \u03bbd kxn yn h n n x n n x n f \u03bbdkf n \u03bb c c f k q \u03bbdkf v u u t. k n x proof of theorem let \u03b3n gn where pn are produced by the probability forecasting strategy of corollary based on and . following and using the previous two lemmas we obtain \u03bb \u03bb n n x \u03bb n n x n n x n n x n n x n n x n n x n n x n n x \u03bb \u03bb \u03bb \u03bb c c f n q \u03bb \u03bb c c f n \u03bb \u03bb c c f n \u03bb \u03bb \u03bb \u03bb c c \u03bbdkf n. f k q q q proof of corollary since \u03bb inequality shows that \u03bb never exceeds c\u03bb in absolute value hoe\ufb00ding s n p n x n n x \u03bb \u03bb \u03bb \u03bb t exp t c \u03bbn for every t. choosing t satisfying t c \u03bbn exp \u03b4 t c\u03bb ln n \u03b4 i.e. r we obtain the statement of corollary . general resolution ii to prove theorem we will need the following variation on lemma . lemma the probability forecasting strategy of corollary based on and guarantees n for any f n x. f f f p y z \u03bb c c f f k kf n q proof following the proof of lemma f f pn y z f kxn yn h kxn pn if n n x n n x f k kf v u u t n x f k n n x kf \u03bb c n f kf \u03bb c c f f k n. q proof of theorem as in the proof of theorem we first assume that x \u03b3 and y are compact . let us first see that the prediction algorithm of theorem fed with a suitable rkhs guarantees the consequent of for all continuous d. let be a universal and continuous rkhs on x y with finite imbedding constant cf. f fix a continuous decision rule d x \u03b3 . for any \u01eb we can find a y to \u03bb . following and function f the similar chain in the proof of theorem we obtain that is \u01eb close in c x f \u03bb \u03bb n n x \u03bb n n x n n x n n x n n x n n x n n x n n x n n x n n x n n x \u01ebn . \u03bb \u03bb \u03bb \u03bb c c f n q \u03bb \u03bb c c f n \u03bb \u03bb c c f n \u03bb \u03bb \u03bb \u03bb c c f n f f pn \u01ebn y z \u03bb \u03bb c c f f n q q q q we can see that lim sup n n n n x \u03bb \u03bb \u01eb since this is true for any \u01eb the consequent of holds . n n n x it remains to get rid of the assumption of compactness of x \u03b3 and y. we will need the following lemma . x and b lemma under the conditions of theorem for each pair of compact sets y there exists a compact set c c \u03b3 such that for a each continuous prediction rule d x \u03b3 there exists a continuous prediction rule d x c that dominates d in the sense x a y b \u03bb \u03bb . proof without loss of generality a and b are assumed non empty . fix any \u03b3 \u03b3 . let m sup a b \u03bb let c \u03b3 be a compact set such that x a \u03b3 c y b \u03bb m let m sup a c b \u03bb . and let c \u03b3 be a compact set such that x a \u03b3 c y b \u03bb m. m and \u03b3 c we have \u03bb c. b and \u03b3 c let us now check that c lies inside the interior of c. indeed for any fixed m since \u03bb m it is obvious that m a for all \u03b3 c some neighborhood of \u03b3 will lie completely in c. \u03b3 be a continuous prediction rule . we will show that holds for some continuous prediction rule d taking values in the compact set c. namely we define let d x d \u03c1 if d if d \u03c1 \u03c1 \u03c1 \u03b3 d \u03c1 \u03c1 d \u03b3 c c c c \u03b3 c where \u03c1 is the metric on \u03b3 the denominator \u03c1 \u03c1 d \u03b3 is always positive since already \u03c1 is positive . assuming c convex we can see that d indeed takes values in c. the only points x at which the continuity of d is not obvious are those for which d lies on the boundary of c one has to use the fact that c is covered by the interior of c. if d it remains to check the only non trivial case is d convexity of \u03bb in \u03b3 the inequality in will follow from c c. by the \u03c1 d \u03b3 \u03c1 \u03c1 \u03bb c \u03c1 \u03c1 \u03c1 \u03bb \u03bb i.e. \u03bb \u03bb . since the left hand side of the last inequality is at most m and its right hand side exceeds m it holds true . s a x and b b for given compact a y fix a compact c x and b y as evader . for each pair of compact a \u03b3 as in the lemma . similarly to the proof of theorem predictor s strategy y and ensuring is constructed from remover s winning strategy in g x from predictor s strategies c and c under the assumption that ensuring the consequent of for d a y. remover s moves are y. predictor is b for compact a assumed to be of the form a simultaneously playing the game of removal g x outputting predictions \u03b3n x and b predictor asks remover to make his first move a b in the game of removal . predictor then plays the prediction game using the strategy until reality chooses b. as soon as such is chosen predictor announces in the game of removal and notes remover s response . he then continues playing the prediction game using the strategy until reality chooses let us check that this strategy for predictor will always ensure . if reality chooses outside predictor s current ak bk finitely often the consequent of will be satisfied for all continuous d x c and so by lemma for all continuous d x \u03b3 . bk infinitely often if reality chooses outside predictor s current ak the set of n. will not be precompact and so the antecedent of will be violated . a b etc. s s proof of theorem define \u03bb \u03bb\u03b3 where \u03b3 is a probability measure on \u03b3 . this is the loss function in a new game of prediction with the prediction space for a compact c the loss function is continuous by lemma . we need the following analogue of lemma . when \u03b3 ranges over p p p lemma under the conditions of theorem for each pair of compact sets \u03b3 such that a y there exists a compact set c c x and b z\u03b3 for each continuous randomized prediction rule d x continuous randomized prediction rule d x dominates d on average . p there exists a such that holds . it is clear that d is continuous in the and p \u03bb \u03bbfd \u03bb fd z\u03b3 \u03bbfd z\u03b3 mfd z\u03b3 z\u03b3 \u03bbfd \u03bbfd \u03bb z\u03b3 for all a b. z\u03b3 fix one of the mappings lemma . c whose existence is asserted by the p we will prove that the strategy of the previous subsection with in place of c applied to the new game is universally consistent . let d x be a continuous randomized prediction rule i.e. a continuous prediction rule in the new game . let be remover s last move and let d x be a continuous randomized prediction rule satisfying with a ak and b bk . from some n concentrated on on our randomized prediction algorithm produces \u03b3n p c and they will satisfy p p lim sup n n n n x \u03bb \u03bb n n n x lim sup n n n n x \u03bb \u03bb . n n n x the loss function is bounded in absolute value on the compact set ak bk by a constant c. the law of the iterated logarithm d c implies that n n \u03bb lim sup n n n \u03bb lim sup n p with probability one . combining the last two inequalities with gives lim sup n n n n x \u03bb \u03bb a.s. n n n x this immediately implies . conclusion in this section i will list what i think are interesting directions of further research . the data space as a bottleneck it is easy to see that if we set x n yn and xn p it becomes impossible to compete even with the simplest prediction rules d x y there needs be no connection between the restrictions of d to yn for di\ufb00erent n. the requirement that y. yn should be compressed into an element xn of a locally compact space x restricts the set of possible prediction rules so that it becomes manageable . we can consider x to be the necessary bottleneck in our notion of a prediction rule and the requirement of local compactness of x makes it narrow enough for us to be able to compete with all continuous prediction rules . a natural question is can the requirement of the local compactness of x be weakened while preserving the existence of on line prediction algorithms competitive with the continuous prediction rules randomization it appears that various aspects of randomization in this paper and competitive on line prediction in general deserve further study . for example the bound of corollary is based on the worst possible outcome of predictor s randomization and the best possible outcome of the prediction rule s randomization . this is unfair to predictor . of course comparing the expected values of predictor s and the prediction rule s loss would be an even worse solution this would ignore the magnitude of the likely deviations of the loss from its expected value . it would be too crude to use the variance as the only indicator of the likely deviations and it appears that the right formalization should involve the overall distribution of the deviations . a related observation is that when using a prediction strategy based on defensive forecasting predictor needs randomization only when there are several very di\ufb00erent predictions with similar expected losses with respect to the current probability forecast pn . since pn are guaranteed to agree with reality we would not expect that predictor will often find himself in such a position provided reality is neutral . predictor s strategy will be almost deterministic . it would be interesting to formalize this intuition . limitations of competitive on line prediction in conclusion i will brie\ufb02y discuss two serious limitations of this paper . first the main results of this paper only concern one step ahead prediction . in a more general framework the loss function would depend not only on yn but on other future outcomes as well . there are simple ways of extending our results in this direction e.g. if the loss function \u03bb \u03bb depends on both yn and yn we could run two on line prediction algorithms with the observation space y one responsible for choosing \u03b3n for odd n and the other for even n. however cleaner and more principled approaches are needed . as we noted earlier the general interpretation of dpredictions is that they are decisions made by a small decision maker . to see why the decision maker is assumed small let us consider which the kind of guarantee provided in competitive on line prediction . predictor s and the prediction rule d s losses are compared on the same sequence x y x y. of data and observations . if predictor is a big decision maker the interpretation of becomes problematic presumably x y x y. resulted from predictor s decisions \u03b3n and d s loss should be evaluated on a di\ufb00erent sequence the sequence x. resulting from d s decisions d. x y y the approach of this paper is based on defensive forecasting the ability to produce ideal in important aspects probability forecasts . it is interesting that ideal probability forecasts are not sufficient in big decision making . as a simple example consider the game where there is no x \u03b3 y and the loss function \u03bb is given by the matrix reality s strategy is yn \u03b3n but predictor s initial theory is that reality always chooses yn . y y \u03b3 \u03b3 predictor s optimal strategy based on his initial beliefs is to always choose \u03b3n su\ufb00ering loss at each step . his initial beliefs are reinforced with every move by reality . intuitively it is clear that predictor s mistake in not choosing is that he was being greedy . however he acted optimally given his beliefs his beliefs have been verified by what actually happened . in big decision making we have to worry about what would have happened if we had acted in a di\ufb00erent way . my hope is that game theoretic probability has an important role to play in big decision making as well . a standard picture in the philosophy of science is that science progresses via struggle between theories and it is conceivable that something like this also happens in individual learning . based on good theories we can make good decisions . testing of probabilistic theories is crucial in this process and the game theoretic version of the testing process is much more \ufb02exible than the standard approach to testing statistical hypotheses at each time we know to what degree the theory has been falsified . it is important however that the skeptic testing the theory should not only do this playing the imaginary game with the imaginary capital he should also venture in the real world . predictor s theory that reality always chooses yn would not survive for more than one round had it been tested . big decision making is a worthy goal but it is very difficult to prove anything about it and elegant mathematical results might be beyond our reach for some time . small decision making is also important but much easier in many cases we can do it almost perfectly . acknowledgments i am grateful to the colt co chairs for inviting me to give the talk on which this paper is based . theorems and provide a partial answer to a question asked by nicol o cesa bianchi . this work was partially supported by mrc ."
    ],
    "abstract": [
        "prediction is a complex notion and different predictors can pursue very different goals . in this paper i will review some popular kinds of prediction and argue that the theory of competitive on line learning can benefit from the kinds of prediction that are now foreign to it ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.673469387755102
    ]
}