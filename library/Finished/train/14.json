{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0512050v1",
    "article": [
        "besides the known difficulties of data mining text mining presents specific difficulties due to the structure of natural language . in particular the polysemy and synonymy e\ufb00ects are dealt with by constructing ontologies or terminologies structuring the words and their meanings in the domain application . a preliminary step for ontology construction is to extract the terms or word collocations attached to the concepts defined by the expert . term extraction actually involves two steps the detection of the relevant collocations and their classification according to the concepts . this paper focuses on the detection of relevant collocations and presents a learning algorithm for ranking collocations with respect to their relevance in the spirit of . an evolutionary algorithm termed roger based on the optimization of the receiver operating characteristics curve and already described in previous works is applied to a few collocations manually labelled as relevant irrelevant by the expert . the optimization of the roc curve is directly related to the recall precision tradeo\ufb00 in term extraction . the paper is organized as follows . section brie\ufb02y reviews the main criteria used in te . section presents the roger az e et al. algorithm for the sake of self containedness and describes the bagging of the diverse hypotheses constructed along independent runs . sections et report on the experimental validation of the approach on two real world domain applications and the paper ends with some perspectives for further research . measures for term extraction the choice of a quality measure among the great many criteria used in text mining is currently viewed as a decision making process the expert has to find the criterion most suited to his her corpus and goals . the criteria considered in the rest of the paper are mutual information mutual information with cube dice coefficient log likelihood number of occurrences log likelihood association measure sebag schoenauer j measure conviction least contradiction cote multiplier khi test used in text mining t test used in text mining vivaldi et al. have shown that the search for a quality measure can be formalized as a supervised learning problem . considering a training set where each candidate term is described from its value for a set of statistical criteria and labelled by the expert they used adaboost to automatically construct a classifier . the approach presented in next section mostly di\ufb00ers from as it learns an ordering function instead of a boolean function . learning ranking functions this section first brie\ufb02y recalls the roger algorithm used for learning a ranking hypothesis and first described in . the n roger variant used in this occl is defined by ranking collocations according to their number of occurrences and breaking the ties based on the term log likelihood . preference learning in terminology extraction a roc based approach paper involves two extensions i the use of non linear ranking hypotheii the exploitation of the ensemble of hypotheses learned along inses dependent runs of roger . using the standard notations the dataset e includes n collocation examples where each collocation xi is described by the value of d statistical criteria and its label yi denotes whether collocation xi is relevant . roger the learning criterion used in roger is the wilcoxon rank test measuring the probability that a hypothesis h ranks xi higher than xj when xi is a positive and xj is a negative example w p r this criterion with quadratic complexity in the number n of examples o\ufb00ers an increased stability compared to the misclassification rate p r with linear complexity in n see and references therein . the wilcoxon rank test is equivalent to the area under the roc curve . this curve intensively used in medical data analysis shows the trade o\ufb00 between the true positive rate and the false positive rate achieved by a given hypothesis classifier learning algorithm . therefore the area under the roc curve does not depend on the imbalance of the training set as opposed to other measures such as fscore . the roc curve also shows the misclassification rates achieved depending on the error cost coefficients . for these reasons argues the comparison of the roc curves attached to two learning algorithms to be more fair and informative than comparing their misclassification rates only . accordingly the area under the roc curve defines a new learning criterion used e.g. for the evolutionary optimization of neural nets or the greedy search of decision trees . in an earlier step the search space h considered is that of linear hypotheses . to each vector w in ird is attached hypothesis hw with hw w x where w x denotes the scalar product of w and x. hypothesis h defines an order on ird which is evaluated from the wilcoxon rank test on the training set e measured after cross validation . the combinatorial optimization problem defined by eq . thus mapped onto a numerical optimization problem is tackled by evolution strategies . es are the evolutionary computation algorithms that are best suited actually the computational complexity is in o since w is propor tional to the sum of ranks of the positive examples . az e et al. to parameter optimization the interested reader is referred to for an extensive presentation . in the rest of the paper roger employs a es involving the generation of \u03bb o\ufb00spring from \u00b5 parents through uniform crossover and self adaptive mutation and deterministically selecting the next \u00b5 parents from the best \u00b5 parents \u03bb o\ufb00spring . extensions an extension first presented in concerns the use of nonlinear hypotheses . exploiting the \ufb02exibility of evolutionary computation the search space h is set to ird ird each hypothesis h composed of a weight vector w and a center c associates to x the weighted l distance of x and c h wi xi ci d x i it must be noted that this representation allows roger for searching non linear hypotheses by doubling the size of the linear search space . previous work has shown that non linear roger significantly outperforms linear roger for some text mining applications . a new extension inspired from ensemble learning exploits the hypotheses h. ht learned along t independent runs of roger . the aggregation of the hi referred to as h associates to each example x the median value of . goals of experiments and experimental setting the goal of experiments is twofold . on one hand the ranking efficiency of n roger will be assessed and compared to that of state of the art supervised learning algorithms specifically support vector machines with linear quadratic and gaussian kernels using svmtorch implementation with default options . due to space limitations only ensemble based non linear roger termed n roger will be considered . on the other hand the results provided by n roger will be interpreted and discussed with respect to their intelligibility . the experimental setting is as follows . an experiment is a fold stratified cross validation process on each fold i svm learns a hypothesis hsv m ii roger is launched times and the bagging of the learned hypotheses constitutes the hypothesis hn roger learned by n roger iii both hypotheses are evaluated on the fold test set and the associated roc curve is constructed . the auc curves are averaged over the folds . http www.idiap.ch machine learning.php content torch en oldsvmtorch.txt preference learning in terminology extraction a roc based approach the overall results reported in the next section are averaged over ex periments . the roger parameters are as follows \u00b5 \u03bb the self adapta tive mutation rate is . the uniform crossover rate is . . empirical validation after describing the datasets this section reports on the comparative performances of the algorithms and inspects the results actually provided by n roger . datasets in both domains the data preparation step allows for categorizing the word collocations depending on the grammatical tag of the words . a first corpus related to molecular biology involves paper abstracts in english gathered from queries on medline . the nounnoun collocations occurring more than times are labelled by the expert the dataset includes a huge majority of relevant collocations . a second corpus related to curriculum vitae involves cvs in french . the frequent cv dataset includes the noun adjective collocations with at least occurrences with a huge majority of relevant collocations . the infrequent cv dataset includes the noun adjective collocations occurring once or twice with a significantly di\ufb00erent distribution of relevant irrelevant collocations . examples of relevant vs irrelevant collocations are respectively comp etences informatiques and euros annuels although both collocations make sense only the first one conveys useful information for the management of human resources . collocations molecular biology cv frequent collocations cv infrequent collocations collocations relevant irrelevant . table . relevant and irrelevant collocations . ranking accuracy after the experimental setting described in section table compares the average auc achieved for n roger and svmtorch with linear gaussian http www.ncbi.nlm.nih.gov entrez query.fcgi courtesy of the vediorbis foundation . az e et al. and quadratic kernels . on these domain applications both supervised learning approaches significantly improve on the statistical criteria standalone . further n roger improves significantly on svm using any kernel excepted on the infrequent cv dataset . a tentative interpretation for this result is based on the fact that this dataset is the most balanced one svm has some difficulties to cope with imbalanced datasets . corpus n roger gaussian quadratic molecular biology . frequent cv infrequent cv linear svm table . ranking accuracy of learning algorithms . corpus mi mi dice l occl ass j conv sesc cm lc t test khi mb . f cv . i cv . table . ranking accuracy of statistical criteria . a more detailed picture is provided by fig . showing the roc curve associated to svm n roger and the occl and j measures on the frequent cv dataset on a representative fold . interestingly the major di\ufb00erences between n roger and the other measures are seen at the beginning of the curve i.e. they concern the top ranked collocations . typically a recall of is obtained for false positive with n roger against with occl with j measures and for quadratic svm . in summary n roger improves the accuracy of the top ranked collocations and therefore the satisfaction and productivity of the expert if he she only examines the top results . a proof of principle of the generality of the approach has been presented in as the ranking function learned from one corpus in one language was found to outperform the standard statistical criteria when applied on the other corpus in another language . analysis of a ranking function as shown in the weights associated to distinct features by roger can provide some insights into the relevance of the features . accordingly the hypotheses constructed by n roger are examined . fig . displays the weights and center coordinates of all features for a representative roger hypothesis h learned on a fold of the frequent cv dataset . although auc is lower than that of h it still outpasses that of standalone features . as could have been expected roger detects that the mutual information criterion does badly with a high center cmi and weight wmi values . inversely as the occl criterion does well the center coccl is high associated with a highly negative weight woccl . although these tendencies could have been exploited by linear hypotheses this is no longer the case for the j criterion interestingly az e et al. the center cj takes on a medium value with a high negative weight wj . this is interpreted as collocations with either too low or too high values of j are less relevant everything else being equal . the current limitation of the approach is to provide a conjunctive description of the region of relevant collocations . mi occl collocation exp erience commerciale formation informatique soci et e informatique gestion informatique colonne morris bouygue telecom fromagerie riches mont sauveteur secouriste exp erience professionelle ressource humaine baccalaur eat professionel baccalaur eat scientifique wm i. woccl . n roger cm i. rank table . rank of relevant collocations given with measures and n roger . for each measure the weights used by n roger are given . coccl . rank rank discussion and perspectives the main claim of the paper is that supervised learning can significantly contribute to the term extraction task in text mining . some empirical evidence supporting this claim have been presented related to two corpora with di\ufb00erent domain applications and languages . based on a domain and languageindependent description of the collocations along a set of standard statistical criteria and on a few collocations manually labelled as relevant irrelevant by the expert a ranking hypothesis is learned . the ranking learner n roger used in the experiments is based on the optimization of the combinatorial wilcoxon rank test criterion using an evolutionary computation algorithm . two new features the use of non linear hypotheses and the exploitation of the ensemble of hypotheses learned along independent runs of roger have been exploited in n roger . further research is concerned with enriching the description of collocations e.g. adding typography related indications or distance to the closest noun possibly providing additional cues on the role of relevant collocations . another perspective is to extend roger using multi modal and multi objective evolutionary optimization e.g. enabling to characterize several types of relevant collocations in a single run . a long term goal is to study along a variety of domain applications and expert goals the eventual regularities associated to i the in the sense that a single center c is considered though the condition far from ci actually corresponds to a disjunction . preference learning in terminology extraction a roc based approach description of the relevant collocations ii the ranking hypotheses . acknowledgment we thank oriane matte tailliez for her expertise and labelling of the molecular biology dataset and mary felkin who did her best to improve the readability of this paper . the authors are partially supported by the pascal network of excellence ist ."
    ],
    "abstract": [
        "a key data preparation step in text mining term extraction selects the terms or collocation of words attached to specific concepts . in this paper the task of extracting relevant collocations is achieved through a supervised learning algorithm exploiting a few collocations manually labelled as relevant irrelevant . the candidate terms are described along standard statistical criteria measures . from these examples an evolutionary learning algorithm termed roger based on the optimization of the area under the roc curve criterion extracts an order on the candidate terms . the robustness of the approach is demonstrated on two real world domain applications considering different domains and different languages ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.6203703703703703
    ]
}