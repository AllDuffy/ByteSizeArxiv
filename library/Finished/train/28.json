{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0607136v1",
    "article": [
        "introduction this paper belongs to the area of research known as universal prediction of individual sequences the predictor s goal is to compete with a wide benchmark class of prediction strategies . in the previous papers and we constructed prediction strategies competitive with the important classes of markov and stationary respectively continuous prediction strategies . in this paper we consider competing against possibly discontinuous strategies . our main results assert the existence of prediction strategies competitive with the markov strategies . this paper s idea of transition from continuous to general benchmark classes was motivated by skorokhod s topology for the space d of c adl ag functions most of which are discontinuous . skorokhod s idea was to allow small deformations not only along the vertical axis but also along the horizontal axis when defining neighborhoods . skorokhod s topology was metrized by kolmogorov so that it became a separable space which allows us to apply one of the numerous algorithms for prediction with expert advice to construct a universal algorithm . in section we give the main definitions and state our main results theo rems and their proofs are given in sections and respectively . main results the game of prediction between two players called predictor and reality is played according to the following protocol . prediction protocol for n. reality announces xn predictor announces \u03b3n reality announces yn x. y. \u03b3 . end for . the game proceeds in rounds numbered by the positive integers n. at the beginning of each round n. predictor is given some signal xn relevant to predicting the following observation yn . the signal is taken from the signal space x and the observation from the observation space y. predictor then announces his prediction \u03b3n taken from the prediction space \u03b3 and the prediction s quality r. in light of the actual observation is measured by a loss function \u03bb \u03b3 we will always assume that the signal space x the prediction space \u03b3 and the observation space y are non empty sets x and \u03b3 will often be equipped with additional structures . y markov universal prediction strategies deterministic case predictor s strategies in the prediction protocol will be called prediction strategies . formally such a strategy is a function d n y n x x \u03b3 it maps each history to the chosen prediction . in this paper we will be especially interested in markov strategies which are functions \u03b3 intuitively d is the recommended prediction on round n. d x the restriction to markov strategies is not a severe one since the signal xn can encode as much of the past as we want in particular xn can contain information about the previous observations y. yn . in this paper markov prediction strategies will also be called prediction rules . for both our theorems we will need the notion of approximation to a signal x x intuitively the m approximation of x is another signal \u03c6m which is as close to x as possible but carries only m bits of information . if x a reasonable definition of \u03c6m would be to take the binary expansion of x but remove all the binary digits starting from the th after the binary dot . in general we will have to equip x with an approximation structure we will do this following kolmogorov and tikhomirov . consider a sequence of mappings \u03c6m x x m. such that each \u03c6m is idempotent in the sense \u03c6m \u03c6m for all x x and \u03c6m contains m elements . it is the sequence \u03c6 m. that will be referred to as an approximation structure . \u03c6m \u03bb \u03c6m \u03c1 \u03b4 . from we obtain n n n x \u03bb \u03bb d yn n n n n x \u03bb n n n x n n x n n n n x n n x \u03bb \u03bb dk yn \u03bb dk yn \u01eb \u01eb qk lel ln n \u01eb now is obvious . proof of theorem a convenient pseudo metric on \u03b3 can be defined by \u03c1 sup \u03bb g g \u03b3 . let us redefine \u03b3 as the quotient space obtained from the original \u03b3 by identifying g and g for which \u03c1 in other words we will not distinguish predictions that always lead to identical losses . now \u03c1 becomes a metric on \u03b3 . let \u03b3 be a countable dense subset of the original topological space \u03b3 the condition of equicontinuity implies that \u03b3 remains a dense subset in \u03b3 equipped with the metric \u03c1 . we define the norm of a function f \u03b3 r as f f f k kbl sup g g \u03b3 g g \u03c1 sup g \u03b3 f this norm is finite for bounded lipschitz functions . notice that next define kbl sup \u03bb y y k \u03bb kbl . \u03bb \u03bb\u03b3 z\u03b3 where \u03b3 is a probability measure on \u03b3 . this is the loss function in a new game of prediction with the prediction space it is linear and therefore convex p in \u03b3 . as a metric on we will take the fortet mourier metric defined as p \u03b2 sup z\u03b3 f kf kbl f d induced by this metric is called the topology of weak the topology on convergence . p let us check that the loss function is also bounded lipschitz in the sense of if \u03b3 \u03b3 and y y p kbl \u03b2 . \u03bb k \u03bb \u03bb z\u03b3 it is easy to see that the space \u03bb with metric \u03b2 is separable e.g. the set of probability measures concentrated on finite subsets of \u03b3 and taking rational . let us enumerate the elements values is dense in of a dense countable set in as d d. as in the previous section we will use the waa to merge all experts dk . p p p the convergence of the mixture to a probability measure on \u03b3 is now obvious . the countable convexity now holds with equality \u03bb k x and follows from the general fact that p n dk yn p n \u03bb k x k x z f d pkpk pk f dpk k x z for bounded borel f \u03b3 . r positive p p. summing to and p p. therefore it is easy to check that the chain still works and we can rephrase the previous section s result as follows . for any randomized prediction rule d any m. and any \u01eb there exists nd m \u01eb such that for any n nd m \u01eb and any x y x y. the waa s predictions \u03b3n are guaranteed to satisfy p p \u03bb \u03bb d yn n n n x n n n x \u01eb . the loss function is bounded in absolute value by a constant l and so the law of the iterated logarithm implies that for any \u03b4 there exists n\u03b4 such that the conjunction of \u03bb . ln ln ln n n n x sup n n\u03b4 n and \u03bb \u03bb n x \u03bb sup n n\u03b4 holds with probability at least \u03b4 . combining the last two inequalities with we can see that for any randomized prediction rule d any m. any \u01eb and any \u03b4 there exists nd m \u01eb \u03b4 such that for any x y x y. the waa s responses \u03b3n to x y x y. are guaranteed to satisfy . ln ln ln n sup n nd m \u01eb \u03b4 \u03bb \u03bb \u01eb p n n n x n n n x with probability at least d d. being a markov universal randomized prediction strategy . \u03b4 . this is equivalent to the waa applied to conclusion an interesting theoretical problem is to state more explicit versions of theorems and for example to give an explicit expression for nd m. the field of lossy compression is now well developed and it would be interesting to apply our prediction algorithms to the approximation structures induced by popular lossy compression algorithms . acknowledgments this work was partially supported by mrc ."
    ],
    "abstract": [
        "assuming that the loss function is convex in the prediction we construct a prediction strategy universal for the class of markov prediction strategies not necessarily continuous . allowing randomization we remove the requirement of convexity ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.6388888888888888
    ]
}