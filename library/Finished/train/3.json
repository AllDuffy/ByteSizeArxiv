{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0506004v4",
    "article": [
        "introduction we consider the problem of forecasting a new observation from the available data which may include e.g. all or some of the previous observations and the values of some explanatory variables . to make the process of forecasting more vivid we imagine that the data and observations are chosen by a player called reality and the forecasts are made by a player called forecaster . to establish properties of forecasting algorithms the traditional theory of machine learning makes some assumptions about the way reality generates the observations e.g. statistical learning theory assumes that the data and observations are generated independently from the same probability distribution . a more recent approach prediction with expert advice replaces the assumptions about reality by a comparison class of prediction strategies a typical result of this theory asserts that forecaster can perform almost as well as the best strategies in the comparison class . this paper further explores a third possibility suggested in which requires neither assumptions about reality nor a comparison class of forecaster s strategies . it is shown in that there exists a forecasting strategy which is automatically well calibrated this result has been further developed in e.g. almost all known calibration results however are asymptotic a non asymptotic result about calibration is given in proposition but even this result involves unspecified constants and randomization . the main results of this paper establish simple explicit inequalities characterizing calibration and resolution of our deterministic forecasting algorithm . next we brie\ufb02y describe the main features of our proof techniques and their connections with the literature . the proofs rely on the game theoretic approach to probability suggested in . the forecasting protocol is complemented by another player skeptic whose role is to gamble at the odds given by forecaster s probabilities . it can be said that our approach to forecasting is skeptic based whereas the traditional approach is reality based and prediction with expert advice is forecaster based . the two most popular formalizations of gambling are subsequence selection rules and martingales . the pioneering paper on what we call the skeptic based approach as well as the numerous papers developing it used von mises s notion of gambling appears to be the first paper in this direction to use ville s notion of gambling . another ingredient of this paper s approach considering skeptic s continuous strategies and thus avoiding randomization by forecaster goes back to and is also described in however i learned it from akimichi takemura in june . it should be noted that although our approach was inspired by and papers further developing precise statements of our results and our proof techniques are completely di\ufb00erent they are more in the spirit of levin s result about the existence of neutral measures . this version of this technical report di\ufb00ers from the previous one in that it incorporates the changes made in response to the comments of the reviewers of its journal version . the algorithms of large numbers in this section we describe our learning protocol and the general forecasting algorithm studied in this paper . the protocol is for n. reality i announces xn forecaster announces pn reality ii announces yn x. intuitively forecaster s move pn is the probability he attaches to the set . for a general \u03c6 we can also expect that the probabilities pn contrived by the algorithms of large numbers will have better calibration and resolution than the true probabilities . there is however little doubt that the true probabilities are more useful than any probabilities we are able to come up with . the true probabilities are not as good at calibration and resolution so they must be better in some other equally important respects . it remains unclear what these other respects may be and this is what we call the puzzle of the iterated logarithm . optimality of the k algorithm in this section we establish that the inequalities in theorems and are tight in a natural sense . h equation says that the di\ufb00erences yn pn are small on average even when scattered in a hilbert space by multiplying by \u03c6 . the next result says that it is the best forecaster can do . theorem let \u03c6 where strategy for reality ii which guarantees that h x h is a hilbert space . there is a \u03c6 pn \u03c6 k k h always holds for all n. regardless of what the other players do . n h n x n n x proof set rn \u03c6 n. it is sufficient to show that on the n th round n. reality ii can ensure that r n r n pn \u03c6 n where fix an n. define points a c d \u03c6n \u03c6 kh . k h as c \u03c6 a \u03c6 \u03c6 d \u03c6 \u03c6 or od oa where o is it is up to reality ii whether make rn equal to the origin . assuming without loss of generality that rn max we reduce our task to showing that the maximal value of rn for fixed rn \u03c6n and pn satisfies . it is geometrically obvious that rn attains its maximal value when this is illustrated in figure . let b be the base of the perpendicular dropped from o onto the interval ad and h. since the triangles obd and obc are right angled oa od od ob n n x n n x n n x n n x h r n h \u03c6n r n h \u03c6n pn \u03c6n . a b c d o figure the worst case for reality ii ob ac pn \u03c6n pn \u03c6n cd oa h. od rn oc rn subtracting the second equality from the first we obtain r n r n \u03c6n \u03c6n pn \u03c6n pn \u03c6 n. od oa . assume that in conclusion let us see that the maximum of rn is indeed attained when now allowed to be less than rn with oa rn . because of the compactness of the disk in figure in two dimensional subspaces of the maximum of rn it is however easy to check that no c will be a point of local maximum for the least trivial case is perhaps where o lies on the line ad and c is between o and d. is attained at some point c. supposing od od oc oc h the next result establishes the tightness of the bound in theorem . theorem let x with kernel k. reality ii has a strategy which ensures regardless of what the other players do that for each n. there exists a non zero f be an rkhs on such that f f f k pnk . proof by theorem there exists a strategy for reality ii which ensures kpn xn pnk . n n x n n x taking f n kf v u u t n x n v u u t n x f n n x f kpn xn we obtain n n x n n x f kpn xn f h if n n x kpn xn f kpn xn f n n x n f k kf v u u t n x f k kf f pnk . our task is accomplished . otherwise the right hand side of will if f also be zero and we can take any f. acknowledgments i am grateful to ilia nouretdinov for a discussion that lead to the proof of theorem and to the anonymous reviewers of the conference and journal versions of this paper for their comments . this work was partially supported by mrc and royal society ."
    ],
    "abstract": [
        "we analyze a new algorithm for probability forecasting of binary observations on the basis of the available data without making any assumptions about the way the observations are generated . the algorithm is shown to be well calibrated and to have good resolution for long enough sequences of observations and for a suitable choice of its parameter a kernel on the cartesian product of the forecast space and the data space . our main results are non asymptotic we establish explicit inequalities shown to be tight for the performance of the algorithm ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.5376344086021505
    ]
}