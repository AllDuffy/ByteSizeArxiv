{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0607047v1",
    "article": [
        "we give an inefficient but generalpurpose smoothing method for converting an estimated distribution that is good under the l metric into a distribution that is good under the kl divergence . keywords . bayes error bayes classifier plug in decision function introduction we consider a general approach to pattern classification in which elements of each class are first used to train a probabilistic model via some unsupervised learning method . the resulting models for each class are then used to assign discriminant scores to an unlabeled instance and a label is chosen to be the one associated with the model giving the highest score . for example uses this approach to classify protein sequences via training a well known probabilistic suffix tree model of ron et al. on each sequence class . indeed even where an unsupervised technique is mainly being used to gain insight into the process that generated two or more data sets it is still sometimes instructive to try out the associated classifier since the misclassification rate provides a quantitative measure of the accuracy of the estimated distributions . this work was supported by epsrc grant gr r. this work was supported in part by the ist programme of the european community under the pascal network of excellence ist . this publication only re\ufb02ects the authors views . the work of has led to further related algorithms for learning classes of probabilistic finite state automata in which the objective of learning has been formalized as the estimation of a true underlying distribution with a distribution represented by a hypothesis pdfa . the natural discriminant score to assign to a string is the probability that the hypothesis would generate that string at random . as one might expect the better one s estimates of label class distributions the better should be the associated classifier . the contribution of this paper is to make precise that observation . we give bounds on the risk of the associated bayes classifier in terms of the quality of the estimated distributions . these results are partly motivated by our interest in the relative merits of estimating a class conditional distribution using the variation distance as opposed to the kl divergence . in it has been shown how to learn a class of pdfas using kl divergence in time polynomial in a set of parameters that includes the expected length of strings output by the automaton . in we show how to learn this class with respect to variation distance with a polynomial sample size bound that is independent of the length of output strings . furthermore it can be shown that it is necessary to switch to the weaker criterion of variation distance in order to achieve this . we show here that this leads to a di\ufb00erent but still useful performance guarantee for the bayes classifier . abe and warmuth study the problem of learning probability distributions using the kl divergence via classes of probabilistic automata . their criterion for learnability is that for an unrestricted input distribution d the hypothesis pdfa should be almost as close as possible to d. abe takeuchi and warmuth study the negative log likelihood loss function in the context of learning stochastic rules i.e. rules that associate an element of the domain x to a probability distribution over the range y. we show here that if two or more label class distributions are learnable in the sense of then the resulting stochastic rule is learnable in the sense of . we show that if instead the label class distributions are well estimated using the variation distance then the associated classifier may not have a good negative log likelihood risk but will have a misclassification rate that is close to optimal . this result is for general k class classification where distributions may overlap . we also incorporate variable misclassification penalties and show that this more general loss function is still approximately minimized provided that discriminant likelihood scores are rescaled appropriately . the bayes classifier associated with two or more probability distributions is the function that maps an element x of the domain to the label associated with the probability distribution whose value at x is largest . this is of course a well known approach for classification see . as a result we show that pac learnability and more generally p concept learnability follows from the ability to learn class distributions in the setting of kearns et al. papers such as study the problem of learning various classes of probability distributions with respect to kl divergence and variation distance in this setting . it is well known that learnability with respect to kl divergence is stronger than learnability with respect to variation distance . furthermore the kl divergence is usually used due to the property that when minimized with respect to an sample the empirical likelihood of that sample is maximized . an algorithm that learns with respect to variation distance can sometimes be converted to one that learns with respect to kl divergence by a smoothing technique when the domain is n and n is a parameter of the learning problem . in this paper we give a related smoothing rule that applies to the version of the pdfa learning problem where we seem to need to use the variation distance . however the smoothed distribution does not have an efficient representation and requires the probabilities used in the target pdfa to have limited precision . notation and terminology in k class classification labeled examples are generated by distribution d over x. we consider the problem of predicting the label \u2113 associated with x x where x is generated by the marginal distribution of d on x d x. a non negative cost is incurred for each classification based either on a cost matrix or the negative log likelihood of the true label being assigned . the aim is to optimize the expected cost given by the occurrence of a randomly generated example . we refer to the expected cost associated with any classifier f x as risk denoted as r. let d\u2113 be d restricted to points \u2113 . d is a mixture p where randomly generated data point has label \u2113 . k i gi and g\u2113 is the class prior of class \u2113 the probability that a p in section it is shown that if we have upper bounds on the inaccuracy of the estimated distributions of each class label then we can derive bounds on the risk associated with the classifiers . suppose d and d are probability distributions over the same domain x. we define the l distance as l x d d dx . we usually assume that x is a discrete domain in which case r l d d. k \u2113 g\u2113d\u2113 the kl divergence from d to d is defined as x x x x x x i d log d d. p concepts are functions probabilistically mapping elements of the domain to classes . learning framework in the pac learning framework an algorithm receives labeled samples generated independently according to distribution d over x where distribution d is unknown and where labels are generated by an unknown function f from a known class of functions f. the algorithm must output a hypothesis h from a class of hypotheses h such that with probability at least \u03b4 errh \u01eb where \u01eb and \u03b4 are parameters . notice that in this setting if f h then err where err is the error associated with the optimal hypothesis . we use a variation on the framework used in for learning p concepts which adopts performance measures from the pac model extending this to learn stochastic rules with k classes . therefore it is the case that err inf h h. the aim of the learning algorithm in this framework is to output a hypothesis h h such that with probability of at least \u03b4 the error errh of h satisfies errh err \u01eb . our notion of learning distributions is similar to that of kearns et al. definition . let dn be a class of distributions . dn is said to be efficiently learnable if an algorithm a exists such that given \u01eb and \u03b4 and access to randomly drawn examples from any unknown target distribution and n and returns a probad dn a runs in time polynomial in bility distribution d that with probability at least \u03b4 is within l distance \u01eb of d. \u03b4 \u01eb we define p concepts as introduced by kearns and shapire . this definition is for class classification but generalizes in a natural way to more than classes . definition . a probabilistic concept f on domain x is given by a real valued function pf x. an observation of f consists of some x x together with a label \u2113 with pr pf . results in section . we give bounds on the risk associated with a hypothesis with respect to the accuracy of the approximation of the underlying distribution generating the instances . in section . we show that these bounds are close to optimal and in section . we give corollaries showing what these bounds mean for pac learnability . we define the accuracy of an approximate distribution in terms of l distance and kl divergence both of which are commonly used measurements . it is assumed that the class priors of each class label are known . bounds on increase in risk first we examine the case where the accuracy of the hypothesis distribution is such that the distribution for each class label is within l distance \u01eb of the true distribution for that label for some \u01eb . a cost matrix c specifies the cost associated with any classification where the cost of classifying a data point which has label i as some label j is denoted as cij . it is usually the case that cij for i j. we introduce the following notation given classifier f over discrete domain x f x the risk of f is given by r cif . gi.di . k x x x i x let f be the bayes optimal classifier i.e. the function with the minimal risk or optimal expected cost and f is the function with optimal expected cost with respect to alternative distributions d i i. for x x f arg minj f arg minj k i cij . gi.di k i cij.gi.d i. p p theorem . let f be the bayes optimal classifier and let f be the bayes classifier associated with estimated distributions d i. suppose that for each label i l \u01eb gi . then r r \u01eb.k . maxij . proof . let rf be the contribution from x x towards the total expected cost associated with classifier f. for f such that f j rf cij.gi.di . k i x let \u03c4\u2113 \u2113 be the increase in risk for labelling x as \u2113 instead of \u2113 so that \u03c4\u2113 \u2113 k i ci\u2113 . gi.di k i . gi.di . note that due to the optimality of f on di x x \u03c4f f. in a similar way the expected contribution to the total cost of f from x must i given that f is chosen be less than or equal to that of f with respect to d to be optimal on the d k i ci\u2113.gi.di i values . we have p p p cif . gi.d i cif . gi.d i. k i x k i x this result is essentially a generalization of exercise . of devroye et al s textbook from class to multiple classes and in addition we show here that variable misclassification costs can be incorporated . this is the closest thing we have found to this theorem that has already appeared but we suspect that other related results may have appeared . we would welcome any further information or references on this topic . theorem is another result which we suspect may be known but likewise we have found no statement of it . rearranging we have d i.gi . cif cif . k i x from and it can be seen that \u03c4f f . gi . i . gi . cif cif . let di be the di\ufb00erence between the probability densities of di and d i at x x di di d p i. therefore k i x \u03c4f f cif cif . gi.di \u03c4f f max j . gi.di . in order to bound the expected cost it is necessary to sum over the range of x x k i x k x x x i x \u03c4f f max j . gi.di max j . gi . di . i \u01eb gi for all i ie . x x di \u01eb gi it follows from x x x x x x since l di d that k i x this expression gives an upper bound on expected cost for labelling x as f instead of f. by definition \u03c4 . gi . k p max j i x. \u01eb gi x x x \u03c4 r r. therefore it has been shown that r r \u01eb . max j r \u01eb.k . max ij . x x x k i x we next prove a corresponding result in terms of kl divergence which uses the negative log likelihood of the correct label as the cost function . we define pri to be the probability that a data point at x has label i such that . given a function f x rk where pri gi.di k j gj.dj p f is a prediction of the probabilities of x having each label i the risk associated with f can be expressed as p r d log.pri . let f x rk output the true class label distribution for an element of x. from equation it can be seen that x x x k i x k r d log.pri . i x theorem . for f x rk suppose that r is given by . if for each label i i \u01eb gi then r r k\u01eb . x x x proof . let rf be the contribution at x x to the risk associated with classii log . pri . therefore r x x d.rf . fier f rf we define pr i to be the estimated probability that a data point at x x p k has label i from distributions d i such that pr p i gi.d i k j gj.d j. p rf d. log pr i . pri . k i x let \u03be denote the contribution to additional risk incurred from using f as opposed to f at x x. from it can be seen that \u03be rf d. log . pri d. pri . log log pr i k i x k i x k d. d. i x k p gi.di k j gj.dj log gi.di k j gj.dj gi.di k j gj.dj . log p gi.di gi.d i log log gi.d i k j gj.d k j gj.dj p k j gj.d p. j j i x we define d such that d i gi.d k i gi.di \u03be can be rewritten as p k d p i. since it is the case that p p \u03be d. k i gi.di d. log k i gi.di log di d i p p gi.di gi.d i d log log d d d d. we define i to be the contribution at x x to the kl divergence such that i d log . it follows that \u03be i. k i x x x x we know that the kl divergence between di and d i is bounded by \u01eb gi for each label i so can be rewritten as k \u03be x x x i x gi . \u01eb gi i k.\u01eb i. due to the fact that the kl divergence between two distributions is nonnegative an upper bound on the cost can be obtained by letting i so r r k\u01eb . therefore it has been proved that r r k\u01eb . lower bounds in this section we give lower bounds corresponding to the two upper bounds given in section . example . consider a distribution d over domain x from which data is generated with labels and and there is an equal probability of each label being generated . di denotes the probability that a point is generated at x x given that it has label i. d and d are distributions over x such that at x x d. and d are approximations of d and d and that l \u01eb suppose that d g \u01eb and l. given the following distributions assuming that a misclassification results in a cost of and that a correct classification results in no cost it can be seen that r \u01eb d \u01eb d \u01eb \u01eb d and d that f will misclassify for every value of x x now if we have approximations d d \u01eb . as shown below it can be seen d \u03b3 d \u03b3 d \u03b3 d \u03b3 . this results in r \u01eb . therefore r r \u01eb r. in this example the risk is only \u03b3 under r \u01eb.k . maxj since k. a similar example can be used to give upper bounds to the lower bound given in theorem . example . consider distributions d d d over domain x as defined in example . it can be seen that the kl divergence between each label s distribution and its approximated distribution is and d i i \u01eb log \u01eb \u03b3 \u01eb log \u01eb \u03b3 . the optimal risk measured in terms of negative log likelihood can be ex . the risk in \u03b3 pressed as r log curred by using f as the discriminant function is r \u01eb \u01eb \u01eb . therefore \u01eb \u01eb \u01eb \u03b3 log log log r r \u01eb log \u01eb \u03b3 \u01eb log r \u01eb . \u01eb \u03b3 . learning near optimal classifiers in the pac sense we show that the results of section . imply learnability within the framework defined in section . . the following corollaries refer to algorithms aclass and aclass . these algorithms generate classifier functions f x which label data in a k label classification problem using l distance and kl divergence respectively as measurements of accuracy . corollary shows that a near optimal classifier can be constructed given that an algorithm exists which approximates a distribution over positive data in polynomial time . we are given cost matrix c and assume knowledge of the class priors gi . corollary . if an algorithm al approximates distributions within l distance \u01eb with probability at least \u03b4 in time polynomial in \u01eb and \u03b4 then an algorithm aclass exists which generates a discriminant function f with an associated risk of at most r \u01eb and aclass is polynomial in \u03b4 and \u01eb . proof . aclass is a classification algorithm which uses unsupervised learners to fit a distribution to each label i and then uses the bayes classifier with respect to these estimated distributions to label data . al is a pac algorithm which learns from a sample of positive data to estimate a distribution over that data . aclass generates a sample n of data and divides n into sets such that ni contains all members of n with label i. note that for all labels i ni gi . n. with a probability of at least the distribution di over label i such that l \u01eb al generates an estimate d of . therefore the size of the sample ni must be polynomial in gi.k . maxij \u01eb and k \u03b4 . for all i gi so ni is polynomial in maxij k \u01eb and \u03b4 . when aclass combines the distributions returned by the k iterations of al there is a probability of at least \u03b4 that all of the distributions are within l distance of the true distributions . we allow a probability of \u03b4 that the initial sample n did not contain a good representation of all labels and as such one or more iteration of al may not have received a sufficiently large sample to learn the distribution accurately . therefore with probability at least \u03b4 all approximated distributions are within \u01eb l distance of the true distributions . if we use the classifier which is optimal on these approximated distributions f then the increase in risk associated with using f instead of the bayes optimal classifier f is at most \u01eb . it has been shown that al requires a sample of size polynomial in \u01eb \u03b4 k and maxij . it follows that n ni p k max o p k max . \u01eb \u03b4 ij k k i x i x \u01eb \u03b4 ij corollary shows how a near optimal classifier can be constructed given that an algorithm exists which approximates a distribution over positive data in polynomial time . corollary . if an algorithm akl has a probability of at least \u03b4 of approximating distributions within \u01eb kl divergence in time polynomial in \u01eb and \u03b4 then an algorithm aclass exists which generates a function f that maps x x to a conditional distribution over class labels of x with an associated log likelihood risk of at most r \u01eb and aclass is polynomial in \u03b4 and \u01eb . proof . aclass is a classification algorithm using the same method as aclass in corollary whereby a sample n is divided into sets and each set is passed to algorithm akl where a distribution is estimated over the data in the set . with a probability of at least akl generates an estimate d of the distribution di over label i such that i \u01eb . therefore the size of the sample ni must be polynomial in gi.k \u01eb and k \u03b4 . since gi ni is polynomial in k \u01eb and k \u03b4 . when aclass combines the distributions returned by the k iterations of akl there is a probability of at least \u03b4 that all of the distributions are within \u01eb kl divergence of the true distributions . we allow a probability of \u03b4 that the initial sample n did not contain a good representation of all labels . therefore with probability at least \u03b4 all approximated distributions are within \u01eb kl divergence of the true distributions . if we use the classifier which is optimal on these approximated distributions f then the increase in risk associated with using f instead of the bayes optimal classifier f is at most \u01eb . it has been shown that akl requires a sample of size polynomial in \u01eb \u03b4 and k. let p be an upper bound on the time and sample size used by akl . it follows that k k n ni p o k.p i x i x \u01eb \u03b4 . \u01eb \u03b4 . smoothing from l distance to kl divergence given a distribution that has accuracy \u01eb under the l distance is there a generic way to smooth it so that it has similar accuracy under the kl divergence from this can be done for x n if we are interested in algorithms that are polynomial in n in addition to other parameters . suppose however that the domain is bit strings of unlimited length . here we give a related but weaker result in terms of bit strings that are used to represent distributions as opposed to members of the domain . we define class d of distributions specified by bit strings such that each member of d is a distribution on discrete domain x represented by a discrete probability scale . let ld be the length of the bit string describing distribution d. note that there are at most ld distributions in d represented by strings of length ld . lemma . suppose d d is learnable under l distance in time polynomial in \u03b4 \u01eb and ld . then d is learnable under kl divergence with polynomial sample size . proof . let d be a member of class d represented by a bit string of length ld and let algorithm a be an algorithm which takes an input set s of samples generated i.i.d. from distribution d and with probability at least \u03b4 returns a distribution dl such that l \u01eb . let \u03be . we define algorithm a such that with probability at \u01eb ld least \u03b4 a returns distribution d l \u03be . algorithm a runs a with sample s where s is polynomial in \u03be \u03b4 and ld . l where l \u03be . kl d. members of x contribute l. therefore positively to i d d i p p log kl kl x x d log d x x d d kl x x d log kl log d log log . p kl \u03be so x x \u03be . we have shown that l d d analysing the first term in p log log \u03be max x x log log . x x x note that for all x x d kl \u03be . ld . it follows that max x x log log log ld log . examining the second term in d kl log log x x x x x x d kl log kl log d where hx d d the concavity of the logarithm function it follows that kl which is a positive quantity for all x x. due to x x d kl log kl log d p klhx x x d x x hx \u03be . d dy i h y d kl therefore i \u03be . for values of \u03be \u01eb ld corollary . consider the problem of learning pdfas having n states over alphabet \u03c3 and probabilities represented by bit strings of length \u2113 . using sample size polynomial in n \u03c3 and \u2113 a distribution is this class can be estimated within kl distance \u01eb . p p the proof follows from the observation that such a pdfa can be represented using a bit string whose length is polynomial in the parameters . consequently we can learn the same class of pdfas under the kl divergence that can be learned under the l distance in i.e. pdfas with distinguishable states but no restriction on the expected length of their outputs . however note that the hypothesis is inefficient . conclusion we have shown a close relationship between the error of an estimated input distribution and the error rate of the resulting classifier . in situations where we believe that input distributions may be accurately estimated the resulting information about the data may be more useful than just a near optimal classifier . a general issue of interest is the question of when one can obtain good classifier from estimated distributions that satisfy weaker goodness of approximation criteria than those considered here . suppose for example that elements of a element domain are being labeled by the stochastic rule that assigns labels and to either element of the domain with equal probability . then any classifier does no better than random labeling and so we can use arbitrary distributions d as estimates of the distributions d and d over examples with label and respectively . in we show that in the basic pac framework we can sometimes design discriminant functions based on unlabeled data sets that result in pac classifiers without any guarantee on how well estimated is the input distribution . further work should possibly compromise between the distribution free setting and the objective considered here of approximating the input distributions in a strong sense . and d acknowledgements we would like to thank luc devroye for drawing to our attention the statement of the version of theorem that appears in ."
    ],
    "abstract": [
        "a standard approach in pattern classification is to estimate the distributions of the label classes and then to apply the bayes classifier to the estimates of the distributions in order to classify unlabeled examples . as one might expect the better our estimates of the label class distributions the better the resulting classifier will be . in this paper we make this observation precise by identifying risk bounds of a classifier in terms of the quality of the estimates of the label class distributions . we show how pac learnability relates to estimates of the distributions that have a pac guarantee on their l _ distance from the true distribution and we bound the increase in negative log likelihood risk in terms of pac bounds on the kl divergence . we give an inefficient but general purpose smoothing method for converting an estimated distribution that is good under the l _ metric into a distribution that is good under the kl divergence ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.6809815950920245
    ]
}