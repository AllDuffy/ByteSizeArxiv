{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0507033v2",
    "article": [
        "introduction there is strong evidence that kernel methods can deliver state of the art performance on most classification tasks when the input data lies in a vector space . arguably two factors contribute to this success . first the good ability of kernel algorithms such as the svm to generalize and provide a sparse formulation for the underlying learning problem second the capacity of nonlinear kernels such as the polynomial and rbf kernels to quantify meaningful similarities between vectors notably non linear correlations between their components . using kernel machines with non vectorial data requires more arbitrary choices both to represent the objects and to chose suitable kernels on those representations . the challenge of using kernel methods on real world data has thus recently fostered many proposals for kernels on complex objects notably for strings trees images or graphs to cite a few . a strategy often quoted as the generative approach to this problem takes advantage of a generative model that is an adequate statistical model for the objects to derive feature representations for the objects . in practice this often yields kernels to be used on the histograms of smaller components sampled in the objects where the kernels take into account the geometry of the underlying model in their similarity measures . the previous approaches coupled with svm s combine both the advantages of using discriminative methods with generative ones and produced convincing results on many tasks . one of the drawbacks of such representations is however that they implicitly assume that each component has been generated independently and in a stationary way where the empirical histogram of components is seen as a sample from an underlying stationary measure . while this viewpoint may translate into adequate properties for some learning tasks it might prove too restrictive and hence inadequate for other types of problems . namely tasks which involve a more subtle mix of detecting both conditional and global similarities between the objects . such problems are likely to arise for instance in speech language time series or image processing . in the first three tasks this consideration is notably treated by most state of the art methods through dynamic programming algorithms capable of detecting and penalizing accordingly local matches between the objects . using dynamic programming to produce a kernel yielded fruitful results in di\ufb00erent applications with the limitation that the kernels obtained in practice are not always positive definite as reviewed in . other kernels proposed for sequences directly incorporate a localization information into each component augmenting considerably the size of the component space and then introduce some smoothing to avoid representations that would be too sparse . we propose in this work a di\ufb00erent approach grounded on the generative approach previously quoted managing however to combine both conditional and global similarities when comparing two objects . the motivation behind this approach is both intuitive and computational intuitively the global histogram of components that is the simple bag of components representation of figure may seem inadequate if the components appearance seem to be clearly conditioned by some external events . this phenomenon can be taken into account by considering collections of nested bags or histograms to describe the object . kernels that would only t t t. t. t t. t. t figure from the bag of components representation to a set of nested bags using a set of conditioning events . rely on these detailed resolutions might however miss the bigger picture that is provided by the global histogram . we propose a trade o\ufb00 between both viewpoints through a combination that aims at giving a balanced account of both fine and coarse perspectives hence the name of multiresolution kernels which we introduce formally in section . on the computational side we show how such a theoretical framework can translate into an efficient factorization detailed in section . we then provide experimental results in section on an image retrieval task which shows that the methodology improves the performance of kernel based state of the art techniques in this field . multiresolution kernels in most applications complex objects can be represented as histograms of components such as texts as bags of words or images and sequences as histograms of colors and letters . through this representation objects are cast as probability laws or measures on the space x of components typically multinomials if x is finite and compared as such through kernels on measures . an obvious drawback of this representation is that all contextual information on how the components have been sampled is lost notably any general sense of position in the objects but also more complex conditional information that may be induced from neighboring components such as transitions or long range interactions . in the case of images for instance one may be tempted to consider not only the overall histogram of colors but also more specialized histograms which may be relevant for the task . if some local color overlapping in the images is an interesting or decisive feature of the learning problem these specialized histograms may be generated arbitrarily following a grid dividing for instance the image into equal parts and computing histograms for each corner before comparing them pairwise between two images . if sequences are at stake these may also be sliced into predefined regions to yield local histograms of letters . if the strings are on the contrary assumed to follow some markovian behaviour an interesting index would translate into a set of contexts typically a complete suffix dictionary as detailed in . while the two previous examples may seem opposed in the way the histograms are generated both methodologies stress a particular class of events that give an additional knowledge on how the components were sampled in the objects . since both these two approaches and possibly other ones can be applied within the framework of this paper using a unified formalism we present our methodology using a general notation for the index of events . namely we note t for an arbitrary set of conditioning events assuming these events can be directly observed on the object itself by contrast with the latent variables approach of . considering still following the generative approach that an object can be mapped onto a probability measure \u00b5 on x we have that the realization of an event t t can be interpreted under the light of a joint probability \u00b5 with x x factorized through bayes law as \u00b5\u00b5 to yield the following decomposition of \u00b5 as \u00b5 \u00b5t x t t def \u00b5\u00b5 is an element of the set of sub probability measures where each \u00b5t m s that is the set of positive measures \u03c1 on x such that their total mass \u03c1 denoted as \u03c1 is less than or equal to . to take into account the information brought by the events in t objects can hence be represented as families of measures of m s indexed by t namely elements \u00b5 contained in mt def m s t. local similarities between measures conditioned by sets of events to compare two objects under the light of their respective decompositions as sub probability measures \u00b5t and \u00b5 t we make use of an arbitrary positive definite kernel k on m s to which we will refer to as the base kernel throughout the paper . for interpretation purposes only we may assume in the following sections that k can be written as e d. note also that the kernel is defined not only on probability measures but also on sub probabilities . for two elements \u00b5 \u00b5 of mt and a given element t t the kernel where d is an euclidian distance in m s kt def k measures the similarity of \u00b5 and \u00b5 by quantifying how similarly their components were generated conditionally to event t. for two di\ufb00erent events s and t of t ks and kt can be associated through polynomial combinations with positive factors to result in new kernels notably their sum ks kt or their product kskt . this is particularly adequate if some complementarity is assumed between s and t so that their combination can provide new insights for a given learning task . if on the contrary the events are assumed to be similar then they can be regarded as a unique event and result in the kernel k def k which will measure the similarity of m and m when either s or t occurs . the previous formula can be extended to model kernels indexed on a set t t of similar events through kt def k where \u00b5t def \u00b5t and \u00b5 t def x t t \u00b5 t. x t t note that this equivalent to defining a distance between elements \u00b5 and \u00b5 conditionned by t as d t def d. resolution specific kernels let p be a finite partition of t that is a finite family p of sets of n t such that ti tj if i j n and i ti t. we write p for s the set of all partitions of t. consider now the kernel defined by a partition p as kp def kti . n y i the kernel kp quantifies the similarity between two objects by detecting their joint similarity under all possible events of t given an a priori similarity assumed on the events which is expressed as a partition of t. note that there is some arbitrary in this definition since following the convolution kernels approach for instance a simple multiplication of base kernels kti to define kp is used rather than any other polynomial combination . more precisely the multiplicative structure of equation quantifies how two objects are similar given a partition p in a way that imposes for the objects to be similar according to all subsets ti . if k can be expressed as a function of a distance d kp can be expressed as the exponential of p def d d ti n x i a quantity which penalizes local di\ufb00erences between the decompositions of \u00b5 and \u00b5 over t as opposed to the coarsest approach where p and only d is considered . as illustrated in figure in the case of images expressed as histograms indexed over locations a partition of t re\ufb02ects a given belief on how events should be associated to belong to the same set or dissociated to highlight interesting figure a useful set of events t for images which would focus on pixel localization can be represented by a grid such as the one represented above . in this case p corresponds to the windows presented in the left image p to the larger square obtained when grouping small windows p to the image divided into equal parts and p is simply the whole image . any partition of the image obtained from sets in p such as the one represented above can in turn be used to represent an image as a family of sub probability measures which reduces in the case of two color images to binary histograms as illustrated in the right most image . dissimilarities . hence all partitions contained in the set p of all possible partitions are not likely to be equally meaningful given that some events may if the index is based on location one would look more similar than others . naturally favor mergers between neighboring indexes . for contexts a useful topology might also be derived by grouping contexts with similar suffixes . such meaningful partitions can be obtained in a general case if we assume the existence of a prior hierarchical information on the elements of t translated into a series p . . pd of partitions of t namely a hierarchy on t. to provide a hierarchical content the family d d is such that any subset present in a partition pd is included in a subset included in the coarser partition pd and further assume this inclusion to be strict . this is equivalent to stating that each set t of a partition pd is divided in pd through a partition of t which is not t itself . we note this partition s and name its elements the siblings of t. consider now the subset pd p of all partitions of t obtained by using only sets in def . . the set pd contains both namely pd the coarsest and the finest resolutions respectively p and pd but also all variable resolutions for sets enumerated in p d as can be seen for instance in the third image of figure . which is quite a big space since if t is a finite set of cardinal r the cardinal of the set of partitions is known as the bell number of order r with br u ur u r er ln r. e p p d def d d pd . averaging resolution specific kernels each partition p contained in pd provides a resolution to compare two objects and generates consequently a very large family of kernels kp when p spans pd . some partitions are probably better suited for certain tasks than others which may call for an efficient estimation of an optimal partition given a task . we take in this section a di\ufb00erent direction by considering an averaging of such kernels based on a bayesian prior on the set of partitions . in practice this averaging favours objects which share similarities under a large collection of resolutions . definition . let t be an index set endowed with a hierarchy d d \u03c0 be a prior measure on the corresponding set of partitions pd and k a base kernel on m s. the multiresolution kernel k\u03c0 on mt mt is defined as m s k\u03c0 \u03c0 kp . x p pd note that in equation each resolution specific kernel contributes to the final kernel value and may be regarded as a weighted feature extractor . kernel computation this section aims at characterizing hierarchies d d and priors \u03c0 for which the computation of k\u03c0 is both tractable and meaningful . we first propose a type of hierarchy generated by trees which is then coupled with a branching process prior to fully specify \u03c0 . these settings yield a computational time for expressing k\u03c0 which is loosely upperbounded by d card t c where c is the time required to compute the base kernel . partitions generated by branching processes all partitions p of pd can be generated iteratively through the following rule starting from the initial root partition p p. for each set t of p. either leave the set as it is in p. either replace it by its siblings enumerated in s and reapply this rule to each sibling unless they belong to the finest partition pd . by giving a probabilistic content to the previous rule through a binomial pafor each treated set assign probability \u03b5 of applying rule rameter a candidate prior for pd can be derived depending on the overall coarseness of the considered partition . for all elements t of pd this binomial parameter is equal to whereas it can be individually defined for any element t of the d coarsest partitions as \u03b5t yielding for a partition p pd the weight \u03c0 y t p y p t p gathers all coarser sets bewhere the set longing to coarser resolutions than p and can be regarded as all ancestors in p d of sets enumerated in p. factorization the prior proposed in section . can be used to factorize the formula in which is summarized in this theorem using notations used in definition theorem . for two elements m m of mt define for t spanning recursively pd pd ... p the quantity kt kt \u03b5t y ku . u s then k\u03c0 kt . proof . the proof follows from the prior structure used for the tree generation and can be found in either or . figure underlines the importance of incorporating to each node kt a weighted product of the kernels ku computed by its siblings . kt \u00b5t \u00b5 t kt \u00b5t \u00b5 t kt \u00b5t \u00b5 t kt k \u03b5t kti q \u00b5t \u00b5 t \u00b5ti \u00b5 ti p p figure the update rule for the computation of k\u03c0 takes into account the branching process prior by updating each node corresponding to a set t of any intermediary partitions with the values obtained for higher resolutions in s. if the hierarchy of t is such that the cardinality of s is fixed to a constant \u03b1 for any set t typically \u03b1 for images as seen in figure then the computation of k\u03c0 is upperbounded by c. this computational complexity may even become lower in cases where the histograms become sparse at fine resolutions yielding complexities in linear time with respect to the size of the compared objects quantified by the length of the sequences in for instance . experiments we present in this section experiments inspired by the image retrieval task first considered in and also used in although the images used here are not exactly the same . the dataset was also extracted from the corel stock database and includes families of labelled images each class containing color images each image being coded as pixels with colors coded in bits . the families depict bears african specialty animals monkeys cougars fireworks mountains office interiors bonsais sunsets clouds apes and rocks and gems . the database is randomly split into balanced sets of training images and test images . the task consists in classifying the test images with the rule learned by training one vs all svm s on the learning fold . the object are then classified according to the svm performing the highest score namely with a winner takes all strategy . the results presented in this section are averaged over di\ufb00erent random splits . we used the cimg package to generate histograms and the spider toolbox for the svm experiments . we adopted a coarser representation of bits per color for the pixels of each image rather than the available ones to reduce the size of the rgb color space to from the original set of colors . in this image retrieval experiment we used localization as the conditioning index set dividing the images into and local histograms . to define the branching process prior we simply set an uniform value over all the grid of \u03b5 of \u03b1 an usage motivated by previous experiments led in a similar context . finally we used kernels described in both and to define the base kernel k. these kernels can be directly applied on sub probability measures which is not the case for all kernels on multinomials notably the information di\ufb00usion kernel . we report results for two families of kernels namely the radial basis function expressed for multinomials and the entropy kernel based on the jensen divergence ka b \u03c1 e \u03c1 p \u03b8a i \u03b8 i a b kh e h \u03b8 \u03b8 . for most kernels not presented here the multiresolution approach usually improved the performance in a similar way than the results presented in table . finally we also report that using only the finest resolution available in each setting that is a branching process prior uniformly set to yielded better results than the use of the coarsest histogram without achieving however the same performance of the multiresolution averaging framework which highlights the interest of taking both coarse and fine perspectives into account . when a. for instance this setting produced . and . error rates for \u03b1 and d and . for \u03b1 and d. http cimg.sourceforge.net and http www.kyb.tuebingen.mpg.de bs people spider kernel global histogram d \u03b1 d \u03b1 d \u03b1 d \u03b1 rbf b \u03c1 . a. a. a. jd . table results for the corel image database experiment in terms of error rate with fold cross validation and di\ufb00erent types of tested kernels the rbf and the jensen divergence . acknowledgments mc would like to thank jean philippe vert and arnaud doucet for fruitful discussions as well as xavier dupr e for his help with the cimg toolbox ."
    ],
    "abstract": [
        "we present in this work a new methodology to design kernels on data which is structured with smaller components such as text images or sequences . this methodology is a template procedure which can be applied on most kernels on measures and takes advantage of a more detailed bag of components representation of the objects . to obtain such a detailed description we consider possible decompositions of the original bag into a collection of nested bags following a prior knowledge on the objects structure . we then consider these smaller bags to compare two objects both in a detailed perspective stressing local matches between the smaller bags and in a global or coarse perspective by considering the entire bag . this multiresolution approach is likely to be best suited for tasks where the coarse approach is not precise enough and where a more subtle mixture of both local and global similarities is necessary to compare objects . the approach presented here would not be computationally tractable without a factorization trick that we introduce before presenting promising results on an image retrieval task ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.5027322404371585
    ]
}