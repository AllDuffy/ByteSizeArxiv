{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\9905015v1",
    "article": [
        "category reinforcement learning and control preference oral introduction most work on hierarchical reinforcement learning has focused on temporal abstraction . for example in the options framework the programmer defines a set of macro actions and provides a policy for each . learning algorithms can then treat these temporally abstract actions as if they were primitives and learn a policy for selecting among them . closely related is the ham framework in which the programmer constructs a hierarchy of finitestate controllers . each controller can include non deterministic states . the hamq learning algorithm can then be applied to learn a policy for making choices in the non deterministic states . in both of these approaches and in other studies of hierarchical rl each option or finite state controller must have access to the entire state space . the one exception to this the feudal q method of dayan and hinton introduced state abstractions in an unsafe way such that the resulting learning problem was only partially observable . hence they could not provide any formal results for the convergence or performance of their method . even a brief consideration of human level intelligence shows that such methods can not scale . when deciding how to walk from the bedroom to the kitchen we do not need to think about the location of our car . without state abstractions any rl method that learns value functions must learn a separate value for each state of the world . some argue that this can be solved by clever value function approximation methods and there is some merit in this view . in this paper however we explore a di\ufb00erent approach in which we identify aspects of the mdp that permit state abstractions to be safely incorporated in a hierarchical reinforcement learning method without introducing function approximations . this permits us to obtain the first proof of the convergence of hierarchical rl to an optimal policy in the presence of state abstraction . we introduce these state abstractions within the maxq framework but the basic ideas are general . in our previous work with maxq we brie\ufb02y discussed state abstractions and we employed them in our experiments . however we could not prove that our algorithm converged with state abstractions and we did not have a usable characterization of the situations in which state abstraction could be safely employed . this paper solves these problems and in addition compares the e\ufb00ectiveness of maxq q learning with and without state abstractions . the results show that state abstraction is very important and in most cases essential to the e\ufb00ective application of maxq q learning . the maxq framework let m be a markov decision problem with states s actions a reward function r and probability transition function p. our results apply in both the finite horizon undiscounted case and the infinite horizon discounted case . let be a set of subtasks of m where each subtask mi is defined by a termination predicate ti and a set of actions ai . the goal of subtask mi is to move the environment into a state such that ti is satisfied . the subtasks of m must form a dag with a single root node no subtask may invoke itself directly or indirectly . a hierarchical policy is a set of policies \u03c0 one for each subtask . a hierarchical policy is executed using standard procedure call and return semantics starting with the root task m and unfolding recursively until primitive actions are executed . when the policy for mi is invoked in state s let p be the probability that it terminates in state s after executing n primitive actions . a hierarchical policy is recursively optimal if each policy \u03c0i is optimal given the policies of its descendants in the dag . let v be the value function for subtask i in state s. similarly let q be the q value for subtask i of executing child action j in state s and then executing the current policy until termination . the maxq value function decomposition is based on the observation that each subtask mi can be viewed as a semi markov decision problem in which the reward for performing action j in state s is equal to v the value function for subtask j in state s. to see this consider the sequence of rewards rt that will be received when we execute child action j and then continue with subsequent actions according to hierarchical policy \u03c0 q e the macro action j will execute for some number of steps n and then return . hence we can partition this sum into two terms q e \u03b3urt u \u03b3urt u st s \u03c0 n the first term is the discounted sum of rewards until subtask j terminates v. the second term is the cost of finishing subtask i after j is executed . we call this second term the completion function and denote it c. we can then write the bellman equation as q p s n x v c to terminate this recursion define v for a primitive action a to be the expected reward of performing action a in state s. the maxq q learning algorithm is a simple variation of q learning in which at subtask mi state s we choose a child action j and invoke its policy . when it returns we observe the resulting state s and the number of elapsed time steps n and update c according to c c \u03b1t \u03b3n . to prove convergence we require that the exploration policy executed during learning be an ordered glie policy . an ordered policy is a policy that breaks q value ties among actions by preferring the action that comes first in some fixed ordering . a glie policy is a policy that executes each action infinitely often in every state that is visited infinitely often and converges with probability to a greedy policy . the ordering condition is required to ensure that the recursively optimal policy is unique . without this condition there are potentially many di\ufb00erent recursively optimal policies with di\ufb00erent values depending on how ties are broken within subtasks subsubtasks and so on . theorem let m hs a p ri be either an episodic mdp for which all deterministic policies are proper or a discounted infinite horizon mdp with discount factor \u03b3 . let h be a dag defined over subtasks . let \u03b1t be a sequence of constants for each subtask mi such that lim t t t x lim t t t x \u03b1t and \u03b1 t let \u03c0x be an ordered glie policy at each subtask mi and state s and assume that vt and ct are bounded for all t i s and a. then with probability algorithm maxq q converges to the unique recursively optimal policy for m consistent with h and \u03c0x . proof the proof is based on proposition . from bertsekas and tsitsiklis and follows the standard stochastic approximation argument due to generalized to the case of non stationary noise . there are two key points in the proof . define pt to be the probability transition function that describes the behavior of executing the current policy for subtask j at time t. by an inductive argument we show that this probability transition function converges to the probability transition function of the recursively optimal policy for j. second r y g root get put t source t destination pickup navigate putdown b north south east west figure left the taxi domain . right task graph . we show how to convert the usual weighted max norm contraction for q into a weighted max norm contraction for c. this is straightforward and completes the proof . what is notable about maxq q is that it can learn the value functions of all subtasks simultaneously it does not need to wait for the value function for subtask j to converge before beginning to learn the value function for its parent task i. this gives a completely online learning algorithm with wide applicability . conditions for safe state abstraction to motivate state abstraction consider the simple taxi task shown in figure . there are four special locations in this world marked as r b g and y. in each episode the taxi starts in a randomly chosen square . there is a passenger at one of the four locations and that passenger wishes to be transported to one of the four locations . the taxi must go to the passenger s location pick up the passenger go to the destination location and put down the passenger there . the episode ends when the passenger is deposited at the destination location . there are six primitive actions in this domain four navigation actions that move the taxi one square north south east or west a pickup action and a putdown action . each action is deterministic . there is a reward of for each action and an additional reward of for successfully delivering the passenger . there is a reward of if the taxi attempts to execute the putdown or pickup actions illegally . if a navigation action would cause the taxi to hit a wall the action is a no op and there is only the usual reward of . this task has a hierarchical structure in which there are two main sub tasks get the passenger and deliver the passenger . each of these subtasks in turn involves the subtask of navigating to one of the four locations and then performing a pickup or putdown action . this task illustrates the need to support both temporal abstraction and state abstraction . the temporal abstraction is obvious for example get is a temporally extended action that can take di\ufb00erent numbers of steps to complete depending on the distance to the target . the top level policy can be expressed very simply with these abstractions . the need for state abstraction is perhaps less obvious . consider the get subtask . while this subtask is being solved the destination of the passenger is completely irrelevant it can not a\ufb00ect any of the nagivation or pickup decisions . perhaps more importantly when navigating to a target location only the taxi s location and identity of the target location are important . the fact that in some cases the taxi is carrying the passenger and in other cases it is not is irrelevant . we now introduce the five conditions for state abstraction . we will assume that the state s of the mdp is represented as a vector of state variables . a state abstraction can be defined for each combination of subtask mi and child action j by identifying a subset x of the state variables that are relevant and defining the value function and the policy using only these relevant variables . such value functions and policies are said to be abstract . the first two conditions involve eliminating irrelevant variables within a subtask of the maxq decomposition . condition subtask irrelevance . let mi be a subtask of mdp m. a set of state variables y is irrelevant to subtask i if the state variables of m can be partitioned into two sets x and y such that for any stationary abstract hierarchical policy \u03c0 executed by the descendants of mi the following two properties hold the state transition probability distribution p \u03c0 for each child action j of mi can be factored into the product of two distributions p \u03c0 p \u03c0 p \u03c0 in the taxi problem the source and destination of the passenger are irrelevant to the navigate subtask only the target t and the current taxi position are relevant . condition leaf irrelevance . a set of state variables y is irrelevant for a primitive action a if for any pair of states s and s that di\ufb00er only in their values for the variables in y p rr s a. p s xs xs this condition is satisfied by the primitive actions north south east and west in the taxi task where all state variables are irrelevant because r is constant . the next two conditions involve funnel actions macro actions that move the environment from some large number of possible states to a small number of resulting states . the completion function of such subtasks can be represented using a number of values proportional to the number of resulting states . condition result distribution irrelevance a set of state variables yj is irrelevant for the result distribution of action j if for all abstract policies \u03c0 executed by mj and its descendants in the maxq hierarchy the following holds for all pairs of states s and s that di\ufb00er only in their values for the state variables in yj s p \u03c0 p \u03c0 . consider for example the get subroutine under an optimal policy for the taxi task . regardless of the taxi s position in state s the taxi will be at the passenger s starting location when get finishes executing . hence the taxi s initial position is irrelevant to its resulting position . condition termination . let mj be a child task of mi with the property that whenever mj terminates it causes mi to terminate too . then the completion cost c and does not need to be represented . this is a particular kind of funnel action it funnels all states into terminal states for mi . for example in the taxi task in all states where the taxi is holding the passenger the put subroutine will succeed and result in a terminal state for root . this is because the termination predicate for put implies the termination condition for root . this means that c is uniformly zero for all states s where put is not terminated . condition shielding . consider subtask mi and let s be a state such that for all paths from the root of the dag down to mi there exists a subtask that is terminated . then no c values need to be represented for subtask mi in state s because it can never be executed in s. in the taxi task a simple example of this arises in the put task which is terminated in all states where the passenger is not in the taxi . this means that we do not need to represent c in these states . the result is that when combined with the termination condition above we do not need to explicitly represent the completion function for put at all by applying these abstraction conditions to the taxi task the value function can be represented using values which is much less than the values required by \ufb02at q learning . without state abstractions maxq requires values theorem let h be a maxq task graph that incorporates the five kinds of state abstractions defined above . let \u03c0x be an ordered glie exploration policy that is abstract . then under the same conditions as theorem maxq q converges with probability to the unique recursively optimal policy \u03c0 r defined by \u03c0x and h. proof consider a subtask mi with relevant variables x and two arbitrary states and . we first show that under the five abstraction conditions the value function of \u03c0 r can be represented using c. to learn the values of c q learning algorithm needs samples of x and n drawn according to p. the second part of the proof involves showing that regardless of whether we execute j in state or in the resulting x and n will have the same distribution and hence give the correct expectations . analogous arguments apply for leaf irrelevance and v. the termination and shielding cases are easy . p experimental results we implemented maxq q for a noisy version of the taxi domain and for kaelbling s hdg navigation task using boltzmann exploration . figure shows the performance of \ufb02at q and maxq q with and without state abstractions on these tasks . learning rates and boltzmann cooling rates were separately tuned to optimize the performance of each method . the results show that without state abstractions maxq q learning is slower to converge than \ufb02at q learning but that with state abstraction it is much faster . maxq abstraction maxq abstraction flat q maxq no abstraction d r a w e r e v i t a l u m u c n a e m d r a w e r e v i t a l u m u c n a e m maxq no abstraction flat q conclusion e . e . e primitive actions primitive actions figure comparison of maxq q with and without state abstraction to \ufb02at q learning on a noisy taxi domain and kaelbling s hdg task . the horizontal axis gives the number of primitive actions executed by each method . the vertical axis plots the average of separate runs . this paper has shown that by understanding the reasons that state variables are irrelevant we can obtain a simple proof of the convergence of maxq q learning under state abstraction . this is much more fruitful than previous e\ufb00orts based only on weak notions of state aggregation and it suggests that future research should focus on identifying other conditions that permit safe state abstraction ."
    ],
    "abstract": [
        "many researchers have explored methods for hierarchical reinforcement learning with temporal abstractions in which abstract actions are defined that can perform many primitive actions before terminating . however little is known about learning with state abstractions in which aspects of the state space are ignored . in previous work we developed the maxq method for hierarchical rl . in this paper we define five conditions under which state abstraction can be combined with the maxq value function decomposition . we prove that the maxq q learning algorithm converges under these conditions and show experimentally that state abstraction is important for the successful application of maxq q learning ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.6296296296296297
    ]
}