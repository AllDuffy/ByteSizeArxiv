{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0607134v1",
    "article": [
        "introduction f f it is well known that under some restrictions on is a normed function class of prediction strategies . there exists a master prediction strategy that performs almost as well as the best strategies in whose norm is not too large . the leading prediction strategies constructed in this paper satisfy a stronger property the loss of any prediction strategy in whose norm is not too large exceeds the loss of a leading strategy by the divergence between the predictions output by the two prediction strategies . therefore the leading strategy implicitly serves as a standard for prediction strategies f in whose norm is not too large such a prediction strategy f su\ufb00ers a small loss to the degree that its predictions resemble the leading strategy s predictions and the only way to compete with the leading strategy is to imitate it . f f f we start the formal exposition with a simple asymptotic result asserting the existence of leading strategies in the problem of on line in regression with the quadratic loss function for the class of continuous limitedmemory prediction strategies . to state a non asymptotic version of this result we introduce several general definitions that are used throughout the paper . in the following two sections proposition is generalized in two di and by strictly rections to the loss functions given by bregman divergences in other words we will consider the problem of on line quadratic loss regression . r and \u03bb . \u00b5n n n x proposition is obtained by applying proposition to large and here we will only demonstrate this rkhs . the details will be given in idea with a simple but non trivial example . let k and m be positive integer constants such that m k. a prediction strategy f will be included in if its predictions \u03c6n satisfy f n n x \u03c6n f otherwise if n k where f is a function from the sobolev space w m kf is defined to be f k the sobolev norm of f. every continuous function of can be arbitrarily well approximated by functions in w m and so is a suitable class of prediction strategies if we believe that neither x. xn nor y. yn k are useful in predicting yn . f very large benchmark classes some interesting benchmark classes of prediction strategies are too large to equip with the structure of rkhs . however an analogue of proposition can also be proved for some banach spaces of prediction strategies for which the constant cf defined by is finite . the modulus of convexity of a banach space u is defined as the function f \u03b4u inf u v su ku vku \u01eb \u01eb s k p sition but with f ranging over the sobolev space w s p is covered by this proposition . the parameter s describes the degree of regularity of the elements of w s p and taking sufficiently large p we can reach arbitrarily irregular functions in the sobolev hierarchy . predictions evaluated by bregman diver gences r. notice that for any function \u03c8 p a predictable process is a function f mapping the situation space s to r f r and any prediction strategy s f the composition \u03c8 mapping each situation s to \u03c8 is a predictable process such compositions will be used in theorems below . a hilbert of predictable processes is called space an rkhs with finite embedding constant if is finite . f the notion of bregman divergence was introduced in and is now widely used in competitive on line prediction . suppose r. let \u03c8 and \u03c8 be two real valued functions defined on y. the expression d\u03c8 \u03c8 \u03c8 \u03c8 \u03c8 y z y z. is said to be the corresponding bregman divergence if d\u03c8 \u03c8 whenever y in all our examples \u03c8 will be a strictly convex continuously di\ufb00erentiable function and \u03c8 its derivative in which case we abbreviate d\u03c8 \u03c8 to d\u03c8 . we will be using the standard notation f kc sup y a f k where a is a subset of the domain of f. theorem suppose y p is a bounded subset of r. let be an rkhs of predictable processes with finite embedding constant cf and \u03c8 \u03c8 be real valued functions on y p. there exists a strategy for predictor that guarantees for all prediction strategies f and n. f n n x d\u03c8 \u03c8 d\u03c8 \u03c8 n n x n n x k d\u03c8 \u03c8 \u03c8 kc k kf \u03c8 n diam c f q where \u03c6n are f s predictions . the expression in this case holds vacuously . similar conventions will be made in all following statements . kf in is interpreted as when \u03c8 \u03c8 f k two of the most important bregman divergences are obtained from the y negative convex functions \u03c8 y and \u03c8 y ln y they are the quadratic loss function y ln respectively . if we apply theorem to them leads assuming y that satisfy \u00b5n \u00b5n \u00b5n yn \u03c6n yn n we can use the results of producing predictions \u00b5n \u00b5n y f n c q and n n x n n x \u00b5n \u03c6n yn y c f f k kf n q. replacing and with the corresponding statements for banach func tion spaces we obtain the proof of proposition . remark in we considered only prediction strategies f for which f depends on sn via xn in the terminology of this paper these are markov strategies . it is easy to see that considering only markov strategies does not lead to a loss of generality if we redefine the object xn as xn sn any prediction strategy will become a markov prediction strategy . proof of proposition proposition will follow from the following lemma proved in . lemma let g on z with finite embedding constant such that be a separable set in c. there exists an rkhs is dense in f in metric c. f g proof let f f. be a dense sequence of elements of set . g \u03c6n n c fn fnk k if fn otherwise \u03c6 \u2113 for z z \u03c6 \u03c6 k i\u2113 be the unique rkhs with reproducing kernel k. finite . by lemma below each fn belongs to it is clear that c z f h z z n fnkc en \u03c6 \u2113 e where en . dense in g \u2113 consists of all s except a at the nth position . therefore is f the following lemma was used in the proof . h with the inner product of lemma let \u03c6 z h where h is a hilbert space . the rkhs corresponding to the reproducing kernel k \u03c6 \u03c6 ih consists of all functions h v \u03c6 v \u03c6 ih and v \u03c6 ih span implies v v. the continuity of each evaluation func from the obvious fact that the equality of the functions for v v tional is also obvious . ih and v \u03c6 h v \u03c6 yn \u03c6 \u03c6 \u01eb \u01eb from some n on . since \u01eb can be taken arbitrarily small we have . proof of theorem the proof is based on the generalized law of cosines d\u03c8 \u03c8 d\u03c8 \u03c8 d\u03c8 \u03c8 n n x n n x. from we deduce d\u03c8 \u03c8 d\u03c8 \u03c8 n n n n x d\u03c8 \u03c8 \u00b5n n x \u03c8 yn \u03c8 \u03c8 yn \u03c8 n n x \u00b5n n x n n x n n x from theorem in we can see that there is a prediction strategy guaranteeing \u03c8 yn diam \u03c8 k kc n and from theorem in we can see that there is a prediction strategy guaranteeing \u03c8 diamcf k \u03c8 n. kf we need however a single strategy guaranteeing some versions of and . such a strategy can be obtained by merging a strategy guaranteeing and a strategy guaranteeing . setting \u03c6 \u03c8 \u03c8 k kc ks r f \u00b5 p s s c f and letting \u00b5n be output by the k algorithm based so that c\u03c6 on we obtain p \u03c8 n n x k r f f n c \u03c8 kc diam q from theorem of and we obtain \u00b5n \u03c8 yn ksn \u03c8 \u00b5n ksn \u03c8 h \u03c8 kf k ksn if n n x f f n n x n n x \u03c8 kf k \u03c6 r f kf diam from the proof of theorem and from theorem of . \u03c8 n x k q n f n c combining with and we can see that produces a strategy guaranteeing . remark as we mentioned earlier the leading constant in the bound of theorem is worse than those in other results in this paper in the intersection of their domains of application . the explanation is that theorem is based on the k algorithm whereas all other results are based on the more sophisticated k algorithm . proof sketch of theorem the proof is similar to that of theorem with the role of the generalized law of cosines played by the equation \u03bb a \u03bb b for some a a and b b. since y can take only two possible values suitable a and b are easy to find it suffices to solve the linear system \u03bb a \u03bb b which in turn gives a d\u03bb . therefore gives exp abbreviating exp\u03bb n n x \u03bb d\u03bb n n x \u03bb exp yn n n x there are prediction strategies that guarantee n x exp exp . \u00b5n and there are prediction strategies that guarantee exp merging such strategies as in corollaries and we can easily obtain from and . proof sketch of theorem it is shown in that there is a prediction strategy guaranteeing exp \u00b5n exp k exp kf v u u t n n x exp k \u00b5n exp k. comparing and with where k is the reproducing kernel of we can see that theorem will follow from f \u00b5n exp k n v u u t n x which in turn will follow from c f. n p n n x and n n x it remains to notice that \u00b5 and to calculate \u00b5 ln \u00b5 \u00b5 c f c f. \u00b5 ln \u00b5 sup \u00b5 \u00b5 . . . \u00b5 proof of proposition this proposition immediately follows from the equality in and hoe\ufb00ding s inequality . conclusion the existence of master strategies can be shown for a very wide class of loss functions . on the contrary leading strategies appear to exist for a rather narrow class of loss functions . it would be very interesting to delineate the class of loss functions for which a leading strategy does exist . in particular does this class contain any loss functions except bregman divergences and strictly proper scoring rules even if a leading strategy does not exist one might look for a strategy g such that the loss of any strategy f whose norm is not too large lies between the loss of g plus some measure of di\ufb00erence between f s and g s predictions and the loss of g plus another measure of di\ufb00erence between f s and g s predictions . i am grateful to the anonymous referees of the conference version of this paper for their comments . this work was partially supported by mrc . acknowledgments"
    ],
    "abstract": [
        "we start from a simple asymptotic result for the problem of on line regression with the quadratic loss function the class of continuous limited memory prediction strategies admits a leading prediction strategy which not only asymptotically performs at least as well as any continuous limited memory strategy but also satisfies the property that the excess loss of any continuous limited memory strategy is determined by how closely it imitates the leading strategy . more specifically for any class of prediction strategies constituting a reproducing kernel hilbert space we construct a leading strategy in the sense that the loss of any prediction strategy whose norm is not too large is determined by how closely it imitates the leading strategy . this result is extended to the loss functions given by bregman divergences and by strictly proper scoring rules ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.5507246376811594
    ]
}