{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0607085v2",
    "article": [
        "keywords . pseudo stochastic rational languages multiplicity automata probabilistic grammatical inference . introduction in probabilistic grammatical inference we often consider stochastic languages which define distributions over \u03c3 the set of all the possible words over an alphabet \u03c3 . in general we consider an unknown distribution p and the goal is to find a good approximation given a finite sample of words independently drawn from p. the class of probabilistic automata is often used for modeling such distributions . this class has the same expressiveness as hidden markov models and is identifiable in the limit . however there exists no efficient algorithm for identifying pa . this can be explained by the fact that there exists no canonical representation of these automata which makes it difficult to correctly identify the structure of the target . one solution is to focus on subclasses of pa such as probabilistic deterministic automata but with an important lack of expressiveness . another solution consists in considering this work was partially supported by the marmota project anr mmsa amaury habrard fran\u00e7ois denis and yann esposito the class of multiplicity automata . these models admit a canonical representation which offers good opportunities from a machine learning point of view . ma define functions that compute rational series with values in r. ma are a strict generalization of pa and the stochastic languages generated by pa are special cases of rational stochastic languages . let us denote by srat k the class of rational stochastic languages computed by ma with parameters in k where k. with k q or k r srat k is exactly the class of stochastic languages generated by pa with parameters in k. but when k q or k r we obtain strictly greater classes . this provides several advantages elements of srat k have a minimal normal representation thus elements of srat k may have significantly smaller representation in srat k parameters of these minimal representations are directly related to probabilities of some natural events of the form u\u03c2 which can be efficiently estimated from stochastic samples lastly when k is a field rational series over k form a vector space and efficient linear algebra techniques can be used to deal with rational stochastic languages . p p however the class srat q presents a serious drawback there exists no recursively enumerable subset class of ma which exactly generates it . as a consequence no proper identification algorithm can exist indeed applying a proper identification algorithm to an enumeration of samples of \u03c3 would provide an enumeration of the class of rational stochastic languages over q. in spite of this result there exists an efficient algorithm dees which is able to identify srat k in the limit . but before reaching the target dees can produce ma that do not define stochastic languages . however it has been shown in that with probability one for any rational stochastic language p if dees is given as input a sufficiently large sample s u \u03c3 r converges drawn according to p dees outputs a rational series such that u \u03c3 p r converges to as the size of s inabsolutely to . moreover creases . we show that these ma belong to a broader class of rational series that we call pseudo stochastic rational languages . a pseudo stochastic rational language r has the property that r limn r is defined for any word u and that r. a stochastic language pr can be associated with r in such a way that u \u03c3 r is absolutely convergent . as a first consequence pr r when r is a stochastic language . as a second p consequence for any rational stochastic language p if dees is given as input increasing samples drawn according to p dees outputs pseudo stochastic rational languages r such that u \u03c3 p pr converges to as the size of s increases . the aim of this paper is twofold to provide a theoretical study of the class of pseudo stochastic rational languages and a series of experiments in order to compare the performance of dees to two classical inference algorithms alergia and mdi . we show that the class of pseudo stochastic rational languages is decidable within polynomial time . we provide an algorithm that can be used to compute pr from any ma that computes r. we also show how it is possible to simulate pr using such an automaton . we show that there exist pseudo stochastic rational languages r such that pr is not rational . finally we show that it is undecidable whether two pseudo stochastic rational languages define the same stochastic language . we have carried out a lot of experiments which show that dees outperforms alergia and mdi in most cases . r r when the sum u \u03c3 pr r p p p using pseudo stochastic rational languages in probabilistic grammatical inference these results were expected since alergia and mdi have not the same theoretical expressiveness and since dees aims at producing a minimal representation of the target in the set of ma which can be significantly smaller than the smaller equivalent pda . the paper is organized as follows . in section we introduce some background about multiplicity automata rational series and stochastic languages and present the algorithm dees . section deals with our study of pseudo rational stochastic languages . our experiments are detailed in section . definitions and notations . rational series multiplicity automata and stochastic languages let \u03c3 be the set of words on the finite alphabet \u03c3 . a language is a subset of \u03c3 . the empty word is denoted by \u03b5 and the length of a word u is denoted by u. for any integer k let \u03c3k and \u03c3 k. we denote by the length lexicographic order on \u03c3 and by m inl the minimal element of a non empty language l according to this order . a subset s of \u03c3 is prefix closed if for any u v \u03c3 uv s u s. for any s \u03c3 let pref and f act . a formal power series is a mapping r of \u03c3 into r. the set of all formal power series is denoted by rhh\u03c3ii . it is a vector space . for any series r and any word u let us denote by ur the series defined by ur r for every word w. let us denote by supp the support of r i.e. the set . a stochastic language is a formal series p which takes its values in r and such that w \u03c3 p. the set of all stochastic languages over \u03c3 is denoted by s. for any language l \u03c3 and p any p s let us denote w l p by p. for any p s and u \u03c3 such that p the residual language of p wrt u is the stochastic language defined p by u p by u p p p. we denote by res the set and by res the set . let s be a sample over \u03c3 i.e. a multiset composed of words over \u03c3 . we denote by ps the empirical distribution over \u03c3 associated with s. let s be an infinite sample composed of words independently drawn according to a stochastic language p. we denote by sn the sequence composed of the n first words of s. we introduce now the notion of multiplicity automata . let k. a k multiplicity automaton is a tuple h\u03c2 q \u03d5 \u03b9 \u03c4 i where q is a finite set of states \u03d5 q \u03c3 q k is the transition function \u03b9 q k is the initialization function \u03c4 q k is the termination function . we extend the transition function \u03d5 to q \u03c3 q by \u03d5 s q \u03d5 \u03d5 and \u03d5 if q r and otherwise for any q r q x \u03c3 and w \u03c3 . for any finite subset l \u03c3 and any r q define \u03d5 w l r r \u03d5 . we denote by qi the set of initial states and by qt the set of terminal states . a state q q is accessible if there exists q qi and u \u03c3 such that \u03d5 . an p p amaury habrard fran\u00e7ois denis and yann esposito ma is trimmed if all its states are accessible and co accessible . from now we only consider trimmed ma . the support of an ma a h\u03c2 q \u03d5 \u03b9 \u03c4 i is the non deterministic finite automaton h\u03c2 q qi qt \u03b4i where \u03b4 . the spectral radius of a square matrix m if the maximum magnitude of its eigenvalues . let a h\u03c2 q \u03b9 \u03d5 \u03c4 i be an ma . let us denote by \u03c1 be the spectral radius of the square matrix i j n. if \u03c1 then each sequence ra q converges to a number sq and hence r converges too . let us denote by r the limit of r when it exists . the numbers sq are the unique solutions of the following linear system of equations it is decidable within polynomial time whether \u03c1 . p sq ra q q q \u03d5sq for q q. a probabilistic automaton is a trimmed ma h\u03c2 q \u03d5 \u03b9 \u03c4 i s.t. \u03b9 \u03d5 and \u03c4 take q q \u03b9 and for any state q \u03c4 \u03d5 . their values in s.t. a probabilistic deterministic automaton is a pa whose support is deterministic . it can be shown that probabilistic automata generate stochastic languages . let us denote by sp a the class of all stochastic languages which can be computed by a p a. k resp . sp da p k q r q \u03b9 \u03d5\u03c4 . for any ma a let ra be the series defined by ra for any q q we also define the series ra q by ra q r q \u03d5\u03c4 . an p ma a is reduced if the set is linearly independent in the vector space rhh\u03c3ii . an ma a is prefix closed if its set of states q is a prefix closed subset of \u03c3 qi and u q \u03b4 where \u03b4 is the transition function in the support of a. p rational series have several characterization . here we shall say that a formal power series over \u03c3 is k rational iff there exists a k multiplicity automaton a such that r ra where k. let us denote by k rathh\u03c3ii the set of k rational series over \u03c3 and by srat k k rathh\u03c3ii s the set of rational stochastic languages over k. it can be shown that a series r is r rational iff the set spans a finite dimensional vector subspace of rhh\u03c3ii . as a corollary a stochastic language p is r rational iff the set res spans a finite dimensional vector subspace of rhh\u03c3ii . rational stochastic languages have been studied in from a language theoretical point of view . it is worth noting that sp da r. from now on a rational stochastic r language will always denote an r rational stochastic language . r srat r srat has no solution then s v ps ps \u03b9 do \u03c3 q \u03d5 \u03b9 \u03c4 i \u03c3 and x \u03c3 let w q be a solution of i \u03b1wps ps s algorithm algorithm dees . \u03c4 ps ps vx f res f x \u03b5 a a a a a \u03b5 else \u03b5 initialisation with \u03b5 . creation of a new state . fig . illustration of the different steps of algorithm dees . final automaton . we begin by constructing a state for \u03b5 . we examine ps with v \u03b5a to decide if we need to add a new state for the string a. we obtain the following system which has in fact no solution and we create a new state as shown in figure . ps ps n ps ps ps ps ps ps x\u03b5 b x\u03b5 b ps ps ps ps x\u03b5 b x\u03b5 o. we examine ps with v aa to decide if we need to create a new state for the string aa . we obtain the system below . it is easy to see that this system admits at least one solution x\u03b5 . then we add two transitions to the automaton and we obtain the automaton of figure and the algorithm halts . and xa ps ps ps ps x\u03b5 ps ps ps ps x\u03b5 ps ps xa b ps ps xa ps ps b n ps ps x\u03b5 ps ps xa b x\u03b5 xa since no recursively enumerable subset of ma is capable to generate the set of rational stochastic languages no identification algorithm can be proper . this remark applies to dees . there is no guarantee at any step that the automaton a output by dees computes a stochastic language . however the rational series r computed by the ma output by dees can be used to compute a stochastic language pr that also converges to the target . moreover they have several nice properties which make o using pseudo stochastic rational languages in probabilistic grammatical inference them close to stochastic languages we call them pseudo stochastic rational languages and we study their properties in the next section . pseudo stochastic rational languages the canonical representation a of a rational stochastic language satisfies \u03c1 and w \u03c3 ra . we use this characteristic to define the notion of pseudo stochastic rational language . p definition . we say that a rational series r is a pseudo stochastic language if there exists an ma a which computes r and such that \u03c1 and if r. note that the condition \u03c1 implies that r is defined without ambiguity . a rational stochastic language is a pseudo stochastic rational language but the converse is false . example . let a h\u03c2 \u03d5 \u03b9 \u03c4 i defined by \u03c3 \u03b9 \u03c4 \u03d5 and \u03d5 . we have ra u b. check that \u03c1 and ra u b for every word u. hence ra is a pseudo stochastic language . as indicated in the previous section any canonical representation a of a rational stochastic language satisfies \u03c1 . in fact the next lemma shows that any reduced representation a of a pseudo stochastic language satisfies \u03c1 . lemma . let a be a reduced representation of a pseudo stochastic language . then \u03c1 . proof . the proof is detailed in annex . . proposition . it is decidable within polynomial time whether a given ma computes a pseudo stochastic language . proof . given an ma b compute a reduced representation a of b check whether \u03c1 and then compute ra . it has been shown in that a stochastic language pr can be associated with a pseudo stochastic rational language r the idea is to prune in \u03c3 all subsets u\u03c2 such that r and to normalize in order to obtain a stochastic language . let n be the smallest prefix closed subset of \u03c3 satisfying \u03b5 n and u n x \u03c3 ux n iff r. for every u \u03c3 n define pr . for every u n let \u03bbu m ax x \u03c3 m ax . then define pr m ax \u03bbu . it can be shown that r pr and r r pr . p the difference between r and pr is simple to express when the sum converges absolutely . let nr nr when r is a stochastic language u \u03c3 r w \u03c3 r pr p u \u03c3 nr . note that p u \u03c3 r converges absolutely and nr . as a r nr r r. we have p p p p amaury habrard fran\u00e7ois denis and yann esposito s.t. \u03c1 and ra \u03d5 \u03b9 \u03c4 \u03c3 q h input ma a a word u q. qn p s n i ei\u03c4 \u03c3 do n i j ei\u03d5sj s p s \u00b5 \u03c3 if w u then pra else p s \u03b5 w repeat \u00b5 for x \u00b5 let x w wx \u03c3 s.t. wx is a prefix of u and let \u00b5 s.t. \u03bb\u00b5 \u03bb for i. n do ei s n j ej\u03d5 p end until w u algorithm algorithm computing pr . consequence in that case pr r. we give in algorithm an algorithm that computes pr and pr for any word u from any ma that computes r. this algorithm is linear in the length of the input . it can be slightly modified to generate a word drawn according to pr . a \u03c1\u03b1 b \u03c1 the stochastic languages pr associated with pseudostochastic rational languages r can be not rational . a \u03c1 b \u03c1\u03b2 q \u03c4 q \u03c4 proposition . there exists pseudo stochastic rational languages r such that pr is not rational . p u \u03c3 ra . u \u03c3 ra and proof . suppose that the parameters of the automaton fig . an example of pseudoa described on figure satisfy \u03c1 \u03c4 stochastic languages rational and \u03c1 \u03c4 with \u03b1 \u03b2 . then the series rq and rq are rational which are not rational . stochastic languages and therefore ra rq rq is a rational series which satisfies let us show that pra is not rational . for any u \u03c3 ra \u03c1 u. for any integer n there exists an integer mn such that for any integer i ra iff i mn . moreover it is clear that mn tends to infinity with m. suppose now that pra is rational and let l be its support . from the pumping lemma there exists an integer n such that for any word w uv l satisfying v n there exists v v v such that v vvv and l uvv v is infinite . let n be such that mn n and let u an and v bmn . since w uv l l anb should be infinite which is is false . therefore l is not the support of a rational language . p different rational series may yield the same pseudo rational stochastic language . is it decidable whether two pseudo stochastic rational series define the same stochastic using pseudo stochastic rational languages in probabilistic grammatical inference a\u03b1 a cos \u03b1 a sin \u03b1 a sin \u03b1 \u03bb q a cos \u03b1 a \u03bb q \u03bb q a. q a. a. q a. b. q a. a. a. a. a. a. a. e a. a. a. a. a. c a. fig . a\u03b1 define stochastic language which can be represented by a pa with at least n states when \u03b1 \u03c0 n. with \u03bb \u03bb and \u03bb the ma a\u03c0 defines a stochastic language p whose prefixed reduced representation is the ma b. in fact p can be computed by a pda and the smallest pa computing it is c. language unfortunately the answer is no . the proof relies on the following result it is undecidable whether a multiplicity automaton a over \u03c3 satisfies ra for every u \u03c3 . it is easy to show that this result still holds for the set of ma a which satisfy ra \u03bb u for any \u03bb . proposition . it is undecidable whether two rational series define the same stochastic language . proof . the proof is detailed in annex . . experiments in this section we present a set of experiments allowing us to study the performance of the algorithm dees for learning good stochastic language models . hence we will study the behavior of dees with samples of distributions generated from pda pa and non rational stochastic language . we decide to compare dees to the most well known probabilistic grammatical inference approaches the algorithms alergia and mdi that are able to identify pdas . these algorithms can be tuned by a parameter in the experiments we choose the best parameter which gives the best result on all the samples but we didn t change the parameter according to the size of the sample in order to take into account the impact of the sample sizes . in our experiments we use two performance criteria . we measure the size of the inferred models by the number of states . moreover to evaluate the quality of the automata we use the d norm between two models a and a defined by d u \u03c3 pa pa . d norm is the strongest distance after kullback leibler . in practice we use an approximation by considering a subset of \u03c3 generated by a. p note that we can t use the kullback leibler measure because it is not robust with null probability strings which implies to smooth the learned models and also because automata produced by dees do not always define stochastic language i.e. some strings may have a negative value . amaury habrard fran\u00e7ois denis and yann esposito dees alergia mdi d e c n a t s d i. dees alergia mdi s e t a t s f o r e b m u n size of the learning sample results with distance d size of the learning sample size of the model . fig . results obtained with the prefix reduced multiplicity automaton of three states of figure admitting a representation with a pda of twelve states . a \u03b1 b \u03b1 a \u03b1 b \u03b1 a b b \u03b5 a a a b a b fig . automaton a is a pa with non rational parameters in r. a can be represented by an ma b with rational parameters in q. we carried out a first series of experiment where the target automaton can be represented by a pda . we consider a stochastic language defined by the automaton on figure . this stochastic language can be represented by a multiplicity automaton of three states and by an equivalent minimal pda of twelve states . to compare the performances of the three algorithms we used the following experimental set up . from the target automaton we generate samples from size to . then for each sample we learn an automaton with the three algorithms and compute the norm d between them and the target . we repeat this experimental setup times and give the average results . figure reports the results obtained . if we consider the size of the learned models dees finds quickly the target automaton while mdi only begins to tend to the target pda after examples . the automata produced by alergia are far from this target . this behavior can be explained by the fact that these two algorithms need significantly longer examples to find the correct target and thus larger samples this is also amplified because there are more parameters to estimate . in practise we noticed that the correct structure can be found after more than examples . if we look at the distance d dees outperforms mdi and alergia and begins to converge after examples . we carried out other series of experiments for evaluating dees when the target belongs to the class of pa . first we consider the simple automaton of figure which defines a stochastic language that can be represented by a pa with parameters in r. we follow the same experimental setup as in the first experiment the results are reported on figure . according to our performance criteria dees outperforms again alergia and mdi . in fact the target can not be modeled correctly by alergia and mdi because it can not be represented by a pda . this explains why these algorithms can t find a good model . for them the best answer is to produce a unigram model . alergia even using pseudo stochastic rational languages in probabilistic grammatical inference d e c n a t s d i d e c n a t s d i. dees alergia mdi dees alergia mdi s e t a t s f o r e b m u n t l u s e r e h t f o s e t a t s f o r e b m u n size of the learning sample results with distance d size of the learning sample size of the model . fig . results obtained with the target automaton of figure admitting a representation in the class pa with non rational parameters . dees alergia learning sample dees alergia line y x number of states of the target results with distance d number of states of the target size of the model . fig . results obtained from a set of pa generated randomly . diverge at a given step and mdi returns always the unigram . dees finds the correct structure quickly and begins to converge after examples . this behavior confirms the fact dees can produce better models with small samples because it constructs small representations . on the other hand alergia and mdi seem to need a huge number of examples to find a good approximation of the target even when the target is relatively small . we made another experiment in the class of pa . we study the behavior of dees when the learning samples are generated from different targets randomly generated . for this experiment we take an alphabet of three letters and we generate randomly some pa with a number of states from to . the pa are generated in order to have a prefix representation which guarantees that all the states are reachable . the rest of the transitions and the values of the parameters are chosen randomly . then for each target we generate samples of size times the number of states of the target . we made this choice because we think that for small targets the samples may be sufficient to find a good approximation while for bigger targets there is a clear lack of examples . this last point allows us to see the behaviors of the algorithms with small amounts of data . we learn an automaton from each sample and compare it to the corresponding target . note that we didn t use mdi in this experiment because this algorithm is extremely hard to tune which implies an important cost in time for finding a good parameter . the parameter of alergia is fixed to a reasonable value kept for all the experiment . results for alergia and dees are reported on figure . we also add the empirical distance of amaury habrard fran\u00e7ois denis and yann esposito d e c n a t s d i. dees alergia mdi s e t a t s f o r e b m u n. dees alergia mdi size of the learning sample results with distance d size of the learning sample size of the model . fig . results obtained with samples generated from a non rational stochastic language . the samples to the target automaton . if you consider the d norm the performances of alergia depend highly on the empirical distribution . alergia infers models close or better than those produced by dees only when the empirical distribution is already very good thus when it is not necessary to learn . moreover alergia has a greater variance which implies a weak robustness . on the other hand dees is always able to learn significantly small models almost always better even with small samples . finally we carried out a last experiment where the objective is to study the behavior of the three algorithms with samples generated from a non rational stochastic language . we consider as a target the stochastic language generated using the pr algorithm from the automaton of figure . we took \u03c1 \u03b1 and \u03b2 . we follow the same experimental setup than the first experiment . since we use rational representations we measure the distance d from the automaton of figure using a sample generated by pr . the results are presented on figure . mdi and alergia are clearly not able to build a good estimation of the target distribution and we see that their best answer is to produce a unigram . on the other hand dees is able to identify a structure close to the ma that was used for defining the distribution and produces good automata after examples . this means that dees seems able to produce pseudo stochastic rational languages which are closed to a non rational stochastic distribution . conclusion in this paper we studied the class of pseudo stochastic rational languages that are stochastic languages defined by multiplicity automata which do not define stochastic languages but share some properties with them . we showed that it is possible to decide wether an ma defines a psrl but we can t decide wether two ma define the same psrl . moreover it is possible to define a stochastic language from these ma but this language is not rational in general . despite of these drawbacks we showed experimentally that dees produces ma computing pseudo stochastic rational languages that provide good estimates of a target stochastic language . we recall here that dees is able to output automata with a minimal number of parameters which is clearly an advantage from a machine learning standpoint especially for dealing with small datasets . moreover our experiments showed that dees outperforms standard probabilistic grammat using pseudo stochastic rational languages in probabilistic grammatical inference ical inference approaches . thus we think that the class of pseudo stochastic rational languages is promising for many applications in grammatical inference . beyond the fact to continue the study of this class we also plan to consider methods that could infer a class of ma strictly greater than the class of psrl . we also began to work on an adaptation of the approaches presented in this paper to trees ."
    ],
    "abstract": [
        "in probabilistic grammatical inference a usual goal is to infer a good approximation of an unknown distribution p called a stochastic language . the estimate of p stands in some class of probabilistic models such as probabilistic automata . in this paper we focus on probabilistic models based on multiplicity automata . the stochastic languages generated by ma are called rational stochastic languages they strictly include stochastic languages generated by pa they also admit a very concise canonical representation . despite the fact that this class is not recursively enumerable it is efficiently identifiable in the limit by using the algorithm dees introduced by the authors in a previous paper . however the identification is not proper and before the convergence of the algorithm dees can produce ma that do not define stochastic languages . nevertheless it is possible to use these ma to define stochastic languages . we show that they belong to a broader class of rational series that we call pseudo stochastic rational languages . the aim of this paper is twofold . first we provide a theoretical study of pseudo stochastic rational languages the languages output by dees showing for example that this class is decidable within polynomial time . second we have carried out a lot of experiments in order to compare dees to classical inference algorithms such as alergia and mdi . they show that dees outperforms them in most cases ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.6050420168067226
    ]
}