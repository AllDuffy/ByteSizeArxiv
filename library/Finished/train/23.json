{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0607067v1",
    "article": [
        "introduction this paper belongs to the area of learning theory that has been variously referred to as prediction with expert advice competitive on line prediction prediction of individual sequences and universal on line learning see for a review . there are many proof techniques known in this field this paper is based on kalnishkan and vyugin s weak aggregating algorithm but it is possible that some of the numerous other techniques could be used instead . in section we give the main definitions and state our main results theorems their proofs are given in sections . in section we informally discuss the notion of stationarity and section concludes . main results the game of prediction between predictor and reality is played according to the following protocol . prediction protocol reality announces for n. reality announces xn predictor announces \u03b3n reality announces yn x. y. \u03b3 . end for . after reality s first move the game proceeds in rounds numbered by the positive integers n. at the beginning of each round n. predictor is given some signal xn relevant to predicting the following observation yn . the signal is taken from the signal space x and the observations from the observation space y. predictor then announces his prediction \u03b3n taken from the prediction space \u03b3 and the prediction s quality in light of the actual observation is measured by r. at the beginning of the game reality chooses a loss function \u03bb \u03b3 the infinite past for all n y. in the games of prediction traditionally considered in machine learning there is no infinite past . this situation is modeled in our framework by extending the signal space and observation space by new elements x and y defining \u03bb arbitrarily and making reality announce the infinite past and refrain from announcing xn or yn afterwards . we will always assume that the signal space x the prediction space \u03b3 and the observation space y are non empty topological spaces and that the loss function \u03bb is continuous . moreover we are mainly interested in the case where x \u03b3 and y are locally compact metric spaces the prime examples being euclidean spaces and their open and closed subsets . our first results will be stated for the case where all three spaces x \u03b3 and y are compact . remark our results can be easily extended to the case where the loss on the nth round is allowed to depend in addition to \u03b3n and yn on the past . xn yn xn . this would however complicate the notation . x y. predictor s strategies in the prediction protocol will be called prediction strategies . mathematically such a strategy is a function d x \u03b3 it maps each history and the current time n to the chosen prediction . in this paper we will only be interested in continuous prediction strategies d. an especially natural class of strategies is formed by the stationary prediction strategies d x universal prediction strategies compact deterministic case in this and next subsections we will assume that the spaces x \u03b3 y are all compact . a prediction strategy is cs universal for a loss function \u03bb if its predictions \u03b3n satisfy lim sup n n n n x n n n x \u03bb \u03bb d yn for any continuous stationary prediction strategy d and any biinfinite . x y x y x y. theorem suppose x and y are compact metric spaces \u03b3 is a compact convex subset of a banach space and the loss function \u03bb is continuous in and convex in the variable \u03b3 \u03b3 . there exists a cs universal prediction algorithm . a cs universal prediction algorithm will be constructed in the next section . universal prediction strategies compact randomized case when the loss function \u03bb is not convex in \u03b3 two difficulties appear the conclusion of theorem becomes false if the convexity requirement is removed in some cases the notion of a continuous prediction strategy becomes vacuous e.g. there are no non constant continuous stationary prediction strategies when \u03b3 x is connected . and to overcome these difficulties we consider randomized prediction strategies . the proof of theorem will give a universal in a natural sense randomized prediction algorithm on the other hand there will be a vast supply of continuous stationary prediction strategies . remark in fact the second difficulty is more apparent than real for example there are many non trivial continuous prediction in the binary case y strategies in the canonical form of the prediction game with the prediction space redefined as the boundary of the set of superpredictions . a randomized prediction strategy is a function d x p. mapping the past complemented by the current time to the and . y y y. n are precompact satisfies . c c oy j j ... n in fact the only property of large at infinity loss functions that we will be using is that in the conclusion of lemma . in particular it implies the following lemma . lemma under the conditions of theorem for each compact set b there exists a compact convex set c c stationary prediction strategy d \u03c3 prediction strategy d \u03c3 y \u03b3 such that for each continuous \u03b3 there exists a continuous stationary c that dominates d in the sense \u03c3 y \u03c3 b \u03bb \u03bb . proof without loss of generality b is assumed non empty . fix any \u03b3 let \u03b3 . m sup y b \u03bb let c \u03b3 be a compact set such that let \u03b3 c y b \u03bb m m sup c b \u03bb and let c \u03b3 be a compact set such that \u03bb m. \u03b3 m and \u03b3 it is obvious that m convex . c y b c let us now check that c lies inside the interior of c. indeed for any fixed c b and \u03b3 c. we can and will assume c m since \u03bb m for all \u03b3 c we have \u03bb y some neighborhood of \u03b3 will lie completely in c. let d \u03c3 \u03b3 be a continuous stationary prediction strategy . we will show that holds for some continuous stationary prediction strategy d taking values in the compact convex set c c. namely we define d \u03c1 d \u03c1 \u03c1 d \u03b3 \u03c1 \u03c1 \u03c1 \u03b3 if d if d if d c c c c \u03b3 c is where \u03c1 is the metric on \u03b3 the denominator \u03c1 \u03c1 d \u03b3 positive since already \u03c1 is positive . since c is convex we can see that d indeed takes values in c. the only points x at which the continuity of d is not obvious are those for which d lies on the boundary of c in this case one has to use the fact that c is covered by the interior of c. c it remains to check the only non trivial case is d convexity of \u03bb in \u03b3 the inequality in will follow from c. by the \u03c1 d \u03b3 \u03c1 \u03c1 c \u03bb \u03c1 \u03c1 \u03c1 \u03bb \u03bb i.e. since the left hand side of the last inequality is at most m and its right hand side exceeds m it holds true . \u03bb \u03bb . remark if the loss function is allowed to depend on the infinite past the \u03c3s in lemma will have to be restricted to a compact set a \u03c3 and the compact set c will depend not only on b but also on a. the proof y fix a compact convex c for each compact b \u03b3 as in lemma . predictor s strategy ensuring is constructed from remover s winning strategy y see lemma metric spaces are paracompact by the stone thein g and from predictor s strategies outputting predictions s and ensuring the consequent of for all continuous \u03b3n c d a c s a b for compact a under the assumption that y the existence of such b moves are assumed to be of the form a predictor is simultaneously playing the game of removal g. remover s x and b y. y as evader . at the beginning of the game of prediction predictor asks remover to make b in the game of removal without loss of generality we his first move a b contains all n. predictor then plays the game of pre n diction using the strategy b until reality chooses . as soon as such is chosen predictor announces in the game of removal and notes remover s response . he then continues playing the game of prediction using the strategy until reality chooses let us check that this strategy for predictor will always ensure . if rebk finitely often the ality chooses outside predictor s current ak consequent of will be satisfied for all continuous stationary d \u03c3 c and so if reality chooses by lemma for all continuous stationary d \u03c3 outside predictor s current ak bk infinitely often the set of n. will not be precompact and so the antecedent of will be violated . a a b etc. \u03b3 . s s proof of theorem consisting of the when \u03b3 ranges over \u03b3 the loss function as measures concentrated on c for a compact c we have seen is continuous . the following analogue of lemma will be useful . identified with the subset of p p lemma under the conditions of theorem for each compact set b there exists a compact convex set c c stationary randomized prediction strategy d \u03c3 p uous stationary randomized prediction strategy d \u03c3 holds . y \u03b3 such that for each continuous there exists a contin such that p such that f on c and f on \u03b3 proof define \u03b3 m c m and c as in the proof of lemma . fix a continuous function f \u03b3 c. set be a continuous stationary randomized pref diction strategy . for each \u03c3 \u03c3 split d into two measures on \u03b3 absolutely continuous with respect to d d with radon nikodym density f and d with radon nikodym density f set f. let d \u03c3 p d d d \u03b4\u03b3 p p for p a measure on \u03b3 . letting it is clear that the stationary randomized prediction strategy d is continuous takes values in and p \u03bb \u03bbfd \u03bb fd z\u03b3 z\u03b3 z\u03b3 \u03bbfd mfd \u03bbfd \u03bbfd \u03bb z\u03b3 z\u03b3 z\u03b3 for all \u03c3 fix one of the mappings b b. so we can take c c. c whose existence is asserted by the lemma . we will prove that the prediction strategy of the previous section with replaced by \u03b3n and replaced by p d a p p is cs universal . let d \u03c3 be a continuous stationary randomized prediction strategy i.e. a continuous stationary prediction strategy in the new game of prediction with loss function . let be remover s last move and let d \u03c3 be a continuous stationary randomized prediction strategy satisfying with b bk . from some n on our randomized prediction algorithm produces \u03b3n concentrated on c and they will satisfy p p lim sup n n n n x \u03bb \u03bb n n x n n n n x n n x lim sup n \u03bb n \u03bb . this is an interesting property but slightly di\ufb00erent from what theorem asserts . according to the proof of lemma we can and we will assume that d generates outcomes d n in two steps first dn is generated from d and then it is replaced by \u03b3 with probability f. the loss function is bounded in absolute value on the compact set c bk by a constant l. from the law of the iterated logarithm applied to the losses of \u03b3n and d n we now obtain instead of lim sup n n n n x \u03bb \u03bb n n x n n n n x n n x lim sup n lim sup n n n n n n x n n x \u03bb \u03bb \u03bb \u03bb a.s. it remains to compare this with . stationarity and continuity as we said earlier the assumption of stationarity is very natural for prediction strategies it just means that the arbitrary origin of time is not taken into account . stationary strategies can detect and make use of all kinds of trends and one o\ufb00 phenomena e.g. they can perform well when the rate of environment change is constantly increasing . there need not be stationarity in the environment . interestingly our prediction algorithms are continuous but not stationary . first we discuss the continuity of the prediction algorithms constructed in the proofs of our four theorems . theorem it is easy to check that the waa is continuous by the weierstrass m test converges uniformly and so its sum is continuous . theorem to check that \u03b3n is a continuous function of \u03c3n in the topology f d\u03b3n is a continuous c. this again follows from the weierstrass r of weak convergence we only need to check that function of \u03c3n for each f m test . theorem as described predictor s strategy is not continuous since his behavior changes suddenly when reality outputs outside his current ak bk but it is clear that it can be smoothed around the edges to ensure continuity . theorem the situation is analogous to theorem . for concreteness we will discuss stationarity only in the case of theorem . we know that the waa is a prediction strategy that is continuous as a function \u03b3 . it is not stationary because it has to keep track of the experts losses since the beginning \u03b3 without fixed points . if d were a universal continuous stationary strategy we could define another continuous stationary strategy d f and make reality collude with d. \u03b3 for all \u03b3 stationary reality a standard problem in probability theory is where reality is governed by a stationary probability measure of course only stationary prediction strategies are considered . in this subsection we will list several references for this problem considering for simplicity only the case where the signals xn are absent . the problem of prediction has been studied extensively for both strictly stationary sequences of observations and wide sense stationary sequences . we will first assume that . y y y. form a wide sense stationary sequence of random variables and then a strictly stationary sequence . the natural mode of prediction for wide sense stationary sequences is linear prediction . the problem of linear prediction of wide sense stationary sequences was posed and solved by kolmogorov later but independently this was done by wiener . kolmogorov and wiener assumed the probability distribution of the observations known . there are many efficient ways to estimate the spectral density of this probability distribution see e.g. chapter for a review . the problem of existence of universal prediction strategies for strictly stationary and ergodic sequences of observations was posed by cover and such strategies were found by ornstein for finite y and algoet for y a polish space . papers construct such strategies using techniques very similar to those of this paper . conclusion an interesting direction of further research is to obtain non asymptotic versions of our results . if the benchmark class of continuous stationary prediction strategies is compact loss bounds can be given in terms of \u01eb entropy . in general one can give loss bounds in terms of a nested family of compact sets whose union is dense in the set of continuous stationary prediction strategies . acknowledgments i am grateful to yura kalnishkan and ilia nouretdinov for useful comments . the construction of cs universal prediction strategies is based on alex smola s and g abor lugosi s suggestions . this work was partially supported by mrc ."
    ],
    "abstract": [
        "in this paper we introduce the class of stationary prediction strategies and construct a prediction algorithm that asymptotically performs as well as the best continuous stationary strategy . we make mild compactness assumptions but no stochastic assumptions about the environment . in particular no assumption of stationarity is made about the environment and the stationarity of the considered strategies only means that they do not depend explicitly on time we argue that it is natural to consider only stationary strategies even for highly non stationary environments ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.5862068965517241
    ]
}