{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\9905014v1",
    "article": [
        "the maxq value function decomposition can represent the value function of any policy that is consistent with the given hierarchy . the decomposition also creates opportunities to exploit state abstractions so that individual mdps within the hierarchy can ignore large parts of the state space . this is important for the practical application of the method . this paper defines the maxq hierarchy proves formal results on its representational power and establishes five conditions for the safe use of state abstractions . the paper presents an online model free learning algorithm maxq q and proves that it converges wih probability to a kind of locally optimal policy known as a recursively optimal policy even in the presence of the five kinds of state abstraction . the paper evaluates the maxq representation and maxq q through a series of experiments in three domains and shows experimentally that maxq q converges to a recursively optimal policy much faster than \ufb02at q learning . the fact that maxq learns a representation of the value function has an important benefit it makes it possible to compute and execute an improved non hierarchical policy via a procedure similar to the policy improvement step of policy iteration . the paper demonstrates the e\ufb00ectiveness of this non hierarchical execution experimentally . finally the paper concludes with a comparison to related work and a discussion of the design tradeo\ufb00s in hierarchical reinforcement learning . introduction a central goal of artificial intelligence is to develop techniques for constructing robust autonomous agents that are able to achieve good performance in complex real world environments . one fruitful line of research views agents from an economic perspective an agent interacts with an environment and receives real valued rewards and penalties . the agent s goal is to maximize the total reward it receives . the economic view makes it easy to formalize traditional goals of achievement . but it also makes it easy to formulate goals of prevention and goals of maintenance . goals of achievement can be represented by giving a positive reward for achieving the goal . goals of prevention can be represented by giving a negative reward when bad events occur and goals of maintenance can be represented by giving a positive reward for each time step that the desireable state is maintained . furthermore the economic formalism makes it possible to incorporate uncertainty we can require the agent to maximize the expected value of the total reward in the face of random events in the world . this brief review shows that the economic approach is very expressive a difficult research challenge however is to develop efficient and scalable methods for reasoning planning and learning within the economic ai framework . the area of stochastic planning studies methods for finding optimal or near optimal plans to maximize expected total reward in the case where the agent has complete knowledge of the probabilistic behavior of the environment and the reward function . the basic methods for this case were developed in the s in the field of dynamic programming . unfortunately these methods require time polynomial in the number of states in the state space which makes them prohibitively expensive for most ai problems . hence recent research has focused on methods that can exploit structure within the planning problem to work more efficiently . the area of reinforcement learning studies methods for learning optimal or near optimal plans by interacting directly with the external environment . again the basic methods in reinforcement learning are based on dynamic programming algorithms . however reinforcement learning methods o\ufb00er two important advantages over classical dynamic programming . first the methods are online . this permits them to focus their attention on the parts of the state space that are important and ignore the rest of the space . second the methods can employ function approximation algorithms to represent their knowledge . this allows them to generalize across the state space so that the learning time scales much better . despite the recent advances in both probabilistic planning and reinforcement learning there are still many shortcomings . the biggest of these is the lack of a fully satisfactory method for incorporating hierarchies into these algorithms . research in classical planning has shown that hierarchical methods such as hierarchical task networks macro actions and state abstraction methods can provide exponential reductions in the computational cost of finding good plans . however all of the basic algorithms for probabilistic planning and reinforcement learning are \ufb02at methods they treat the state space as one huge \ufb02at search space . this means that the paths from the start state to the goal state are very long and the length of these paths determines the cost of learning and planning because information about future rewards must be propagated backward along these paths . many researchers have experimented with di\ufb00erent methods of hierarchical reinforcement learning and hierarchical probabilistic planning . this research has explored many di\ufb00erent points in the design space of hierarchical methods but several of these systems were designed for specific situations . we lack crisp definitions of the main approaches and a clear understanding of the relative merits of the di\ufb00erent methods . this paper formalizes and clarifies one approach and attempts to understand how it compares with the other techniques . the approach called the maxq method provides a hierarchical decomposition of the given reinforcement learning problem into a set of subproblems . it simultaneously provides a decomposition of the value function for the given problem into a set of value functions for the subproblems . hence it has both a declarative semantics and a procedural semantics . a review of previous research shows that there are several important design decisions that must be made when constructing a hierarchical reinforcement learning system . as a way of providing an overview of the results in this paper let us review these issues and see how the maxq method approaches each of them . the first issue is how subtasks should be specified . hierarchical reinforcement learning involves breaking the target markov decision problem into a hierarchy of subproblems or subtasks . there are three general approaches to defining these subtasks . one approach is to define each subtask in terms of a fixed policy that is provided by the programmer . the option method of sutton precup and singh takes this approach . the second approach is to define each subtask in terms of a non deterministic finite state controller . the hierarchy of abstract machines method of parr and russell takes this approach . this method permits the programmer to provide a partial policy that constrains the set of permitted actions at each point but does not specify a complete policy for each subtask . the third approach is to define each subtask in terms of a termination predicate and a local reward function . these define what it means for the subtask to be completed and what the final reward should be for completing the subtask . the maxq method described in this paper follows this approach building upon previous work by singh kaelbling dayan and hinton and dean and lin . an advantage of the option and partial policy approaches is that the subtask can be defined in terms of an amount of e\ufb00ort or a course of action rather than in terms of achieving a particular goal condition . however the option approach requires the programmer to provide complete policies for the subtasks which can be a difficult programming task in real world problems . on the other hand the termination predicate method requires the programmer to guess the relative desirability of the di\ufb00erent states in which the subtask might terminate . this can also be difficult although dean and lin show how these guesses can be revised automatically by the learning algorithm . a potential drawback of all hierarchical methods is that the learned policy may be suboptimal . the programmer provided hierarchy constrains the set of possible policies that can be considered . if these constraints are poorly chosen the resulting policy will be suboptimal . nonetheless the learning algorithms that have been developed for the option and partial policy approaches guarantee that the learned policy will be the best possible policy consistent with these constraints . the termination predicate method su\ufb00ers from an additional source of suboptimality . the learning algorithm described in this paper converges to a form of local optimality that we call recursive optimality . this means that the policy of each subtask is locally optimal given the policies of its children . but there might exist better hierarchical policies where the policy for a subtask must be locally suboptimal so that the overall policy is optimal . this problem can be avoided by careful definition of termination predicates and local reward functions but this is an added burden on the programmer . the second design issue is whether to employ state abstractions within subtasks . a subtask employs state abstraction if it ignores some aspects of the state of the environment . for example in many robot navigation problems choices about what route to take to reach a goal location are independent of what the robot is currently carrying . with few exceptions state abstraction has not been explored previously . we will see that the maxq method creates many opportunities to exploit state abstraction and that these abstractions can have a huge impact in accelerating learning . we will also see that there is an important design tradeo\ufb00 the successful use of state abstraction requires that subtasks be defined in terms of termination predicates rather than using the option or partial policy methods . this is why the maxq method must employ termination predicates despite the problems that this can create . the third design issue concerns the non hierarchical execution of a learned hierarchical policy . kaelbling was the first to point out that a value function learned from a hierarchical policy could be evaluated incrementally to yield a potentially much better non hierarchical policy . dietterich and sutton singh precup and ravindran generalized this to show how arbitrary subroutines could be executed non hierarchically to yield improved policies . however in order to support this non hierarchical execution extra learning is required . ordinarily in hierarchical reinforcement learning the only states where learning is required at the higher levels of the hierarchy are states where one or more of the subroutines could terminate . but to support non hierarchical execution learning is required in all states . in general this requires additional exploration as well as additional computation and memory . as a consequence of the hierarchical decomposition of the value function the maxq method is able to support either form of execution and we will see that there are many problems where the improvement from non hierarchical execution is worth the added cost . the fourth and final issue is what form of learning algorithm to employ . an important advantage of reinforcement learning algorithms is that they typically operate online . however finding online algorithms that work for general hierarchical reinforcement learning has been difficult particularly within the termination predicate family of methods . singh s method relied on each subtask having a unique terminal state kaelbling employed a mix of online and batch algorithms to train her hierarchy and work within the options framework usually assumes that the policies for the subproblems are given and do not need to be learned at all . the best previous online algorithms are the hamq q learning algorithm of parr and russell and the feudal q algorithm of dayan and hinton . unfortunately the hamq method requires \ufb02attening the hierarchy and this has several undesirable consequences . the feudal q algorithm is tailored to a specific kind of problem and it does not converge to any well defined optimal policy . in this paper we present a general algorithm called maxq q for fully online learning of a hierarchical value function . we show experimentally and theoretically that the algorithm converges to a recursively optimal policy . we also show that it is substantially faster than \ufb02at q learning when state abstractions are employed . without state abstractions it gives performance similar to the hamq algorithm . the remainder of this paper is organized as follows . after introducing our notation in section we define the maxq value function decomposition in section and illustrate it with a simple example markov decision problem . section presents an analytically tractable version of the maxq q learning algorithm called the maxq algorithm and proves its convergence to a recursively optimal policy . it then shows how to extend maxq to produce the maxq q algorithm and shows how to extend the theorem similarly . section takes up the issue of state abstraction and formalizes a series of five conditions under which state abstractions can be safely incorporated into the maxq representation . state abstraction can give rise to a hierarchical credit assignment problem and the paper brie\ufb02y discusses one solution to this problem . finally section presents experiments with three example domains . these experiments give some idea of the generality of the maxq representation . they also provide results on the relative importance of temporal and state abstractions and on the importance of non hierarchical execution . the paper concludes with further discussion of the design issues that were brie\ufb02y described above and in particular it tackles the question of the tradeo\ufb00 between the method of defining subtasks and the ability to exploit state abstractions . some readers may be disappointed that maxq provides no way of learning the structure of the hierarchy . our philosophy in developing maxq has been to draw inspiration from the development of belief networks . belief networks were first introduced as a formalism in which the knowledge engineer would describe the structure of the networks and domain experts would provide the necessary probability estimates . subsequently methods were developed for learning the probability values directly from observational data . most recently several methods have been developed for learning the structure of the belief networks from data so that the dependence on the knowledge engineer is reduced . in this paper we will likewise require that the programmer provide the structure of the hierarchy . the programmer will also need to make several important design decisions . we will see below that a maxq representation is very much like a computer program and we will rely on the programmer to design each of the modules and indicate the permissible ways in which the modules can invoke each other . our learning algorithms will fill in implementations of each module in such a way that the overall program will work well . we believe that this approach will provide a practical tool for solving large real world mdps . we also believe that it will help us understand the structure of hierarchical learning algorithms . it is our hope that subsequent research will be able to automate most of the work that we are currently requiring the programmer to do . formal definitions . markov decision problems and semi markov decision problems we employ the standard definitions for markov decision problems and semi markov decision problems . in this paper we restrict our attention to situations in which an agent is interacting with a fullyobservable stochastic environment . this situation can be modeled as a markov decision problem hs a p r pi defined as follows s this is the set of states of the environment . at each point in time the agent can observe the complete state of the environment . a this is a finite set of actions . technically the set of available actions depends on the current state s but we will suppress this dependence in our notation . p when an action a a is performed the environment makes a probabilistic transition from its current state s to a resulting state s according to the probability distribution p. r similarly when action a is performed and the environment makes its transition from s to s the agent receives a real valued reward r. to simplify the notation it is customary to treat this reward as being given at the time that action a is initiated even though it may in general depend on s as well as on s and a. p this is the starting state distribution . when the mdp is initialized it is in state s with probability p. a policy \u03c0 is a mapping from states to actions that tells what action a \u03c0 to perform when the environment is in state s. we will consider two settings episodic and infinite horizon . in the episodic setting all rewards are finite and there is at least one zero cost absorbing terminal state . an absorbing terminal state is a state in which all actions lead back to the same state with probability and zero reward . we will only consider problems where all deterministic policies are proper that is all deterministic policies have a non zero probability of reaching a terminal state when started in an arbitrary state . in this setting the goal of the agent is to find a policy that maximizes the expected cumulative reward . in the special case where all rewards are non positive these problems are referred to as stochastic shortest path problems because the rewards can be viewed as costs and the policy attempts to move the agent along the path of minimum expected cost . in the infinite horizon setting all rewards are also finite . in addition there is a discount factor \u03b3 and the agent s goal is to find a policy that minimizes the infinite discounted sum of future rewards . the value function v \u03c0 for policy \u03c0 is a function that tells for each state s what the expected cumulative reward will be of executing that policy . let rt be a random variable that tells the reward that the agent receives at time step t while following policy \u03c0 . we can define the value function in the episodic setting as v \u03c0 e. in the discounted setting the value function is v \u03c0 e rt \u03b3rt \u03b3rt st t \u03c0 . we can see that this equation reduces to the previous one when \u03b3 . however in the infinite horizon case this infinite sum will not converge unless \u03b3 . the value function satisfies the bellman equation for a fixed policy o v \u03c0 p r \u03b3v \u03c0 . the quantity on the right hand side is called the backed up value of performing action a in state s. for each possible successor state s it computes the reward that would be received and the value of the resulting state and then weights those according to the probability of ending up in s. the optimal value function v is the value function that simultaneously maximizes the expected cumulative reward in all states s s. bellman proved that it is the unique solution to what is now known as the bellman equation v max p r \u03b3v . a xs there may be many optimal policies that achieve this value . any policy that chooses a in s to achieve the maximum on the right hand side of this equation is an optimal policy . we will denote an optimal policy by \u03c0 . note that all optimal policies are greedy with respect to the backed up value of the available actions . closely related to the value function is the so called action value function or q function . this function q\u03c0 gives the expected cumulative reward of performing n xs xs xs action a in state s and then following policy \u03c0 thereafter . the q function also satisfies a bellman equation q\u03c0 p r \u03b3q\u03c0 . the optimal action value function is written q and it satisfies the equation q p r \u03b3 max a note that any policy that is greedy with respect to q is an optimal policy . there may be many such optimal policies they di\ufb00er only in how they break ties between actions with identical q values . an action order denoted \u03c9 is a total order over the actions within an mdp . that is \u03c9 is an anti symmetric transitive relation such that \u03c9 is true i\ufb00 a is preferred to a. an ordered greedy policy \u03c0\u03c9 is a greedy policy that breaks ties using \u03c9 . for example suppose that the two best actions at state s are a and a that q q and that \u03c9 . then the ordered greedy policy \u03c0\u03c9 will choose a \u03c0\u03c9 a. note that although there may be many optimal policies for a given mdp the ordered greedy policy \u03c0 \u03c9 is unique . a discrete time semi markov decision process is a generalization of the markov decision process in which the actions can take a variable amount of time to complete . in particular let the random variable n denote the number of time steps that action a takes when it is executed in state s. we can extend the state transition probability function to be the joint distribution of the result states s and the number of time steps n when action a is performed in state s p. similarly the reward function can be changed to be r. it is straightforward to modify the bellman equation to define the value function for a fixed policy \u03c0 as v \u03c0 p r \u03b3n v \u03c0 . xs n h the only change is that the expected value on the right hand side is taken with respect to both s and n and \u03b3 is raised to the power n to re\ufb02ect the variable amount of time that may elapse while executing action a. i note that because expectation is a linear operator we can write each of these bellman equations as the sum of the expected reward for performing action a and the expected value of the resulting state s. for example we can rewrite the equation above as v \u03c0 r p \u03b3n v \u03c0 . where r is the expected reward of performing action \u03c0 in state s where the expectation is taken with respect to s and n. note that for the episodic case there is no di\ufb00erence between a mdp and a semi markov decision process . this formalization is slightly di\ufb00erent than the standard formulation of smdps which separates p and f where f is the cumulative distribution function for the probability that a will terminate in t time units where t is real valued rather than integer valued . in our case it is important to consider the joint distribution of s and n but we do not need to consider actions with arbitrary real valued durations . xs n. reinforcement learning algorithms a reinforcement learning algorithm is an algorithm that is given access to an unknown mdp via the following reinforcement learning protocol . at each time step t the algorithm is told the current state s of the mdp and the set of actions a a that are executable in that state . the algorithm chooses an action a a and the mdp executes this action and returns a real valued reward r. if s is an absorbing terminal state the set of actions a contains only the special action reset which causes the mdp to move to one of its initial states drawn according to p. the learning algorithm is evaluated based on its observed cumulative reward . the cumulative reward of a good learning algorithm should converge to the cumulative reward of the optimal policy for the mdp . in this paper we will make use of two well known learning algorithms q learning and sarsa . both of these algorithms maintain a tabular representation of the action value function q. every entry of the table is initialized arbitrarily . in q learning after the algorithm has observed s chosen a received r and observed s it performs the following update qt qt \u03b1t where \u03b1t is a learning rate parameter . jaakkola jordan and singh and bertsekas and tsitsiklis prove that if the agent follows an exploration policy that tries every action in every state infinitely often and if lim t t t x \u03b1t and lim t t xt \u03b1 t then qt converges to the optimal action value function q with probability . their proof holds in both settings discussed in this paper . the sarsa algorithm is very similar . after observing s choosing a observing r observing s and choosing a the algorithm performs the following update qt qt \u03b1t where \u03b1t is a learning rate parameter . the key di\ufb00erence is that the q value of the chosen action a q appears on the right hand side in the place where q learning uses the q value of the best action . singh jaakkola littman and szepesv ari provide two important convergence results first if a fixed policy \u03c0 is employed to choose actions sarsa will converge to the value function of that policy provided \u03b1t decreases according to equation . second if a so called glie policy is employed to choose actions sarsa will converge to the value function of the optimal policy provided again that \u03b1t decreases according to equation . a glie policy is defined as follows definition a glie policy is any policy satisfying . each action is executed infinitely often in every state that is visited infinitely often . in the limit the policy is greedy with respect to the q value function with probability . r y g b figure the taxi domain the maxq value function decomposition at the center of the maxq method for hierarchical reinforcement learning is the maxq value function decomposition . maxq describes how to decompose the overall value function for a policy into a collection of value functions for individual subtasks . a motivating example to make the discussion concrete let us consider the following simple example . figure shows a by grid world inhabited by a taxi agent . there are four specially designated locations in in this world marked as r b g and y. the taxi problem is episodic . each episode the taxi starts in a randomly chosen square . there is a passenger at one of the four locations and that passenger wishes to be transported to one of the four locations . the taxi must go to the passenger s location pick up the passenger go to the destination location and put down the passenger there . the episode ends when the passenger is deposited at the destination location . there are six primitive actions in this domain four navigation actions that move the taxi one square north south east or west a pickup action and a putdown action . each action is deterministic . there is a reward of for each action and an additional reward of for successfully delivering the passenger . there is a reward of if the taxi attempts to execute the putdown or pickup actions illegally . if a navigation action would cause the taxi to hit a wall the action is a no op and there is only the usual reward of . we seek a policy that maximizes the total reward per episode . there are possible states squares locations for the passenger and destinations . this task has a simple hierarchical structure in which there are two main sub tasks get the passenger and deliver the passenger . each of these subtasks in turn involves the subtask of navigating to one of the four locations and then performing a pickup or putdown action . this task illustrates the need to support temporal abstraction state abstraction and subtask sharing . the temporal abstraction is obvious for example the process of navigating to the passenger s location and picking up the passenger is a temporally extended action that can take di\ufb00erent numbers of steps to complete depending on the distance to the target . the top level policy can be expressed very simply if these temporal abstractions can be employed . the need for state abstraction is perhaps less obvious . consider the subtask of getting the passenger . while this subtask is being solved the destination of the passenger is completely irrelevant it can not a\ufb00ect any of the nagivation or pickup decisions . perhaps more importantly when navigating to a target location only the identity of the target location is important . the fact that in some cases the taxi is carrying the passenger and in other cases it is not is irrelevant . finally support for subtask sharing is critical . if the system could learn how to solve the navigation subtask once then the solution could be shared by both of the get the passenger and deliver the passenger subtasks . we will show below that the maxq method provides a value function representation and learning algorithm that supports temporal abstraction state abstraction and subtask sharing . to construct a maxq decomposition for the taxi problem we must identify a set of individual subtasks that we believe will be important for solving the overall task . in this case let us define the following four tasks navigate . in this subtask the goal is to move the taxi from its current location to one of the four target locations which will be indicated by the formal parameter t. get . in this subtask the goal is to move the taxi from its current location to the passenger s current location and pick up the passenger . put . the goal of this subtask is to move the taxi from the current location to the passenger s destination location and drop o\ufb00 the passenger . root . this is the whole taxi task . each of these subtasks is defined by a subgoal and each subtask terminates when the subgoal is achieved . after defining these subtasks we must indicate for each subtask which other subtasks or primitive actions it should employ to reach its goal . for example the navigate subtask should use the four primitive actions north south east and west . the get subtask should use the navigate subtask and the pickup primitive action and so on . all of this information can be summarized by a directed acyclic graph called the task graph which is shown in figure . in this graph each node corresponds to a subtask or a primitive action and each edge corresponds to a potential way in which one subtask can call one of its child tasks . the notation f ormal actual tells how a formal parameter is to be bound to an actual parameter . now suppose that for each of these subtasks we write a policy to achieve the subtask . we will refer to the policy for a subtask as a subroutine and we can view the parent subroutine as invoking the child subroutine via ordinary subroutine call and return semantics . if we have a policy for each subtask then this gives us an overall policy for the taxi mdp . the root subtask executes its policy by calling subroutines that are policies for the get and put subtasks . the get policy calls subroutines for the pickup primitive action and the navigate subtask . and so on . we will call this collection of policies a hierarchical policy . in a hierarchical policy each subroutine executes until it enters a terminal state for its subtask . definitions let us formalize the discussion so far . pickup navigate putdown root get put t source t destination north south east west figure a task graph for the taxi problem . the maxq decomposition takes a given mdp m and decomposes it into a set of subtasks with the convention that m is the root subtask . definition an unparameterized subtask is a three tuple hti ai rii defined as follows . ti is a termination predicate that partitions s into a set of active states si and a set of terminal states ti . the policy for subtask mi can only be executed if the current state s is in si . ai is a set of actions that can be performed to achieve subtask mi . these actions can either be primitive actions from a the set of primitive actions for the mdp or they can be other subtasks which we will denote by their indexes i. we will refer to these actions as the children of subtask i. if a child subtask mj has formal parameters then it can occur multiple times in ai and each such occurrence must specify the actual values that will be bound to the formal parameters . the set of actions ai may di\ufb00er from one state to another so technically ai is a function of s. however we will suppress this dependence in our notation . ri is the pseudo reward function which specifies a pseudo reward for each transition from a state s si to a terminal state s ti . this pseudo reward tells how desirable each of the terminal states is for this subtask . it is typically employed to give goal terminal states a pseudo reward of and any non goal terminal states a negative reward . each primitive action a from m is a primitive subtask in the maxq decomposition such that a is always executable it always terminates immediately after execution and its pseudo reward function is uniformly zero . if a subtask has formal parameters then each possible binding of actual values to the formal parameters specifies a distinct subtask . we can think of the values of the formal parameters as being part of the name of the subtask . in practice of course we implement a parameterized subtask by parameterizing the various components of the task . if b specifies the actual parameter values for task mi then we can define a parameterized termination predicate ti and a parameterized pseudo reward function ri . to simplify notation in the rest of the paper we will usually omit these parameter bindings from our notation . table pseudo code for execution of a hierarchical policy st is the state of the world at time t kt is the state of the execution stack at time t while top is not a primitive action let top where i is the name of the current subroutine and fi gives the parameter bindings for i let \u03c0i where push onto the stack kt a is the action and fa gives the parameter bindings chosen by policy \u03c0i let pop be the primitive action on the top of the stack . execute primitive action a and update st to be the resulting state of the environment . while top specifies a terminated subtask do pop kt kt is the resulting execution stack . definition a hierarchical policy \u03c0 is a set containing a policy for each of the subtasks in the problem \u03c0 . each subtask policy \u03c0i takes a state and returns the name of a primitive action to execute or the name of a subroutine to invoke . in the terminology of sutton precup and singh a subtask policy is a deterministic option and its probability of terminating in state s is if s si and if s ti . in a parameterized task the policy must be parameterized as well so that \u03c0 takes a state and the bindings of formal parameters and returns a chosen action and the bindings of its formal parameters . table gives a pseudo code description of the procedure for executing a hierarchical policy . the hierarchical policy is executed using a stack discipline as in ordinary programming languages . let kt denote the contents of the pushdown stack at time t. when a subroutine is invoked its name and actual parameters are pushed onto the stack . when a subroutine terminates its name and actual parameters are popped o\ufb00 the stack . it is sometimes useful to think of the contents of the stack as being an additional part of the state space for the problem . hence a hierarchical policy implicitly defines a mapping from the current state st and current stack contents kt to a primitive action a. this action is executed and this yields a resulting state st and a resulting stack contents kt . because of the added state information in the stack the hierarchical policy is non markovian with respect to the original mdp . because a hierarchical policy maps from states s and stack contents k to actions the value function for a hierarchical policy must in general also assign values to all combinations of states s and stack contents k. definition a hierarchical value function denoted v \u03c0 gives the expected cumulative reward of following the hierarchical policy \u03c0 starting in state s with stack contents k. in this paper we will primarily be interested only in the top level value of the hierarchical policy that is the value when the stack k is empty v \u03c0 . this is the value of executing the hierarchical policy beginning in state s and starting at the top level of the hierarchy . definition the projected value function denoted v \u03c0 is the value of executing hierarchical policy \u03c0 starting in state s and starting at the root of the task hierarchy . decomposition of the projected value function now that we have defined a hierarchical policy and its projected value function we can show how that value function can be decomposed hierarchically . the decomposition is based on the following theorem theorem given a task graph over tasks m. mn and a hierarchical policy \u03c0 each subtask mi defines a semi markov decision process with states si actions ai probability transition function i and expected reward function r v \u03c0 where v \u03c0 is the projected p \u03c0 value function for child task ma in state s. if a is a primitive action v \u03c0 is defined as the expected immediate reward of executing a in s v \u03c0 s p r. proof consider all of the subroutines that are descendants of task mi in the task graph . because all of these subroutines are executing fixed policies the i is a well defined stationary distribution for each child probability transition function p \u03c0 subroutine a. the set of states si and the set of actions ai are obvious . the interesting part of this theorem is the fact that the expected reward function r of the smdp is the projected value function of the child task ma . p to see this let us write out the value of v \u03c0 v \u03c0 e this sum continues until the subroutine for task mi enters a state in ti . now let us suppose that the first action chosen by \u03c0i is a subroutine a. this subroutine is ini . voked and it executes for a number of steps n and terminates in state s according to p \u03c0 we can rewrite equation as v \u03c0 e \u03b3urt u \u03b3urt u st s \u03c0 n the first summation on the right hand side of equation is the discounted sum of rewards for executing subroutine a starting in state s until it terminates in other words it is v \u03c0 the projected value function for the child task ma . the second term on the right hand side of the equation is the value of s for the current task i v \u03c0 discounted by \u03b3n where s is the current state when subroutine a terminates . we can write this in the form of a bellman equation v \u03c0 v \u03c0 p \u03c0 i \u03b3n v \u03c0 xs n this has the same form as equation which is the bellman equation for an smdp where the first term is the expected reward r. q.e.d. to obtain a hierarchical decomposition of the projected value function let us switch to the action value representation . first we need to extend the q notation to handle the task hierarchy . let q\u03c0 be the expected cumulative reward for subtask mi of performing action a in state s and then following hierarchical policy \u03c0 until subtask mi terminates . with this notation we can re state equation as follows q\u03c0 v \u03c0 p \u03c0 i \u03b3n q\u03c0 xs n the right most term in this equation is the expected discounted reward of completing task mi after executing action a in state s. this term only depends on i s and a because the summation marginalizes away the dependence on s and n. let us define c \u03c0 to be equal to this term definition the completion function c \u03c0 is the expected discounted cumulative reward of completing subtask mi after invoking the subroutine for subtask ma in state s. the reward is discounted back to the point in time where a begins execution . c \u03c0 p \u03c0 i \u03b3n q\u03c0 xs n with this definition we can express the q function recursively as q\u03c0 v \u03c0 c \u03c0 . finally we can re express the definition for v \u03c0 as v \u03c0 q\u03c0 s p r if i is composite if i is primitive p we will refer to equations and as the decomposition equations for the maxq hierarchy under a fixed hierarchical policy \u03c0 . these equations recursively decompose the projected value function for the root v \u03c0 into the projected value functions for the individual subtasks m. mn and the individual completion functions c \u03c0 for j. n. the fundamental quantities that must be stored to represent the value function decomposition are just the c values for all non primitive subtasks and the v values for all primitive actions . to make it easier for programmers to design and debug maxq decompositions we have developed a graphical representation that we call the maxq graph . a maxq graph for the taxi domain is shown in figure . the graph contains two kinds of nodes max nodes and q nodes . the max nodes correspond to the subtasks in the task decomposition there is one max node for each primitive action and one max node for each subtask task . each primitive max node i stores the value of v \u03c0 . the q nodes correspond to the actions that are available for each subtask . each q node for parent task i state s and subtask a stores the value of c \u03c0 . in addition to storing information the max nodes and q nodes can be viewed as performing parts of the computation described by the decomposition equations . specifically each max node i can be viewed as computing the projected value function v \u03c0 for its subtask . for primitive max nodes this information is stored in the node . for composite max nodes this information is obtained by asking the q node corresponding to \u03c0i . each q node with parent task i and child task a can be viewed as computing the value of q\u03c0 . it does this by asking its child task a for its projected value function v \u03c0 and then adding its completion function c \u03c0 . as an example consider the situation shown in figure which we will denote by s. suppose that the passenger is at r and wishes to go to b. let the hierarchical policy we are evaluating be an optimal policy denoted by \u03c0 . the value of this state under \u03c0 is because it will cost unit to move the taxi to r unit to pickup the passenger units to move the taxi to b and unit to putdown the passenger for a total of units . when the passenger is delivered the agent gets a reward of so the net value is . figure shows how the maxq hierarchy computes this value . to compute the value v \u03c0 maxroot consults its policy and finds that \u03c0root is get . hence it asks the q node qget maxroot qget qput maxget maxput qpickup qnavigateforget qnavigateforput qputdown t source t destination pickup putdown maxnavigate qnorth qeast qsouth qwest north east south west figure a maxq graph for the taxi domain to compute q\u03c0 . the completion cost for the root task after performing a get c \u03c0 is because it will cost units to deliver the customer after completing the get subtask . however this is just the reward after completing the get so it must ask maxget to estimate the expected reward of performing the get itself . the policy for maxget dictates that in s the navigate subroutine should be invoked with t bound to r so maxget consults the q node qnavigateforget to compute the expected reward . qnavigateforget knows that after completing the navigate task one more action will be required to complete the get so c \u03c0 . it then asks maxnavigate to compute the expected reward of performing a navigate to location r. the policy for maxnavigate chooses the north action so maxnavigate asks qnorth to compute the value . qnorth looks up its completion cost and finds that c \u03c0 is . it consults maxnorth to determine the expected cost of performing the north action itself . because maxnorth is a primitive action it looks up its expected reward which is . now this series of recursive computations can conclude as follows q\u03c0 maxroot qput maxput qget maxget qpickup qnavigateforget qnavigateforput qputdown pickup putdown maxnavigate qnorth qeast qsouth qwest north east south west figure computing the value of a state using the maxq hierarchy . the c value of each q node is shown to the left of the node . all other numbers show the values being returned up the graph . v \u03c0 q\u03c0 to perform the navigate plus to complete the get . v \u03c0 q\u03c0 . the end result of all of this is that the value of v \u03c0 is decomposed into a sum of c terms plus the expected reward of the chosen primitive action v \u03c0 v \u03c0 c \u03c0 c \u03c0 c \u03c0 v \u03c0 xxxxxxx ppppp v \u03c0 . v \u03c0 z z zz v \u03c0 c \u03c0 c \u03c0 c \u03c0 r r r r r r r r r r r r. figure the maxq decomposition r. r denote the sequence of rewards received from primitive actions at times . in general the maxq value function decomposition has the form v \u03c0 v \u03c0 c \u03c0 . c \u03c0 c \u03c0 where a a. am is the path of max nodes chosen by the hierarchical policy going from the root down to a primitive leaf node . we can summarize the presentation of this section by the following theorem theorem let \u03c0 be a hierarchical policy defined for a given maxq graph with subtasks m. mn and let i be the root node of the graph . then there exist values for c \u03c0 and v \u03c0 such that v \u03c0 is the expected discounted cumulative reward of following policy \u03c0 starting in state s. proof the proof is by induction on the number of levels in the task graph . at each level i we compute values for c \u03c0 according to the decomposition equations . we can apply the decomposition equations again to compute q\u03c0 and apply equation and theorem to conclude that q\u03c0 gives the value function for level i. when i we obtain the value function for the entire hierarchical policy . q. e. d. it is important to note that this representation theorem does not mention the pseudo reward function because the pseudo reward is used only during learning . this theorem captures the representational power of the maxq decomposition but it does not address the question of whether there is a learning algorithm that can find a given policy . that is the subject of the next section . a learning algorithm for the maxq decomposition in order to develop a learning algorithm for the maxq decomposition we must consider exactly what we are hoping to achieve . of course for any mdp m we would like to find an optimal policy \u03c0 . however in the maxq method the programmer imposes a hierarchy on the problem . this hierarchy constrains the space of possible policies so that it may not be possible to represent the optimal policy or its value function . in the maxq method the constraints take two forms . first within a subtask only some of the possible primitive actions may be permitted . for example in the taxi task during a navigate only the north south east and west actions are available the pickup and putdown actions are not allowed . second consider a max node mj with child nodes . the policy learned for mj must involve executing the learned policies of these child nodes . when the policy for child node mji is executed it will run until it enters a state in tji . hence any policy learned for mj must pass through some subset of these terminal state sets . the ham method shares these same two constraints and in addition it imposes a partial policy on each node so that the policy for any subtask mi must be a deterministic refinement of the given non deterministic initial policy for node i. in the option approach the policy is even further constrained . in this approach there are only two non primitive levels in the hierarchy and the subtasks at the lower level are given complete policies by the programmer . hence any learned policy must be constructed by concatenating the given lower level policies in some order . the purpose of imposing these constraints on the policy is to incorporate prior knowledge and thereby reduce the size of the space that must be searched to find a good policy . however these constraints may make it impossible to learn the optimal policy . if we can t learn the optimal policy the next best target would be to learn the best policy that is consistent with the given hierarchy . definition a hierarchically optimal policy for mdp m is a policy that achieves the highest cumulative reward among all policies consistent with the given hierarchy . parr proves that his hamq learning algorithm converges with probability to a hierarchically optimal policy . similarly given a fixed set of options sutton precup and singh prove that their smdp learning algorithm converges to a hierarchically optimal value function . with the maxq method we will seek an even weaker form of optimality recursive optimality . definition a recursively optimal policy for mdp m with maxq decomposition is a hierarchical policy \u03c0 such that for each subtask mi the corresponding policy \u03c0i is optimal for the smdp defined by the set of states si the set of actions ai the state transition probability function p \u03c0 and the reward function given by the sum of the original reward function r and the pseudo reward function ri . note that in this definition the state transition probability distribution is defined by the locally optimal policies of all subtasks that are descendants of mi in the maxq graph . hence recursive optimality is a kind of local optimality in which the policy at each node is optimal given the policies of its children . the reason to seek recursive optimality rather than hierarchical optimality is that recursive optimality makes it possible to solve each subtask without reference to the context in which it is executed . this context free property makes it easier to share and re use subtasks . it will also turn out to be essential for the successful use of state abstraction . before we proceed to describe our learning algorithm for recursive optimality let us see how recursive optimality di\ufb00ers from hierarchical optimality . maxroot qexit qgotogoal maxexit maxgotogoal qexitnorth qexitsouth qexiteast qnorthg qsouthg qeastg g figure a simple mdp and its associated maxq graph . the policy shown in the left diagram is recursively optimal but not hierarchically optimal . the shaded cells indicate points where the locally optimal policy is not globally optimal . north south east it is easy to construct examples of policies that are recursively optimal but not hierarchically optimal . consider the simple maze problem and its associated maxq graph shown in figures . suppose a robot starts somewhere in the left room and it must reach the goal g in the right room . the robot has three actions north south and east and these actions are deterministic . the robot receives a reward of for each move . let us define two subtasks exit . this task terminates when the robot exits the left room . we can set the pseudo reward function r to be for the two terminal states . gotogoal . this task terminates when the robot reaches the goal g. the arrows in figure show the locally optimal policy within each room . the arrows on the left seek to exit the left room by the shortest path because this is what we specified when we set the pseudo reward function to . the arrows on the right follow the shortest path to the goal which is fine . however the resulting policy is neither hierarchically optimal nor optimal . there exists a hierarchical policy that would always exit the left room by the upper door . the maxq value function decomposition can represent the value function of this policy but such a policy would not be locally optimal . fix this problem . the value of the upper starred state under the optimal hierarchical policy is and the value of the lower starred state is . hence if we set r to have these values then the recursively optimal policy would be hierarchically optimal . in other words if the programmer can guess the right values for the terminal states of a subtask then the recursively optimal policy will be hierarchically optimal . this basic idea was first pointed out by dean and lin . they describe an algorithm that makes initial guesses for the values of these starred states and then updates those guesses based on the computed values of the starred states under the resulting recursively optimal policy . they proved that this will converge to a hierarchically optimal policy . the drawback of their method is that it requires repeated solution of the resulting hierarchical learning problem and this does not always yield a speedup over just solving the original \ufb02at problem . parr proposed an interesting approach that constructs a set of di\ufb00erent r functions and computes the recursively optimal policy under each of them for each subtask . his method chooses the r functions in such a way that the hierarchically optimal policy can be approximated to any desired degree . unfortunately the method is quite inefficient because it relies on solving a series of linear programming problems each of which requires time polynomial in several parameters including the number of states si within the subtask . this discussion suggests that while in principle it is possible to learn good values for the pseudoreward function in practice we must rely on the programmer to specify a single pseudo reward function r. if the programmer wishes to consider a small number of alternative pseudo reward functions they can be handled by defining a small number of subtasks that are identical except for their r functions and permitting the learning algorithm to choose the one that gives the best recursively optimal policy . in practice we have employed the following simplified approach to defining r. for each subtask mi we define two predicates the termination predicate ti and a goal predicate gi . the goal predicate defines a subset of the terminated states that are goal states and these have a pseudoreward of . all other terminal states have a fixed constant pseudo reward that is set so that it is always better to terminate in a goal state than in a non goal state . for the problems on which we have tested the maxq method this worked very well . in our experiments with maxq we have found that it is easy to make mistakes in defining ti and gi . if the goal is not defined carefully it is easy to create a set of subtasks that lead to infinite looping . for example consider again the problem in figure . suppose we permit a fourth action west in the mdp and let us define the termination and goal predicates for the right hand room to be satisfied i\ufb00 either the robot reaches the goal or it exits the room . this is a very natural definition since it is quite similar to the definition for the left hand room . however the resulting locally optimal policy for this room will attempt to move to the nearest of these three locations the goal the upper door or the lower door . we can easily see that for all but a few states near the goal the only policies that can be constructed by maxroot will loop forever first trying to leave the left room by entering the right room and then trying to leave the right room by entering the left room . this problem is easily fixed by defining the goal predicate gi for the right room to be true if and only if the robot reaches the goal g. but avoiding such undesired termination bugs can be hard in more complex domains . now that we have an understanding of recursively optimal policies we present two learning algorithms . the first one called maxq applies only in the case when the pseudo reward function r is always zero . we will first prove its convergence properties and then show how it can be extended to give the second algorithm maxq q which works with general pseudo reward functions . table gives pseudo code for maxq . maxq is a recursive function that executes the current exploration policy starting at max node i in state s. it performs actions until it reaches a terminal state at which point it returns a count of the total number of primitive actions that have been executed . to execute an action maxq calls itself recursively . when the recursive call returns it updates the value of the completion function for node i. it uses the count of the number of primitive actions to appropriately discount the value of the resulting state s. at leaf nodes maxq updates the estimated one step expected reward v. the value \u03b1t is a learning rate parameter that should be gradually decreased to zero in the limit . there are two things that must be specified in order to make this algorithm description complete . first we must specify how to compute vt in line since it is not stored in the max node . it is computed by the following modified versions of the decomposition equations table the maxq learning algorithm . function maxq if i is a primitive maxnode execute i receive r and observe result state s vt vt \u03b1t rt return else let count while ti is false do choose an action a according to the current exploration policy \u03c0x let n maxq observe result state s ct ct \u03b1t \u03b3n vt end maxq table pseudo code for greedy execution of the maxq graph function evaluatemaxnode if i is a primitive max node return hvt ii else for each j ai let hv aji evaluatemaxnode let jhg argmaxj vt ct return hvt ajhg i end evaluatemaxnode vt maxa qt vt if i is composite if i is primitive qt vt ct. these equations re\ufb02ect two important changes compared with equations and . first in the first equation vt is defined in terms of the q value of the best action a rather than of the action chosen by a fixed hierarchical policy . second there are no \u03c0 superscripts because the current value function vt is not based on a fixed hierarchical policy \u03c0 . to compute vt using these equations we must perform a complete search of all paths through the maxq graph starting at node i and ending at the leaf nodes . table gives pseudocode for a recursive function evaluatemaxnode that implements a depth first search . in addition to returning vt evaluatemaxnode also returns the action at the leaf node that achieves this value . this information is not needed for maxq but it will be useful later when we consider non hierarchical execution of the learned recursively optimal policy . the second thing that must be specified to complete our definition of maxq is the exploration policy \u03c0x . we require that \u03c0x be an ordered glie policy . definition an ordered glie policy is a glie policy that converges in the limit to an ordered greedy policy which is a greedy policy that imposes an arbitrary fixed order \u03c9 on the available actions and breaks ties in favor of the action a that appears earliest in that order . we need this property in order to ensure that maxq converges to a uniquely defined recursively optimal policy . a fundamental problem with recursive optimality is that in general each max node i will have a choice of many di\ufb00erent locally optimal policies given the policies adopted by its descendant nodes . these di\ufb00erent locally optimal policies will all achieve the same locally optimal value function but they can give rise to di\ufb00erent probability transition functions p. the result will be that the semi markov decision problem defined at the next level above node i in the maxq graph will di\ufb00er depending on which of these various locally optimal policies is chosen by node i. however if we establish a fixed ordering over the max nodes in the maxq graph and break ties in favor of the lowest numbered action then this defines a unique policy at each max node . and consequently by induction it defines a unique policy for the entire maxq graph . let us call this policy \u03c0 r. we will use the r subscript to denote recursively optimal quantities under an ordered greedy policy . hence the corresponding value function is v r denote the corresponding completion function and action value function . we now prove that the maxq algorithm converges to \u03c0 r. r and c r and q theorem let m hs a p r pi be either an episodic mdp for which all deterministic policies are proper or a discounted infinite horizon mdp with discount factor \u03b3 . let h be a maxq graph defined over subtasks such that the pseudo reward function ri is zero for all i s a and s. let \u03b1t be a sequence of constants for each max node i such that lim t t t x lim t t t x \u03b1t and \u03b1 t let \u03c0x be an ordered glie policy at each node i and state s and assume that vt and ct are bounded for all t i s and a. then with probability algorithm maxq converges to \u03c0 r the unique recursively optimal policy for m consistent with h and \u03c0x . proof the proof follows an argument similar to those introduced to prove the convergence of q learning and sarsa . we will employ the following result from stochastic approximation theory lemma consider the iteration rt rt \u03b1t . let ft be the entire history of the iteration . if the \u03b1t satisfy conditions for every i and t the noise terms wt satisfy e given any norm on rn there exist constants a and b such that e a b rt. there exists a vector r a positive vector \u03be and a scalar \u03b2 such that for all t u rt r \u03be \u03b2 rt r \u03be there exists a nonnegative random sequence \u03b8t that converges to zero with probability and is such that for all t ut \u03b8t then rt converges to r with probability . the notation \u03be denotes a weighted maximum norm a \u03be max x a \u03be . the structure of the proof of theorem will be inductive starting at the leaves of the maxq graph and working toward the root . we will employ a di\ufb00erent time clock at each node i to count the number of update steps performed by maxq at that node . the variable t will always refer to the time clock of the current node i. to prove the base case for any primitive max node we note that line of maxq is just the standard stochastic approximation algorithm for computing the expected reward for performing action a in state s and therefore it converges under the conditions given above . to prove the recursive case consider any composite max node i with child node j. let pt be the transition probability distribution for performing child action j in state s at time t. by the inductive assumption maxq applied to j will converge to the recursively optimal value function v r with probability . furthermore because maxq is following an ordered glie policy for j and its descendants pt will converge to p r the unique transition probability function for executing child j under the locally optimal policy \u03c0 r. what remains to be shown is that the update assignment for c will converge to the optimal c r function with probability . to prove this we will apply lemma . we will identify the x in the lemma with a state action pair . the vector rt will be the completion cost table ct for all s a and fixed i after t update steps . the vector r will be the optimal completion cost c r. define the mapping u to be r \u03b3n a xs this is a c update under the mdp mi assuming that all descendant value functions v transition probabilities p r have converged . r and to apply the lemma we must first express the c update formula in the form of the update rule in the lemma . let s be the state that results from performing a in state s. line can be written ct ct \u03b1t \u03b3n ct \u03b1t where wt \u03b3n max a ut pt\u03b3n pt\u03b3n xs n xs n xs n max a max a max a r \u03b3n here wt is the di\ufb00erence between doing an update at node i using the single sample point s drawn according to pt and doing an update using the full distribution pt . the value of ut captures the di\ufb00erence between doing an update using the current probability transitions pt and current value functions of the children vt and doing an update using the optimal probability transitions p r and the optimal values of the children r. v we now verify the conditions of lemma . condition is assumed in the conditions of the theorem with \u03b1t \u03b1t . condition is satisfied because s is sampled from pt so the expected value of the di\ufb00erence is zero . condition follows directly from the assumption that the ct and vt are bounded . condition is the condition that u is a weighted max norm pseudo contraction . we can derive this by starting with the weighted max norm for q learning . it is well known that q is a weighted max norm pseudo contraction in both the episodic case where all deterministic policies are proper and in the infinite horizon discounted case . that is there exists a positive vector \u03be and a scalar \u03b2 such that for all t t qt q \u03be \u03b2 qt q \u03be where t is the operator p \u03b3n . xs n now we will show how to derive the contraction for the c update operator u. our plan is to show first how to express the u operator for learning c in terms of the t operator for updating q values . then we will replace t q in the contraction equation for q learning with u c and show that u is a weighted max norm contraction under the same weights \u03be and the same \u03b2 . recall from eqn . that q c v. furthermore the u operator performs its updates using the optimal value functions of the child nodes so we can write this as qt ct v. now once the children of node i have converged the q function version of the bellman equation for mdp mi can be written as q r \u03b3n . as we have noted before v for node i the t operator can be rewritten as r plays the role of the immediate reward function for mi . therefore r \u03b3n . xs n xs n now we replace q by c v r and obtain r \u03b3n . xs n note that v obtain r does not depend on s or n so we can move it outside the expectation and v r r \u03b3n xs n r v abusing notation slightly we will express this in vector form as t q v we can write qt ct v r in vector form as qt ct v r. r u c. similarly now we can substitute these two formulas into the max norm pseudo contraction formula for t eqn . to obtain a a v r u ct \u03be \u03b2 v r ct \u03be . the v terms cancel on both sides of the equation and we get u ct c r \u03be \u03b2 ct c r \u03be . finally it is easy verify the most important condition . by assumption the ordered glie policies in the child nodes converge with probability to locally optimal policies for the children . therefore pt converges to p r for all s n s and a with probability and vt converges with probability to v r for all child actions a. therefore ut converges to zero with probability . we can trivially construct a sequence \u03b8t ut that bounds this convergence so ut \u03b8t \u03b8t . we have verified all of the conditions of lemma so we can conclude that ct converges to c r with probability . by induction we can conclude that this holds for all nodes in the maxq including the root node so the value function represented by the maxq graph converges to the unique value function of the recursively optimal policy \u03c0 r. q.e.d. algorithm maxq can be extended to accelerate learning in the higher nodes of the graph by a technique that we call all states updating . when an action a is chosen for max node i in state s the execution of a will move the environment through a sequence of states s s. sn sn s. if a was indeed the best abstract action to choose in s then it should also be the best action to choose in states s through sn . hence we can execute a version of line in maxq for each of these intermediate states as shown in this replacement pseudo code a b c for j from to n do ct ct \u03b1t \u03b3maxa qt end for in our implementation as each composite action is executed by maxq it constructs a linked list of the sequence of primitive states that were visited . this list is returned when the composite action terminates . the parent max node can then process each state in this list as shown above . the parent max node appends the state lists that it receives from its children and passes them to its parent when it terminates . all experiments in this paper employ all states updating . kaelbling introduced a related but more powerful method for accelerating hierarchical reinforcement learning that she calls all goals updating . this method is suitable for a maxq hierarchy containing only a root task and one level of composite tasks . to understand all goals updating suppose that for each primitive action there are several composite tasks that could have invoked that primitive action . in all goals updating whenever a primitive action is executed the equivalent of line of maxq is applied in every composite task that could have invoked that primitive action . sutton precup and singh prove that each of the composite tasks will converge to the optimal q values under all goals updating . all goals updating would work in the maxq hierarchy for composite tasks all of whose children are primitive actions . however as we have seen at higher levels in the hierarchy node i needs to obtain samples of result states drawn according to p for composite tasks a. all goals updating can not provide these samples so it can not be applied at these higher levels . now that we have shown the convergence of maxq let us design a learning algorithm for arbitrary pseudo reward functions ri . we could just add the pseudo reward into maxq but this has the e\ufb00ect of changing the mdp m to have a di\ufb00erent reward function . the pseudorewards contaminate the values of all of the completion functions computed in the hierarchy . the resulting learned policy will not be recursively optimal for the original mdp . this problem can be solved by learning two completion functions . the first one c is the completion function that we have been discussing so far in this paper . it computes the expected reward for completing task mi after performing action a in state s and then following the learned policy for mi . it is computed without any reference to ri . this completion function will be used by parent tasks to compute v the expected reward for performing action i starting in state s. the second completion function c is a completion function that we will use only inside node i in order to discover the locally optimal policy for task mi . this function will incorporate rewards both from the real reward function r and from the pseudo reward function ri . we will employ two di\ufb00erent update rules to learn these two completion functions . the c function will be learned using an update rule similar to the q learning rule in line of maxq . but the c function will be learned using an update rule similar to sarsa its purpose is to learn the value function for the policy that is discovered by optimizing c. pseudo code for the resulting algorithm maxq q is shown in table . the key step is at lines and . in line maxq q first updates c using the value of the greedy action a in the resulting state . this update includes the pseudo reward ri . then in line maxq q updates c using this same greedy action a even if this would not be the greedy action according to the uncontaminated value function . this update of course does not include the pseudo reward function . it is important to note that whereever vt appears in this pseudo code it refers to the uncontaminated value function of state s when executing the max node a. this is computed recursively in exactly the same way as in maxq . finally note that the pseudo code also incorporates all states updating so each call to maxqq returns a list of all of the states that were visited during its execution and the updates of lines and are performed for each of those states . the list of states is ordered most recent first so the states are updated starting with the last state visited and working backward to the starting state which helps speed up the algorithm . when maxq q has converged the resulting recursively optimal policy is computed at each node by choosing the action a that maximizes q c v. it is for this reason that we gave the name max nodes to the nodes that represent subtasks within the maxq table the maxq q learning algorithm . function maxq q let seq be the sequence of states visited while executing i if i is a primitive maxnode execute i receive r and observe result state s vt vt \u03b1t rt push s into the beginning of seq else let count while ti is false do choose an action a according to the current exploration policy \u03c0x let childseq maxq q where childseq is the sequence of states visited while executing action a. observe result state s let a argmaxa let n length for each s in childseq do ct ct \u03b1t \u03b3n append childseq onto the front of seq s s end while end else return seq end maxq graph . each q node j with parent node i stores both c and c and it computes both q and q by invoking its child max node j. each max node i takes the maximum of these q values and computes either v or computes the best action a using q. corollary under the same conditions as theorem maxq q converges the unique recursively optimal policy for mdp m defined by maxq graph h pseudo reward functions r and ordered glie exploration policy \u03c0x . proof the argument is identical to but more tedious than the proof of theorem . the proof of convergence of the c values is identical to the original proof for the c values but it relies on proving convergence of the new c values as well which follows from the same weighted max norm pseudo contraction argument . q.e.d. state abstraction there are many reasons to introduce hierarchical reinforcement learning but perhaps the most important reason is to create opportunities for state abstraction . when we introduced the simple taxi problem in figure we pointed out that within each subtask we can ignore certain aspects of the state space . for example while performing a maxnavigate the taxi should make the same navigation decisions regardless of whether the passenger is in the taxi . the purpose of this section is to formalize the conditions under which it is safe to introduce such state abstractions and to show how the convergence proofs for maxq q can be extended to prove convergence in the presence of state abstraction . specifically we will identify five conditions that permit the safe introduction of state abstractions . throughout this section we will use the taxi problem as a running example and we will see how each of the five conditions will permit us to reduce the number of distinct values that must be stored in order to represent the maxq value function decomposition . to establish a starting point let us compute the number of values that must be stored for the taxi problem without any state abstraction . the maxq representation must have tables for each of the c functions at the internal nodes and the v functions at the leaves . first at the six leaf nodes to store v we must store values at each node . second at the root node there are two children which requires values . third at the maxget and maxput nodes we have actions each so each one requires values for a total of . finally at maxnavigate we have four actions but now we must also consider the target parameter t which can take four possible values . hence there are e\ufb00ectively combinations of states and t values for each action or total values that must be represented . in total therefore the maxq representation requires separate quantities to represent the value function . to place this number in perspective consider that a \ufb02at q learning representation must store a separate value for each of the six primitive actions in each of the possible states for a total of values . hence we can see that without state abstraction the maxq representation requires more than four times the memory of a \ufb02at q table . five conditions that permit state abstraction we now introduce five conditions that permit the introduction of state abstractions . for each condition we give a definition and then prove a lemma which states that if the condition is satisfied then the value function for some corresponding class of policies can be represented abstractly . for each condition we then provide some rules for identifying when that condition can be satisfied and give examples from the taxi domain . we begin by introducing some definitions and notation . definition let m be a mdp and h be a maxq graph defined over m. suppose that each state s can be written as a vector of values of a set of state variables . at each max node i suppose the state variables are partitioned into two sets xi and yi and let \u03c7i be a function that projects a state s onto only the values of the variables in xi . then h combined with \u03c7i is called a state abstracted maxq graph . in cases where the state variables can be partitioned we will often write s to mean that a state s is represented by a vector of values for the state variables in x and a vector of values for the state variables in y. similarly we will sometimes write p v and ra in place of p v and ra respectively . definition an abstract hierarchical policy for mdp m with state abstracted maxq graph h and associated abstraction functions \u03c7i is a hierarchical policy in which each policy \u03c0i satisfies the condition that for any two states s and s such that \u03c7i \u03c7i \u03c0i \u03c0i . in order for maxq q to converge in the presence of state abstractions we will require that at all times t its exploration policy is an abstract hierarchical policy . one way to achieve this is to construct the exploration policy so that it only uses information from the relevant state variables in deciding what action to perform . boltzmann exploration based on the q values \u01eb greedy exploration and counter based exploration based on abstracted states are all abstract exploration policies . counter based exploration based on the full state space is not an abstract exploration policy . now that we have introduced our notation let us describe and analyze the five abstraction conditions . we have identified three di\ufb00erent kinds of conditions under which abstractions can be introduced . the first kind involves eliminating irrelevant variables within a subtask of the maxq graph . under this form of abstraction nodes toward the leaves of the maxq graph tend to have very few relevant variables and nodes higher in the graph have more relevant variables . hence this kind of abstraction is most useful at the lower levels of the maxq graph . the second kind of abstraction arises from funnel actions . these are macro actions that move the environment from some large number of initial states to a small number of resulting states . the completion cost of such subtasks can be represented using a number of values proportional to the number of resulting states . funnel actions tend to appear higher in the maxq graph so this form of abstraction is most useful near the root of the graph . the third kind of abstraction arises from the structure of the maxq graph itself . it exploits the fact that large parts of the state space for a subtask may not be reachable because of the termination conditions of its ancestors in the maxq graph . we begin by describing two abstraction conditions of the first type . then we will present two conditions of the second type . and finally we describe one condition of the third type . . . condition max node irrelevance the first condition arises when a set of state variables is irrelevant to a max node . definition let mi be a max node in a maxq graph h for mdp m. a set of state variables y is irrelevant to node i if the state variables of m can be partitioned into two sets x and y such that for any stationary abstract hierarchical policy \u03c0 executed by the descendants of i the following two properties hold the state transition probability distribution p \u03c0 at node i can be factored into the product of two distributions p \u03c0 p \u03c0 p \u03c0 where y and y give values for the variables in y and x and x give values for the variables in x. for any pair of states s and s such that \u03c7 \u03c7 x and any child action a v \u03c0 v \u03c0 and ri ri . lemma let m be an mdp with full state maxq graph h and suppose that state variables yi are irrelevant for max node i. let \u03c7i x be the associated abstraction function that maps s onto the remaining relevant variables xi . let \u03c0 be any abstract hierarchical policy . then the actionvalue function q\u03c0 at node i can be represented compactly with only one value of the completion function c \u03c0 for each equivalence class of states s that share the same values on the relevant variables . specifically q\u03c0 can be computed as follows q\u03c0 v \u03c0 c \u03c0 where c \u03c0 p \u03c0 \u03b3n xx n where v \u03c0 v \u03c0 ri ri and \u03c0 \u03c0 for some arbitrary value y for the irrelevant state variables yi . proof define a new mdp \u03c7i at node i as follows states x. actions a. transition probabilities p \u03c0 reward function v \u03c0 ri because \u03c0 is an abstract policy its decisions are the same for all states s such that \u03c7i x for some x. therefore it is also a well defined policy over \u03c7i . the action value function for \u03c0 over \u03c7i is the unique solution to the following bellman equation q\u03c0 v \u03c0 p \u03c0 \u03b3n compare this to the bellman equation over mi q\u03c0 v \u03c0 p \u03c0 \u03b3n and note that v \u03c0 v \u03c0 v \u03c0 and ri ri ri . furthermore we know that the distribution p \u03c0 can be factored into separate distributions for yi and xi . hence we can rewrite as q\u03c0 v \u03c0 p p \u03c0 \u03b3n xy xx n the right most sum does not depend on y or y so the sum over y evaluates to and can be eliminated to give q\u03c0 v \u03c0 p \u03c0 \u03b3n . xx n xs n xx n finally note that equations and are identical except for the expressions for the q values . since the solution to the bellman equation is unique we must conclude that q\u03c0 q\u03c0 . we can rewrite the right hand side to obtain q\u03c0 v \u03c0 c \u03c0 c \u03c0 p \u03b3n . where q.e.d. xx n of course we are primarily interested in being able to discover and represent the optimal policy at each node i. the following corollary shows that the optimal policy is an abstract policy and hence that it can be represented abstractly . corollary consider the same conditions as lemma but with the change that the abstract hierarchical policy \u03c0 is executed only by the descendants of node i but not by node i. let \u03c1 be an ordering over actions . then the optimal ordered policy \u03c0 \u03c1 at node i is an abstract policy and its action value function can be represented abstracted . proof define the policy \u03c9 \u03c1 to be the optimal ordered policy over the abstract mdp \u03c7 and let q be the corresponding optimal action value function . then by the same argument given above q is also a solution to the optimal bellman equation for the original mdp . this means that the policy \u03c0 \u03c1 defined by \u03c0 \u03c1 \u03c9 is an optimal ordered policy and by construction it is an abstract policy . q.e.d. as stated this condition appears quite difficult to satisfy since it requires that the state transition probability distribution factor into x and y components for all possible abstract hierarchical policies . however in practice this condition is often satisfied . for example let us consider the navigate subtask . the source and destination of the passenger are irrelevant to the achievement of this subtask . any policy that successfully completes this subtask will have the same value function regardless of the source and destination locations of the passenger . by abstracting away the passenger source and destination we obtain a huge savings in space . instead of requiring values to represent the c functions for this task we require only values . one rule for noticing cases where this abstraction condition holds is to examine the subgraph rooted at the given max node i. if a set of state variables is irrelevant to the leaf state transition probabilities and reward functions and also to all pseudo reward functions and termination conditions in the subgraph then those variables satisfy the max node irrelevance condition lemma let m be an mdp with associated maxq graph h and let i be a max node in h. let xi and yi be a partition of the state variables for m. a set of state variables yi is irrelevant to node i if for each primitive leaf node a that is a descendant of i p p p and r r for each internal node j that is equal to node i or is a descendent of i rj rj and the termination predicate tj is true i\ufb00 tj . proof we must show that any abstract hierarchical policy will give rise to an smdp at node i whose transition probability distribution factors and whose reward function depends only on xi . by definition any abstract hierarchical policy will choose actions based only upon information in xi . because the primitive probability transition functions factor into an independent component for xi and since the termination conditions at all nodes below i are based only on the variables in xi the probability transition function pi must also factor into pi and pi . similarly all of the reward functions v must be equal to v because all rewards received within the subtree depend only on the variables in xi . therefore the variables in yi are irrelevant for max node i. q.e.d. in the taxi task the primitive navigation actions north south east and west only depend on the location of the taxi and not on the location of the passenger . the pseudo reward function and termination condition for the maxnavigate node only depend on the location of the taxi . hence this lemma applies and the passenger source and destination are irrelevant for the maxnavigate node . . . condition leaf irrelevance the second abstraction condition describes situations under which we can apply state abstractions to leaf nodes of the maxq graph . for leaf nodes we can obtain a stronger result than lemma by using a slightly weaker definition of irrelevance . definition a set of state variables y is irrelevant for a primitive action a of a maxq graph if for all states s the expected value of the reward function v p r xs does not depend on any of the values of the state variables in y. in other words for any pair of states s and s that di\ufb00er only in their values for the variables in y p r p r. xs if this condition is satisfied at leaf a then the following lemma shows that we can represent its xs value function v compactly . lemma let m be an mdp with full state maxq graph h and suppose that state variables y are irrelevant for leaf node a. let \u03c7 x be the associated abstraction function that maps s onto the remaining relevant variables x. then we can represent v for any state s by an abstracted value function v v. proof according to the definition of leaf irrelevance any two states that di\ufb00er only on the irrelevant state variables have the same value for v. hence we can represent this unique value by v. q.e.d. here are two rules for finding cases where leaf irrelevance applies . the first rule shows that if the probability distribution factors then we have leaf irrelevance . lemma suppose the probability transition function for primitive action a p factors as p p p and the reward function satisfies r r. then the variables in y are irrelevant to the leaf node a. proof plug in to the definition of v and simplify . v p r xs xx y xy xx p p r p r p xx x a r x p hence the expected reward for the action a depends only on the variables in x and not on the variables in y. q.e.d. lemma let r ra be the reward function for action a in mdp m which is always equal to a constant ra . then the entire state s is irrelevant to the primitive action a. proof v p r p ra xs xs ra . this does not depend on s so the entire state is irrelevant to the primitive action a. q.e.d. this lemma is satisfied by the four leaf nodes north south east and west in the taxi task because their one step reward is a constant . hence instead of requiring values to store the v functions we only need values one for each action . similarly the expected rewards of the pickup and putdown actions each require only values depending on whether the corresponding actions are legal or illegal . hence together they require values instead of values . . . condition result distribution irrelevance now we consider a condition that results from funnel actions . definition . a set of state variables yj is irrelevant for the result distribution of action j if for all abstract policies \u03c0 executed by node j and its descendants in the maxq hierarchy the following holds for all pairs of states s and s that di\ufb00er only in their values for the state variables in yj p \u03c0 p \u03c0 for all s and n. lemma let m be an mdp with full state maxq graph h and suppose that the set of state variables yj is irrelevant to the result distribution of action j which is a child of max node i. let \u03c7ij be the associated abstraction function \u03c7ij x. then we can define an abstract completion cost function c \u03c0 such that for all states s c \u03c0 c \u03c0 . proof the completion function for fixed policy \u03c0 is defined as follows c \u03c0 p \u03b3n . xs n consider any two states s and s such that \u03c7ij \u03c7ij x. under result distribution irrelevance their transition probability distributions are the same . hence the right hand sides of have the same value and we can conclude that c \u03c0 c \u03c0 . therefore we can define an abstract completion function c \u03c0 to represent this quantity . q.e.d. it might appear that this condition would rarely be satisfied and indeed for infinite horizon discounted problems this is true . consider for example the get subroutine under an optimal policy for the taxi task . no matter what location that taxi has in state s the taxi will be at the passenger s starting location when the get finishes executing . hence the starting location is irrelevant to the resulting location of the taxi . in the discounted cumulative reward setting however the number of steps n required to complete the get action will depend very much on the starting location of the taxi . consequently p is not necessarily the same for any two states s with di\ufb00erent starting locations even though s is always the same . the important lesson to draw from this is that discounting interferes with introducing state abstractions based on funnel operators the maxq framework is therefore less e\ufb00ective when applied in the discounted setting . however if we restrict attention to the episodic undiscounted setting then the result distribution p no longer depends on n and the result distribution irrelevance condition is satisfied . fortunately the taxi task is an undiscounted finite horizon task so we can represent c using distinct values because there are equivalence classes of states . this is much less than the quantities in the unabstracted representation . funnel actions arise in many hierarchical reinforcement learning problems . for example abstract actions that move a robot to a doorway or that move a car onto the entrance ramp of a freeway have this property . the result distribution irrelevance condition is applicable in all such situations as long as we are in the undiscounted setting . . . condition termination the fourth condition is closely related to the funnel property . it applies when a subtask is guaranteed to cause its parent task to terminate in a goal state . in a sense the subtask is funneling the environment into the set of states described by the goal predicate of the parent task . lemma . let mi be a task in a maxq graph such that for all states s where the goal predicate gi is true the pseudo reward function ri . suppose there is a child task a and state s such that for all hierarchical policies \u03c0 s p \u03c0 i gi . then for any policy executed at node i the completion cost c is zero and does not need to be explicitly represented . proof by the assumptions in the lemma with probability the completion cost is zero for any action that results in a goal terminal state . q.e.d. for example in the taxi task in all states where the taxi is holding the passenger the put subroutine will succeed and result in a goal terminal state for root . this is because the termination predicate for put implies the goal condition for root . this means that c is uniformly zero for all states s where put is not terminated . it is easy to detect cases where the termination condition is satisfied . we only need to compare the termination predicate of a subtask with the goal predicate of the parent task . if the first implies the second then the termination condition is satisfied . . . condition shielding the shielding condition arises from the structure of the maxq graph . lemma . let mi be a task in a maxq graph and s be a state such that for all paths from the root of the graph down to node mi there exists a subtask j whose termination predicate tj is true then the q nodes of mi do not need to represent c values for state s. proof task i can not be executed in state s so no c values need to be estimated . q.e.d. as with the termination condition the shielding condition can be verified by analyzing the structure of the maxq graph and identifying nodes whose ancestor tasks are terminated . in the taxi task a simple example of this arises in the put task which is terminated in all states where the passenger is not in the taxi . this means that we do not need to represent c in these states . the result is that when combined with the termination condition above we do not need to explcitly represent the completion function for put at all . . dicussion by applying these five abstraction conditions we obtain the following safe state abstractions for the taxi task north south east and west . these terminal nodes require one quantity each for a total of four values . pickup and putdown each require values for a total of four . qnorth qsouth qeast and qwest each require values . qnavigateforget requires values . qpickup requires possible values possible source locations and possible taxi locations . qget requires possible values . qnavigateforput requires only values . qputdown requires possible values . qput requires values . this gives a total of distinct values which is much less than the values required by \ufb02at q learning . hence we can see that by applying state abstractions the maxq representation can give a much more compact representation of the value function . a key thing to note is that these state abstractions can not be exploited with the \ufb02at representation of the value function . what prior knowledge is required on the part of a programmer in order to introduce these state abstractions it suffices to know some general constraints on the one step reward functions the one step transition probabilities and termination predicates goal predicates and pseudo reward functions within the maxq graph . specifically the max node irrelevance and leaf irrelevance conditions require simple analysis of the one step transition function and the reward and pseudoreward functions . opportunities to apply the result distribution irrelevance condition can be found by identifying funnel e\ufb00ects that result from the definitions of the termination conditions for operators . similarly the shielding and termination conditions only require analysis of the termination predicates of the various subtasks . hence applying these five conditions to introduce state abstractions is a straightforward process and once a model of the one step transition and reward functions has been learned the abstraction conditions can be checked to see if they were satisfied . convergence of maxq q with state abstraction we have shown that state abstractions can be safely introduced into the maxq value function decomposition under the five conditions described above . however these conditions only guarantee that the value function of any fixed abstract hierarchical policy can be represented they do not show that the optimal policy can be represented nor do they show that the maxq q learning algorithm will find the optimal policy . the goal of this section is to prove these two results that the ordered recursively optimal policy is an abstract policy and that maxq q will converge to this policy when applied to a maxq graph with safe state abstractions . lemma let m be an mdp with full state maxq graph h and abstract state maxq graph \u03c7 where the abstractions satisfy the five conditions given above . let \u03c1 be an ordering over all actions in the maxq graph . then the following statements are true the unique ordered recursively optimal policy \u03c0 r defined by m h and \u03c1 is an abstract policy the c and v functions in \u03c7 can represent the projected value function of \u03c0 r. proof the five abstraction lemmas tell us that if the ordered recursively optimal policy is abstract then the c and v functions of \u03c7 can represent its value function . hence the heart of this lemma is the first claim . the last two forms of abstraction do not place any restrictions on abstract policies so we ignore them in this proof . the proof is by induction on the levels of the maxq graph starting at the leaves . as a base case let us consider a max node i all of whose children are primitive actions . in this case there are no policies executed within the children of the max node . hence if variables yi are irrelevant for node i then we can apply our abstraction lemmas to represent the value function of any policy at node i not just abstract policies . consequently the value function of any optimal policy for node i can be represented and it will have the property that q q for any states s and s such that \u03c7i \u03c7i . now let us impose the action ordering \u03c1 to compute the optimal ordered policy . consider two actions a and a such that \u03c1 and suppose that there is a tie in the q function at state s such that the values q q and they are the only two actions that maximize q in this state . then the optimal ordered policy must choose a. now in all other states s such that \u03c7i \u03c7i we know that the q values will be the same . hence the same tie will exist between a and a and hence the optimal ordered policy must make the same choice in all such states . hence the optimal ordered policy for node i is an abstract policy . now let us turn to the recursive case at max node i. make the inductive assumption that the ordered recursively optimal policy is abstract within all descendant nodes and consider the locally optimal policy at node i. if y is a set of state variables that are irrelevant to node i corollary tells us that q q for all states s and s such that \u03c7i \u03c7i . similarly if y is a set of variables irrelevant to the result distribution of a particular action j then lemma tells us the same thing . hence by the same ordering argument given above the ordered optimal policy at node i must be abstract . by induction this proves the lemma . q.e.d. with this lemma we have established that the combination of an mdp m an abstract maxq graph h and an action ordering defines a unique recursively optimal ordered abstract policy . we are now ready to prove that maxq q will converge to this policy . theorem let m hs a p r pi be either an episodic mdp for which all deterministic policies are proper or a discounted infinite horizon mdp with discount factor \u03b3 . let h be an unabstracted maxq graph defined over subtasks with pseudo reward functions ri . let \u03c7 be a state abstracted maxq graph defined by applying state abstractions \u03c7i to each node i of h under the five conditions given above . let \u03c0x be an abstract ordered glie exploration policy at each node i and state s whose decisions depend only on the relevant state variables at each node i. let \u03c0 r be the unique recursively optimal hierarchical policy defined by \u03c0x m and r. then with probability algorithm maxq q applied to \u03c7 converges to \u03c0 r provided that the learning rates \u03b1t satisfy equation and vt and ct are bounded for all t i \u03c7i and a. proof rather than repeating the entire proof for maxq q we will only describe what must change under state abstraction . the last two forms of state abstraction refer to states whose values can be inferred from the structure of the maxq graph and therefore do not need to be represented at all . since these values are not updated by maxq q we can ignore them . we will now consider the first three forms of state abstraction in turn . we begin by considering primitive leaf nodes . let a be a leaf node and let y be a set of state variables that are leaf irrelevant for a. let s and s be two states that di\ufb00er only in their values for y. under leaf irrelevance the probability transitions p and p need not be the same but the expected reward of performing a in both states must be the same . when maxq q visits an abstract state x it does not know the value of y the part of the state that has been abstracted away . nonetheless it draws a sample according to p receives a reward r and updates its estimate of v. let pt be the probability that maxq q is visiting given that the unabstracted part of the state is x. then line of maxq q is computing a stochastic approximation to we can write this as pt ptr . ptptr . xs n y y x xs n according to leaf irrelevance the inner sum has the same value for all states s such that \u03c7 x. call this value r. this gives ptr y x which is equal to r for any distribution pt . hence maxq q converges under leaf irrelevance abstractions . now let us turn to the two forms of abstraction that apply to internal nodes node irrelevance and result distribution irrelevance . consider the smdp defined at each node i of the abstracted maxq graph at time t during maxq q. this would be an ordinary smdp with transition probability function pt and reward function vt ri except that when maxq q draws samples of state transitions they are drawn according to the distribution pt over the original state space . to prove the theorem we must show that drawing according to this second distribution is equivalent to drawing according to the first distribution . for max node irrelevance we know that for all abstract policies applied to node i and its descendants the transition probability distribution factors as p p p. because the exploration policy is an abstract policy pt factors in this way . this means that the xi and yi components of the state are independent of each other and hence sampling from pt gives samples for pt . therefore maxq q will converge under max node irrelevance abstractions . finally consider result distribution irrelevance . let j be a child of node i and suppose yj is a set of state variables that are irrelevant to the result distribution of j. when the smdp at node i wishes to draw a sample from pt it does not know the current value of y the irrelevant part of the current state . however this does not matter because result distribution irrelevance means that for all possible values of y pt is the same . hence maxq q will converge under result distribution irrelevance abstractions . in each of these three cases maxq q will converge to a locally optimal ordered policy at node i in the maxq graph . by lemma this can be extended to produce a locally optimal ordered policy for the unabstracted smdp at node i. hence by induction maxq q will converge to the unique ordered recursively optimal policy \u03c0 exploration policy \u03c0x . q.e.d. r defined by maxq q h mdp m and ordered . the hierarchical credit assignment problem there are still some situations where we would like to introduce state abstractions but where the five properties described above do not permit them . consider the following modification of the taxi problem . suppose that the taxi has a fuel tank and that each time the taxi moves one square it costs one unit of fuel . if the taxi runs out of fuel before delivering the passenger to his or her destination it receives a reward of and the trial ends . fortunately there is a filling station where the taxi can execute a fillup action to fill the fuel tank . to solve this modified problem using the maxq hierarchy we can introduce another subtask refuel which has the goal of moving the taxi to the filling station and filling the tank . maxrefuel is a child of maxroot and it invokes navigate to move the taxi to the filling station . the introduction of fuel and the possibility that we might run out of fuel means that we must include the current amount of fuel as a feature in representing every c value and v value . this is unfortunate because our intuition tells us that the amount of fuel should have no in\ufb02uence on our decisions inside the navigate subtask . the amount of fuel should be taken into account by the top level q nodes which must decide whether to go refuel go pick up the passenger or go deliver the passenger . given this intuition it is natural to try abstracting away the amount of remaining fuel within the navigate subtask . however this doesn t work because when the taxi runs out of fuel and a reward is given the qnorth qsouth qeast and qwest nodes can not explain why this reward was received that is they have no consistent way of setting their c tables to predict when this negative reward will occur . stated more formally the difficulty is that the max node irrelevance condition is not satisfied because the one step reward function r for these actions depends on the amount of fuel . we call this the hierarchical credit assignment problem . the fundamental issue here is that in the maxq decomposition all information about rewards is stored in the leaf nodes of the hierarchy . we would like to separate out the basic rewards received for navigation from the reward received for exhausting fuel . if we make the reward at the leaves only depend on the location of the taxi then the max node irrelevance condition will be satisfied . one way to do this is to have the programmer manually decompose the reward function and indicate which nodes in the hierarchy will receive each reward . let r i r be a decomposition of the reward function such that r specifies that part of the reward that must be handled by max node i. in the modified taxi problem for example we can decompose the reward so that the leaf nodes receive all of the original penalties but the out of fuel rewards must be handled by maxroot . lines and of the maxq q algorithm are easily modified to include r. p in most domains we believe it will be easy for the designer of the hierarchy to decompose the reward function . it has been straightforward in all of the problems we have studied . however an interesting problem for future research is to develop an algorithm that can solve the hierarchical credit assignment problem autonomously . non hierarchical execution of the maxq hierarchy up to this point in the paper we have focused exclusively on representing and learning hierarchical policies . however often the optimal policy for a mdp is not a strictly hierarchical policy . kaelbling first introduced the idea of deriving a non hierarchical policy from the value function of a hierarchical policy . in this section we exploit the maxq decomposition to generalize her ideas and apply them recursively at all levels of the hierarchy . the first method is based on the dynamic programming algorithm known as policy iteration . the policy iteration algorithm starts with an initial policy \u03c0 . it then repeats the following two steps until the policy converges . in the policy evaluation step it computes the value function v \u03c0k of the current policy \u03c0k . then in the policy improvement step it computes a new policy \u03c0k according to the rule \u03c0k argmax p. a xs howard proved that if \u03c0k is not an optimal policy then \u03c0k is guaranteed to be an improvement . note that in order to apply this method we need to know the transition probability distribution p and the reward function r. if we know p and r we can use the maxq representation of the value function to perform one step of policy iteration . we start with a hierarchical policy \u03c0 and represent its value function using the maxq hierarchy . then we can perform one step of policy improvement by applying equation using v \u03c0 to compute v \u03c0 . s p where v \u03c0 is the value corollary let \u03c0g argmaxa function computed by the maxq hierarchy . then if \u03c0 was not an optimal policy \u03c0g is strictly better for at least one state in s. p proof this is a direct consequence of howard s policy improvement theorem . q.e.d. unfortunately we can t iterate this policy improvement process because the new policy \u03c0g is very unlikely to be a hierarchical policy . nonetheless one step of policy improvement can give very significant improvements . this approach to non hierarchical execution ignores the internal structure of the maxq graph . in e\ufb00ect the maxq hierarchy is just viewed as a kind of function approximator for representing v \u03c0 any other representation would give the same one step improved policy \u03c0g . the second approach to non hierarchical execution borrows an idea from q learning . one of the great beauties of the q representation for value functions is that we can compute one step of policy improvement without knowing p simply by taking the new policy to be \u03c0g argmaxa q. this gives us the same one step greedy policy as we computed above using onestep lookahead . with the maxq decomposition we can perform these policy improvement steps at all levels of the hierarchy . we have already defined the function that we need . in table we presented the function evaluatemaxnode which given the current state s conducts a search along all paths from a given max node i to the leaves of the maxq graph and finds the path with the best value . in addition evaluatemaxnode returns the primitive action a at the end of this best path . this action a would table the procedure for executing the one step greedy policy . procedure executehgpolicy repeat let hv ai evaluatemaxnode execute primitive action a let s be the resulting state end executehgpolicy be the first primitive action to be executed if the learned hierarchical policy were executed starting in the current state s. our second method for non hierarchical execution of the maxq graph is to call evaluatemaxnode in each state and execute the primitive action a that is returned . the pseudo code is shown in table . we will call the policy computed by executehgpolicy the hierarchical greedy policy and denote it \u03c0hg where the superscript indicates that we are computing the greedy action at each time step . the following theorem shows that this can give a better policy than the original hierarchical policy . theorem let g be a maxq graph representing the value function of hierarchical policy \u03c0 . let v hg be the value computed by executehgpolicy and let \u03c0hg be the resulting policy . define v hg to be the value function of \u03c0hg . then for all states s it is the case that v \u03c0 v hg v hg . proof the left inequality in equation is satisfied by construction by line of evaluatemaxnode . to see this consider that the original hierarchical policy \u03c0 can be viewed as choosing a path through the maxq graph running from the root to one of the leaf nodes and v \u03c0 is the sum of the c \u03c0 values along this chosen path . in contrast evaluatemaxnode performs a traversal of all paths through the maxq graph and finds the best path that is the path with the largest sum of c \u03c0 values . hence v hg must be at least as large as v \u03c0 . to establish the right inequality note that by construction v hg is the value function of a policy call it \u03c0hg that chooses one action greedily at each level of the maxq graph and then follows \u03c0 thereafter . this is a consequence of the fact that line of evaluatemaxnode has c \u03c0 on its right hand side and c \u03c0 represents the cost of completing each subroutine by following \u03c0 not by following some other greedier policy . however when we execute executehgpolicy we have an opportunity to improve upon \u03c0 and \u03c0hg at each time step . hence v hg is an underestimate of the actual value of \u03c0hg . q.e.d. note that this theorem only works in one direction . it says that if we can find a state where v hg v \u03c0 then the greedy policy \u03c0hg will be strictly better than \u03c0 . however it could be that \u03c0 is not an optimal policy and yet the structure of the maxq graph prevents us from considering an action that would improve \u03c0 . hence unlike the policy improvement theorem of howard we do not have a guarantee that if \u03c0 is suboptimal then the hierarchically greedy policy is a strict improvement . in contrast if we perform one step policy improvement as discussed at the start of this section corollary guarantees that we will improve the policy . so we can see that in general neither of these two methods for non hierarchical execution dominates the other . nonetheless the first method only operates at the level of individual primitive actions so it is not able to produce very large improvements in the policy . in contrast the hierarchical greedy method can obtain very large improvements in the policy by changing which actions are chosen near the root of the hierarchy . hence in general hierarchical greedy execution is probably the better method . sutton singh precup and ravindran have simultaneously developed a closely related method for non hierarchical execution of macros . their method is equivalent to executehgpolicy for the special case where the maxq hierarchy has only one level of subtasks . the interesting aspect of executehgpolicy is that it permits greedy improvements at all levels of the tree to in\ufb02uence which action is chosen . some care must be taken in applying theorem to a maxq hierarchy whose c values have been learned via maxq q. being an online algorithm maxq q will not have correctly learned the values of all states at all nodes of the maxq graph . for example in the taxi problem the value of c will not have been learned very well except at the four special locations . this is because the put subtask can not be executed until the passenger is in the taxi and this usually means that a get has just been completed so the taxi is at the passenger s source location . during exploration both children of put will be tried in such states . the putdown will usually fail whereas the navigate will eventually succeed and take the taxi to the destination location . now because of all states updating the values for c will have been learned at all of the states but the c values for the putdown action will not . hence if we train the maxq representation using hierarchical execution and then switch to hierarchically greedy execution the results will be quite bad . in particular we need to introduce hierarchically greedy execution early enough so that the exploration policy is still actively exploring . of course an alternative would be to use hierarchically greedy execution from the very beginning of learning . however remember that the higher nodes in the maxq hierarchy need to obtain samples of p for each child action a. if the hierarchical greedy execution interrupts child a before it has reached a terminal state then these samples can not be obtained . hence it is important to begin with purely hierarchical execution during training and make a transition to greedy execution at some point . the approach we have taken is to implement maxq q in such a way that we can specify a number of primitive actions l that can be taken hierarchically before the hierarchical execution is interrupted and control returns to the top level . we start with l set very large so that execution is completely hierarchical when a child action is invoked we are committed to execute that action until it terminates . however gradually we reduce l until it becomes at which point we have hierarchical greedy execution . we time this so that it reaches at about the same time our boltzmann exploration cools to a temperature of . as the experimental results will show this generally gives excellent results with very little added exploration cost . experimental evaluation of the maxq method we have performed a series of experiments with the maxq method with three goals in mind to understand the expressive power of the value function decomposition to characterize the behavior of the maxq q learning algorithm and to assess the relative importance of temporal abstraction state abstraction and non hierarchical execution . in this section we describe these experiments and present the results . the fickle taxi task our first experiments were performed on a modified version of the taxi task . this version incorporates two changes to the task described in section . . first each of the four navigation actions is noisy so that with probability . it moves in the intended direction but with probability . it instead moves to the right and with probability . it moves to the left . the second change is that after the taxi has picked up the passenger and moved one square away from the passenger s source location the passenger changes his or her destination location with probability . . the purpose of this change is to create a situation where the optimal policy is not a hierarchical policy so that the e\ufb00ectiveness of non hierarchical execution can be measured . we compared four di\ufb00erent configurations of the learning algorithm \ufb02at q learning maxq q learning without any form of state abstraction maxq q learning with state abstraction and maxq q learning with state abstraction and greedy execution . these configurations are controlled by many parameters . these include the following the initial values of the q and c functions the learning rate the cooling schedule for boltzmann exploration and for non hierarchical execution the schedule for decreasing l the number of steps of consecutive hierarchical execution . we optimized these settings separately for each configuration with the goal of matching or exceeding the best policy that we could code by hand . for boltzmann exploration we established an initial temperature and then a cooling rate . a separate temperature is maintained for each max node in the maxq graph and its temperature is reduced by multiplying by the cooling rate each time that subtask terminates in a goal state . the following parameters were chosen . for \ufb02at q learning initial q values of . learning rate . and boltzmann exploration with an initial temperature of and a cooling rate of . . for maxq q learning without state abstraction we used initial values of . a learning rate of . and boltzmann exploration with an initial temperature of and cooling rates of . at maxroot and maxput . at maxget and . at maxnavigate . for maxq q learning with state abstraction we used initial values of . a learning rate of . and boltzmann exploration with an initial temperature of and cooling rates of . at maxroot . at maxput . at maxget and . at maxnavigate . for maxq q learning with non hierarchical execution we used the same settings as with state in addition we initialized l to and decreased it by with each trial until it abstraction . reached . so after trials execution was completely greedy . figure shows the averaged results of training trials . the first thing to note is that all forms of maxq learning have better initial performance than \ufb02at q learning . this is because of the constraints introduced by the maxq hierarchy . for example while the agent is executing a navigate subtask it will never attempt to pickup or putdown the passenger . similarly it will never attempt to putdown the passenger until it has first picked up the passenger . the second thing to notice is that without state abstractions maxq q learning actually takes d r a w e r e v i t a l u m u c n a e m hierarchical q learning with state abstraction and greedy execution hierarchical q learning with state abstraction hierarchical q learning without state abstraction flat q learning primitive actions figure comparison of performance of hierarchical q learning with \ufb02at q learning with and without state abstractions and with and without greedy evaluation . longer to converge so that the flat q curve crosses the maxq no abstraction curve . this shows that without state abstraction the cost of learning the huge number of parameters in the maxq representation is not really worth the benefits . the third thing to notice is that with state abstractions maxq q converges very quickly to a hierarchically optimal policy . this can be seen more clearly in figure which focuses on the range of reward values in the neighborhood of the optimal policy . here we can see that maxq with abstractions attains the hierarchically optimal policy after approximately steps whereas \ufb02at q learning requires roughly twice as long to reach the same level . however \ufb02at q learning of course can continue onward and reach optimal performance whereas with the maxq hierarchy the best hierarchical policy is slow to respond to the fickle behavior of the passenger when he she changes the destination . the last thing to notice is that with greedy execution the maxq policy is also able to attain optimal performance . but as the execution becomes more greedy there is a drop in performance because maxq q must learn c values in new regions of the state space that were not visited by the recursively optimal policy . despite this drop in performance greedy maxq q recovers rapidly and reaches hierarchically optimal performance faster than purely hierarchical maxq q learning . hence there is no added cost in terms of exploration for introducing greedy execution . this experiment presents evidence in favor of three claims first that hierarchical reinforcement learning can be much faster than \ufb02at q learning second that state abstraction is required by maxq for good performance and third that non hierarchical execution can produce significant improvements in performance with little or no added exploration cost . maxq abstract greedy flat q optimal policy hier optimal policy maxq abstract maxq no abstract d r a w e r e v i t a l u m u c n a e m primitive actions figure close up view of the previous figure . this figure also shows two horizontal lines indicating optimal performance and hierarchically optimal performance in this domain . to make this figure more readable we have applied a step moving average to the data points . kaelbling s hdg method the second task that we will consider is a simple maze task introduced by leslie kaelbling and shown in figure . in each trial of this task the agent starts in a randomly chosen state and must move to a randomly chosen goal state using the usual north south east and west operators . there is a small cost for each move and the agent must maximize the undiscounted sum of these costs . because the goal state can be in any of di\ufb00erent locations there are actually di\ufb00erent mdps . kaelbling s hdg method starts by choosing an arbitrary set of landmark states and defining a voronoi partition of the state space based on the manhattan distances to these landmarks . the method then defines one subtask for each landmark l. the subtask is to move from any state in the current voronoi cell or in any neighboring voronoi cell to the landmark l. optimal policies for these subtasks are then computed . once hdg has the policies for these subtasks it can solve the abstract markov decision problem of moving from each landmark state to any other landmark state using the subtask solutions as macro actions . so it computes a value function for this mdp . finally for each possible destination location g within a voronoi cell for landmark l the hdg method computes the optimal policy of getting from l to g. by combining these subtasks the hdg method can construct a good approximation to the optimal policy as follows . in addition to the value functions discussed above the agent maintains two other functions n l the name of the landmark nearest to state s and n a list of the figure kaelbling s by navigation task . each circled state is a landmark state and the heavy lines show the boundaries of the voronoi cells . in each episode a start state and a goal state are chosen at random . in this figure the start state is shown by the shaded hexagon and the goal state is shown by the shaded square . landmarks that are in the cells that are immediate neighbors of cell l. by combining these the agent can build a list for each state s of the current landmark and the landmarks of the neighboring cells . for each such landmark the agent computes the sum of three terms the expected cost of reaching that landmark the expected cost of moving from that landmark to the landmark in the goal cell and the expected cost of moving from the goal cell landmark to the goal state . note that while terms and can be exact estimates term is computed using the landmark subtasks as subroutines . this means that the corresponding path must pass through the intermediate landmark states rather than going directly to the goal landmark . hence term is typically an overestimate of the required distance . given this information the agent then chooses to move toward the best of the landmarks . for example in figure term is the cost of reaching the landmark in row column which is . term is the cost of getting from row column to the landmark at row column . in this case the best landmark to landmark path is from row column to row column and then to row column . hence term is . term is the cost of getting from row column to the goal which is . the sum of these is . for comparison the optimal path has length . in kaelbling s experiments she employed a variation of q learning to learn terms and and she computed at regular intervals via the floyd warshall all sources shortest paths algorithm . maxroot gl nl qgotogoallmk qgotogoal maxgotogoallmk qgotolmk maxgotolmk maxgotogoal qnorthlmk qsouthlmk qeastlmk qwestlmk qnorthg qsouthg qeastg qwestg north south east west figure a maxq graph for the hdg navigation task . figure shows a maxq approach to solving this problem . the overall task root takes one argument g which specifies the goal cell . there are three subtasks gotogoallmk go to the landmark nearest to the goal location . the termination for the predicate is true if the agent reaches the landmark nearest to the goal . the goal predicate is the same as the termination predicate . gotolmk go to landmark l. the termination predicate for this is true if either the agent reaches landmark l or the agent is outside of the region defined by the voronoi cell for l and the neighboring voronoi cells n. the goal predicate for this subtask is true only for condition . gotogoal go to the goal location g. the termination predicate for this subtask is true if either the agent is in the goal location or the agent is outside of the voronoi cell n l that contains g. the goal predicate for this subtask is true if the agent is in the goal location . the maxq decomposition is essentially the same as kaelbling s method but somewhat redundant . consider a state where the agent is not inside the same voronoi cell as the goal g. in such states hdg decomposes the value function into three terms and . similarly maxq also decomposes it into these same three terms v the cost of getting to landmark l. actually the sum of v and c. mark gl nearest the goal . c the cost of getting from landmark l to the land c the cost of getting to the goal location after reaching gl . when the agent is inside the goal voronoi cell then again hdg and maxq store essentially the same information . hdg stores q while maxq breaks this into two terms c and v and then sums these two quantities to compute the q value . note that this maxq decomposition stores some information twice specifically the cost of getting from the goal landmark gl to the goal is stored both as c and as c v. let us compare the amount of memory required by \ufb02at q learning hdg and maxq . there are locations possible actions and possible goal states so \ufb02at q learning must store values . to compute quantity hdg must store q values for each state s with respect to its own landmark and the landmarks in n. this gives a total of values that must be stored . to compute quantity hdg must store for each landmark information on the shortest path to every other landmark . there are landmarks . consider the landmark at row column . it has neighboring landmarks which constitute the five macro actions that the agent can perform to move to another landmark . the nearest landmark to the goal cell could be any of the other landmarks so this gives a total of q values that must be stored . similar computations for all landmarks give a total of values that must be stored . finally to compute quantity hdg must store information for each square inside each voronoi cell about how to get to each of the other squares inside the same voronoi cell . this requires values . hence the grand total for hdg is which is a huge savings over \ufb02at q learning . now let s consider the maxq hierarchy with and without state abstractions . v this is the expected reward of each primitive action in each state . there are states and primitive actions so this requires values . however because the reward is constant we can apply leaf irrelevance to store only a single value . c where a is one of the four primitive actions . this requires the same amount of space as in kaelbling s representation indeed combined with v this represents exactly the same information as . it requires values . no state abstractions can be applied . c this is the cost of completing the gotogoallmk task after going to landmark l. if the primitive actions are deterministic then gotolmk will always terminate at location l and hence we only need to store this for each pair of l and gl . this is exactly the same as kaelbling s quantity which requires values . however if the primitive actions are stochastic as they were in kaelbling s original paper then we must store this value for each possible terminal state of each gotolmk action . each of these actions table comparison of the number of values that must be stored to represent the value function using the hdg and maxq methods . hdg maxq item item v c c c c c total number of values required maxq hdg maxq maxq no abs values safe abs unsafe abs could terminate at its target landmark l or in one of the states bordering the set of voronoi cells that are the neighbors of the cell for l. this requires values . when kaelbling stores values only for she is e\ufb00ectively making the assumption that gotolmk will never fail to reach landmark l. this is an approximation which we can introduce into the maxq representation by our choice of state abstraction at this node . c this is the cost of completing the gotogoal task after making one of the primitive actions a. this is the same as quantity in the hdg representation and it requires the same amoount of space values . c this is the cost of reaching the goal once we have reached the landmark nearest the goal . maxq must represent this for all combinations of goal landmarks and goals . this requires values . note that these values are the same as the values of c v for each of the primitive actions . this means that the maxq representation stores this information twice whereas the hdg representation only stores it once . c. this is the cost of completing the root task after we have executed the gotogoal task . if the primitive action are deterministic this is always zero because gotogoal will have reached the goal . hence we can apply the termination condition and not store any values at all . however if the primitive actions are stochastic then we must store this value for each possible state that borders the voronoi cell that contains the goal . this requires di\ufb00erent values . again in kaelbling s hdg representation of the value function she is ignoring the probability that gotogoal will terminate in a non goal state . because maxq is an exact representation of the value function it does not ignore this possibility . if we apply the termination condition in this case the maxq representation becomes a function approximation . in the stochastic case without state abstractions the maxq representation requires values . with safe state abstractions it requires values . with the approximations employed by kaelbling the maxq representation with state abstractions requires values . these numbers are summarized in table . we can see that with the unsafe state abstractions the maxq representation requires only slightly more space than the hdg representation because of the redundancy in storing c. this example shows that for the hdg task we can start with the fully general formulation provided by maxq and impose assumptions to obtain a method that is similar to hdg . the maxq formulation guarantees that the value function of the hierarchical policy will be represented exactly . the assumptions will introduce approximations into the value function representation . this might be useful as a general design methodology for building application specific hierarchical representations . our long term goal is to develop such methods so that each new application does not require inventing a new set of techniques . instead o\ufb00 the shelf tools could be specialized by imposing assumptions and state abstractions to produce more efficient special purpose systems . one of the most important contributions of the hdg method was that it introduced a form of non hierarchical execution . as soon as the agent crosses from one voronoi cell into another the current subtask is interrupted and the agent recomputes the current target landmark . the e\ufb00ect of this is that the agent is always aiming for a landmark outside of its current voronoi cell . hence although the agent aims for a sequence of landmark states it typically does not visit many of these states on its way to the goal . the states just provide a convenient set of intermediate targets . by taking these shortcuts hdg compensates for the fact that in general it has overestimated the cost of getting to the goal because its computed value function is based on a policy where the agent goes from one landmark to another . the same e\ufb00ect is obtained by hierarchical greedy execution of the maxq graph . note that by storing the n l function kaelbing s hdg method can detect very efficiently when the current subtask should be interrupted . this technique only works for navigation problems in a space with a distance metric . in contrast executehgpolicy performs a kind of polling where it checks after each primitive action whether it should interrupt the current subroutine and invoke a new one . an important goal for future research on maxq is to find a general purpose mechanism for avoiding unnecessary polling that is a mechanism that can discover efficiently evaluable interrupt conditions . figure shows the results of our experiments with hdg using the maxq q learning algorithm . we employed the following parameters for flat q learning initial values of . a learning rate of . initial temperature of and cooling rate of . for maxq q without initial values of . learning rate of . initial temperature of and state abstractions cooling rates of . for maxroot . for maxgotogoallmk . for maxgotogoal and . for maxgotolmk for maxq q with state abstractions initial values of . learning rate of . initial temperature of and cooling rates of . for maxroot . for maxgotogoal . for maxgotogoallmk and . for maxgotolmk . hierarchical greedy execution was introduced by starting with primitive actions per trial and reducing this every trial by actions so that after trials execution is completely greedy . the figure confirms the observations made in our experiments with the fickle taxi task . without state abstractions maxq q converges much more slowly than \ufb02at q learning . with state abstractions it converges roughly three times as fast . figure shows a close up view of figure that allows us to compare the di\ufb00erences in the final levels of performance of the methods . here we can see that maxq q with no state abstractions was not able to reach the quality of our handcoded hierarchical policy presumably even more exploration would be required to achieve this whereas with state abstractions maxq q is able to do slightly better than our hand coded policy . with hierarchical greedy execution maxq q is able to reach the goal using one fewer action on the average so that it approaches the performance of the best hierarchical greedy policy . notice however that the best performance that can be obtained by hierarchical greedy execution of the best recursively optimal policy can not match optimal performance . hence flat q learning achieves a policy that reaches the goal state on the average with maxq flat q maxq no abstractions d r a w e r e v i t a l u m u c n a e m e . e . e primitive actions figure comparison of flat q learning with maxq q learning with and without state abstraction . about one fewer primitive action . finally notice that as in the taxi domain there was no added exploration cost for shifting to greedy execution . parr and russell hierarchies of abstract machines in his dissertation work ron parr considered an approach to hierarchical reinforcement learning in which the programmer encodes prior knowledge in the form of a hierarchy of finite state controllers called a ham . the hierarchy is executed using a procedure call and return discipline and it provides a partial policy for the task . the policy is partial because each machine can include non deterministic choice machine states in which the machine lists several options for action but does not specify which one should be chosen . the programmer puts choice states at any point where he she does not know what action should be performed . given this partial policy parr s goal is to find the best policy for making choices in the choice states . in other words his goal is to learn a hierarchical value function v where s is a state and m contains all of the internal state of the hierarchy . a key observation is that it is only necessary to learn this value function at choice states hs mi . parr s algorithm does not learn a decomposition of the value function . instead it \ufb02attens the hierarchy to create a new markov decision problem over the choice states hs mi . hence it is hierarchical primarily in the sense that the programmer structures the prior knowledge hierarchically . an advantage of this is that parr s method can find the optimal hierarchical policy subject to constraints provided by the programmer . a disadvantage is that the method can not be executed non hierarchically to produce a better policy . maxq greedy maxq optimal policy hierarchical greedy optimal policy hierarchical hand coded policy flat q maxq no abstractions d r a w e r e v i t a l u m u c n a e m e . e . e primitive actions figure expanded view comparing flat q learning with maxq q learning with and without state abstraction and with and without hierarchical greedy execution . parr illustrated his work using the maze shown in figure . this maze has a high level structure and a low level structure . in each trial the agent starts in the top left corner and it must move to any state in the bottom right corner room . the agent has the usual four primitive actions north south east and west . the actions are stochastic with probability . they succeed but with probability . the action will move to the left and with probability . the action will move to the right instead . if an action would collide with a wall or an obstacle it has no e\ufb00ect . the maze is structured as a series of rooms each containing a by block of states . some rooms are parts of hallways because they contain walls on two opposite sides and they are open on the other two sides . other rooms are intersections where two or more hallways meet . to test the representational power of the maxq hierarchy we want to see how well it can represent the prior knowledge that parr is able to represent using the ham . we begin by describing parr s ham for his maze task and then we will present a maxq hierarchy that captures much of the same prior knowledge . parr s top level machine mroot consists of a loop with a single choice state that chooses among four possible child machines mgo mgo mgo and mgo . the loop terminates when the agent reaches a goal state . mroot will only invoke a particular machine if there is a hallway in the specified direction . hence in the start state it will only consider mgo the author thanks ron parr for providing the details of the ham for this task . and mgo . the mgo machine begins executing when the agent is in an intersection . so the first thing it tries to do is to exit the intersection into a hallway in specified direction d. then it attempts to traverse the hallway until it reaches another intersection . it does this by first invoking a exitintersection machine . when that machine returns it then invokes a mexithallway machine . when that machine returns mgo also returns . the mexitintersection and mexithallway machines are identical except for their termination conditions . both machines consist of a loop with one choice state that chooses among four possible subroutines . to simplify their description suppose that mgo has chosen mexitintersection . then the four possible subroutines are msni\ufb00 msni\ufb00 mback and mback . the msni\ufb00 machine always moves in direction d until it encounters a wall . then it moves in perpendicular direction p until it reaches the end of the wall . a wall can end in two ways either the agent is now trapped in a corner with walls in both directions d and p or else there is no longer a wall in direction d. in the first case the msni\ufb00 machine terminates in the second case it resumes moving in direction d. the mback machine moves one step backwards and then moves five steps in direction p. these moves may or may not succeed because the actions are stochastic and there may be walls blocking the way . but the actions are carried out in any case and then the mback machine returns . the msni\ufb00 and mback machines also terminate if they reach the end of a hall or the end of an intersection . these finite state controllers define a highly constrained partial policy . the mback msni\ufb00 and mgo machines contain no choice states at all . the only choice points are in mroot which must choose the direction in which to move and in mexitintersection and mexithall which must decide when to call msni\ufb00 when to call mback and which perpendicular direction to tell these machines to try when they can not move forward . figure shows a maxq graph that encodes a similar set of constraints on the policy . the subtasks are defined as follows root . this is exactly the same as the mroot machine . it must choose a direction d and invoke go . it terminates when the agent enters a terminal state . this is also its goal condition . go . the parameter r is bound to the current by room in which the agent is located . go terminates when the agent enters the room at the end of the hallway in direction d or when it leaves the desired hallway . the goal condition for go is satisfied only if the agent reaches the desired intersection . exitinter . this terminates when the agent has exited room r. the goal condition is that the agent exit room r in direction d. exithall . this terminates when the agent has exited the current hall . the goal condition is that the agent has entered the desired intersection in direction d. sni\ufb00 . this encodes a subtask that is equivalent to the msni\ufb00 machine . however sni\ufb00 must have two child subtasks towall and followwall that were simply internal states of msni\ufb00 . this is necessary because a subtask in the maxq framework can not contain any internal state whereas a finite state controller in the ham representation can contain as many internal states as necessary . in particular it can have one state for when it is moving forward and another state for when it is following a wall sideways . towall . this is equivalent to part of msni\ufb00 and it terminates when there is a wall in front of the agent in direction d. the goal condition is the same as the termination condition . followwall . this is equivalent to the other part of msni\ufb00 . it moves in direction p until the wall in direction d ends . the goal condition is the same as the termination condition . back . this attempts to encode the same information as the mback machine but this is a case where the maxq hierarchy can not capture the same information . mback simply executes a sequence of primitive actions . but to do this mback must have internal states which maxq does not allow . instead the back subtask is has the subgoal of moving the agent at least one square backwards and at least squares in the direction p. in order to determine whether it has achieved this subgoal it must remember the x and y position where it started to execute so these are bound as parameters to back . back terminates if it achieves this subgoal or if it runs into walls that prevent it from achieving the subgoal . the goal condition is the same as the termination condition . backone . this moves the agent one step backwards in the direction opposite to d. it needs the starting x and y position in order to tell when it has succeeded . it terminates if it has moved at least one unit in direction d or if there is a wall in this direction . its goal condition is the same as its termination condition . perpthree . this moves the agent three steps in the direction p. it needs the starting x and y positions in order to tell when it has succeeded . it terminates when it has moved at least three units in the direction p or if there is a wall in that direction . the goal condition is the same as the termination condition . move . this is a parameterized primitive action . it executes one primitive move in direction d and terminates immediately . from this we can see that there are three major di\ufb00erences between the maxq representation and the ham representation . first a ham finite state controller can contain internal states . to convert them into a maxq subtask graph we must make a separate subtask for each internal state in the ham . second a ham can terminate based on an amount of e\ufb00ort whereas a maxq subtask must terminate based on some change in the state of the world . it is impossible to define a maxq subtask that performs k steps and then terminate regardless of the e\ufb00ects of those steps . third it is more difficult to formulate the termination conditions for maxq subtasks than for ham machines . for example in the ham it was not necessary to specify that the mexithallway machine terminates when it has entered a di\ufb00erent intersection than the one where the mgo was executed . however this is important for the maxq method because in maxq each subtask learns its own value function and policy independent of its parent tasks . for example without the requirement to enter a di\ufb00erent intersection the learning algorithms for maxq will always prefer to have maxexithall take one step backward and return to the room in which the go action was started . this problem does not arise in the ham approach because the policy learned for a subtask depends on the whole \ufb02attened hierarchy of machines and returning to the state where the go action was started does not help solve the overall problem of reaching the goal state in the lower right corner . to construct the maxq graph for this problem we have introduced three programming tricks binding parameters to aspects of the current state having a parameterized primitive action and employing inheritance of termination conditions that is each subtask in this maxq graph inherits the termination conditions of all of its ancestor tasks . hence if the agent is in the middle of executing a towall action when it leaves an intersection the towall subroutine terminates because the exitinter subroutine has terminated . if this satisfies the goal condition of exitinter then it is also considered to satisfy the goal condition of towall . this inheritance made it easier to write the maxq graph because the parents did not need to pass down to their children all of the information necessary to define the complete termination and goal predicates . there are essentially no opportunities for state abstraction in this task because there are no irrelevant features of the state . there are some opportunities to apply the shielding and termination properties however . in particular exithall is guaranteed to cause its parent task maxgo to terminate so it does not require any stored c values . there are many states where some subtasks are terminated and so no c values need to be stored . nonetheless even after applying the state elimination conditions the maxq representation for this task requires much more space than a \ufb02at representation . an exact computation is difficult but after applying maxq q learning the maxq representation required values whereas \ufb02at q learning requires fewer than values . parr states that his method requires only values . to test the relative e\ufb00ectiveness of the maxq representation we compare maxq q learning with \ufb02at q learning . because of the very large negative values that some states acquire we were unable to get boltzmann exploration to work well one very bad trial would cause an action to receive such a low q value that it would never be tried again . hence we experimented with both \u01eb greedy exploration and counter based exploration . the \u01eb greedy exploration policy is an ordered abstract glie policy in which a random action is chosen with probability \u01eb and \u01eb is gradually decreased over time . the counter based exploration policy keeps track of how many times each action a has been executed in each state s. to choose an action in state s it selects the action that has been executed the fewest times until all actions have been executed t times . then it switches to greedy execution . hence it is not a genuine glie policy . parr employed counter based exploration policies in his experiments with this task . learning rate . initial value for \u01eb of for flat q learning we chose the following parameters . \u01eb decreased by . after each successful execution of a max node and initial q values of . . for maxq q learning we chose the following parameters counter based exploration with t learning rate equal to the reciprocal of the number of times an action had been performed and initial values for the c values selected carefully to provide underestimates of the true c values . for example the initial values for qexitinter were . because in the worst case after completing an exitinter task it takes about steps to complete the subsequent exithall task and hence complete the go parent task . figure plots the results . we can see that maxq q learning converges about times faster than flat q learning . we do not know whether maxq q has converged to a recursively optimal policy . for comparison we also show the performance of a hierarchical policy that we coded by hand but in our hand coded policy we used knowledge of contextual information to choose operators so this policy is surely better than the best recursively optimal policy . hamq learning should converge to a policy equal to or slightly better than our hand coded policy . this experiment demonstrates that the maxq representation can capture most but not all of the prior knowledge that can be represented by the hamq hierarchy . it also shows that the maxq representation requires much more care in the design of the goal conditions for the subtasks . other domains in addition to the three domains discussed above we have developed maxq graphs for singh s \ufb02ag task the treasure hunter task described by tadepalli and dietterich and dayan and hinton s fuedal q learning task . all of these tasks can be easily and naturally placed into the maxq framework indeed all of them fit more easily than the parr and russell maze task . maxq is able to exactly duplicate singh s work and his decomposition of the value function while using exactly the same amount of space to represent the value function . maxq can also duplicate the results from tadepalli and dietterich however because maxq is not an explanationbased method it is considerably slower and requires substantially more space to represent the value function . in the feudal q task maxq is able to give better performance than feudal q learning . the reason is that in feudal q learning each subroutine makes decisions using only a q function learned at that level that is without information about the estimated costs of the actions of its in contrast the maxq value function decomposition permits each max node to descendants . make decisions based on the sum of its completion function c and the costs estimated by its descendants v. of course maxq also supports non hierarchical execution which is not possible for feudal q because it does not learn a value function decomposition . discussion design tradeo\ufb00s in hierarchical reinforcement learning at the start of this paper we discussed four issues concerning the design of hierarchical reinforcement learning architectures . in this section we want to highlight a tradeo\ufb00 between two of those issues the method for defining subtasks and the use of state abstraction . maxq defines subtasks using a termination predicate ti and a pseudo reward function r. there are at least two drawbacks of this method . first it can be hard for the programmer to define ti and r correctly since this essentially requires guessing the value function of the optimal policy for the mdp at all states where the subtask terminates . second it leads us to seek a recursively optimal policy rather than a hierarchically optimal policy . recursively optimal policies may be much worse than hierarchically optimal ones so we may be giving up substantial performance . however in return for these two drawbacks maxq obtains a very important benefit the policies and value functions for subtasks become context free . in other words they do not depend on their parent tasks or the larger context in which they are invoked . to understand this point consider again the mdp shown in figure . it is clear that the optimal policy for exiting the left hand room depends on the location of the goal . if it is at the top of the right hand room then the agent should prefer to exit via the upper door whereas if it is at the bottom of the right hand room the agent should prefer to exit by the lower door . however if we define the subtask of exiting the left hand room using a pseudo reward of zero for both doors then we obtain a policy that is not optimal in either case but a policy that we can re use in both cases . furthermore this policy does not depend on the location of the goal . hence we can apply max node irrelevance to solve the exit subtask using only the location of the robot and ignore the location of the goal . this example shows that we obtain the benefits of subtask reuse and state abstraction because we define the subtask using a termination predicate and a pseudo reward function . the termination predicate and pseudo reward function provide a barrier that prevents communication of value information between the exit subtask and its context . compare this to parr s ham method . the hamq algorithm finds the best policy consistent with the hierarchy . to achieve this it must permit information to propagate into the exit subtask from its environment . but this means that if any state that is reached after leaving the exit subtask has di\ufb00erent values depending on the location of the goal then these di\ufb00erent values will propagate back into the exit subtask . to represent these di\ufb00erent values the exit subtask must know the location of the goal . in short to achieve a hierarchically optimal policy within the exit subtask we must represent its value function using the entire state space . we can see therefore that there is a direct tradeo\ufb00 between achieving hierarchical optimality and achieving recursive optimality . methods for hierarchical optimality have more freedom in defining subtasks . but they can not employ state abstractions within subtasks and in general they can not reuse the solution of one subtask in multiple contexts . methods for recursive optimality on the other hand must define subtasks using some method that isolates the subtask from its context . but in return they can apply state abstraction and the learned policy can be reused in many contexts . it is interesting that the iterative method described by dean and lin can be viewed as a method for moving along this tradeo\ufb00 . in the dean and lin method the programmer makes an initial guess for the values of the terminal states of each subtask . based on this initial guess the locally optimal policies for the subtasks are computed . then the locally optimal policy for the parent task is computed while holding the subtask policies fixed . at this point their algorithm has computed the recursively optimal solution to the original problem given the initial guesses . instead of solving the various subproblems sequentially via an o\ufb04ine algorithm we could use the maxq q learning algorithm . but the method of dean and lin does not stop here . instead it computes new values of the terminal states of each subtask based on the learned value function for the entire problem . this allows it to update its guesses for the values of the terminal states . the entire solution process can now be repeated . to obtain a new recursively optimal solution based on the new guesses . they prove that if this process is iterated indefinitely it will converge to the recursively optimal policy . this suggests an extension to maxq q learning that adapts the r values online . each time a subtask terminates we could update the r function based on the computed value of the terminated state . to be precise if j is a subtask of i then when j terminates in state s we should update r to be equal to v maxa q. however this will only work if r is represented using the full state s. if subtask j is employing state abstractions x \u03c7 then r will need to be the average value of v where the average is taken over all states s such that x \u03c7 . this is easily accomplished by performing a stochastic approximation update of the form r r \u03b1t v each time subtask j terminates . such an algorithm could be expected to converge to the best hierarchical policy consistent with the given state abstractions . this also suggests that in some problems it may be worthwhile to first learn a recursively optimal policy using very aggressive state abstractions and then use the learned value function to initialize a maxq representation with a more detailed representation of the states . these progressive refinements of the state space could be guided by monitoring the degree to which the values of v vary for a single abstract state x. if they have a large variance this means that the state abstractions are failing to make important distinctions in the values of the states and they should be refined . both of these kinds of adaptive algorithms will take longer to converge than the basic maxq method described in this paper . but for tasks that an agent must solve many times in its lifetime it is worthwhile to have learning algorithms that provide an initial useful solution but gradually improve that solution until it is optimal . an important goal for future research is to find methods for diagnosing and repairing errors in the initial hierarchy so that ultimately the optimal policy is discovered . concluding remarks this paper has introduced a new representation for the value function in hierarchical reinforcement learning the maxq value function decomposition . we have proved that the maxq decomposition can represent the value function of any hierarchical policy under both the finite horizon undiscounted cumulative reward criterion and the infinite horizon discounted reward criterion . this representation supports subtask sharing and re use because the overall value function is decomposed into value functions for individual subtasks . the paper introduced a learning algorithm maxq q learning and proved that it converges with probability to a recursively optimal policy . the paper argued that although recursive optimality is weaker than either hierarchical optimality or global optimality it is an important form of optimality because it permits each subtask to learn a locally optimal policy while ignoring the behavior of its ancestors in the maxq graph . this increases the opportunities for subtask sharing and state abstraction . we have shown that the maxq decomposition creates opportunities for state abstraction and we identified a set of five properties that allow us to ignore large parts of the state space within subtasks . we proved that maxq q still converges in the presence of these forms of state abstraction and we showed experimentally that state abstraction is important in practice for the successful application of maxq q learning at least in the taxi and kaelbling hdg tasks . the paper presented two di\ufb00erent methods for deriving improved non hierarchical policies from the maxq value function representation and it has formalized the conditions under which these methods can improve over the hierarchical policy . the paper verified experimentally that nonhierarchical execution gives improved performance in the fickle taxi task and in the hdg task . finally the paper has argued that there is a tradeo\ufb00 governing the design of hierarchical reinforcement learning methods . at one end of the design spectrum are context free methods such as maxq . they provide good support for state abstraction and subtask sharing but they can only learn recursively optimal policies . at the other end of the spectrum are context sensitive methods such as hamq the options framework and the early work of dean and lin . these methods can discover hierarchically optimal policies but their drawback is that they can not easily exploit state abstractions or share subtasks . because of the great speedups that are enabled by state abstraction this paper has argued that the context free approach is to be preferred and that it can be relaxed as needed to obtain improved policies . acknowledgements the author gratefully acknowledges the support of the national science foundation under grant number iri the office of naval research under grant number n the air force office of scientific research under grant number f and the spanish council for scientific research . in addition the author is indebted to many colleagues for helping develop and clarify the ideas in this paper including valentina bayer leslie kaelbling bill langford wes pinchot rich sutton prasad tadepalli and sebastian thrun . i particularly want to thank eric chown for encouraging me to study feudal reinforcement learning ron parr for providing the details of his ham machines and sebastian thrun encouraging me to write a single comprehensive paper . i also thank the anonymous reviewers of previous drafts of this paper for their suggestions and careful reading which have improved the paper immeasurably ."
    ],
    "abstract": [
        "this paper presents the maxq approach to hierarchical reinforcement learning based on decomposing the target markov decision process into a hierarchy of smaller mdps and decomposing the value function of the target mdp into an additive combination of the value functions of the smaller mdps . the paper defines the maxq hierarchy proves formal results on its representational power and establishes five conditions for the safe use of state abstractions . the paper presents an online model free learning algorithm maxq q and proves that it converges wih probability to a kind of locally optimal policy known as a recursively optimal policy even in the presence of the five kinds of state abstraction . the paper evaluates the maxq representation and maxq q through a series of experiments in three domains and shows experimentally that maxq q converges to a recursively optimal policy much faster than flat q learning . the fact that maxq learns a representation of the value function has an important benefit it makes it possible to compute and execute an improved non hierarchical policy via a procedure similar to the policy improvement step of policy iteration . the paper demonstrates the effectiveness of this non hierarchical execution experimentally . finally the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.8311111111111111
    ]
}