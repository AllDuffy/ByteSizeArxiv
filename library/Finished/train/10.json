{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0509055v1",
    "article": [
        "keogh and pazzani define augmented bayes networks in which the augmenting arcs form a forest on the attributes and present heuristic search methods for learning good though not optimal augmenting arc sets . in this paper we present a simple polynomial time greedy algorithm for learning an optimal augmented bayes network with respect to mdl score . categories and subject descriptors i. pattern recognition i. . classifier design and evaluation keywords bayesian networks classification augmented bayes networks tan mdl introduction . classification is a machine learning task that requires construction of a function that classifies examples into one of a discrete set of possible categories . formally the examples are vectors of attribute values and the discrete categories are the class labels . the construction of the classifier function is done by training on preclassified instances of a set of attributes . this kind of learning is called supervised learning as the learning is based on labeled data . a few of the various approaches for supervised learning are artificial neural networks decision tree learning support vector machines and bayesian networks . all these methods are comparable in terms of classification accuracy . bayesian networks are especially important because they provide us with useful information about the structure of the problem itself . one highly simple and e\ufb00ective classifier is the naive bayes classifier . the naive bayes classifier is based on the assumption that the attribute values are conditionally independent of each other given the class label . the classifier learns the probability of each attribute xi given the class c from the preclassified instances . classification is done by calculating the probability of the class c given all attributes x x ... xn . the computation of this probability is made simple by application of bayes rule and the rather naive assumption of attribute independence . in practical classification problems we hardly come across a situation where the attributes are truly conditionally independent of each other . yet the naive bayes classifier performs well as compared to other state of art classifiers . an obvious question that comes to mind is whether relaxing the attribute independence assumption of the naive bayes classifier will help improve the classification accuracy of bayesian classifiers . in general learning a structure that represents the appropriate attribute dependencies is an np hard problem . several authors have examined the possibilities of adding arcs between attributes of a naive bayes classifier that obey certain structural restrictions . for instance friedman geiger and goldszmidt define the tan structure in which the augmenting arcs form a tree on the attributes . they present a polynomial time algorithm that learns an optimal tan with respect to mdl score . keogh and pazzani define augmented bayes networks in which the augmenting arcs form a forest on the attributes and present heuristic search methods for learning good though not optimal augmenting arc sets . the authors however evaluate the learned structure only in terms of observed misclassification error and not against a scoring metric such as mdl . sacha in his dissertation defines the same problem as forest augmented c figure a simple augmented bayes network naive bayes and presents polynomial time algorithm for finding good classifiers with respect to various quality measures . the author however does not claim the learned structure to be optimal with respect to any quality measure . in this paper we present a polynomial time algorithm for finding optimal augmented bayes networks forest augmented naive bayes with respect to mdl score . the rest of the paper is organized as follows . in section we define the augmented bayes structure . section defines the mdl score for bayesian networks . the reader is referred to the friedman paper for details on mdl score as we present only the necessary details in section . section provides intuition about the problem and section and present the polynomial time algorithm and prove that its optimal . augmented bayes networks the augmented bayes network structure is defined by keogh and pazzani as follows every attribute xi has the class attribute c as its an attribute xi may have at most one other attribute parent . as its parent . note that the definition is similar to the tan definition given in . the di\ufb00erence is that whereas tan necessarily adds n augmenting arcs abn adds any number of augmenting arcs up to n. figure shows a simple abn . the dashed arcs represent augmenting arcs . note that attributes and in the figure do not have any incoming augmenting arcs . thus the abn structure does not enforce the tree structure of tan giving more model \ufb02exibility . background in this section we present the definitions of bayesian network and its mdl score . this section is derived from the friedman paper . we refer the reader to the paper for more information as we only present the necessary details . a bayesian network is an annotated directed acyclic graph that encodes a joint probability distribution of a do main composed of a set of random variables . let u be a set of n discrete attributes where each attribute xi takes values from a finite domain . then the bayesian network for u is the pair b g \u03b8 where g is a dag whose nodes correspond to the attributes x ... xn and whose arcs represent direct dependencies between the attributes . the graph structure g encodes the following set of independence assumptions each node xi is independent of its non descendants given its parents in g. the second component of the pair \u03b8 contains a parame p for each possible value xi of xi and ter \u03b8xi \u03c0xi \u03c0xi of \u03c0xi . b defines a unique joint probability distribution over u defined by pb pb n yi the problem of learning a bayesian network can be stated as follows . given a training set d of instances of u find a network that best fits d. we now review the minimum description length of a bayesian network . as mentioned before our algorithm learns optimal abns with respect to mdl score . the mdl score casts learning in terms of data compression . the goal of the learner is to find a structure that facilitates the shortest description of the given data . intuitively data having regularities can be described in a compressed form . in context of bayesian network learning we describe the data using dags that represent dependencies between attributes . a bayesian network with the least mdl score is said to model the underlying distribution in the best possible way . thus the problem of learning bayesian networks using mdl score becomes an optimization problem . the mdl score of a bayesian network b is defined as m dl n i b log n n xi where n is the number of instances of the set of attributes b is number of parameters in the bayesian network b n is number of attributes and i is the mutual information between an attribute xi and its parents in the network . as per the definition of the abn structure the class attribute does not have any parents . hence we have i. also each attribute has as its parents the class attribute and at most one other attribute . hence for the abn structure we have n xi n n i i i xi \u03c0 xi \u03c0 the first term on r.h.s in equation represents all attributes with an incoming augmenting arc . the second term represents attributes without an incoming augmenting arc . consider the chain law for mutual information given below i i i applying the chain law to the first term on r.h.s of equation we get n xi n xi \u03c0 i i i n xi for any abn structure the second term of equation n i i is a constant . this is because the term represents the arcs from the class attribute to all other attributes p in the network and these arcs are common to all abn structures . using equations and we rewrite the non constant terms of the mdl score for abn structures as follows m dl baug log n n n xi \u03c0 i where baug denotes an abn structure . some insights looking at the mdl score given in equation we present a few insights on the learning abn problem . the first term of the mdl equation baug log n represents the length of the abn structure . note that the length of any abn structure depends only on the number of augmenting arcs as the rest of the structure is the same for all abns . if we annotate the augmenting arcs with mutual information between the respective head and tail attributes then the second term n i \u03c0 i represents the sum of costs of n all augmenting arcs . since the best mdl score is the minimum score our problem can be thought of as balancing the number of augmenting arcs against the sum of costs of all augmenting arcs where we wish to maximize the total cost . p the mdl score for abn structures is decomposable on attributes . we can rewrite equation as note that this equivalence implies that the overall change in mdl score is independent of the arc direction . that is adding an augmenting arc changes the network score identically to adding the arc . thus any augmenting arc is eligible to be added to an abn structure if it has a cost at least the defined threshold tr and if it does not violate the abn structure . note that this threshold depends only on the number of discrete states of the attributes and the number of cases in the input database and is independent of the direction of the augmenting arc . we now present a polynomial time greedy algorithm for learning optimal abn with respect to mdl score . the algorithm . construct a complete undirected graph g such that v is the set of attributes . for each edge e g compute cost i. annotate e with cost . remove from the graph g any edges that have a cost less than the threshold tr . this will possibly make the graph g unconnected . run the kruskal s maximum spanning tree algorithm on each of the connected components of g. this will make g a maximum cost forest . for each tree in g choose a root attribute and set directions of all edges to be outward from the root attribute . n xi xi log n n i. add the class variable as a vertex c to the set v and add directed edges from c to all other vertices in g. return g. where xi are the number of parameters stored at attribute xi . the number of parameters stored at attribute xi depends on the number of parents of xi in baug and hence on whether xi has an incoming augmenting arc . since we want to minimize the mdl score of our network we should add an augmenting arc to an attribute xi only if its cost i dominates the increase in the number of parameters of xi . for example consider an attribute xi with no augmenting arc incident on it . then the number of parameters stored at the attribute xi in abn will be c where c and xi are the number of states of the attributes c and xi respectively . thus xi c. if now an augmenting arc e having a cost of cost i i is made incident on the attribute xi then the number of parameters stored at xi will be xi xj . c. where xj is the number of states of the attribute xj . note that the addition of the augmenting arc has increased the number of parameters of the network . since we want to add an augmenting arc on xi only if it reduces the mdl score the following condition must be satisfied c log n log n n cost which is equivalent to cost c n log n tr the algorithm constructs an undirected graph g in which all edges have costs above the defined threshold tr . as seen in the previous section all edges having costs greater than the threshold improve the overall score of the abn structure . running the maximum spanning tree algorithm on each of the connected components of g ensures that the abn structure is preserved and at the same time maximizes the second term of the mdl score given in equation . note that if in step of the algorithm the graph g remains connected our algorithm outputs a tan structure . in this sense our algorithm can be thought of as a generalization of the tan algorithm given in . the next section proves that the augmented bayes structure output by our algorithm is optimal with respect to the mdl score . proof we prove that the abn output by our algorithm is optimal by making the observation that no optimal abn can contain any edge that was removed in step of the algorithm . this is because removing any such edge lowers the mdl score and leaves the structure an abn . consequently an optimal abn can contain only those edges that remain after step of the algorithm . if an optimal abn does not connect some connected component of the graph g that results following step edges with costs greater than or equal to tr can be added without increasing overall mdl score until the component is spanned . hence there exists an optimal abn that spans each component of the graph g that results from step . by the correctness of kruskal s algorithm run on each connected component to find a maximum cost spanning tree an optimal abn is found . thus the abn output by our algorithm is an optimal abn . references r. duda and p. hart . pattern classification and scene analysis . john wiley and sons new york . n. friedman d. geiger and m. goldszmidt . bayesian network classifiers . machine learning . n. friedman and m. goldszmidt . discretization of continuous attributes while learning bayesian networks . in proceedings of the thirteenth international conference on machine learning pages . e. keogh and m. pazzani . learning augmented bayesian classifiers a comparison of distribution based and classification based approaches . in proceedings of the seventh international workshop on artificial intelligence and statistics pages . t. mitchell . machine learning . wcb mcgraw hill . j. rissanen . modelling by shortest data description . automatica ."
    ],
    "abstract": [
        "naive bayes is a simple bayesian classifier with strong independence assumptions among the attributes . this classifier desipte its strong independence assumptions often performs well in practice . it is believed that relaxing the independence assumptions of a naive bayes classifier may improve the classification accuracy of the resulting structure . while finding an optimal unconstrained bayesian network is an np hard problem it is possible to learn in polynomial time optimal networks obeying various structural restrictions . several authors have examined the possibilities of adding augmenting arcs between attributes of a naive bayes classifier . friedman geiger and goldszmidt define the tan structure in which the augmenting arcs form a tree on the attributes and present a polynomial time algorithm that learns an optimal tan with respect to mdl score . keogh and pazzani define augmented bayes networks in which the augmenting arcs form a forest on the attributes and present heuristic search methods for learning good though not optimal augmenting arc sets . the authors however evaluate the learned structure only in terms of observed misclassification error and not against a scoring metric such as mdl . in this paper we present a simple polynomial time greedy algorithm for learning an optimal augmented bayes network with respect to mdl score ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.7746478873239436
    ]
}