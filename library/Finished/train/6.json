{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0506085v1",
    "article": [
        "introduction this paper proposes a new way of modelling machine learning inspired by streambased active learning but more accurately re\ufb02ecting strategies humans employ when working with trainers and more directly addressing the needs of users reluctant to risk effort integrating ml algorithms . like active learning it aims to reduce labeling overhead but unlike existing models it removes the separation between training and testing leading to systems which can quickly adapt to their tasks as well as receive ongoing training throughout their useful lives . we call our approach on the job training . traditional paradigms assume that an administrator prepares training and test sets with distributions similar to that of the real problem assesses performance of the agent then deploys a fixed instance of the classifier once it reaches a satisfactory level of performance . however this train test use paradigm does not adequately model the needs of systems which must begin performing immediately or living systems whose datasets change and for which additional classified data becomes available over time . the ojt framework explicitly addresses this by formally defining a concurrent train use process along with performance metrics which indicate both immediate and long term performance . traditional classifications distinguish between active and passive systems and within active learning between pool based and stream based designs . we expand this space to include synchronous and asynchronous relationships between agent and trainer the presence or absence of intervention from the trainer and temporally evolving distributions of interest . this is a framework paper exploring one particular niche in a spectrum of learning paradigms which we claim re\ufb02ects the needs of many real world practitioners . like stream based active learning our model lets the agent decide how confident it feels in its classification ability allowing it to take over for the trainer when easy problems arise or to ask for help when a difficult query comes along maximizing the value of the trainer s time . our performance metrics are unique in measuring how rapidly the agent gains the ability to relieve the trainer from tedious tasks without neglecting long term accuracy . we also assume that the trainer is willing to work interactively with the agent giving unsolicited advice or allowing the agent to specify what points it would most like to have classified as a pool based active learner would . these points will often identify interesting or problematic areas for the trainer to research . consider how people tend to evaluate software . users almost never invest a large amount of initial effort reading a program s manual prefering instead to start working immediately to see if the software does what they need . while they have some initial patience they expect to see results quickly and if the package isn t working when their patience runs out they ll discard it regardless of how it might have done in the long run . conversely users also expect software that they use over time to remember their changing preferences and get frustrated with software that continually requires overrides to its initial settings . our metrics are designed to address these human requirements emphasizing the need to solve initial problems correctly with human aid minimizing up front training demands yet still requiring satisfactory results in the long run in short suitable for on going real time interaction with humans . consider also the problem of constructing security policies . low security installations tend to use overly broad policies rather than expend the resources required to handle the numerous exceptions and permission grants which occur over time . administrators will be unwilling to spend time with ml assistants unless they can adapt to their task very rapidly responding to correction asking for help only for boundary conditions and adapting to later policy changes . high security installations have to balance the need for tight control with the danger of overly complex policies which will have subtle errors . their administrators are unwilling to allow an agent to make security decisions but can still benefit from an agent which can point out parts of the access space which seem most unusual . in both cases users want an agent which can quickly give useful results yet perform in the long term as conditions change . there is little work directly related to this paper . active learning traditionally focuses on different learning techniques as opposed to exploring new performance requirements . closest in spirit to ojt are stream based active learners but these have generally been evaluated in terms of overall classifier accuracy rather than with respect to their ability to address the immediate human needs we have illustrated . in addition to defining our class of agents and corresponding performance metrics we will specify how to build an ideal but impractical ojt learner and consider an obvious approximation to that algorithm based on knn . we conclude with promising performance results and discuss future research . system specification define s a sample set of zero or more unclassified points which may be used as questions by the selective sampling algorithm . s is provided to the agent at the start of the execution phase . define c a function which returns the true classification of any point q. let gt denote the agent s guess at the classification of any point q at a particular time t. let v the set representing the queries which have arrived . let t be the loop iteration counter . define p erf an performance metric which keeps track of the agent s classifier throughout the execution phase . an ojt agent executes a series of five basic steps which may be arranged differently depending on the system design . the steps are query . a query q is sent to the agent . set v v q. q can be null for asynchronous agents meaning that no query needs to be answered at this time . question . the agent outputs a question q an unclassified point it wishes the trainer to classify . in selective sampling implementations q s v. q implies the agent chooses not to ask a question . fact . the trainer sends the agent f hq ci or f if q. f can be null for asynchronous agents meaning that no fact is forthcoming at this time . real systems might also allow the trainer to set q providing an unsolicited fact . answer . the agent outputs gt . assess . call p erf to assess the agent s performance . here we make our first distinctions between agents . a synchronous system requires a query to be provided at each loop iteration and that the trainer always answers questions asked by the agent during the iteration in which they re asked . note that agents are not required to ask a question during a particular loop iteration . real world problems which benefit from on the job training are likely to have an asynchronous nature however in which queries may arrive at any time and in which trainers may not always be available or willing to answer questions from the agent . furthermore the facts provided by the trainer are not required to correspond to the questions asked by the agent . the order of operations in the loop is critical and motivates the distinction between systems with and without intervention . if the answer phase is placed after the fact phase the agent has no opportunity to choose its question so as to aid in answering the query at hand and is said to be a system without intervention . if the query set v follows a different distribution than the sample set s then the agent can still use v to specialize on part of the problem space . but if the distributions of v and s are the same then the agent in a system without intervention has no opportunity to improve its accuracy for an unanswered query . an agent with intervention therefore executes the steps in the following order query question fact answer assess . an agent without intervention executes the steps in this order query answer question fact assess . synchronous and asynchronous systems are distinguished by whether or not null queries and facts are permitted . performance metrics since ojt aims to encourage creation of both agents which respond quickly to new tasks and agents which are used and trained over long periods of time we propose two new performance metrics . the first fixes all but two variables giving the algorithm a fixed budget of questions to ask . the second metric uses an arbitrary utility function allowing question and wrong answer values to be externally provided . a crucial point is that these metrics are made available to the agent which allows it to optimize with respect to the performance measure it is being evaluated by . both metrics calculate agent accuracy cumulatively as the queries are answered . because of the lack of a formal testing phase we calculate accuracy using the answers provided at each step . the focus on accuracy from the start requires a finite limit on the number of rounds considered if there is also to be a finite limit to the number of questions . otherwise as we will show later in this section cumulative accuracy becomes indistinguishable from traditional accuracy measurements . for the first metric this requires the user to decide how important the agent s learning rate is as a function of the amount of training he is willing to provide . this is natural decision a user trying to solve a problem in one hour wants an algorithm which will very quickly be of some assistance while a user preparing an agent for high volume long term service will be willing to offer more training to an algorithm in exchange for high overall accuracy . budget metric . our first metric mirrors the traditional active learning model in which an algorithm chooses a preset number of points for classification . since this metric s utility lies in the simplicity of choosing only question and query limits we specify it with respect to a synchronous system . the metric computes the agent s cumulative error over the course of execution . agents are expected to use all available questions no reward is given for unused questions at termination . define kq to be the query limit and kq to be the question budget . let c be the counter for incorrectly classified queries and qrem kq be the number of questions remaining . the budget metric is then defined as the cumulative error rate or c kq . this simple measure would be no different from traditional error calculations except that c is accumulated after each query is answered in the assess phase . in section we give two theorems which demonstrate the significance of this seemingly small difference . note that since this metric considers the rate at which each learner learns our metric is quite useful for evaluating active learners since their strength is generally considered to be in their fast convergence . utility metric . in the utility metric the assess phase calculates c the cumulative cost as \u03c3cq cw where cq and cw denote the cost of asking a particular question or being wrong about a particular query . if possible these cost functions should be made available to the agent . at the risk of being too general this metric illustrates that agents should take advantage of questions that are easy for a trainer to answer and should be willing to guess when a wrong answer will not be expensive . intervention and kq in this section we show how two of the seemingly minor elements of our system and metric definitions have a dramatic impact on agent strategy and performance . first we show that systems without intervention have advantages over traditional active learners only when the distribution of queries over time reveals information about the task at hand . second we emphasize that ojt s distinctions from traditional active learning fade once questions are no longer being answered and that consequently the choice of kq in our proposed metrics is critical in making useful assessments of an ojt system . theorem . assuming queries are chosen uniformly at random with replacement from a set t whose distribution is the same as a sufficiently large unlabeled sample set s an agent in a synchronous ojt system without intervention has no statistical advantage over a traditional active learning agent with respect to the budget metric . proof consider the first iteration for an agent a. a is forced to answer a query q before receiving the classification for any point . once it provides its guess it is allowed to choose any point as its question . since the trainer returns a fact only after the agent has output its guess the fact clearly can not in\ufb02uence that guess . but since q was chosen randomly from t and t has the same distribution as s it provides no information in a statistical sense about any future query . consequently the agent can discard q after guessing its classification . but this again happens before the question phase so q provides no useful information about what question to ask . by induction we see that the agent has the same dilemma for all future iterations of the loop . the agent can therefore equivalently output its classifier gt in each iteration of the loop before the query arrives then ignore the query once it does since the query provides no utility to the agent for current or future rounds . thus a can equivalently operate as either an ojt agent or a traditional active learner with respect to the budget metric . in the event that t and s have different distributions ojt agents have an advantage over active learners even in systems without intervention . but also note that this proof doesn t apply to systems with intervention in which ojt can outperform baseline even for problems which are traditionally considered unlearnable . the importance of kq while ojt systems should generally produce agents which have good overall accuracy their emphasis is on learning the query set at hand . while this seems natural for many applications it is useful to note that if all you want is a system to train once and then deploy ojt has little to offer over traditional active learning as this theorem demonstrates . theorem . define an optimal agent as one whose classifier accuracy monotonically increases as it learns queries and facts and which achieves cumulative error less than or equal to the cumulative error of any other practical classifier . then given a finite kq and an infinite query set t the difference in cumulative error \u01eb between optimal agent a in an ojt system with and intervention and optimal agent b in an ojt system without intervention approaches zero as kq approaches infinity . proof assume that an optimal agent a in an ojt system with intervention can achieve a cumulative error of ea by asking no more than kq questions over kq queries from t. an optimal agent b in a system without intervention can receive precisely the same information available to a but only at a later time so its cumulative error eb can never be lower than ea . let \u01eb eb ea . assume that \u01eb is maximized so that a answers kq queries correctly which b misclassifies as a result of not receiving facts until later in its loop . then for a given kq kq \u01eb kq kq which approaches as kq approaches infinity . despite our questionable definition of an optimal agent it should be clear that the advantages of intervention fade as one considers cumulative accuracy past the point where the question budget has been exhausted . this should not be taken to mean that ojt is no different from other types of ml rather it should emphasize that ojt s strengths lie in adaptability to temporally evolving conditions and ability to minimize load on trainers . implementation here we show how to create both an ideal but impractical ojt agent as well as a practical agent which generally outperforms its active learning counterpart . both implementations assume a budget metric and synchronous systems with intervention . modeling ojt agent strategies is easier if the agent can predict how accurate it will become if given the classification of a particular unlabeled point . the implementations we propose all assume that the underlying classifier has the ability to recursively predict how it will classify future points and how confident it will be in that classification . let such an algorithm provide three functions u nc returns the probability that p will be misclassified . add assumes that p s label is known when calculating u nc . remove means that the algorithm should no longer assume that p s label is known . ideal ojt given an agent with perfect foresight and unlimited computing power and some simplifying assumptions about the system it is straightforward to construct an ideal system in the sense that it minimizes cumulative error with respect to the budget metric . we begin by assuming the entire test set t and remaining test set r are known and that queries are chosen from r uniformly at random . let q be the query just selected in the query phase and qrem be the number of questions remaining in the question budget . let p ermute calculate all permutations of the elements of t calling f on each permutation let average returns the average of the elements in set s. then the ideal ojt algorithm for a selective sampling synchronous ojt system with intervention is as follows . define a function expectedw rong which returns the number of queries it expects to misclassify by the time r is exhausted . assume expectedw rong is called during the question phase of system execution and that the value q is the question it selects . letting the function avgpenalty return average p ermute expectedwrong if q r set q return . start assuming we ask no question let q qbest let m inp enalty u nc avgp enalty then consider all possible questions we might ask if qrem return m inp enalty loop for all q set p enalty u nc avgp enalty if set m inp enalty p enalty set qbest q set q qbest return m inp enalty note that since we assume questions are chosen randomly from r we can average the penalties over all possible futures choosing the question in this round which leads to an optimal outcome on average since all sequences are equally likely . note that this algorithm is entirely intractable p ermute calls expectedw rong for every possible future and expectedw rong is itself a recursive function . but it does suggest a general form which a practical algorithm might take if it can approximate or efficiently calculate the expected penalty across reasonable futures . a tractable approximation the knn classifier described in provides an excellent starting point for implementing ojt classifiers . their classifier implements active learning using selective sampling and outperforms similar passive learners as well as several active learning approaches adapted from other sources it also makes it simple to construct the add remove and u nc functions required by our implementation . in fact conditional uncertainty with lookahead is the basis for their utility metric . our first approximation of the ideal algorithm effectively answers the question if i have only one question to ask and i ask it now what should it be it approximates this implementation of expectedw rong by substituting s v as an approximation to r. using u nc as a shorthand form of add u nc remove the algorithm is min q s v u nc qrem qrem r x p r u nc the summation of uncertainties divided by r gives the expected overall classifier error given the true classification of the q under consideration . multiplying by qrem would give the expected number of misclassifications given no further questions but first we subtract qrem since at least that many additional queries can be classified correctly just by passing them along as questions . we then add the penalty of misclassifying the current query explicitly since a penalty will certainly be incurred if it is misclassified as opposed to the possible future queries whose uncertainty is amortized . most obviously missing from this implementation is the option of asking no question at all in a given round since we have not yet found a reliable efficient way to predict when this behavior will be worthwhile . figure on the left kq kq . with . times more queries than questions the ojt agent s advantage in both cumulative and overall error is sharply defined . the always ask algorithm suffers from its poor overall classifier while the activer learner can not take advantage of the first queries . on the right kq kq . with times more queries than questions we see cumulative error beginning to converge with overall error as our second theorem predicts . ojt still wins in the cumulative metric although the active learner closes the gap in the overall metric and in an independent overall metric would be performing just as well as the ojt agent . all results averaged over runs . active learner ojt active learner ojt r o r r e e v i t a l u m u c. e t a r s s i m. queries figure performance results on the ionosphere dataset . on the left kq kq . here we see how the active learner performs for the first kq queries gradually converging toward its overall accuracy . the ojt agent however quickly becomes quite good at the query set since it frequently chooses the queries themselves as questions for the trainer . once it runs out of questions the cumulative error rate will gradually increase as it approaches its overall accuracy threshold . on the right kq kq . this graph shows the rate at which each classifier misses each of the first kq queries . here we can see even more strikingly the advantage of the ojt agent in adapting to queries while still building a good overall classifier . dataset ionosphere ionosphere metric cumulative overall cumulative overall segmentation cumulative overall segmentation cumulative overall pima indians cumulative overall pima indians cumulative overall kq ojt . active . always ask . table error rates averaged over runs . performance we compared our classifier to two others . the first uses the simple strategy of using the first kq queries as questions as they arrive . this ensures cumulative error for the first kq rounds and performs as a passive learner would for the remaining queries . the second strategy is a simple adaptation of the active learning algorithm proposed in . it adds queries to its unlabeled set as they come in but otherwise behaves normally . in all cases we used the budget metric and an overall accuracy metric with kq for evaluation purposes since the results in used approximately training examples for each classifier they tested and because the budget metric is easier to directly compare with overall accuracy metrics . note that we were unable to reproduce the exact results in . figure shows our results on the ionosphere dataset from uci . the active learning algorithm s representation in the overall metric is not entirely fair since the overall metric uses the same queries sent to the agents during execution . an entirely separate test set would give a better picture of the true generalized accuracy of each classifier although our metric is a good halfway point between such a general metric and our cumulative metric . but as our second theorem points out as kq increases such advantages disappear anyway . this can be seen on the right graph where the cumulative metric more closely matches the overall metric and the gap between the active and ojt learners decreases slightly . we expect that gap to disappear entirely given a large enough kq . correcting for this inequity analytically we see that the results are also consistent with what we expect . given the active learner s. overall accuracy we would expect it to be wrong on about . of the first queries while the ojt agent might get all correct . the test set used had elements so we expect the ojt learner s results to be about . higher than they would be on a completely independent test set would indicate . since this is almost exactly the gap between the active and ojt learners we suspect such a test would show that their long term accuracies are almost identical . figure shows how the ojt agent quickly adapts to the query set achieving a low cumulative error rates by the time the question budget is exhausted . finally table lists our results from two other datasets . the image segmentation dataset produced results comparable to the ionosphere set although the active learner edged out the ojt agent even with the handicap discussed earlier . the pima indians database was surprising however in that the always ask strategy outperformed both other agents in both cumulative and overall accuracy investigating this result is left for future research . conclusions this is a wide open research area . our practical implementation provides an initial attempt at maximizing the goals of ojt and can be vastly improved . asynchronous systems combined with creative cost functions in the utility metric provide a host of possible directions for further research including consideration of costs which vary over time and burstiness of fact input when trainers are only intermittently available in asynchronous systems ."
    ],
    "abstract": [
        "we propose a new framework for building and evaluating machine learning algorithms . we argue that many real world problems require an agent which must quickly learn to respond to demands yet can continue to perform and respond to new training throughout its useful life . we give a framework for how such agents can be built describe several metrics for evaluating them and show that subtle changes in system construction can significantly affect agent performance ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.5844155844155844
    ]
}