{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0511088v1",
    "article": [
        "ted variance as in the proposed algorithm . note the existence of this algorithm implies that the earlier bounds are tight . interestingly the algorithm does not require knowledge of a or \u03c3 which are used only in the analysis . due to the statistics of the situation stderr \u02c6x t scales appropriately with a and \u03c3 . tocorollary the tal regret prior to time t defined by rt p\u03c4 t f f incurred by an unbiased feed back process is bounded below in expectation by discussion e \u03c3 t if the fastest possible convergence bound were not of this form then we would obtain a valid bound but not a tight one . however we constructively show that the bound obtained is tight . although the above theorems all assume unbiased estimates integration of prior information would assuming that the prior is smooth only change an initial transient response of the system leaving the asymptotic behaviour unchanged . the limits on regret would change by only a small additive psfrag replacements psfrag replacements xt \u02c6x t n greedy xt \u02c6x t t r t r xt \u02c6x t xt \u02c6x n t greedy xt \u02c6x t n stderr \u02c6x stderr \u02c6x n t t. n t xt \u02c6x t xt \u02c6x xt \u02c6x t t xt \u02c6x t n t xt \u02c6x t n psfrag replacements psfrag replacements greedy xt \u02c6x t xt \u02c6x t xt \u02c6x t n n greedy xt \u02c6x t xt \u02c6x xt \u02c6x t t t stderr \u02c6x n t. n t figure total regret as a function of time for overlaid runs of the algorithm of theorem which optimally trades off exploration and exploitation with p. for more query noise resulting in less between run variation but more regret with p. for less query noise resulting in more between run variation and for the greedy strategy zero query noise in which runs rapidly converge to incorrect estimates . all runs used \u03c3 a b c and were initialised with two queries at x x. constant whose value would dependant upon the details of the prior . the above exploration exploitation tradeoff and bound holds when using noisy measurements and the cost of an evaluation is the value of the function being optimised . the result is robust in that small changes to the model will not change their character . however a related situation finding the zero x of a linear function using noisy measurements where the expected loss of a measurement xt x has a surprisingly difis quadratic in xt ferent result . in this matching shoulders lobpass case formalised by abe and takeuchi based on the foraging theory question posed by herrnstein a convergence rate of e o and thus an expected regret of e o can be achieved . this is because the measurements in that t r t r r. greedy p figure bar graph of total regret after queries averaged over runs for the algorithm of theorem with \u03c3 and a. bars shown for values of p both above and below the optimal p and also for the greedy algorithm of zero injected noise . risers show sample standard deviations . setting serve the purpose of gradient information . procedures which do not insert sufficient variability into their queries acquire only finite leverage resulting in convergence to a non optimum . this is seen in the upper simulations of fig . the minimal total regret in fig . is for an algorithm injecting slightly less query than stderr \u02c6x t. this is due to the slight additional leverage caused by \ufb02uctuation of the estimate \u02c6x t over time . some procedures used in practise for problems of this character appear to attempt to exceed the convergence bound established here for instance in medical treatment optimisation . the above bounds should serve as a caution concerning the ease with which a seemingly reasonable optimisation procedure can converge to a non optimum . in the setting considered here when insufficient query variance is used convergence to a nonoptimum occurs and standard statistical analysis of the ongoing measurements will fail to give any hint of a problem . query variability must be in jected when the setting itself requires it rather than only in response to empirical signs of premature convergence . in business the best selling price should be faster to estimate than the supply or demand curves which seem potentially subject to this bound . this would argue that firms that set their prices by first estimating supply and demand curves may be at a disadvantage against those that set prices directly . more speculatively regulatory regimes have surprising variability considering that all are designed to further similar goals . legal systems have similar diversity . the ultimate cause of this variability may be the intrinsic difficulty of gradient free noisy query optimisation . even more speculatively sexual selection for adaptive traits may provide a proxy for gradient information thus speeding evolution . acknowledgements supported by science foundation ireland grant pi . c. thanks to tony zador ken duffy and susanna still for helpful comments ."
    ],
    "abstract": [
        "the problem of finding an optimum using noisy evaluations of a smooth cost function arises in many contexts including economics business medicine experiment design and foraging theory . we derive an asymptotic bound e ^ o on the rate of convergence of a sequence generated by an unbiased feedback process observing noisy evaluations of an unknown quadratic function maximised at x. the bound is tight as the proof leads to a simple algorithm which meets it . we further establish a bound on the total regret e sum _ ^ o these bounds may impose practical limitations on an agent s performance as o eps ^ queries are made before the queries converge to x with eps accuracy ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.31932773109243695
    ]
}