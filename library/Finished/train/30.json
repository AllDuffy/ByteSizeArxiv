{
    "id": "C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library\\trainTokenized\\0609007v1",
    "article": [
        "keywords classification rules lazy learning global optimization con\ufb02icting rules resolution strategy . introduction extraction of structural information from raw data is a problem which is of great interest for both fundamental and applied studies . this paper will focus on one specific example of this problem classification . the goal is to predict a class of a particular event . this problem was approached from a number of di\ufb00erent disciplines including statistical data analysis machine learning fuzzy logic operations research and data mining . as a result a variety of learning techniques was developed . the result of learning can be represented in a number of di\ufb00erent forms . the form that we are interested in working with is a set of rules . it should be stressed that some other forms are equivalent to a set of rules . a set of rules is often a preferred form of knowledge representation because it allows for a simple answer to the question what was learned this specific set of rules was learned from the data . for an algorithm which produces only an answer it is often impossible to understand what was really learned and why this specific answer was produced . the model based techniques such as developed in take training data as input and produce a set of rules which can classify any event . the lazy instance based techniques such as developed in return a result tailored to the specific event we want to classify . with such techniques the events similar to the given one are usually found first then a prediction based on found instances is made . an interesting attempt to combine model based and lazy instance based learning was presented in . in a greedy lazy model based approach for classification was developed in which the result was a rule tailored to the specific observation . while such an approach gives a simple rule as an answer and often works faster for classification of a single event it as every greedy algorithm is not guaranteed to find the best rule because the algorithm may not reach the global maximum of the quality criterion and a sub optimal rule may be returned . in the work an approach based on the brute force of rule space scanning was developed . it was used for finding the nuggets of knowledge in the data . in contrast with greedy type algorithms massive search algorithms are guaranteed to find the best rule . in our early work we presented an approach which combined the massive model based rule search approach with lazy instance based learning . in that work we were also interested in nuggets of knowledge but only those which were applicable for the instance we wanted to classify . the result was a set of rules which were applicable for classification of the given event . one may think about these rules as a projection of a global classification rules set to the given instance of the event . art.tex p. a massive local rules search approach to the classification problem in the current paper this approach is taken to the next level and a practical algorithm applicable to a variety of problems is presented . a number of significant improvements have been made since that early version . the current algorithm includes the following new features . highly optimized rule space scanning which allows problems with significant number of attributes to be solved . integration of levels selection procedure for ordered attributes with the rule search algorithm . information about dependent attributes directly included into the tree search algorithm thus significantly reducing computational complexity and . an original con\ufb02icting rules resolution strategy which was especially built to work with automatically generated rules . to create a practical algorithm the three aspects logical statistical and computational complexity need to be addressed . in section we formulate the problem and discuss the logical formulas which represent the rules we are interested in finding . in section we discuss the statistical quality criterion which can be used for evaluation of rule quality and specify the criteria which we use in this work . we also present a con\ufb02icting rules resolution strategy for automatically generated rules . at the end of section a sketch of the algorithm is presented . in section we discuss the selection of attributes for analysis it should be stressed that some attributes as they are built in section are not independent and this fact is known in advance . in section we discuss computational complexity issues an approach which includes information about dependence of the attributes into the algorithm is proposed . in section we discuss error estimation . in section we present the data analysis results and compare our results with the results of c.r. in section a discussion is presented . logical formulas as a result of statistical analysis in this section we describe logical formulas obtained as a result of data analysis . representation of knowledge after it has been learned from the data can vary depending on the approach used . however di\ufb00erent forms of knowledge representation are equivalent to some logical formulas . formulas obtained during data analysis are usually quite complex when applied to prediction or classification . this complicates the understanding of the results . the major source of complexity is the fact that the formulas are usually built to be applicable to all data observations . as we show below the complexity of the rules can be significantly reduced if instead art.tex p. vladislav g. malyshkin et al of building global rules we build local rules which are defined on a subset of observation data this subset must include the data point where we want to perform a prediction classification . such an approach combines the best of both instance based and model based learning . it can be described as an approach working with projections of global formulas to local observations . a drawback of such an approach is the need to recalculate the rules for every event we want to classify . this is the cost of using simple local rules instead of complex global rules . in the simplest form the problem can be represented as the following we have a random variable g and a random vector x of m components x m. m. random variables g and x are assumed to take two di\ufb00erent values true and f alse we have a finite number of observations n each observation gives specific values of x and gn . index n. n numerates the observations . the value of antecedent x is known for n. n the value of consequent gn is known for n. n at the point n the value of consequent is unknown . the problem is predicting the value of g at n. again we are interested in finding a prediction of g only at one point n not in building a universal prediction formula which is applicable at any n. this allows us to build a prediction which is easier to build understand and interpret . n n the prediction is represented as a set of conjunctive forms which are correct with a high degree of confidence this set of conjunctives may be considered as a distinctive conjunctive form of a logical formula . the criterion of acceptance rejection will be described in section . consider all possible expressions of the form f ym n x x in the eq . each term is a match of x antecedent component at a given point n with the value of the x at the point we want to make a prediction n index m belongs to a given set of indexes we have logical and in between all these terms i.e. the formula represents a fact of simultaneous matches of several antecedent components with their values at the prediction point n. each formula of type is completely defined by a set . in total there are m possible sets . art.tex p. a massive local rules search approach to the classification problem the goal is to find the conjunctive of form which can give an implication with high degree of confidence . x x g g ym formula represents an implication rule when a simultaneous match of given antecedent components with their values at a point to predict n gives specific values of consequent . the value g is the value that the rule predicts . note that the rule of form is defined on a subset of all available observations . we do not consider the rules which can not be applied at n. this drastically reduces the number of rules we may accept . in the next section we discuss statistical criteria used for the evaluation of each rule quality and for resolving the problem of con\ufb02icting rules . local prediction rules statistical evaluation of quality and con\ufb02icts resolution quality evaluation of a rule is based upon its statistical characteristics . in this paper we use canonical statistics statistics which can be expressed via components of matrix of joint distribution p p p p here g is the consequent and f is a logical formula for example one from eq . the is x matrix . probability p can be defined in a number of di\ufb00erent ways . in this paper the probability is defined in a standard combinatoric way . almost any of the commonly used type of criteria can be expressed via the components of a matrix . there are many di\ufb00erent statistics which can be used for quality evaluation of a logical formula . in the work an approach of logical formulas transformation was developed which may solve an exponential complexity combinatoric problem in polynomial time . a similar approach was developed in where only statistics allowing formula transformations increasing quality criterion were used . art.tex p. vladislav g. malyshkin et al a statistic commonly used as a quality criterion is information gain . the information gain based criterion was used in a number of machine learning studies . this criterion usually works well for the evaluation of global rules but much less e\ufb00ectively for local rules . in the case of local rules the major problem with information gain criterion is the fact that f g and f g are equally important for this criterion . for the rules of type we know in advance that f true and this asymmetry should be included into the quality criterion . information gain criterion work well in the case of a seldom event . for example for an event which happens in out of cases a rule which predicts that the event will never happen has . correctness . at the same time an information gain based criterion gives no value to such a rule because we get no extra information beyond what we already know . p g g the most widely used statistics for estimation of a logical formula quality are ones of type the coverage is defined as p and the correctness is defined as p p. in a criterion based on high correctness has been used . a criterion based on the f measure from information retrieval theory can also be used as a quality criterion . an important characteristic of the f measure is the presence of a parameter allowing the adjustment of relative importance of coverage and correctness . in this paper we use a quality criterion which has properties similar to one of type . the quality \u03b1 of implication rule is defined as following \u03b1 \u03bb p f true g g p p f true g g p g p is maximal in this paper we focus on predicting the events not the probabilities so for a given f we first select the value of g which gives maximum of p eq . then evaluate the quality of implication rule using quality criterion . the value of \u03b1 is equal to for implications giving totally correct predictions for every observation . for implications with non perfect correctness and or coverage the value of \u03b1 is lower than . the parameter \u03bb determines the relative importance of coverage and correctness . the value \u03bb . makes coverage and correctness equally important art.tex p. a massive local rules search approach to the classification problem characteristics of a rule . the values \u03bb . make correctness more important than coverage . while di\ufb00erent statistics give very similar results on data which does not produce con\ufb02icting rules the di\ufb00erence between di\ufb00erent statistics may become significant when analyzing data producing con\ufb02icting rules . our experiments with di\ufb00erent types of data have shown that quality criterion works well for the di\ufb00erent data that we tested . to resolve a problem of con\ufb02icting rules we separate the process of making a prediction on two steps . on the first step we do not predict the specific value of g we just find all implication rules of high quality . on the second step we use all found implication rules to obtain a prediction . let us assume we found all rules of high enough quality for example with a quality better than a given acceptance level \u03b1 . each rule predicts its own g at n. if we have no con\ufb02icting rules everything is very simple this value is the value we predict at n. if we have con\ufb02icting rules the situation is more complicated and a con\ufb02ict resolution strategy must be developed . this is a special problem which has been considered in a number of publications . most studies focus on resolving con\ufb02icts between hand crafted rather than automatically generated rules . the con\ufb02ict resolution of automatically generated rules has its own specifics . the simplest approach is to accept only one rule . the problem is the fact that it is common to have a number of rules of similar quality and the idea of taking a single rule and leaving a number of rules of similar quality out of consideration often causes a significant bias in data analysis . an approach often used to resolve such con\ufb02icts is the idea of ordering rules but it gives away an extremely useful property of rules based predictions the ability to evaluate rules in arbitrary order . the approach we use in this paper di\ufb00ers from the ones mentioned above in a very significant way . we assume that all accepted rules must be incorporated into the prediction formula . if we do not have con\ufb02icting rules prediction quality usually increases by combining all rules . if we do have con\ufb02icting rules prediction quality may decrease when the rules are combined . for resolving the problem of con\ufb02icting rules consider the following problem let s be a set of observations on which the value of f from is true . the p is the probability of an observation to give true value of f and p is the probability of an observation to have g equal to g under the condition that the observation belongs . art.tex p. vladislav g. malyshkin et al. s or p g g to s. note that these two probabilities are just equal to p and p respectively but for con\ufb02ict resolution it is much more convenient to work with a set of observations than with individual rules . the problem of resolving con\ufb02icting rules is equivalent to the following for a number of sets sq q. q determines probabilities of di\ufb00erent outcomes of g under the condition that all sq are true . for a single rule the answer is trivial this is either p depending on whether we accepted or rejected a rule . for more than one rule a formal answer can be also written this is either p or p depending on whether we accepted the rules or not . the problem is that the probability p can not even be estimated because the set s s. sq often has few observations insufficient for probability calculation . there is an example of this assume we have observations of g and observations of x m. in the point to predict antecedent x. let g take the value of true on observations and f alse on the other . suppose we have two implication rules and both give perfect prediction on these observations . what will be the probability of di\ufb00erent values of g in the point to predict x we have two perfect rules . the first one predicts g f alse and the second one predicts g true . the probability p g g s s can not be calculated because we have no observation with known g when x true and x true simultaneously . to resolve such con\ufb02icts we build a set s from all sq sets and then apply a quality criterion to a single combined rule which is defined on s. this way the problem of con\ufb02icting rules is resolved by introducing a new combined rule and the answer is the same as the one mentioned above for a single rule the probability is either p or p depending on whether we accepted or rejected a combined rule . having only one rule we may use a number of di\ufb00erent criteria to evaluate this combined rule quality for example in addition to criterion we may use \u03c7 criterion or any other criteria . di\ufb00erent criteria usually give similar results in the case of a single rule . it should be stressed here that the quality of combined rule may be lower than . art.tex p. a massive local rules search approach to the classification problem individual rule quality . if this happens this often indicates the presence of rule con\ufb02icts or data overfitting . the only problem left to discuss is how to obtain the set s from individual sets sq. there is no universal way to do this because the sets sq are sensitive to the quality criteria . the simplest way is to choose the set s as a union of all sq s s s. sq returning to a simple example above with two perfect con\ufb02icting rules the set s s s covers all observations and the criterion produces . value which is a very low value . the combined rule must be rejected and unconditional probabilities p should be used for prediction . this is what we intuitively expect in such an extreme case of con\ufb02icting rules . there are several other ways to select the set of observations s. we will not discuss all the variants here . the way to select s in form seems to work the best for the quality criterion . in addition to that the way to select s is well protected against data overfitting because overfitted rules often produce di\ufb00erent values of g which drastically reduce combined rule quality . let us return to the original problem we formulated in the beginning of section . now we can present an algorithm for predicting the value of consequent at n. select acceptance level \u03b1 . initialize set s to an empty set . for every set of antecedent indexes do a build implication and evaluate quality \u03b1 of it . b if \u03b1 \u03b1 add all observation points for which f from eq . is true to the set s. evaluate the quality of a combined rule the rule which is defined on observations from s. this can be done by using the same criterion \u03c7 or any other type of criterion . if the combined s if rejected use p g g rule is accepted use p probability to predict the fact of g taking value g the predicted value g probability . at n. corresponds to the event with maximal . the algorithm described above is of exponential complexity . as we will show in section the art.tex p. vladislav g. malyshkin et al complexity may be significantly reduced in an average case . before we start discussing computational complexity let us discuss the procedure of attributes selection for antecedent and consequent . selection of attributes for analysis in all of the considerations above we always assumed that consequent g and antecedent components x are boolean attributes . there are many cases in which the data contain attributes of other types . in addition to boolean variables in this paper we consider continuous variables and discrete variables . the requirement of ordering is very important for analysis because this allows us to build an e\ufb00ective algorithm of levels selection . the case with non ordered values is much less interesting because in this case for a descrete variable the algorithm described above will use the following boolean attribute whether the value of the attribute is equal to its value at n or not . let us consider a variable rn taking values from some ordered set . we convert rn to a number of boolean attributes which will be used as the components of vector x. this transformation is performed by selecting a grid yl l. l and comparing the value of r with levels yl that gives antecedent components x rn yl . the question is how to select levels yl to use in n implication . the most commonly used approach is to take a single level . people usually do this because an increase in the number of levels increases the number of antecedent components that can drastically increase computational complexity . the most common criterion used for selection of the split level is information gain criterion . in several works this criterion was successfully applied for determination of levels of comparison . we propose a new approach for antecedent attributes selection . the major new characteristics of proposed approach is integration of two usually independent steps into one step so the inference algorithm described in section will perform not only data analysis but will also select levels to compare . we do not limit ourselves to one or two levels that we can compare with we use a number of levels and determine the real levels to use directly during data analysis . art.tex p. a massive local rules search approach to the classification problem one may think about this as automatic selection of levels in singleton mamdani rules in fuzzy logic see . the first step is to take an ordered grid yl l. l which has many di\ufb00erent levels . then we obtain l antecedent components x r yl . the attributes x are not independent . from the fact of ordering of yl follows that if x n is true then x n is also false for p l. n is also true for p l. also if x n is false then x the second step is to find the highest index l for which r yl is false and mark this index as h. then yl with l h h h. may be considered as lower boundaries of r and yl while l h h. l may be considered as upper boundaries of r. these upper and lower boundaries can be considered as fuzzy levels for r. instead of determining specific values for upper lower levels from some ad hoc special procedure we select them during data analysis by using the inference algorithm we described in the previous section . such integration allows us to automatically select the best level for a rule . while it may look like we have increased the number of antecedent components and exponentially increased computational complexity this is not really the case . the di\ufb00erence between standard approach p. when a k valued variable is replaced by k synthetic boolean variables and our approach is that we incorporate the knowledge about the dependence of these k variables into the inference algorithm in section we show that this knowledge can drastically reduce computational complexity in average case . the problem of consequent variable selection is usually more straightforward than that for antecedents . if consequent j is a boolean nothing special should be done about consequent selection and we use j as consequent g. if j is an ordered variable then we take a grid yl l. l yl yl and just run the analysis for every g. additional testing on monotonic increase of the predicted probability of true value of g with increase of l may be performed to test the consistency of the predictor . the algorithm in section can also be applied to g taking more than two values because the quality criterion may be generalized to such g. art.tex p. vladislav g. malyshkin et al. estimation of computational complexity for brute force rules analysis as we showed in section a brute force algorithm is of exponential complexity . however the implication rules we consider are not independent . it is often possible to determine from one rule s characteristics that a set of rules does not have a member of required quality so that set of rules can be taken out of consideration . the requirement of preventing data overfitting also helps because it eliminates rules that are too complex . in addition some optimization techniques can be applied . this way we can often perform brute force analysis for a problem with a significant number of components . let us discuss the properties which allow us to reduce computational complexity . preventing data overfitting . this usually requires taking out of consideration overly complex rules . we do this by considering only rules with less that mmax terms in implication . this immediately reduces the number of rules to consider from m m mmax to c mmax which is still too high . m. c mmax m c m. taking into account dependent antecedent components . in this paper we consider the simplest case upper and lower boundary antecedent variables as we build them in section . antecedent attributes as we build them in section from variable r are not independent . for example the components of lower boundary rn yl with l h h h. have the following property x n x x n x n x l max x the property follows from the fact of ordering of yl the way of h selection which leads to x f alse and the following equation x an equation very similar to can be also written for upper boundary set rn yl with l h h. l. this means that only one attribute from the upper boundary set needs to be included art.tex p. a massive local rules search approach to the classification problem in implication . if we put two components from the upper boundary set of attributes then by applying a type of transformation we can always replace two terms by a single one . this property which is known directly from the antecedent allows us to reduce the number of implications we need to consider . increase in computational complexity when adding one set with nd dependent antecedent components selected as described above in terms of computational complexity is equivalent to adding much fewer independent components . this is why we can integrate selection of fuzzy levels with the inference algorithm without much increase in computational complexity . addition of l levels to test is equivalent to adding about log log independent boolean attributes . if an implication rule of form has a perfect correctness then the quality of this rule can not be improved by adding more elements to set because by adding more conditions we just decrease coverage while correctness can not be further improved . this means we do not need to consider the subsets of rules with close to perfect correctness . as it has been shown in an implication rule with coverage below some level can not produce a rule of the required quality . this requirement can be slightly improved by using minimal probability p for every consequent value . specifically for at least one v we must have p p. if we have no single v for which this condition holds then the implication can not produce a rule of the required quality . for the quality criterion the value of p can be easily obtained p p \u03bb x. an implication rule must not have redundant conditions . an extreme example of redundant condition is a situation when a term x is added to implication twice . this does not n change any property of a rule it just increases the complexity of it . to check for redundancy of a rule with m conjunctions we may compare the rule with m rules obtained by taking out one condition from the original rule see section . . ref . specifically in our case this criterion can be formulated as following having a set with m elements consider art.tex p. vladislav g. malyshkin et al m formulas fm of type each one is obtained by taking out one of m element . if for at least one fm there is no v for which the condition p p mism holds then this rule have redundant conditions and should not be considered . the value of p mism can be obtained from the same formula which was used for p. the only di\ufb00erence is the di\ufb00erent value of threshold \u03b1 . for mismatches the threshold \u03b1 is usually chosen lower that \u03b1 . the five properties presented above allow us to build an algorithm of polynomial complexity . this comes from the fact that we are interested only in rules applicable at n what reduces the number of rules to consider from m to m and from the properties and which limit the maximal tree depth in a typical case . in the worst case the tree depth is limited by the value of mmax from item . the other properties reduce the complexity further . this algorithm which in typical case is of polinomial complexity on n and m can be applied for solving a variety of practical problems . the algorithm can be applied to a brute force analysis for a problem with a significant number of components . a sketch for the algorithm is the following all possible implication rules may be represented as a tree . each node has an antecedent index assigned to it . every node can be mapped to a set . this property means that if node a is an ancestor of node b then fb fa x where fa and fb are formulas of type obtained from a set corresponding to nodes a and b respectively that allows us to implement the algorithm as a recursive tree scanning algorithm and directly incorporate five properties above as indicators for a branch not having a rule of the required quality . we discuss di\ufb00erent applications in section . predictor error estimation an estimation of predictor correctness usually involves building a global rule on training data and then evaluating this rule s quality on testing data . while this testing approach suits well for testing global rules it is not very convenient when considering local rules because for every prediction point we may have di\ufb00erent local rules . it is nice to know the quality of a local rule but this information is not useful for error estimation at the other prediction points . the best way to perform testing in such a case is to test the average performance of the predictor . one may consider a predictor as some art.tex p. a massive local rules search approach to the classification problem kind of global rule and estimate its quality . the quality of such a global rule is equivalent to the predictor average quality . a common problem of errors estimation is the limited number of observations . techniques such as bootstrap and cross validation are commonly used for performing error estimation with a limited number of observations . for local predictors a leave one out type of cross validation is very promising when working with a limited number of observations . this type of testing includes creation of a set with n observations and this data is used for predicting the value at one left point with known value of g. the procedure is repeated n times and average predictor performance is obtained . mentioned in the non stratification problem of testing data is much less an issue in the case of local predictions than in the case of global predictions because the predictor was specifically built to be applicable at the point where it tested . in case we have plenty of data we can estimate predictor average performance without leave one out cross validation . the fact of the local nature of the predictor should be taken into account when performing the tests . assume we have a training set of n observations and testing set of t observations . to determine predictor average performance we predict the value for every observation in a testing set using all the observations from the training set . in total we run predictor t times each time using the same training set with n observations and estimate predictor average performance from these t predictor runs . predictor average correctness c is defined as c pjj xj true f alse t pjk t the probabilities in are calculated in the testing space the value is the number of tests when the value of the consequent which really happened was equal to j and the predicted value was k. one of the problems with and similar types of criteria is its dependence on unconditional probabilities of di\ufb00erent outcomes of g. for example if we have an event which happens in out of cases art.tex p. vladislav g. malyshkin et al then a predictor predicting that the event will never happen has . correctness . this high value of correctness is not a result of predictor quality but of the distribution of g. one may use information gain based criteria but using several criteria simultaneously complicates the analysis . this problem does not arise when the criterion is used for relative comparison of di\ufb00erent predicting techniques on the same data because in this case we have identical distribution of g. results of data analysis the real algorithm has a number of features not presented in the basic algorithm description which we gave in sections and . these are some of them . the acceptance level \u03b1 is dynamically adjusted . first we set an initial acceptance level . then during tree scanning required acceptance level gets automatically increased to \u03ba\u03b1 if we find a rule with quality \u03b1 such us \u03b1 \u03ba\u03b1 i.e. we keep only rules with quality better than \u03ba fraction of the best rule quality all rules with the quality below this value are pruned . the value \u03ba corresponds to the case when only the best rule is accepted . dependent variables are also handled in a slightly more complex way than described because of additional optimization . these and other details which are not described here make the algorithm practical . this algorithm was implemented in the mls program the complete source code of which is available from . the following parameters were used during all trials . the parameter \u03bb in quality criterion was set to . making correctness more important than coverage . the maximal tree depth mmax was set to . the cmin was set to . i.e. we accept only rules with quality better than the quality of a perfectly correct rule covering . of positive samples . the minimal number of mismatches was also determined on a base of minimal coverage the value of c was set to . this threshold stayed the same during tree scanning and was not adjusted as it was for the matches . min for non ordered input variables antecedent components were built as a fact of the exact match of variable value with its value at a point to predict . for ordered input variables antecedent art.tex p. a massive local rules search approach to the classification problem components were built as described in section . for ordered literal variables we used all possible values as initial levels yl l. l. for continuous variables a discretization was performed first to build ordered literal variables then the same attribute selection procedure used for ordered literal variables was applied . the initial levels for continuous attributes may be selected in a number of di\ufb00erent ways for sufficiently big l di\ufb00erent supervised and unsupervised techniques give very similar results for predictor quality . from a computational complexity point of view it is good to have low values of l. the entropy based discretization gives a very good balance of quality and levels numbers . in this work the entropy based discretization from was used for initial selection of levels yl for continuous variables then the procedure described in section was applied . the utility we used is available from . a rich collection of data from uci repository allows a comprehensive data analysis on data from di\ufb00erent domains to be performed . predictor correctness was estimated using fold cross validation with stratification . obtained results were compared with ones produced by widely used program c.r with default settings . in table i we present the comparison of mls with c.r. for comprehensive comparison with the other predicting techniques we refer to where a variety of predicting techniques were tested on the same data from uci repository . the error estimation from these works can be directly compared with ones from table i of this paper which allows our technique to be easily compared with the other predicting techniques . the exceptions mentioned above in algorithm parameter values for some datasets were required to reduce computation time . the higher values of cmin and c the min earlier tree scanning algorithm will reach termination criteria . the first column of table i identifies the data set . the second and third columns contain predictor correctness c for c.r and our program mls respectively . the fourth column contains the total number of observations . the number of antecedent variables is presented in the last column . this value is for estimation of computational complexity . the trials are usually executed much faster in c.r than mls . first because c.r is written in c while our program mls is written in java . second because we need to re run the predictor for every test while c.r does this only once . this slowdown is important only when doing predictor testing because we are especially interested in the tasks when just a few predictions not about the same number as the training set is necessary . third massive search algorithms are generally slower than decision tree divide and conquer type of algorithms . despite running more slowly the proposed algorithm is fast enough to solve practical problems . the monk monk and monk are the problems usually tried first by di\ufb00erent predictor algorithms . from table i it follows that on monk tests mls performs about the same or slightly better than c.r. on chess mls performs noticeably worse that c.r. this is because the values of cmin and c used for this trial e\ufb00ectively reduce maximal tree depth to a value of about . at the same time c.r generates a number of rules with more than conditions . this chess problem is an example of a problem for which massive search approach min art.tex p. a massive local rules search approach to the classification problem is not e\ufb00ective a large number of attributes produce complex rules . a similar e\ufb00ect occurs in the mushroom trial . on crx diabetes hepatitis horse colic labor neg pima tic tactoe and vote mls performs about the same as or better than c.r. these trials also have significant number of attributes but the rules do not have too many conditions and global optimization algorithm easily catches the best rule without any major slowdown in calculations . our tests also show that in some trials di\ufb00erent values of cmin and parameter \u03bb . from eq . may result in better correctness than presented in table i. the higher cmin the higher is the required quality for a rule to be accepted . as shown in section the value of cmin a\ufb00ects computational complexity . the increase of cmin and c decreases computational complexity by reducing the e\ufb00ective min tree scanning depth . we expect that automatic adjustment of parameter \u03bb and required minimal coverage cmin based on available data will make noticeable improvement to mls . in addition to predictor quality on di\ufb00erent datasets another thing we are interested in testing is an e\ufb00ect of attribute selection methodology from section to predictor quality . to test this we ran the predictor twice on some datasets the first time all antecedents were selected as a fact of exact match of variable value with its value at a point to predict and the second time all antecedent components were selected as a comparison with upper and lower levels in the way described in section . note that the former selection can be always obtained from the latter one because the condition r a is the same as . this way we tested how the quality of a predictor is a\ufb00ected by the increase of rule expressive power when we go from exact match type of attributes to the type of attributes built in the way described in section . we performed this testing on five datasets with ordered attributes and two datasets with literal non ordered attributes . the results are presented in table ii from these trials it follows that for datasets with ordered attributes the transition from exact match to levels comparison may significantly increase predictor quality . in monk no single rule found for an exact match but increased expressive power of generated rules allows us to generate high quality rules which obey the condition of minimal coverage . at the same time in trials where art.tex p. vladislav g. malyshkin et al table ii . predictor results for exact match and level comparison type of attribute selection for mls . data exact match levels comparison monk monk monk pima diabetes tic tac toe vote . antecedent attributes were obtained as a result of entropy discretization there is no strong e\ufb00ect of automatic selection of upper lower boundaries . because of computational complexity for ionosphere and spambase we did only exact match type of attribute selection trials but preliminary results show that exact match type of attributes may produce even better results than levels comparison . for the datasets with non ordered attributes such transition may either not a\ufb00ect or even decrease predictor quality . this is because increased expressive power of the rules may cause an e\ufb00ect similar to data overfitting . the most clear example is tic tac toe trial where the global optimization algorithm finds many false rules a combination of conditions which by chance happened to give a high value of quality criterion . such false rules can be significantly reduced by increasing the value of minimal coverage . from these trials it follows that the approach to antecedent attributes selection from section may give better results only for ordered attributes and even in this case an exact match of attributes may produce better results in some instances . presented test trials show that the massive search algorithm often performs about the same or better than c.r on many datasets . we attribute this to global optimization . there are also cases when mls is less e\ufb00ective than decision tree divide and conquer type of algorithms . this usually happens on the datasets with a large number of attributes producing complex rules . art.tex p. a massive local rules search approach to the classification problem . discussion the described approach proves that a massive local rules search global optimization algorithm can be applied to problems with a significant number of attributes . the computational complexity can be greatly reduced by building rules which are specific to a prediction point and by using the optimization technique described above . the massive search algorithm is guaranteed to find the global maximum which makes it especially valuable for testing various predicting systems . in this work we have shown that the process of attributes selection can be integrated with the process of rules search . this allows us to perform data analysis in a uniform way without separation of the attribute selection and the rules search stages . from a fuzzy logic approach this may be considered as automatic selection of levels in singleton mamdani rules . such a method of attribute selection usually allows us to build more expressive rules . this is related to the fact that in many problems the comparison of the value with a level is a natural method of attribute selection for the problem . another distinctive feature of the proposed algorithm is a con\ufb02icting rules resolution strategy . we accept a number of rules then build a single rule for prediction based on accepted rules . the quality of this single rule may significantly decrease if accepted rules predict di\ufb00erent values of consequence . while the described approach is already practical and was applied in the solution of a number of di\ufb00erent problems it can be further improved . from our point of view there are two improvements which would improve the algorithm . firstly the quality criterion is different than commonly used criteria . the major advantage of the criterion is the fact that its calculation can be optimized . the problem of probability calculation of a logical expression is a problem actively studied from a computational complexity point of view see and references therein . because calculation of is equivalent to calculation of a probability various optimizations used in reliability theory can be applied . another improvement which can be added to the algorithm is automatic selection of minimal coverage cmin and relative importance of coverage and correctness \u03bb . a \ufb02exible selection of these parameters often improves the results . these improvements in our opinion can further increase the correctness and decrease the computational complexity of the algorithm . art.tex p. vladislav g. malyshkin et al acknowledgements vladislav malyshkin greatly appreciates columbus advisors llc s support for this study especially the support from emilio j. lamar during vladislav s employment with columbus advisors llc . the authors would also like to thank alexander rybalov for many fruitful discussions ."
    ],
    "abstract": [
        "an approach to the classification problem of machine learning based on building local classification rules is developed . the local rules are considered as projections of the global classification rules to the event we want to classify . a massive global optimization algorithm is used for optimization of quality criterion . the algorithm which has polynomial complexity in typical case is used to find all high quality local rules . the other distinctive feature of the algorithm is the integration of attributes levels selection with rules searching and original conflicting rules resolution strategy . the algorithm is practical it was tested on a number of data sets from uci repository and a comparison with the other predicting techniques is presented ."
    ],
    "extracted": [
        0
    ],
    "score": [
        0.7851239669421488
    ]
}