the multiplicative newton like method developed by the author et al. is extended to the situation where the dynamics is restricted to the orthogonal group. a general framework is constructed without specifying the cost function. though the restriction to the orthogonal groups makes the problem somewhat complicated an explicit expression for the amount of individual jumps is obtained. this algorithm is exactly second order convergent. the global instability inherent in the newton method is remedied by a levenberg marquardt type variation. the method thus constructed can readily be applied to the independent component analysis. its remarkable performance is illustrated by a numerical simulation.
#####
 overview many optimization problems take the form find an optimal matrix under the constraints .. etc. some of these can be considered as optimizations on lie groups. for groups the .. in considerafundamental manipulation is a multiplication whereas an addition is unnatural. tion of this fact we have constructed a multiplicative newton like algorithm for maximizing the akuzawa brain.riken.go.jp kurtosis in. there the dynamics takes place on the coset gln gl. we can apply the techniques developed in to many other optimization problems. the coset structure gln gl is however characteristic of the independent component analysis. it is understood by the fact that the independence is nothing to do with the scaling. the redundancy resulting from the invariance of the model under the componentwise scaling must be eliminated for a rigorous discussion and this redundancy corresponds to gln. another way to eliminate this redundancy is the prewhitening. the prewhitening is a linear transformation of the observed data which maps the covariance matrix to the unit matrix. if we deal with prewhitened data we can legitimately narrow the sweeping range to the orthogonal group. the aim of this letter is the construction of a multiplicative algorithm for the orthogonal groups. the framework is as follows. n dimensional prewhitened random variables are available and it is anticipated that their origins are some unknown mutually independent components. the goal of the ica is the map. we restrict ourselves to the linear independent component analysis. there we want to ﬁnd a linear transformation y c x which minimizes some cost function c x that measures the independence. since we are assuming that the data is already prewhitened the covariance matrix of x is the n n unit matrix. if we do not take into account errors in the prewhitening the optimal point c must belong to o. giving up the analytical solution we consider a sequence c c c c which converges to the optimal solution c. the sequence is generated by the leftmultiplication of another sequence of orthogonal matrices. each d is speciﬁed by the coordinate which satisﬁes d e. we assume that is an n n skew symmetric matrix which implies that d belongs to the identity component of o. in practice the procedure is as follows. as an initial condition we set c. for t we introduce and denote c as c e c. under these settings we determine by using the newton method with respect to the matrix elements of. that is we evaluate the cost function at c by expanding it around c in terms of the elements of up to the second order. then is choosen as the critical point of this second order expansion. we iteratively follow these procedures until we obtain a satisfactory solution. this letter is organized as follows. in section we will give a complete description of a new multiplicative updating method for the orthogonal groups. this section is the main part of this letter. since our formulation does not depend on the details of the cost function the method can be useful for many problems other than the ica. the performance of our method including the second order convergence is discussed in section. section is a survey of possible applications of our method. the algorithm constructed in section is considered as a pure newton method on the orthogonal groups. to achive the global convergence we must modify the method. this is accomplished in section. section also includes a numerical examination of the performance of our method. section is a summary. multiplicative updating on o we assume that the cost function f takes the form f e n i x where each fi r r is an unspeciﬁed function. through this letter we denote by e the expectation. we will determine the concrete procedures after the newton manner. first we introduce maps r and s from n dimensional dataset to n n matrices by and ki e fi yi yk kl uikl e fi y i ykyl. the goal is the construction of a sequence of the estimates of the independent components which converges to the optimal point y. within the framework of the linear analysis we consider that this sequence is derived from another sequence of the linear transformation by the relation y cx where x are the original data. thus if we restate the problem the task is to determine a sequence. we assume that for each t n the estimates of the independent components at time t and and the estimates at time t are related by or equivalently y dy c dc where d is some orthogonal matrix to be ﬁxed. our method is characterized by this leftmultiplicative updating rule. as mentioned in the previous section we assume that each d always belongs to the identity component of the orthogonal group o. this assumption is reasonable for example if the original data x are already prewhitened in the case of the ica. anyway under this restriction d is speciﬁed by an n n anti symmetric matrix which satisﬁes exp d. for brevity s sake we will omit the argument and denote y by z. f is expanded in terms of as f f tr tr r ik iluikl o. xi k l through the letter we denote by o polynomials of matrix elements of which does not contain terms with degrees less than k. do not confuse this with the symbol for the orthogonal groups such as o. as in the usual newton method we truncate the expansion at the second order with respect to. then in this step is determined as the coordinate of the critical point of this truncated expansion. the partial derivative of is more convenient for the purpose. it reads f kl rlk lk kpuklp o p x where we have omitted the argument y for r and u. now let us introduce a map cs as in the previous article mat fn an a a a. an n an a cs where mat is n n matrices on some unspeciﬁed ﬁeld f. we denote by the upper subscript the transposition and by the complex conjugate. for the orthogonal groups it is rather simple to move to the framework of the column string as compared to the case of gln gl by neglecting o terms the right hand side of is straightforwardly rewritten as rlk lk kpuklp l p x cs r in in r cs uk t cs mk l n where the symbol stands for the direct sum u u. un un uk n k m t is an n n matrix deﬁned by cs t cs for a mat and in is the n n unit matrix. we denote the tensor product by as usual. the transposition t is also considered as an intertwiner between two equivalent representations the orthogonal group o has less degrees of freedom than the general linear group. the canonical basis of the lie algebra o of o is n anti symmetric matrices. we will introduce some operators which enable us to move to the coordinates based on the canonical basis on o. in the ﬁrst place we introduce an n n matrix h by t t b a. h h i j x where h is a π rotation between the j n th component and the i n th component h kl the projection operator pd for k j n for k j n for k i n for k i n l j m l i m l j m l i m otherwise. pd diag pk pk for k n i i n otherwise is used to extract the diagonal elements of a matrix from its image by cs. then the coordinate transformation is realized by a multiplication of to column string vectors. we need to introduce two more projection operators ps and pa deﬁned by ps diag pa diag h pd where if otherwise. pk j i and k i n by the left action of ps and pa to column string vectors rotated by h pd we can extract respectively the symmetric components and the anti symmetric components of the matrices. then the conditions for the critical point of the second order expansion which must be satisﬁed by are translated into the following two conditions. first symmetric components of must vanish. this condition is expressed as j n for i j pscs. secondly for the anti symmetric components the condition for the critical point is transformed to j n for i j where we have set w r in in r uk t. the conditions and are combined into an equation mk pacs paw pa ps cs. note that pa pah. the optimal is immediately obtained from cs paw pa ps pacs h pahw h pa ps pahcs. thus we have obtained the explicit updating rule. by iterating the procedure in this section from a starting point suﬃciently close to the optimal one the sequences and converge to the optimal solutions. performance the second order convergence is one of the main advantages of this method. indeed this algorithm is rigorously second order convergent. the proof can be given almost in the same way as in. so we omit the proof in this letter. sometimes we have to deal with large matrices to apply the technique here constructed. let us examine the situation. the n n matrix pahw h pa ps is a direct sum of an n n matrix and an n n unit matrix. within the n n block the number of non zero oﬀ diagonal elements is no more than n. so this is a very sparse matrix when n becomes large. of course if n becomes extremely large our method requires quite large memories. but due to the sparseness it remains to be a practical tool for problems with considerably large n. nz figure n. the black dots denote non zero elements of pahw h pa ps. as is often the case with the newton method the global convergence is not assured by this algorithm. fortunately it is possible to cure this fault. we will show the prescription to the global instability in section. applications to ica so far we have not speciﬁed the cost function beyond the assumption that the cost function is a sum of the form. many of the cost functions for the independent component analysis belong to this class. kullback leibler information the kullback leibler information n i x n i x n z i y dyip ln p ln pi is a good measure for the independence. here p is the joint probability density function of and pi is the probability density function of the i th component. we have already restricted ourselves to the case where the jacobian of the transformation equals one. then the minimization of the kullback leibler information is equivalent to the minimization of dyip ln pi e. n i x z y i thus we can legitimately transform the kullback leibler information to a cost function of the form where we should set s as fi ln pi. we must evaluate s their derivatives and so on to determine the optimal solution. a robust estimation of these quantities is possibly not an easy task. cumulant of fourth order the kurtosis of a random variable a is deﬁned by e. the kurtosis is related to the cumulant of the fourth order κ cum e by κ cum. for prewhitened data the kurtosis equals the cumulant of the fourth order. as is wellknown we can grab independent components in many cases by seeking the maximum of the absolute values of the kurtoses. our method is applicable by setting fi κ for all i. if it is known a priori that all the sources have positive kurtoses we may use the kurtosis itself and set fi κ. for these cost functions r and other quantities needed for determining each step are calculated easily from the observed data. thus applying our method for this cost function is highly practical and reasonable choice. levenberg marquardt type variation and performance in practice the pure newton updating rule has a poor global convergence property. this drawback is remedied by the levenberg marquardt type variation. first we modify as cs h pahw h pa ps λin pahcs. the initial value λ for λ is ﬁxed at some positive value. we also ﬁx a real number α. then the procedure at time t is as follows i calculate by. ii if f is larger than f multiply λ by α and go back to i. iii otherwise multiply λ by α and proceed to the next time step t. other parts of the algorithm is completely the same as in the pure newton version in section. let us examine the real performance of our method under this setting. for the cost function we choose the kurtosis as in subsection .. the source signals are three synthesizer generated wav ﬁles. pseudo observed data are generated by mixing the source by a random matrix a i s where each element of s is distributed uniformly on. the residual crosstalk of the signals demixed by our method is. on average. it takes about seconds for one hundred iteration of the same problem on our workstation. for reference we have also solved the same demixing problem by the fastica. in this case the residual crosstalk is. on average and it takes about seconds for one hundred iteration on the same workstation. since the author s knowledge about the fastica package is limited one should not take this result seriously. it can however be said that our method is quite good also in practice. samisen. bell. drum. x x x figure sample data generated by a synthesizer. summary we have constructed a new algorithm for ﬁnding a critical point of broad classes of cost functions on the orthogonal groups. this method is second order convergent since it is in essence the newton method. the method here constructed is an extension of the multiplicative updating method developed in our previous work. the constraint for from the nature of the orthogonal groups makes the problem a little complicated. we have however obtained a rigorous and explicit updating rule. we have also constructed a levenbergmarquardt type variation which is suitable for practical purpose. the global instability inherent in the newton method is remedied in this version. since our discussion does not depend on the detail of the cost function this method is applicable to many concrete problems. the relatively mild assumption on the form of the cost function however implies that our algorithm is especially suitable for the ica. its practical utility for the ica have been illustrated here by a numerical simulation. to summarize our algorithm has numerous theoretical virtues such as the rigorous second order convergence the explicit and strict formulation and so on. it provides also in practice fast and powerful tools for the ica and many other problems. acknowledgments the author would like to thank noboru murata and shun ichi amari for valuable discussions and comments. references a.hyv arinen. a fast fixed point algorithm for independent compo nent analysis. neural computation. b.w.sliverman. density estimation for statistics and data analysis. london chapman hall. d.cox d. a penalty method for nonparametric estimation of the logarith mic derivative of a density function. ann.inst.statist.math. hurri j. g avert h. s alel a j. hyv arinen a. fastica package for matlab. http www.cis.hut.ﬁ projects ica fastica. t.akuzawa n.murata. multiplicative nonholo nomic newton like algorithm. preprint. w.h.press b.p.flannery s.a.teukolsky w.t.vetterling. nu merical recipes in c. cambridge cambridge u.p.