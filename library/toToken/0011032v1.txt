an approach to clustering is presented that adapts the basic top down induction of decision trees method towards clustering. to this aim it employs the principles of instance based learning. the resulting methodology is implemented in the tic system for first order clustering. the tic system employs the first order logical decision tree representation of the inductive logic programming system tilde. various experiments with tic are presented in both propositional and relational domains.
#####
 introduction decision trees are usually regarded as representing theories for classiﬁcation. the leaves of the tree contain the classes and the branches from the root to a leaf contain suﬃcient conditions for classiﬁcation. a diﬀerent viewpoint is taken in elements of machine learning. according to langley each node of a tree corresponds to a concept or a cluster and the tree as a whole thus represents a kind of taxonomy or a hierarchy. such taxonomies are not only output by decision tree algorithms but typically also by clustering algorithms such as e.g. cobweb. therefore langley views both clustering and concept learning as instantiations of the same general technique the induction of concept hierarchies. the similarity between classiﬁcation trees and clustering trees has also been noted by fisher who points to the possibility of using tdidt the authors are listed in alphabetical order. in the clustering context and mentions a few clustering systems that work in a tdidt like fashion. following these views we study top down induction of clustering trees. a clustering tree is a decision tree where the leaves do not contain classes and where each node as well as each leaf corresponds to a cluster. to induce clustering trees we employ principles from instance based learning and decision tree induction. more speciﬁcally we assume that a distance measure is given that computes the distance between two examples. furthermore in order to compute the distance between two clusters we employ a function that computes a prototype of a set examples. a prototype is then regarded as an example which allows to deﬁne the distance between two clusters as the distance between their prototypes. given a distance measure for clusters and the view that each node of a tree corresponds to a cluster the decision tree algorithm is then adapted to select in each node the test that will maximize the distance between the resulting clusters in its subnodes. depending on the examples and the distance measure employed one can distinguish two modes. in supervised learning the distance measure only takes into account the class information of each example. also regression trees should be considered supervised learning. in unsupervised learning the examples may not be classiﬁed and the distance measure does not take into account any class information. rather all attributes or features of the examples are taken into account in the distance measure. the top down induction of clustering trees approach is implemented in the tic system. tic is a ﬁrst order clustering system as it does not employ the classical attribute value representation but that of ﬁrst order logical decision trees as in srt and tilde. so the clusters corresponding to the tree will have ﬁrst order deﬁnitions. on the other hand in the current implementation of tic we only employ propositional distance measures. using tic we report on a number of experiments. these experiments demonstrate the power of top down induction of clustering trees. more speciﬁcally we show that tic can be used for clustering for regression and for learning classiﬁers. this paper signiﬁcantly expands on an earlier extended abstract in that tic now contains a pruning method and also that this paper provides new experimental evidence. this paper is structured as follows. in section we discuss the representation of the data and the induced theories. section identiﬁes possible applications of clustering. the tic system is presented in section. in section we empirically evaluate tic for the proposed applications. section presents conclusions and related work. the learning problem. representing examples we employ the learning from interpretations setting for inductive logic programming. for the purposes of this paper it is suﬃcient to regard each example as a small relational database i.e. as a set of facts. within learning from interpretations one may also specify background knowledge in the form of a prolog program which can be used to derive additional features of the examples. see for more details on learning from interpretations. for instance examples for the well known mutagenesis problem can be described by interpretations. here an interpretation is simply an enumeration of all the facts we lumo know about one single molecule and logp values the atoms and bonds occurring in it certain high level structures. we can rep. first order logical decision trees first order logical decision trees are similar to standard decision trees except that the test in each node is a conjunction of literals instead of an test on an attribute. they are always binary as the test can only succeed or fail. a detailed discussion of these trees is beyond the scope of this paper but can be found in. we will use these trees to represent clustering trees. an example of a clustering tree in the mutagenesis context is shown in figure. note that in a classical logical decision tree leaves would contain classes. here leaves simply contain sets of examples that belong together. also note that variables occurring in tests are existentially quantiﬁed. the root test for instance tests whether there occurs an atom of type in the molecule. the whole set of examples is thus divided into two clusters a cluster of molecules containing an atom and a cluster of molecules not containing any. this view is in correspondence with langley s viewpoint that a test in a node is not just a decision criterion but also a description of the subclusters formed in this node. in we show how a logical decision tree can be transformed into an equivalent logic program which could alternatively be used to sort examples into clusters. the logic program contains invented predicates that correspond to the clusters. instance based learning and distances the purpose of conceptual clustering is to obtain clusters such that intra cluster distance is as small as possible and the inter cluster distance is as large as possible. in this paper we assume that a distance measure d that computes the distance d between examples e and e is given. furthermore there is also a need for measuring the distance between diﬀerent clusters. therefore we will assume as well the existence of a prototype function p that computes the prototype p of a set of examples e. the distance between two clusters c and c is then deﬁned as the distance d between the prototypes of the clusters. this shows that the prototypes should be considered as partial example descriptions. the prototypes should be sufﬁciently detailed as to allow the computation of the distances. for instance the distance could be the euclidean distance d between the values of one or more numerical attributes or it could be the distance d as measured by a ﬁrst order distance measure such as used in ribl or kbg or. given the distance at the level of the examples the principles of instance based learning can be used to compute the prototypes. e.g. d would result in a prototype function p that would simply compute the mean for the cluster whereas d could result in function p that would compute the least general generalisation of the examples in the cluster. throughout this paper we employ only propositional distance measures and the prototype functions that correspond to the instance averaging methods along the lines of. however we stress that in principle we could use any distance measure. notice that although we employ only propositional distance measures we obtain ﬁrst order descriptions of the clusters through the representation of ﬁrst order logical decision trees. problem specification by now we are able to formally specify the clustering problem given a set of examples e a background theory b in the form of a prolog program a distance measure d that computes the distance between two examples or prototypes a prototype function p that computes the proto type of a set of examples find a ﬁrst order clustering tree. before discussing how this problem can be solved we take a look at possible applications of clustering trees. applications of clustering trees following langley s viewpoint a system such as c. can be considered a supervised clustering system where the distance metric is the class entropy within lower class entropy within a cluster the clusters means that the examples in that cluster are more similar with respect to their classes. since c. employs class information it is a supervised learner. clustering can also be done in an unsupervised manner however. when making use of a distance metric to form clusters this distance metric may or may not use information about the classes of the examples. even if it does not use class information clusters may be coherent with respect to the class of the examples in them. this principle leads to a classiﬁcation technique that is very robust with respect to missing class information. indeed even if only a small percentage of the examples is labelled with a class one could perform unsupervised clustering and assign to each leaf in the if concept hierarchy the majority class in that leaf. the leaves are coherent with respect to classes this method would yield relatively high classiﬁcation accuracy with a minimum of class information available. this is quite similar in spirit to emde s method for learning from few classiﬁed examples implemented in the cola system. a similar reasoning can be followed for regression leading to unsupervised regression again this may be useful in the case of partially missing information. we conclude that clustering can extend classiﬁcation and regression towards unsupervised learning. an other extension in the predictive context is that clusters can be used to predict many or all attributes of an example at once. depending on the application one has in mind measuring the quality of a clustering tree is done in diﬀerent ways. for classiﬁcation purposes predictive accuracy on unseen cases is typically used. for regression an often used criterion is the relative error which is the mean squared error of predictions divided by the mean squared error of a default hypothesis always predicting the mean. this can be extended towards the clustering context if a distance measure and prototype function are available re n i d p n i d p with ei the examples ˆei the predictions and p the prototype. if clustering is considered as unsupervised learning of classiﬁcation or regression trees the relative error of only the predicted variable or the accuracy with which the class variable can be predicted is a suitable quality criterion. in this case classes should be available for the evaluation of the clustering tree though not during learning. such an evaluation is often done for clusters see e.g. tic top down induction of clustering trees a system for top down induction of clustering trees called tic has been implemented as a subsystem of the ilp system tilde. tic employs the basic tdidt framework as it is also incorporated in the tilde system. the main point where tic and tilde diﬀer from the propositional tdidt algorithm is in the computation of the tests to be placed in a node see for details. furthermore tic diﬀers from tilde in that it uses other heuristics for splitting nodes an alternative stopping criterion and alternative tree post pruning methods. we discuss these topics below. splitting the splitting criterion used in tic works as follows. given a cluster c and a test t that will result in two disjoint subclusters c and c of c tic computes the distance d where p is the prototype function. the best test t is then the one that maximizes this distance. this reﬂects the principle that the inter cluster distance should be as large as possible. if the prototype is simply the mean then maximizing inter cluster distances corresponds to minimizing intra cluster distances and splitting heuristics such as information gain or gini index can be seen as special cases of the above principle as they minimize intra cluster class diversity. in the regression context minimizing intra cluster variance is another instance of this principle. note that our distance based approach has the advantage of being applicable to both numeric and symbolic data and thus generalises over regression and classiﬁcation. stopping criteria stopping criteria are often based on signiﬁcance in the classiﬁcation context a χ test is often tests. used to check whether the class distributions in the subtrees diﬀer signiﬁcantly. since regression and clustering use variance as a heuristic for choosing the best split a reasonable heuristic for the stopping criterion seems to be the f test. if a set of examples is split into two subsets the variance should decrease signiﬁcantly i.e. f ss should be signiﬁcantly large. pruning using a validation set the principle of using a validation set to prune trees is very simple. after using the training set to build a tree the quality of the tree is computed on the validation set. the f test is only theoretically correct for normally distributed populations. since this assumption may not hold it should here be considered a heuristic for deciding when to stop growing a branch not a real statistical test. for each node of the tree the quality of the tree if it were pruned at that node q is compared with the quality q of the unpruned tree. if q q then the tree is pruned. such a strategy has been successfully followed in the context of classiﬁcation and regression as well as clustering. fisher s method is more complex than ours in that for each individual variable a diﬀerent subset of the original tree will be used for prediction. in the current implementation of tilde validation set based pruning is available for all settings. for clustering and regression it is the only pruning criterion that is implemented. it is only reliable for reasonably large data sets though. when learning from small data sets performance decreases because the training set becomes even smaller and with a small validation set a lot of pruning is due to random inﬂuences. experiments. data sets we used the following data sets for our experiments soybeans this database contains descriptions of diseased soybean plants. every plant is described by attributes. a small data set and a large one are available at the uci repository. iris a simple database of descriptions of iris plants available at the uci repository. it contains classes of examples each. there are numerical attributes. mutagenesis this database contains descriptions of molecules for which the mutagenic activity has to be predicted. originally mutagenicity was measured by a real number but in most experiments with ilp systems this has been discretized into two values. the database is available at the ilp repository. molecules the other contain higher level information. for our experiments the tests allowed in the trees can make use of structural information only though for the heuristics numerical information from background can be used. biodegradability a set of molecules of which structural descriptions and molecular weights are given. the biodegradability of the molecules is to be predicted. this is a real number but has been discretized into four values in most past experiments. the dataset was provided to us by s. dˇzeroski but is not yet in the public domain. the data sets were deliberately chosen to include both propositional and relational data sets. for each individual experiment the most suitable data sets were chosen. distances were always computed from all numerical attributes except when stated otherwise. for the soybeans data sets all nominal attributes were converted into numbers ﬁrst. experiment pruning in this ﬁrst experiment we want to evaluate the eﬀect of pruning in tic on both predictive accuracy and tree complexity. we have applied tic to two databases soybeans and mutagenesis. we chose these two because they are relatively large. for both data sets tenfold crossvalidations were performed. in each run the algorithm divides the learning set in a training set and a validation set. clustering trees are built and pruned in an unsupervised manner. the clustering hierarchy before and after pruning is evaluated by predicting the class of each test example. in figure the average accuracy of the clustering hierarchies before and after pruning is plotted against the size of the validation set and the same is done for the tree complexity. the same results for the mutagenesis database are summarised in figure. introduce four levels of background knowledge the ﬁrst contain only structural information s e d o n s e d o n accuracy before and after pruning b number of nodes before and after pruning figure mutagenesis accuracy and size of the clustering trees nodes. the pruning strategy seems relatively stable w.r.t. the size of the validation set. the mutagenesis experiment conﬁrms these ﬁndings. experiment comparison with other learners in this experiment we compare tic with propositional clustering systems and with classiﬁcation and regression systems. a comparison with propositional clustering systems is hard to make because few quantitative results are available in the literature therefore we also compare with supervised learners. we applied tic to the soybean and iris databases performing tenfold crossvalidations. learning is unsupervised but classes are assumed to be known at evaluation time. table compares the results with those obtained with the supervised learner tilde. we see that tic obtains high accuracies for these problems. the only clustering result we know of is for cobweb which obtained on the soybean data set. this diﬀerence is not signiﬁcant. tilde s ac database soybean iris tic tree size acc. nodes nodes tilde acc. tree size nodes nodes table comparison of tic with a supervised learner. curacies don t diﬀer much from those of tic which induced the hierarchy without knowledge of the classes. tree sizes are smaller though. we have also performed an experiment on the biodegradability data set predicting numbers. for this dataset the f test stopping criterion was used but no validation set was used given the small size of the data set. the distance used is the diﬀerence between class values. table compares tic s performance with tilde s and srt s. our conclusions are that a for unsupervised learning tic performs almost as well as other unsupervised or supervised learners if classiﬁcation accuracy is measured and b while there is clearly room for improvement with respect to using tic for regression postdiscretization of the regression predictions shows that this approach is competitive with classical approaches to classiﬁcation. l.o.o. tilde l.o.o. tic l.o.o. tic fold srt fold tic classiﬁcation regression classif. via regression regression regression acc. re. acc. re. re. table comparison of regression and classiﬁcation on the biodegradability data. experiment predicting multiple attributes clustering allows to predict multiple attributes. since examples in a leaf must resemble each other as much as possible attributes must also agree as much as possible. by sorting unseen examples down a cluster tree and comparing all attributes of the example with the prototype attributes we get an idea of how good the tree is. this is an extension of the classical evaluation as each attribute in turn is a class now. we did a tenfold crossvalidation for the following experiment using the training set a clustering tree is induced. then all examples of the test set are sorted in this hierarchy and the prediction for all of their attributes is evaluated. for each attribute the value that occurs most frequently in a leaf is predicted for all test examples sorted in that leaf. we used the large soybean database with pruning. table summarizes the accuracies obtained for each attribute and compares with the accuracy of majority prediction. the high accuracies show that most attributes can be predicted very well which means the clusters are very coherent. the mean accuracy of. does not diﬀer signiﬁcantly from the reported in. experiment handling missing information it can be expected that clustering making use of more attributes than just class attributes is more robust with respect to missing values. we showed in experiment that unsupervised learners can yield trees with predictive accuracies close to those of supervised learners but all class information was still available for assigning classes to leaves after the tree was built. in this experiment we measure the predictive accu name date plant stand precip temp hail crop hist area damaged severity seed tmt germination plant growth leaves leafspots halo leafspots marg leafspots size leaf shread leaf malf leaf mild stem lodging stem cankers canker lesion fruiting bodies external decay mycelium int discolor sclerotia fruit pods fruit spots seed mold growth seed discolor seed size shriveling roots mean range default. acc. table prediction of all attributes together in the soybean data set racy of trees when class information as well as other information may be missing not only for learning but also for assigning classes to leaves afterwards and this for several levels of missing information. our aim is to investigate how predictive accuracy deteriorates with missing information and to compare clustering systems that use only class information with systems that use more information. we have used the mutagenesis data set for this experiment comparing the use of only class information with the use of three numerical variables for computing dis available numerical data logmutag. all three. table classiﬁcation accuracies obtained for mutagenesis with several distance functions and on several levels of missing information. tances. this experiment is similar in spirits to the ones performed with cola. table shows the results. as expected performance degrades less quickly when more information is available which supports the claim that the use of more than just class information can improve performance in the presence of missing information. conclusions and related work we have presented a novel ﬁrst order clustering system tic within the tdidt class of algorithms. tic integrates ideas from concept learning from instance based learning and from inductive logic programming to obtain a clustering system. several experiments were performed that illustrate the type of tasks tic is useful for. as far as related work is concerned our work is related to kbg which also performs ﬁrst order clustering. in contrast to the current version of tic kbg does use a ﬁrst order similarity measure which could also be used within tic. furthermore kbg is an agglomerative clustering algorithm and tic a divisive one. the divisive nature of tic makes tic as eﬃcient as classical tdidt algorithms. a ﬁnal diﬀerence with kbg is that tic directly obtains logical descriptions of the clusters through the use of the logical decision tree format. for kbg these descriptions have to be derived in a separate step because the clustering process only produces the clusters and not their description. instance based the ribl uses an advanced ﬁrst order distance metric that might be a good candidate for incorporation in tic. learner work is closely related to srt who builds regression trees in a supervised manner. tic can be considered a generalization of srt in that tic can also build trees in an unsupervised manner and can predict multiple values. finally we should also refer to a number of other approaches to ﬁrst order clustering which include klus ter and. future work on tic includes extending the system so that it can employ ﬁrst order distance measures and investigating the limitations of this approach. acknowledgements hendrik blockeel is supported by the flemish institute for the promotion of scientiﬁc and technological research in industry. luc de raedt is supported by the fund for scientiﬁc research of flanders. this work is part of the european community esprit project no. inductive logic programming. the authors thank stefan kramer who performed the srt experiments saˇso dˇzeroski who provided the biodegradability database luc dehaspe and kurt driessens for proofreading the paper and the anonymous referees for their very valuable comments.