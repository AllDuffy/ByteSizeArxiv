cross validation is a useful and generally applicable technique often employed in machine learning including decision tree induction. an important disadvantage of straightforward implementation of the technique is its computational overhead. in this paper we show that for decision trees the computational overhead of cross validation can be reduced significantly by integrating the cross validation with the normal decision tree induction process. we discuss how existing decision tree algorithms can be adapted to this aim and provide an analysis of the speedups these adaptations may yield. the analysis is supported by experimental results.
#####
 introduction cross validation is a generally applicable and very useful technique for many tasks often encountered in machine learning such as accuracy estimation feature selection or parameter tuning. it consists of partitioning a data set d into n subsets di and then running a given algorithm n times each time using a diﬀerent training set d di and validating the results on di. cross validation is used within a wide range of machine learning approaches such as instance based learning artiﬁcial neural networks or decision tree induction. as an example of its use within decision tree induction the cart system employs a tree pruning method that is based on trading oﬀ predictive accuracy versus tree complexity this trade oﬀ is governed by a parameter that is optimized using crossvalidation. while cross validation has many advantages for certain tasks an often mentioned disadvantage is that it is computationally expensive. indeed n fold crossvalidation is typically implemented by running the same learning system n times each time on a diﬀerent training set of size n times the size of the original data set. because of this computational cost cross validation is sometimes avoided even when it is agreed that the method would be useful. it is clear however that when a speciﬁc decision tree induction algorithm is run several times on highly similar datasets there will be redundancy in the computations. e.g. when selecting the best test in a node of a tree the test needs to be evaluated against each individual example in the training set. in an nfold cross validation each example occurs n times as a training example which means that each test will be evaluated on each training example n times. the question naturally arises whether it would be possible to avoid such redundant computations thereby speedin this text we ing up the cross validation process. provide an aﬃrmative answer to this question. this paper is organised as follows. in section we focus on reﬁnement of a single node of the tree we identify the computations that are prone to the kind of redundancy mentioned above indicate how this redundancy can be reduced and analyse to what extent performance can thus be improved. in section we discuss the whole tree induction process showing how our adapted node reﬁnement algorithm ﬁts in several tree induction algorithms. in section we present experimental results for one of these algorithms that support our complexity analysis supporting our main claim that cross validation can be integrated with decision tree induction in such a way that it causes only a small overhead. in section we brieﬂy discuss to what extent the results generalize to other machine learning techniques and mention the limitations of our approach. in section we conclude. eﬃcient cross validation. decision tree induction we describe decision tree induction algorithms only in such detail as needed for the remainder of this text for more details see quinlan or breiman et al. function grow tree returns decision tree t optimal test p partition induced on t by t if stop criterion then return leaf else for all pj in p trj grow tree return node figure. a generic algorithm for top down induction of decision trees. decision trees are usually built top down using an algorithm similar to the one shown in figure. basically given a data set a node is created and a test is selected for that node. a test is a function from the example space to some ﬁnite domain. each test induces a partition of the data set and typically that test is selected for which the subsets of the partition are maximally homogeneous w.r.t. some target attribute. for each subset of the partition the procedure is repeated and the created nodes become children of the current node. the procedure stops when stop criterion succeeds this is typically the case when no good test can be found or when the data set is suﬃciently homogeneous already. in that case the subset becomes a leaf of the tree and in this leaf information about the subset is stored. the result of the initial call of the algorithm is the full decision tree. the reﬁnement of a single node can in more detail be described as follows for all tests t that can be put in the node for all examples e in the training set t update statistics q compute quality t argmaxt q partition t according to t the computation of the quality of a test t is split into two phases here one phase where statistics on t are computed and stored into an array s and a second phase where the quality is computed from the statistics. for instance for classiﬁcation trees phase one could compute the class distribution for each outcome of the test. quality criteria such as information gain or gain ratio can easily be computed from this in phase two. for regression using variance as a quality criterion a similar two phase process can be deﬁned the variance can be computed from p where the yi are the target values. removing redundancy .. overlapping data sets now assume that the node reﬁnement process as described above is repeated several times each time on a slightly diﬀerent data set ti. we assume here that the same set of tests is considered in all these nodes. then instead of running the process n times with n the number of data sets the following algorithm can be used for each test t that can be put in the node for each example e in si ti for each i such that e ti update statistics for each ti q compute quality for each ti t i argmaxt q for each diﬀerent test t among the t i partition si according to t this algorithm performs the same computations as running the original one once on each data set except for two diﬀerences for each test t each single example e is tested only once instead of m times where m is the number of data sets the example occurs in. each single example e is sorted into a child node f times instead of m times with f the number of diﬀerent best tests for all the data sets where the example occurred. note that in each node of the tree multiple tests and correspondingly multiple sets of child nodes may now be stored instead of just one. s is then a matrix indexed on classes and results of t and update statistics just increments st class by. sorting examples into child nodes corresponds to par titioning the data set. .. cross validation for an n fold cross validation each single example occurs exactly n times as a training example. hence the time needed to compute the statistics of all tests is reduced by a factor n compared to running the original algorithm n times. the time needed to sort examples into child nodes is reduced by n if the same test is selected in all folds otherwise a smaller reduction occurs. besides this speedup there are no changes in the computational complexity of the algorithm. speciﬁcally for cross validation the algorithm can be further improved if the employed statistics s for any data set d can be computed from the corresponding statistics of its subsets in a partition. this holds for all statistics that are essentially sums since in that case s pi s. such statistics could also be called additive. in an n fold cross validation the data set d is partitioned into n sets di and the training sets ti can be deﬁned as d di. it is then suﬃcient to compute statistics just for the di those for the ti can be easily computed from this without further reference to the data. since each example occurs in exactly of the di updating statistics has to be done only n times instead of n times. .. cross validation combined with actual tree induction in practice cross validation is usually performed in addition to building a tree from the whole data set this tree is then considered to be the actual hypothesis proposed by the algorithm and the cross validation is done just to estimate the predictive accuracy of the hypothesis or for parameter tuning. the algorithm for eﬃcient cross validation can easily be extended so that it builds a tree from the whole data set in addition to the cross validation trees. adopting this change we obtain the algorithm in figure. in the remainder of this text we will refer to this algorithm as the parallel algorithm as opposed to the straightforward method of running all cross validation folds and the actual tree induction serially. at this point we have discussed the major issues related to the reﬁnement of a single node. the next step for each test t that can be put in the node choose i such that e di update statistics compute s from all s for each ti for each example e in d. for each diﬀerent test t among the t i. for each ti t i argmaxt q partition si according to t figure. performing cross validation in parallel with induction of the actual tree. is to include this process into a full tree induction algorithm. this will be discussed in the next section but ﬁrst we take a look at the complexity of the node reﬁnement step. computational complexity of node reﬁnement let te be the time for extracting relevant information from a single example and updating the statistics matrix s tp the time needed to test an example and sort it into the correct subset during partitioning n the number of examples in the data set n the number of folds and a the number of tests. then we obtain the following times for reﬁning a single node when building one tree from the full data set tactual an te n tp c n c when performing cross validation serially n n c t fold n tn folds n c when serially building the actual tree and per forming a cross validation tserial tactual tn folds nn c when using the parallel algorithm worst case tparallel an te nn tp c n c when using the parallel algorithm best case t parallel n c our analysis gives rise to approximate upper bounds on the speedup factors that can be achieved. assuming large n so that the ci terms can be ignored for the worst case we get tserial tparallel n ate tp ate ntp n and tserial tparallel ate tp ate n tp ate tp tp a te tp hence the worst case speedup factor is bounded by min. it will approximate n when a n becomes large and b tp is small compared to ate. in the best case where the same test is selected for all folds we just get tserial t parallel n the speedup factor approaches n as soon as n becomes large. another way to look at this is to observe that t parallel tactual approaches one in other words for large n and a stable problem the overhead caused by performing cross validation becomes negligible. an algorithm for building trees in parallel we now describe how the above algorithms for node reﬁnement ﬁt in decision tree induction algorithms. first we describe the data structures which are more complicated than when growing individual trees. next we discuss several decision tree induction techniques and show how they can exploit the above algorithms. data structures since the parallel cross validation algorithm builds multiple trees at the same time we need a data structure to store all these trees together. we refer to this structure as a forest although this might be somewhat misleading as the trees are not disjoint but may share some parts. an example of a forest is shown in figure. in this ﬁgure two kinds of internal nodes are represented. the small squares represent bifurcation points points where the trees of diﬀerent folds start to diﬀer because diﬀerent tests were selected. the larger rectangles represent tests that partition the relevant data set. the way in which the trees in the forest split the data sets is illustrated by means of an example data set of elements on which a three fold cross validation is performed. figure. an example forest for a fold cross validation. note that the memory consumption of a forest is at most n times that of a single tree which in practice is not problematic since n usually is small. when in the following we refer to nodes in the forest we always refer to the test nodes making abstraction of bifurcation points. e.g. in figure the root node has ﬁve children three of which are leaves. tree induction algorithms .. depth first tree induction probably the best known approach to decision tree induction is quinlan s id algorithm later developed into c. id basically follows the depthﬁrst approach of figure. the simplest way to adapt an id like algorithm to perform cross validation in parallel with the actual tree building is to make it use the node reﬁnement algorithm of figure and call the algorithm recursively for each child node created. note that the number of such child nodes is now pf i ri with f the number of diﬀerent tests selected as best test in some fold and ri the number of possible results of the i th test. in this way the above mentioned speedup is obtained as long as the same test is chosen in all crossvalidations and in the actual tree. the more diﬀerent tests are selected the less speedup is achieved and when in each fold a diﬀerent test is selected the speedup factor goes to. to see how this process inﬂuences the total forest in duction time let us deﬁne tr as the average time that is needed to reﬁne all the nodes of a single tree on level i for a data set of size d and f as the average number of diﬀerent tests selected on level i of the forest. the computational complexity of the whole forest building process can then be approximated as tparallel tr f tr f tr for the parallel version and assuming that reﬁnement time is linear in the number of examples in nodes that are to be reﬁned tserial ntr ntr ntr for the serial version. thus the total speedup will be between and n and will be higher for stable problems than for unstable problems. .. level wise tree induction most decision tree induction algorithms assume that all data reside in main memory. when inducing a tree from a large database this may not be realistic data have to be loaded from disk into main memory when needed and then for eﬃciency reasons it is important to minimize the number of times each example needs to be loaded. to that aim alternative tree induction algorithms have been proposed that build the tree one whole level at a time where for each level one pass through the data is required. the idea is to go over the data and for each example update statistics for all possible tests in the node where the example belongs. for each node the best test is then selected from these statistics without more access to the data. since in these approaches too the computation of the quality of tests is split up into two phases it is easy to see how such level wise algorithms can be adapted. when processing one example instead of looking up the single node in the tree where the example belongs one should look up all the nodes in the forest where the example belongs and update the statistics in all these nodes. when data reside on disk the number of examples is typically large and both te and tp are large. the constant terms ci then become negligible very quickly and the speedup factor can approach n if a n tp. assuming that tp and te te are comparable this will be true as soon as a n which in practice often holds. further optimisations as soon as diﬀerent tests are selected for diﬀerent folds the forest induction process bifurcates in the sense that from that point onwards diﬀerent trees in the forest will be handled independently. a further optimisation that comes to mind is removing redundancy among computations in these independently handled trees as well. referring to figure among the diﬀerent branches created by a bifurcation point there may still be some overlap with respect to the tests that will be considered in the child nodes as well as the relevant examples. for instance in the lower right of the forest in figure in the children of the test b node one needs to consider all tests except a and b and in the children of the test c node one needs to consider all tests except a and c. since the relevant example set for fold f at that point overlaps with that of folds f and f all tests besides a b and c will give rise to some redundant computations. removing this redundancy as well would give rise to a more thorough redesign of the forest induction process it seems that for best results the depth ﬁrst tree induction method should be abandoned and a levelwise method adopted instead. here we will not discuss this optimisation any further but focus on the above described algorithm which is simple and compatible with both tree induction approaches and can easily be integrated in existing tree induction systems. experimental evaluation. implementation we implemented algorithm as a module of tilde an ilp system that induces ﬁrst order decision trees brieﬂy these are decision trees where a test in a node is a ﬁrst order literal or conjunction and a path from root to leaf can be interpreted as a horn clause. literals belonging to diﬀerent nodes in such a path may share variables. a typical property of ilp systems in general and tilde is no exception is that because tests are ﬁrst order conjunctions both the number of tests and the time needed to perform a test may be large. this translates to large a te and tp values in our complex ity analysis which makes it reasonable to expect a speedup factor close to n for reﬁnement of the top node of the tree and close to n f for nodes on level i. experimental setup for these experiments we used the version of tilde as implemented within the ace data mining tool this version is a depth ﬁrst id like algorithm that keeps all data in main memory. with these experiments we aim at a better understanding of the behaviour of the parallel crossvalidation process. we measure how much speedup the parallel procedure yields compared to the serial one how the overhead of the parallel procedure varies with the number of folds and how much time is spent by both procedures on diﬀerent levels of the tree. the parallel and serial procedures make use of exactly the same implementation of tilde except for the differences between parallel and serial execution as described in this text. the diﬀerent procedures are compared pairwise for the following data sets sb and cb called bongard problems. sb contains examples with a simple underlying theory cb examples with a more complex theory. muta the mutagenesis data set an ilp benchmark asm a subset of examples of the so called adaptive systems management data set kindly provided to us by perot systems nederland. mach machines a tiny data set described in the number of tests in each node varied from to a few hundred. results table compares the actual tree building time ta the time for serially performing fold cross validation in addition to the actual tree building ts and the time in addition to needed by the parallel algorithm tp. these the speedup factor s ts tp is shown as as ace is available for academic purposes upon request. ta. sb cb mach asm muta ts. tp. s. op os table. timings of parallel and serial execution on various data sets. figure. ts and tp relative to ta. the part above the horizontal line is the overhead os respectively op. well as the overhead caused by performing the crossvalidation. os and op are plotted graphically in figure. the lowest overhead is achieved for simple bongard which has a relatively large number of examples and a simple theory. the simplicity of the true theory causes the induced trees to be exactly the same in most folds yielding little bifurcation. for complex bongard the eﬀect of bifurcation is more prominent. for asm a real world data set for which a perfect theory may not exist the overhead of cross validation is relatively high. for machines the overhead is relatively large but still smaller than for the serial algorithm i.e. even for small example sets the parallel algorithm yields a speedup. for mutagenesis we obtained less good results. two factors turned out to be responsible for this instability of the trees but also high variance in the complexity of testing examples. the latter is due to the fact that ﬁrst order queries have exponential worst case complexity most of them are reasonably fast but a very few of them may dominate the others time wise. such behaviour typically occurs at lower levels of the tree as will be conﬁrmed when we look at figure. figure shows how cross validation overhead varies with the number of folds for the cb and asm data sets. the result for cb conﬁrms our expectation that n has a small inﬂuence on the total time but for asm the overhead increases with increasing n. figure. overhead in function of number of folds the latter result can be understood by looking at the graphs in figure where the total time spent on each level of the tree by the parallel and the serial procedure is plotted together with the f values as deﬁned previously. the graphs clearly show that when f goes up the per level speedup factor is reduced. for cb this happens at a point where the total reﬁnement time is already small so it does not inﬂuence the overall speedup factor much but for asm and muta f increases almost immediately. note that in the part where f is high many folds are handled independently and cross validation becomes linear in n which explains the increase of the asm data in figure. it is also clear in figure how the time spent on some lower levels suddenly goes up this is the eﬀect of stumbling upon some very complex tests. applicability and limitations although we have studied eﬃcient cross validation in the context of decision trees the principles explained here are also applicable outside this domain. for instance rule set induction systems typically build a rule by consecutively adding a best condition to it until no further improvement occurs. similar to our forestbuilding algorithm cross validation of such rules could be performed in parallel with the construction of the actual rule set avoiding redundant computations. it is less clear however how the technique could be used with models that contain only continuous parameters such as neural networks. we obtain the greatest speedups for stable trees where the same test is chosen in diﬀerent folds. with continuous models no compu figure. total reﬁnement time per level. tations will ever be exactly the same hence removal of exactly redundant computations as explained here will in general not be possible. also within decision tree induction a number of limitations exist. a ﬁrst one is related to the use of continuous parameters in the tree. decision tree induction systems often construct inequality tests for continuous attributes where the constant is generated from the currently relevant data. even for stable problems where the same test is usually selected for diﬀerent folds there may be small diﬀerences in the constants that make the tests look diﬀerent. solving this problem requires extra optimisations. a second limitation is that the proposed techniques concern the tree building phase only. this phase is typically followed by tree post pruning and may be preceded by data pre processing such as discretization of attributes. while these other phases usually take much less time than the tree building phase when they are not negligible and n is large they may become the bottleneck limiting the usefulness of our approach. conclusions we have shown that in the context of decision tree induction the beneﬁts of cross validation are available for a relatively low overhead if the cross validation is carefully integrated with the normal tree building process. comparing experimental results with an analytical estimate of this overhead we have identiﬁed a number of disturbing factors such as variance in test complexity and tree instability. these factors increase the overhead but in all cases it was still smaller than for the serial crossvalidation procedure and in the best cases there was only a small overhead over the normal tree induction process. the ideas underlying our approach are also applicable outside the decision tree context e.g. for rule induction but not immediately for induction of models that have only continuous parameters. possible further improvements to the technique include speciﬁc adaptations for handling tests with continuous values. also the algorithms we have discussed are fairly simple versions the sprint system for instance is much more sophisticated with respect to the statistics it keeps and adaptations to the system along the lines of this paper would be correspondingly complex to implement. related work includes that of moore and lee who discuss eﬃcient cross validation in the context of model selection. their approach diﬀers substantially from ours in that they obtain eﬃciency by quickly abandoning models that after seeing some examples have low probability of ever becoming the best model i.e. they save on the number of cases a model is evaluated on during cross validation whereas our work focuses on removing redundancy in the model building process itself. blockeel et al. discuss a technique similar to the one described here. the main diﬀerence is is in the kind of redundancies that are removed here the redundancies arise from running the same test in different folds of a cross validation whereas in blockeel et al. they are caused by similarities in diﬀerent tests. both approaches can easily be combined and such work is in progress. acknowledgements the authors are a post doctoral fellow respectively research assistant of the fund for scientiﬁc research of flanders. they thank perot systems nederland syllogic for providing the asm data. the cooperation between perot systems nederland and the authors was supported by the european union s esprit project .