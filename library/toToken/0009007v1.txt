in real world environments it usually is difficult to specify target operating conditions precisely for example target misclassification costs. this uncertainty makes building robust classification systems problematic. we show that it is possible to build a hybrid classifier that will perform at least as well as the best available classifier for any target conditions. in some cases the performance of the hybrid actually can surpass that of the best known classifier. this robust performance extends across a wide variety of comparison frameworks including the optimization of metrics such as accuracy expected cost lift precision recall and workforce utilization. the hybrid also is efficient to build to store and to update. the hybrid is based on a method for the comparison of classifier performance that is robust to imprecise class distributions and misclassification costs. the roc convex hull method combines techniques from roc analysis decision analysis and computational geometry and adapts them to the particulars of analyzing learned classifiers. the method is efficient and incremental minimizes the management of classifier performance data and allows for clear visual comparisons and sensitivity analyses. finally we point to empirical evidence that a robust hybrid classifier indeed is needed for many real world problems.
#####
 keywords classiﬁcation learning uncertainty evaluation comparison multiple models cost sensitive learning skewed distributions to appear in machine learning journal introduction traditionally classiﬁcation systems have been built by experimenting with many diﬀerent classiﬁers comparing their performance and choosing the best. experimenting with diﬀerent induction algorithms parameter settings and training regimes yields a large number of classiﬁers to be evaluated and compared. unfortunately comparison often is diﬃcult in real world environments because key parameters of the target environment are not known. the optimal cost beneﬁt tradeoﬀs and the target class priors seldom are known precisely and often are subject to change. for example in fraud detection we cannot ignore misclassiﬁcation costs or the skewed class distribution nor can we assume that our estimates are precise or static. we need a method for the management comparison and application of multiple classiﬁers that is robust in imprecise and changing environments. we describe the roc convex hull method which combines techniques from roc analysis decision analysis and computational geometry. the roc convex hull decouples classiﬁer performance from speciﬁc class and cost distributions and may be used to specify the subset of methods that are potentially optimal under any combination of cost assumptions and class distribution assumptions. the rocch method is eﬃcient so it facilitates the comparison of a large number of classiﬁers. it minimizes the management of classiﬁer performance data because it can specify exactly those classiﬁers that are potentially optimal and it is incremental easily incorporating new and varied classiﬁers without having to reevaluate all prior classiﬁers. we demonstrate that it is possible and desirable to avoid complete commitment to a single best classiﬁer during system construction. instead the rocch can be used to build from the available classiﬁers a hybrid classiﬁcation system that will perform best under any target cost beneﬁt and class distributions. target conditions can then be speciﬁed at run time. moreover in cases where precise information is still unavailable when the system is run the hybrid system can be tuned easily based on feedback from its actual performance. the paper is structured as follows. first we sketch brieﬂy the traditional approach to building such systems in order to demonstrate that it is brittle under the types of imprecision common in realworld problems. we then introduce and describe the rocch and its properties for comparing and visualizing classiﬁer performance in imprecise environments. in the following sections we formalize the notion of a robust classiﬁcation system and show that the rocch is an elegant method for constructing one automatically. the solution is elegant because the resulting hybrid classiﬁer is robust for a wide variety of problem formulations including the optimization of metrics such as accuracy expected cost lift precision recall and workforce utilization and it is eﬃcient to build to store and to update. we then show that the hybrid actually can do better than the best known classiﬁer in certain situations. finally by citing results from empirical studies we provide evidence that this type of system indeed is needed. an example a systems building team wants to create a system that will take a large number of instances and identify those for which an action should be taken. the instances could be potential cases of fraudulent account behavior of faulty equipment of responsive customers of interesting science etc. we consider problems for which the best method for classifying or ranking instances is not well deﬁned so the system builders may consider machine learning methods neural networks casebased systems and hand crafted knowledge bases as potential classiﬁcation models. ignoring for the moment issues of eﬃciency the foremost question facing the system builders is which of the available models performs best at classiﬁcation traditionally an experimental approach has been taken to answer this question because the distribution of instances can be sampled if it is not known a priori. the standard approach is to estimate the error rate of each model statistically and then to choose the model with the lowest error rate. this strategy is common in machine learning pattern recognition data mining expert systems and medical diagnosis. in some cases other measures such as cost or beneﬁt are used as well. applied statistics provides methods such as cross validation and the bootstrap for estimating model error rates and recent studies have compared the eﬀectiveness of diﬀerent methods. unfortunately this experimental approach is brittle under two types of imprecision that are common in real world environments. speciﬁcally costs and beneﬁts usually are not known precisely and target class distributions often are known only approximately as well. this observation has been made by many authors and is in fact the concern of a large subﬁeld of decision analysis. imprecision also arises because the environment may change between the time the system is conceived and the time it is used and even as it is used. for example levels of fraud and levels of customer responsiveness change continually over time and from place to place. basic terminology in this paper we address two class problems. formally each instance i is mapped to one element of the set of positive and negative classes. a classiﬁcation model is a mapping from instances to predicted classes. some classiﬁcation models produce a continuous output to which diﬀerent thresholds may be applied to predict class membership. to distinguish between the actual class and the predicted class of an instance we will use the labels for the classiﬁcations produced by a model. for our discussion let c be a two place error cost function where c is the cost of a false positive error and c is the cost of a false negative error. we represent class distributions by the classes prior probabilities p and p p. for this paper we consider error costs to include beneﬁts not realized and ignore the costs of correct classiﬁcations. e t a r e v i t i s o p e u r t. c c c fp. c c c c c c fp. false positive rates fp. figure three classiﬁers under three diﬀerent neyman pearson decision criteria the true positive rate or hit rate of a classiﬁer is t p p positives correctly classiﬁed total positives the false positive rate or false alarm rate of a classiﬁer is f p p negatives incorrectly classiﬁed total negatives the traditional experimental approach is brittle because it chooses one model as best with respect to a speciﬁc set of cost functions and class distribution. if the target conditions change this system may no longer perform optimally or even acceptably. as an example assume that we have a maximum false positive rate f p that must not be exceeded. we want to ﬁnd the classiﬁer with the highest possible true positive rate t p that does not exceed the f p limit. this is the neyman pearson decision criterion. three classiﬁers under three such f p limits are shown in ﬁgure. a diﬀerent classiﬁer is best for each f p limit any system built with a single best classiﬁer is brittle if the f p requirement can change. evaluating and visualizing classiﬁer performance. classiﬁer comparison decision analysis and roc analysis most prior work on building classiﬁers uses classiﬁcation accuracy as the primary evaluation metric. the use of accuracy assumes that the class priors in the target environment will be constant and relatively balanced. in the real world this rarely is the case. classiﬁers often are used to sift through a large population of normal or uninteresting entities in order to ﬁnd a relatively small number of unusual ones for example looking for defrauded accounts among a large population of customers screening medical tests for rare diseases and checking an assembly line for defective parts. because the unusual or interesting class is rare among the general population the class distribution is very skewed. as the class distribution becomes more skewed evaluation based on accuracy breaks down. consider a domain where the classes appear in a ratio. a simple rule always classify as. e t a r e v i t i s o p e u r t classifier classifier classifier. false positive rate figure roc graph of three classiﬁers the maximum likelihood class gives a. accuracy. this accuracy may be quite diﬃcult for an induction algorithm to beat though the simple rule presumably is unacceptable if a non trivial solution is sought. skews of are common in fraud detection and skews exceeding have been reported in other applications. evaluation by classiﬁcation accuracy also assumes equal error costs c c. in the real world classiﬁcations lead to actions which have consequences. actions can be as diverse as denying a credit charge discarding a manufactured part moving a control surface on an airplane or informing a patient of a cancer diagnosis. the consequences may be grave and performing an in mushroom incorrect action may be very costly. rarely are the costs of mistakes equivalent. classiﬁcation for example judging a poisonous mushroom to be edible is far worse than judging an edible mushroom to be poisonous. indeed it is hard to imagine a domain in which a classiﬁcation system may be indiﬀerent to whether it makes a false positive or a false negative error. in such cases accuracy maximization should be replaced with cost minimization. the problems of unequal error costs and uneven class distributions are related. it has been suggested that for training high cost instances can be compensated for by increasing their prevalence in an instance set. unfortunately little work has been published on either problem. there exist several dozen articles in which techniques for cost sensitive learning are suggested but few studies evaluate and compare them. the literature provides even less guidance in situations where distributions are imprecise or can change. given an estimate of p the posterior probability of an instance s class membership decision analysis gives us a way to produce cost sensitive classiﬁcations. classiﬁer error frequencies can be used to approximate such probabilities. for an instance i the decision to emit a positive classiﬁcation from a particular classiﬁer is c p c regardless of whether a classiﬁer produces probabilistic or binary classiﬁcations its normalized cost on a test set can be evaluated empirically as cost f p c c most published work on cost sensitive classiﬁcation uses an equation such as this to rank classiﬁers. given a set of classiﬁers a set of examples and a precise cost function each classiﬁer s cost is computed and the minimum cost classiﬁer is chosen. however as discussed above such analyses assume that the distributions are precisely known and static. more general comparisons can be made with receiver operating characteristic analysis a classic methodology from signal detection theory that is common in medical diagnosis and has recently begun to be used more generally in ai classiﬁer work. roc graphs depict tradeoﬀs between hit rate and false alarm rate. we use the term roc space to denote the coordinate system used for visualizing classiﬁer performance. in roc space t p is represented on the y axis and f p is represented on the x axis. each classiﬁer is represented by the point in roc space corresponding to its pair. for models that produce a continuous output e.g. posterior probabilities t p and f p vary together as a threshold on the output is varied between its extremes the resulting curve is called the roc curve. an roc curve illustrates the error tradeoﬀs available with a given model. figure shows a graph of three typical roc curves in fact these are the complete roc curves of the classiﬁers shown in ﬁgure. for orientation several points on an roc graph should be noted. the lower left point represents the strategy of never alarming the upper right point represents the strategy of always alarming the point represents perfect classiﬁcation and the line y x represents the strategy of randomly guessing the class. informally one point in roc space is better than another if it is to the northwest. an roc graph allows an informal visual comparison of a set of classiﬁers. roc graphs illustrate the behavior of a classiﬁer without regard to class distribution or error cost and so they decouple classiﬁcation performance from these factors. unfortunately while an roc graph is a valuable visualization technique it does a poor job of aiding the choice of classiﬁers. only when one classiﬁer clearly dominates another over the entire performance space can it be declared better. the roc convex hull method in this section we combine decision analysis with roc analysis and adapt them for comparing the performance of a set of learned classiﬁers. the method is based on three high level principles. first roc space is used to separate classiﬁcation performance from class and cost distribution information. second decision analytic information is projected onto the roc space. third the convex hull in roc space is used to identify the subset of classiﬁers that are potentially optimal. .. iso performance lines by separating classiﬁcation performance from class and cost distribution assumptions the decision goal can be projected onto roc space for a neat visualization. speciﬁcally the expected cost of applying the classiﬁer represented by a point in roc space is p c p f p c therefore two points and have the same performance if t p t p f p f p cp cp this equation deﬁnes the slope of an iso performance line. that is all classiﬁers corresponding to points on the line have the same expected cost. each set of class and cost distributions deﬁnes a family of iso performance lines. lines more northwest are better because they correspond to classiﬁers with lower expected cost. .. the roc convex hull because in most real world cases the target distributions are not known precisely it is valuable to be able to identify those classiﬁers that potentially are optimal. each possible set of distributions d convex hull a c. b e t a r e v i t i s o p e u r t. false positive rate figure the roc convex hull identiﬁes potentially optimal classiﬁers. deﬁnes a family of iso performance lines and for a given family the optimal methods are those that lie on the most northwest iso performance line. thus a classiﬁer is optimal for some conditions if and only if it lies on the northwest boundary of the convex hull of the set of points in roc space. we discuss this in detail in section. we call the convex hull of the set of points in roc space the roc convex hull of the corresponding set of classiﬁers. figure shows four roc curves with the roc convex hull drawn as the border between the shaded and unshaded areas. d is clearly not optimal. perhaps surprisingly b can never be optimal either because none of the points of its roc curve lies on the convex hull. we can also remove from consideration any points of a and c that do not lie on the hull. consider these classiﬁers under two distribution scenarios. in each negative examples outnumber positives by. in scenario a false positive and false negative errors have equal cost. in scenario b a false negative is times as expensive as a false positive. each scenario deﬁnes a family of iso performance lines. the lines corresponding to scenario a have slope those for b have slope. figure shows the convex hull and two iso performance lines α and β. line α is the best line with slope that intersects the convex hull line β is the best line with slope that intersects the convex hull. each line identiﬁes the optimal classiﬁer under the given distribution. figure shows the three roc curves from our initial example with the convex hull drawn. .. generating the roc convex hull the roc convex hull method selects the potentially optimal classiﬁers based on the roc convex hull and iso performance lines. for each classiﬁer plot t p and f p in roc space. for continuous output classiﬁers vary a threshold over the output range and plot the roc curve. table shows an algorithm for producing such an roc curve in a single pass. the convex hull of a set of points is the smallest convex set that contains the points. there is a subtle complication to producing roc curves from ranked test set data which is reﬂected in the algorithm shown in table. speciﬁcally consecutive examples with the same score can give overly optimistic or overly pessimistic roc curves depending on the ordering of positive and negative examples. the roc curve generating algorithm shown here waits until all examples with the same score have been tallied before computing the next point of the roc curve. the result is a segment that bisects the area that would have resulted from the most optimistic and most pessimistic orderings. β c α a e t a r e v i t i s o p e u r t. false positive rate figure lines α and β show the optimal classiﬁer under diﬀerent sets of conditions. find the convex hull of the set of points representing the predictive behavior of all classiﬁers of interest for example by using the quickhull algorithm. for each set of class and cost distributions of interest ﬁnd the slope of the corresponding iso performance lines. for each set of class and cost distributions the optimal classiﬁer will be the point on the convex hull that intersects the iso performance line with largest t p intercept. ranges of slopes specify hull segments. figures and demonstrate how the subset of classiﬁers that are potentially optimal can be identiﬁed and how classiﬁers can be compared under diﬀerent cost and class distributions. .. comparing a variety of classiﬁers the roc convex hull method accommodates both binary and continuous classiﬁers. binary classiﬁers are represented by individual points in roc space. continuous classiﬁers produce numeric outputs to which thresholds can be applied yielding a series of pairs forming an roc curve. each point may or may not contribute to the roc convex hull. figure depicts the binary classiﬁers e f and g added to the previous hull. e may be optimal under some circumstances because it extends the convex hull. classiﬁers f and g never will be optimal because they do not extend the hull. new classiﬁers can be added incrementally to an rocch analysis as demonstrated in ﬁgure by the addition of classiﬁers e f and g. each new classiﬁer either extends the existing hull or it does not. in the former case the hull must be updated accordingly but in the latter case the new classiﬁer can be ignored. therefore the method does not require saving every classiﬁer for re analysis under diﬀerent conditions only those points on the convex hull. recall that each point is a classiﬁer and might take up considerable space. further the management of knowledge about many classiﬁers and their statistics from many diﬀerent runs of learning programs can be a substantial undertaking. classiﬁers not on the rocch can never be optimal so they need not be saved. every classiﬁer that does lie on the convex hull must be saved. in section. we demonstrate the rocch in use managing the results of many learning experiments. e t a r e v i t i s o p e u r t classifier classifier classifier convex hull. false positive rate figure roc curves with convex hull .. changing distributions and costs class and cost distributions that change over time necessitate the reevaluation of classiﬁer choice. in fraud detection costs change based on workforce and reimbursement issues the amount of fraud changes monthly. with the roc convex hull method comparing under a new distribution involves only calculating the slope of the corresponding iso performance lines and intersecting them with the hull as shown in ﬁgure. the roc convex hull method scales gracefully to any degree of precision in specifying the cost and class distributions. if nothing is known about a distribution the roc convex hull shows all classiﬁers that may be optimal under any conditions. figure showed that given classiﬁers a b c and d only a and c can ever be optimal. with complete information the method identiﬁes the optimal classiﬁer. in ﬁgure we saw that classiﬁer a is optimal under scenario a and classiﬁer c is optimal under scenario b. next we will see that with less precise information the roc convex hull can show the subset of possibly optimal classiﬁers. .. sensitivity analysis imprecise distribution information deﬁnes a range of slopes for iso performance lines. this range of slopes intersects a segment of the roc convex hull which facilitates sensitivity analysis. for example if the segment deﬁned by a range of slopes corresponds to a single point in roc space or a small threshold range for a single classiﬁer then there is no sensitivity to the distribution assumptions in question. consider a scenario similar to a and b in that negative examples are times as prevalent as positive ones. in this scenario consider the cost of dealing with a false alarm to be between and and the cost of missing a positive example to be between and. these conditions deﬁne a range of slopes for iso performance lines. figure a depicts this range of slopes and the corresponding segment of the roc convex hull. the ﬁgure shows that the choice of classiﬁer is insensitive to changes within this range. figure b depicts a scenario with a wider range of slopes m. the ﬁgure shows that under this scenario the choice of classiﬁer is very sensitive to the distribution. classiﬁers a c and e each are optimal for some subrange. m table algorithm for generating an roc curve from a set of ranked examples. given e list of tuples hi pi where i labeled example p numeric ranking assigned to i by the classiﬁer p n count of positive and negative examples in e respectively. output r list of points on the roc curve. t count f count plast r hi sort e in decreasing order by p values while do current tp tally current fp tally last score seen list of roc points remove tuple hi pi from head of e if then add point to end of r end if if then t count t count else end if f count f count end while add point to end of r i is a negative example building robust classiﬁers up to this point we have concentrated on the use of the rocch for visualizing and evaluating sets of classiﬁers. the rocch helps to delay classiﬁer selection as long as possible yet provides a rich performance comparison. however once system building incorporates a particular classiﬁer the problem of brittleness resurfaces. this is important because the delay between system building and deployment may be large and because many systems must survive for years. in fact in many domains a precise static speciﬁcation of future costs and class distributions is not just unlikely it is impossible. we address this brittleness by using the rocch to produce robust classiﬁers deﬁned as satisfying the following. under any target cost and class distributions a robust classiﬁer will perform at least as well as the best classiﬁer for those conditions. our statements about optimality are practical the best classiﬁer may not be the bayes optimal classiﬁer but it is at least as good as any known classiﬁer. srinivasan calls this fapp optimal. stating that a classiﬁer is robust is stronger than stating that it is optimal for a speciﬁc set of conditions. a robust classiﬁer is optimal under all possible conditions. in principle classiﬁcation brittleness could be overcome by saving all possible classiﬁers and then performing an automated run time comparison under the desired target conditions. however such a system is not feasible because of time and space limitations there are myriad possible classiﬁcation models arising from the many diﬀerent learning methods under their many diﬀerent parameter settings. storing all the classiﬁers is not feasible and tuning the system by comparing classiﬁers on the ﬂy under diﬀerent e g a c f e t a r e v i t i s o p e u r t. false positive rate figure classiﬁer e may be optimal for some conditions because it extends the roc convex hull. f and g cannot be optimal they are not on the hull nor do they extend it. conditions is not feasible. fortunately doing so is not necessary. moreover we will show that it is sometimes possible to do better than any of these classiﬁers. rocch hybrid classiﬁers we now show that robust hybrid classiﬁers can be built using the rocch. deﬁnition let i be the space of possible instances and let c be the space of sets of classiﬁcation models. let a µ hybrid classiﬁer comprise a set of classiﬁcation models c c and a function µ i ℜ c. a µ hybrid classiﬁer takes as input an instance i i for classiﬁcation and a number x ℜ. as output it produces the classiﬁcation produced by µ. things will get more involved later but for the time being consider that each set of cost and class distributions deﬁnes a value for x which is used to select the best classiﬁer for those conditions. to build a µ hybrid classiﬁer we must deﬁne µ and the set c. we would like c to include only those models that perform optimally under some conditions since these will be stored by the system and we would like µ to be general enough to apply to a variety of problem formulations. the models comprising the rocch can be combined to form a µ hybrid classiﬁer that is an elegant robust classiﬁer. deﬁnition the rocch hybrid is a µ hybrid classiﬁer where c is the set of classiﬁers that form the rocch and µ makes classiﬁcations using the classiﬁer on the rocch with f p x. note that for the moment the rocch hybrid is deﬁned only for f p values corresponding to rocch vertices. robust classiﬁcation our deﬁnition of robust classiﬁers was intentionally vague about what it means for one classiﬁer to be better than another because diﬀerent situations call for diﬀerent comparison frameworks. we. false positive rate. a. low sensitivity e a e a. e t a r e v i t i s o p e u r t. e t a r e v i t i s o p e u r t c c. false positive rate b. high sensitivity figure sensitivity analysis using the roc convex hull low sensitivity high sensitivity now continue with minimizing expected cost because the process of proving that the rocch hybrid minimizes expected cost for any cost and class distributions provides a deep understanding of why and how the rocch hybrid works. later we generalize to a wide variety of comparison frameworks. the rocch hybrid can be seen as an application of multi criteria optimization to classiﬁer design and construction. the classiﬁers on the rocch are edgeworth pareto optimal with respect to tp fp and the objective functions we discuss. multi criteria optimization was used previously in machine learning by tcheng lambert lu and rendell for the selection of inductive bias. alternatively the rocch can be seen as an application of the theory of games and statistical decisions for which convex sets represent optimal strategies. edgeworth pareto optimality is the century old notion that in a multidimensional space of criteria optimal performance is the frontier of achievable performance in this space. in cases where performance is a linear combination of the criteria the optimality frontier is the convex hull. .. minimizing expected cost from above the expected cost of applying a classiﬁer is ec p c p f p c for a particular set of cost and class distributions the slope of the corresponding iso performance lines is mec cp cp every set of conditions will deﬁne an mec. we now can show that the rocch hybrid is robust for problems where the best classiﬁer is the classiﬁer with the minimum expected cost. the slope of the rocch is an important tool in our argument. the rocch is a piecewise linear concave down curve. therefore as x increases the slope of the rocch is monotonically nonincreasing with k discrete values where k is the number of rocch component classiﬁers including the degenerate classiﬁers that deﬁne the rocch endpoints. where there will be no confusion we use phrases such as points in roc space as a shorthand for the more cumbersome classiﬁers corresponding to points in roc space. for this subsection unless otherwise noted points on the rocch refer to vertices of the rocch. deﬁnition for any real number m the point where the slope of the rocch is m is one of the endpoints of the segment of the rocch with slope m if such a segment exists. otherwise it is the vertex for which the left adjacent segment has slope greater than m and the right adjacent segment has slope less than m. for completeness the leftmost endpoint of the rocch is considered to be attached to a segment with inﬁnite slope and the rightmost endpoint of the rocch is considered to be attached to a segment with zero slope. note that every m deﬁnes at least one point on the rocch. lemma for any set of cost and class distributions there is a point on the rocch with minimum expected cost. proof assume that for some conditions there exists a point c with smaller expected cost than any point on the rocch. by equations and a point has the same expected cost as a point if t p t p f p f p mec therefore for conditions corresponding to mec all points with equal expected cost form an isoperformance line in roc space with slope mec. also by and points on lines with larger y intercept have lower expected cost. now point c is not on the rocch so it is either above the curve or below the curve. if it is above the curve then the rocch is not a convex set enclosing all points which is a contradiction. if it is below the curve then the iso performance line through c also contains a point p that is on the rocch. if this iso performance line intersects no rocch vertex then consider the vertices at the endpoints of the rocch segment containing p one of these vertices must intersect a better iso performance line than does c. in either case since all points on an iso performance line have the same expected cost point c does not have smaller expected cost than all points on the rocch which is also a contradiction. although it is not necessary for our purposes here it can be shown that all of the minimum expected cost classiﬁers are on the rocch. deﬁnition an iso performance line with slope m is an m iso performance line. lemma for any cost and class distributions that translate to mec a point on the rocch has minimum expected cost only if the slope of the rocch at that point is mec. proof suppose that there is a point d on the rocch where the slope is not mec but the point does have minimum expected cost. by deﬁnition either the segment to the left of d has slope less than mec or the segment to the right of d has slope greater than mec. for case consider point n the vertex of the rocch that neighbors d to the left and consider the mec iso performance lines ld and ln through d and n. because n is to the left of d and the line connecting them has slope less than mec the y intercept of ln will be greater than the y intercept of ld. this means that n will have lower expected cost than d which is a contradiction. the argument for is analogous. lemma if the slope of the rocch at a point is mec then the point has minimum expected cost. proof if this point is the only point where the slope of the rocch is mec then the proof follows directly from lemma and lemma. if there are multiple such points then by deﬁnition they are connected by an mec iso performance line so they have the same expected cost and once again the proof follows directly from lemma and lemma. it is straightforward now to show that the rocch hybrid is robust for the problem of minimizing expected cost. theorem the rocch hybrid minimizes expected cost for any cost distribution and any class distribution. proof because the rocch hybrid is composed of the classiﬁers corresponding to the points on the rocch this follows directly from lemmas and. now we have shown that the rocch hybrid is robust when the goal is to provide the minimum expected cost classiﬁcation. this result is important even for accuracy maximization because the preferred classiﬁer may be diﬀerent for diﬀerent target class distributions. this rarely is taken into account in experimental comparisons of classiﬁers. corollary the rocch hybrid minimizes error rate for any target class distribution. proof error rate minimization is cost minimization with uniform error costs. robust classiﬁcation for other common metrics showing that the rocch hybrid is robust not only helps us with understanding the rocch method generally it also shows us how the rocch hybrid will pick the best classiﬁer in order to produce the best classiﬁcations which we will return to later. if we ignore the need to specify how to pick the best component classiﬁer we can show that the rocch applies more generally. f p then there exists a point on the rocch with an f value at least as high as theorem for any classiﬁer evaluation metric f if f t p and f that of any known classiﬁer. proof assume that there exists a classiﬁer co not on the rocch with an f value higher than that of any point on the rocch. co is either above or below the rocch. in case the rocch is not a convex set enclosing all the points which is a contradiction. in case let co be represented in roc space by. because co is below the rocch there exist points call one on the rocch with t pp t po and f pp f po. however by the restriction on the partial derivatives for any such point f f which again is a contradiction. there are two complications to the more general use of the rocch both of which are illustrated by the decision criterion from our very ﬁrst example. recall that the neyman pearson criterion speciﬁes a maximum acceptable f p rate. standard roc analysis uses roc curves to select a single parameterized classiﬁcation model the parameter allows the user to select the operating point for a decision making task usually a threshold on a probabilistic output that will allow for optimal decision making. under the neyman pearson criterion selecting the single best model from a set is easy plot the roc curves draw a vertical line at the desired maximum f p and pick the model whose curve has the largest t p at the intersection with this line. classifier classifier classifier hull neyman pearson e t a r e v i t i s o p e u r t. false positive rate figure the roc convex hull used to select a classiﬁer under the neyman pearson criterion with the rocch hybrid making the best classiﬁcations under the neyman pearson criterion is not so straightforward. for minimizing expected cost it was suﬃcient for the rocch hybrid to choose a vertex from the rocch for any mec value. for problem formulations such as the neymanpearson criterion the performance statistics at a non vertex point on the rocch may be preferable. fortunately with a slight extension the rocch hybrid can yield a classiﬁer with these performance statistics. theorem an rocch hybrid can achieve the t p f p tradeoﬀ represented by any point on the rocch not just the vertices. proof extend µ to non vertex points as follows. pick the point p on the rocch with f p x. let t px be the t p value of this point. if is an rocch vertex use the corresponding classiﬁer. if it is not a vertex call the left endpoint of the hull segment on which p lies cl and the right endpoint cr. let d be the distance between cl and cr and let p be the distance between cl and p. make classiﬁcations as follows. for each input instance ﬂip a weighted coin and choose the answer given by classiﬁer cr with probability p d and that given by classiﬁer cl with probability p d. it is straightforward to show that f p and t p for this classiﬁer will be x and t px. the second complication is that as illustrated by the neyman pearson criterion many practical classiﬁer comparison frameworks include constrained optimization problems. arbitrarily constrained optimizations are problematic for the rocch hybrid. given total freedom it is possible to devise constraints on classiﬁer performance such that even with the restriction on the partial derivatives an interior point scores higher than any acceptable point on the hull. for example two linear constraints can enclose a subset of the interior and exclude the entire rocch there will be no acceptable points on the rocch. however many realistic constraints do not thwart the optimality of the rocch hybrid. theorem for any classiﬁer evaluation metric f if f t p and f f p and no constraint on classiﬁer performance eliminates any point on the rocch without also eliminating all higher scoring interior points then the rocch hybrid can perform at least as well as any known classiﬁer. proof follows directly from theorem and theorem. linear constraints on classiﬁers f p t p performance are common for real world problems so the following is useful. corollary for any classiﬁer evaluation metric f if t p and f f f p and there is a single constraint on classiﬁer performance of the form a t p b f p c with a and b non negative then the rocch hybrid can perform at least as well as any known classiﬁer. proof the single constraint eliminates from contention all points that do not fall to the left of or below a line with non positive slope. by the restriction on the partial derivatives such a constraint will not eliminate a point on the rocch without also eliminating all interior points with higher f values. thus the proof follows directly from theorem. so ﬁnally we have the following corollary for the neyman pearson criterion the rocch hybrid can perform at least as well as that of any known classiﬁer. proof for the neyman pearson criterion the evaluation metric is f t p that is a higher t p is better. the constraint on classiﬁer performance is f p f pmax. these satisfy the conditions for corollary and therefore this corollary follows. all the foregoing eﬀort may seem misplaced for a simple criterion like neyman pearson. however there are many other realistic problem formulations. for example consider the decision support problem of optimizing workforce utilization in which a workforce is available that can process a ﬁxed number of cases. too few cases will under utilize the workforce but too many cases will leave some cases unattended. if the workforce can handle k cases the system should present the best possible set of k cases. this is similar to the neyman pearson criterion but with an absolute cutoﬀ instead of a percentage cutoﬀ. theorem for workforce utilization the rocch hybrid will provide the best set of k cases for any choice of k. proof the decision criterion is to maximize t p subject to the constraint the theorem therefore follows from corollary. t p p f p n k in fact many screening problems such as are found in marketing and information retrieval use exactly this linear constraint. it follows that for maximizing lift precision or recall subject to absolute or percentage cutoﬀs on case presentation the rocch hybrid will provide the best set of cases. as with minimizing expected cost imprecision in the environment forces us to favor a robust solution for these other comparison frameworks. for many real world problems the precise desired cutoﬀ will be unknown or will change. what is worse for a ﬁxed cutoﬀ merely changing the size of the universe of cases may change the preferred classiﬁer because it will change the constraint line. the rocch hybrid provides a robust solution because it gives the optimal subset of cases for any constraint line. for example for document retrieval the rocch hybrid will yield the best n documents for any n for any prior class distribution and for any target corpus size. ranking cases an apparent solution to the problem of robust classiﬁcation is to use a model that ranks cases and just work down the ranked list. this approach appears to sidestep the brittleness demonstrated with binary classiﬁers since the choice of a cutoﬀ point can be deferred to classiﬁcation time. however choosing the best ranking model is still problematic. for most practical situations choosing the best ranking model is equivalent to choosing which classiﬁer is best for the cutoﬀ that will be used. rb ra random n cases e t a r e v i t i s o p e u r t. false positive rate figure the roc curves of the two ranking classiﬁers ra and rb described in section .. an example will illustrate this. consider two ranking functions ra and rb applied to a classbalanced set of cases. assume ra is able to recognize a common aspect unique to positive cases that occurs in of the population and it ranks these highest. assume rb is able to recognize a common aspect unique to negative cases occurring in of the population and it ranks these lowest. so ra ranks the highest correctly and performs randomly on the remainder while rb ranks the lowest correctly and performs randomly on the remainder. which model is better the answer depends entirely upon how far down the list the system will go before it stops that if fewer than cases are to be selected then ra should be is upon what cutoﬀ will be used. used whereas rb is better if more than cases will be selected. figure shows the roc curves corresponding to these two classiﬁers and the point corresponding to n where the curves cross in roc space. the rocch method can be used to organize such ranking models as we have seen. recall that roc curves are formed from case rankings by moving the cutoﬀ from one extreme to the other. the rocch hybrid comprises the ranking models that are best for all possible conditions. whole curve metrics in situations where either the target cost distribution or class distribution is completely unknown some researchers advocate choosing the classiﬁer that maximizes a single number metric representing the average performance over the entire curve. a common whole curve metric is auc the area under the curve. the auc is equivalent to the probability that a randomly chosen positive instance will be rated higher than a negative instance and thereby is also estimated by the wilcoxon test of ranks. a criticism of auc is that for speciﬁc target conditions the classiﬁer with the maximum auc may be suboptimal. indeed this criticism may be made of any single number metric. fortunately not only is the rocch hybrid optimal for any speciﬁc target conditions it has the maximum auc there is no classiﬁer with auc larger than that of the rocch hybrid. using the rocch hybrid to use the rocch hybrid for classiﬁcation we need to translate environmental conditions to x values to plug into µ. for minimizing expected cost equation shows how to translate conditions to mec. for any mec by lemma we want the f p value of the point where the slope of the rocch is mec which is straightforward to calculate. for the neyman pearson criterion the conditions are deﬁned as f p values. for workforce utilization with conditions corresponding to a cutoﬀ k the f p value is found by intersecting the line t p p f p n k with the rocch. we have argued that target conditions are rarely known. it may be confusing that we now seem to require exact knowledge of these conditions. the rocch hybrid gives us two important capabilities. first the need for precise knowledge of target conditions is deferred until run time. second in the absence of precise knowledge even at run time the system can be optimized easily with minimal feedback. by using the rocch hybrid information on target conditions is not needed to train and compare classiﬁers. this is important because of imprecision caused by temporal geographic or other diﬀerences that may exist between training and use. for example building a system for a real world problem introduces a non trivial delay between the time data are gathered and the time the learned models will be used. the problem is exacerbated in domains where error costs or class distributions change over time even with slow drift a brittle model may become suboptimal quickly. in many such scenarios costs and class distributions can be speciﬁed at run time with reasonable precision by sampling from the current population and used to ensure that the rocch hybrid always performs optimally. in some cases even at run time these quantities are not known exactly. a further beneﬁt of the rocch hybrid is that it can be tuned easily to yield optimal performance with only minimal feedback from the environment. conceptually the rocch hybrid has one knob that varies x in µ from one extreme to the other. for any knob setting the rocch hybrid will give the optimal t p f p tradeoﬀ for the target conditions corresponding to that setting. turning the knob to the right increases t p turning the knob to the left decreases f p. because of the monotonicity of the rocch hybrid simple hill climbing can guarantee optimal performance. for example if the system produces too many false alarms turn the knob to the left if the system is presenting too few cases turn the knob to the right. beating the component classiﬁers perhaps surprisingly in many realistic situations an rocch hybrid system can do better than any of its component classiﬁers. consider the neyman pearson decision criterion. the rocch may intersect the f p line above the highest component roc curve. this occurs when the f p line intersects the rocch between vertices therefore there is no component classiﬁer that actually produces these particular statistics as in ﬁgure. by theorem the rocch hybrid can achieve any t p on the hull. only a small number of f p values correspond to hull vertices. the same holds for other common problem formulations such as workforce utilization lift maximization precision maximization and recall maximization. time and space eﬃciency we have argued that the rocch hybrid is robust for a wide variety of problem formulations. it is also eﬃcient to build to store and to update. the time eﬃciency of building the rocch hybrid depends ﬁrst on the eﬃciency of building the component models which varies widely by model type. some models built by machine learning methods can be built in seconds. hand built models can take years to build. however we presume that this is work that would be done anyway. the rocch hybrid can be built with whatever methods are available be there two or two thousand. as described below as new classiﬁers become available the rocch hybrid can be updated incrementally. the time eﬃciency depends also on the eﬃciency of the experimental evaluation of the classiﬁers. once again we presume that this is work that would be done anyway. finally the time eﬃciency of the rocch hybrid depends on the eﬃciency of building the rocch which can be done in o time using the quickhull algorithm where n is the number of classiﬁers. the rocch is space eﬃcient too because it comprises only classiﬁers that might be optimal under some target conditions. the number of classiﬁers that must be stored can be reduced if bounds can be placed on the potential target conditions. as described above ranges of conditions deﬁne segments of the rocch. thus the rocch hybrid may need only a subset of c. adding new classiﬁers to the rocch hybrid also is eﬃcient. adding a classiﬁer to the rocch will either extend the hull adding to the rocch hybrid or conclude that the new classiﬁers are not superior to the existing classiﬁers in any portion of roc space and can be discarded. the run time complexity of the rocch hybrid is never worse than that of the in situations where run time complexity is crucial the rocch should be component classiﬁers. constructed without prohibitively expensive classiﬁcation models. it then will ﬁnd the best subset of the computationally eﬃcient models. empirical demonstration of need robust classiﬁcation is of fundamental interest because it weakens two very strong assumptions the availability of precise knowledge of costs and of class distributions. however might it not be that existing classiﬁers already are robust for example if a given classiﬁer is optimal under one set of conditions might it not be optimal under all it is beyond the scope of this paper to oﬀer an in depth experimental study answering this question. however we can provide solid evidence that the answer is no by referring to the results of two prior studies. one is a comprehensive roc analysis of medical domains recently conducted by andrew bradley. the other is a published roc analysis of uci database domains that we undertook last year with ron kohavi. note that a classiﬁer dominates if its roc curve completely deﬁnes the rocch. therefore if there exist more than a trivially few domains where no single classiﬁer dominates then techniques like the rocch hybrid are essential if robust classiﬁers are desired. bradley s study bradley studied six medical data sets noting that unfortunately we rarely know what the individual misclassiﬁcation costs are. he plotted the roc curves of six classiﬁer learning algorithms. on not one of these data sets was there a dominating classiﬁer. this means that for each domain there exist diﬀerent sets of conditions for which diﬀerent classiﬁers are preferable. in fact the running example in the present article is based on the three best classiﬁers from bradley s results on the heart bleeding data his results for the full set of six classiﬁers can be found in ﬁgure. classiﬁers constructed for the cleveland heart disease data are shown in ﬁgure. bradley s results show clearly that for many domains the classiﬁer that maximizes any single metric be it accuracy cost or the area under the roc curve will be the best for some cost and class distributions and will not be the best for others. we have shown that the rocch hybrid will be the best for all. our study in the study we performed with ron kohavi we chose ten datasets from the uci repository each of which contains at least instances but for which the accuracy for decision trees was less than. for each domain we induced classiﬁers for the minority class. we selected several induction algorithms from mlc a decision tree learner naive bayes with discretization k nearest neighbor for several k values and bagged mc. mc is similar to c. probabilistic predictions are made by using a laplace correction at the leaves. nb discretizes the data based on entropy minimization and then builds the bradley s purpose was not to answer this question fortunately his published results do anyway. e t a r e v i t i s o p e u r t. bayes k nn mlp c. msc perceptron. false positive rate figure bradley s classiﬁer results for the heart bleeding data. naive bayes model. ibk votes the closest k neighbors each neighbor votes with a weight equal to one over its distance from the test instance. some of the roc curves are shown in figure. for only one of these ten domains was there an absolute dominator. in general very few of the runs performed had dominating classiﬁers. some cases are very close for example adult and waveform. in other cases a curve that dominates in one area of roc space is dominated in another. these results also support the need for methods like the rocch hybrid which produce robust classiﬁers. as examples of what expected cost minimizing rocch hybrids would look like internally table shows the component classiﬁers that make up the rocch for the four uci domains of ﬁgure. for example in the road domain naive bayes would be chosen for any target conditions corresponding to a slope less than. and bagged mc would be chosen for slopes greater than .. they perform equally well at .. limitations and future work there are limitations to the rocch method as we have presented it here. we have deﬁned it here only for two class problems. srinivasan shows that it can be extended to multiple dimensions. it should be noted that the dimensionality of the roc hyperspace grows quadratically in the number of classes so both eﬃciency and visualization capability are called into question. we have assumed constant error costs for a given type of error e.g. all false positives cost the same. for some problems diﬀerent errors of the same type have diﬀerent costs. in many cases such a problem can be transformed for evaluation into an equivalent problem with uniform intra type error costs by duplicating instances in proportion to their costs. we also have assumed for this paper that the estimates of the classiﬁers performance statistics are very good. as mentioned above much work has addressed the production of good estimates for simple performance statistics such as error rate. much less work has addressed the production of good roc curve estimates. as with simpler statistics care should be taken to avoid over ﬁtting the training data and to ensure that diﬀerences between roc curves are meaningful. one solution is to use cross validation with averaging of roc curves which is the procedure used to produce the roc curves in section .. to our knowledge the issue is e t a r e v i t i s o p e u r t. bayes k nn mlp c. msc perceptron. false positive rate figure bradley s classiﬁer results for the cleveland heart disease data open of how best to produce conﬁdence bands appropriate to a particular problem. those shown in section. are appropriate for the neyman pearson decision criterion. also we have addressed predictive performance and computational performance. these are not the only concerns in choosing a classiﬁcation model. what if comprehensibility is important the easy answer is that for any particular setting the rocch hybrid is as comprehensible as the underlying model it is using. however this answer falls short if the rocch hybrid is interpolating between two models or if one wants to understand the multiple model system as a whole. although roc analysis and the rocch method were speciﬁcally designed for classiﬁcation domains we have extended them to activity monitoring domains. such domains involve monitoring the behavior of a population of entities for interesting events requiring action. these problems are substantially diﬀerent from standard classiﬁcation because timeliness of classiﬁcation is important and dependencies exist among instances both factors complicate evaluation. this work is fundamentally diﬀerent from other recent machine learning work on combining multiple models. that work combines models in order to boost performance for a ﬁxed cost and class distribution. the rocch hybrid combines models for robustness across diﬀerent cost and class distributions. in principle these methods should be independent multiple model classiﬁers are candidates for extending the rocch. however it may be that some multiple model classiﬁers achieve increased performance for a speciﬁc set of conditions by interpolating along edges of the rocch. cherikh uses roc analysis to study decision making where the decisions of multiple models are present. unlike our work the goal is to ﬁnd optimal combinations of models for speciﬁc conditions. however it seems that the two methods may be combined proﬁtably well chosen combinations of models should extend the rocch yielding a better robust classiﬁer. the rocch method also complements research on cost sensitive learning. existing cost sensitive learning methods are brittle with respect to imprecise cost knowledge. thus the rocch is an essential evaluation tool. furthermore cost sensitive learning may be used to ﬁnd better components for the rocch hybrid by searching explicitly for classiﬁers that extend the rocch. false positive a. vehicle false positive b. crx e v i t i s o p e u r t e v i t i s o p e u r t. e v i t i s o p e u r t e v i t i s o p e u r t. mc nb ib ib ib bag mc mc nb ib ib ib bag mc mc nb ib ib ib bag mc mc nb ib ib ib bag mc. false positive c. roadgrass false positive d. satimage figure smoothed roc curves from uci database domains conclusion the roc convex hull method is a robust eﬃcient solution to the problem of comparing multiple classiﬁers in imprecise and changing environments. it is intuitive can compare classiﬁers both in general and under speciﬁc distribution assumptions and provides crisp visualizations. it minimizes the management of classiﬁer performance data by selecting exactly those classiﬁers that are potentially optimal thus only these need to be saved in preparation for changing conditions. moreover due to its incremental nature new classiﬁers can be incorporated easily e.g. when trying a new parameter setting. the rocch hybrid performs optimally under any target conditions for many realistic problem formulations including the optimization of metrics such as accuracy expected cost lift precision recall and workforce utilization. it is eﬃcient to build in terms of time and space and can be updated incrementally. furthermore it can sometimes classify better than any known model. therefore we conclude that it is an elegant robust classiﬁcation system. we believe that this work has important implications for both machine learning applications and machine learning research. for applications it helps free system designers table locally dominating classiﬁers for four uci domains domain vehicle road crx satimage slope range dominator bagged mc nb bagged mc. bagged mc nb bagged mc nb. nb bagged mc ib ib ib ib bagged mc. from the need to choose comparison metrics before precise knowledge of key evaluation parameters is available. indeed such knowledge may never be available yet robust systems still can be built. for machine learning research it frees researchers from the need to have precise class and cost in particular work on distribution information in order to study important related phenomena. cost sensitive learning has been impeded by the diﬃculty of specifying costs and by the tenuous nature of conclusions based on a single cost metric. researchers need not be held back by either. cost sensitive learning can be studied generally without specifying costs precisely. the same goes for research on learning with highly skewed distributions. which methods are eﬀective for which levels of distribution skew the rocch will provide a detailed answer. recently drummond and holte have demonstrated an intriguing dual to the rocch. their cost curves represent expected costs explicitly rather than as slopes of iso performance lines and thereby provide an insightful alternative perspective for visualization. note an implementation of the rocch method in perl is publicly available. the code and related papers may be found at http www.hpl.hp.com personal tom_fawcett rocch. acknowledgments much of this work was done while the authors were employed at the bell atlantic science and technology center. we thank the many with whom we have discussed roc analysis and classiﬁer comparison especially rob holte george john ron kohavi ron rymon and peter turney. we thank andrew bradley for supplying data from his analysis.