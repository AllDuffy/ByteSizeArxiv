{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import urllib.request as libreq\n",
    "import feedparser\n",
    "import pdfminer.layout\n",
    "import pdfminer.high_level\n",
    "from io import StringIO\n",
    "from pdfminer.layout import LAParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed title: ArXiv Query: search_query=cat:cs.LG&id_list=&start=0&max_results=5\n",
      "Feed last updated: 2020-04-21T00:00:00-04:00\n",
      "totalResults for this query: 53710\n",
      "itemsPerPage for this query: 5\n",
      "startIndex for this query: 0\n",
      "e-print metadata\n",
      "arxiv-id: cs/9905014v1\n",
      "Published: 1999-05-21T14:26:07Z\n",
      "Title:  Hierarchical Reinforcement Learning with the MAXQ Value Function\n",
      "  Decomposition\n",
      "Last Author:  Thomas G. Dietterich\n",
      "Authors:  Thomas G. Dietterich\n",
      "abs page link: http://arxiv.org/abs/cs/9905014v1\n",
      "pdf link: http://arxiv.org/pdf/cs/9905014v1\n",
      "Journal reference: No journal ref found\n",
      "Comments: 63 pages, 15 figures\n",
      "Primary Category: cs.LG\n",
      "All Categories: cs.LG, I.2.6\n",
      "Abstract: This paper presents the MAXQ approach to hierarchical reinforcement learning\n",
      "based on decomposing the target Markov decision process (MDP) into a hierarchy\n",
      "of smaller MDPs and decomposing the value function of the target MDP into an\n",
      "additive combination of the value functions of the smaller MDPs. The paper\n",
      "defines the MAXQ hierarchy, proves formal results on its representational\n",
      "power, and establishes five conditions for the safe use of state abstractions.\n",
      "The paper presents an online model-free learning algorithm, MAXQ-Q, and proves\n",
      "that it converges wih probability 1 to a kind of locally-optimal policy known\n",
      "as a recursively optimal policy, even in the presence of the five kinds of\n",
      "state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q\n",
      "through a series of experiments in three domains and shows experimentally that\n",
      "MAXQ-Q (with state abstractions) converges to a recursively optimal policy much\n",
      "faster than flat Q learning. The fact that MAXQ learns a representation of the\n",
      "value function has an important benefit: it makes it possible to compute and\n",
      "execute an improved, non-hierarchical policy via a procedure similar to the\n",
      "policy improvement step of policy iteration. The paper demonstrates the\n",
      "effectiveness of this non-hierarchical execution experimentally. Finally, the\n",
      "paper concludes with a comparison to related work and a discussion of the\n",
      "design tradeoffs in hierarchical reinforcement learning.\n",
      "e-print metadata\n",
      "arxiv-id: cs/9905015v1\n",
      "Published: 1999-05-21T14:49:39Z\n",
      "Title:  State Abstraction in MAXQ Hierarchical Reinforcement Learning\n",
      "Last Author:  Thomas G. Dietterich\n",
      "Authors:  Thomas G. Dietterich\n",
      "abs page link: http://arxiv.org/abs/cs/9905015v1\n",
      "pdf link: http://arxiv.org/pdf/cs/9905015v1\n",
      "Journal reference: No journal ref found\n",
      "Comments: 7 pages, 2 figures\n",
      "Primary Category: cs.LG\n",
      "All Categories: cs.LG, I.2.6\n",
      "Abstract: Many researchers have explored methods for hierarchical reinforcement\n",
      "learning (RL) with temporal abstractions, in which abstract actions are defined\n",
      "that can perform many primitive actions before terminating. However, little is\n",
      "known about learning with state abstractions, in which aspects of the state\n",
      "space are ignored. In previous work, we developed the MAXQ method for\n",
      "hierarchical RL. In this paper, we define five conditions under which state\n",
      "abstraction can be combined with the MAXQ value function decomposition. We\n",
      "prove that the MAXQ-Q learning algorithm converges under these conditions and\n",
      "show experimentally that state abstraction is important for the successful\n",
      "application of MAXQ-Q learning.\n",
      "e-print metadata\n",
      "arxiv-id: cs/0001004v1\n",
      "Published: 2000-01-07T06:20:53Z\n",
      "Title:  Multiplicative Algorithm for Orthgonal Groups and Independent Component\n",
      "  Analysis\n",
      "Last Author:  Toshinao Akuzawa (RIKEN BSI)\n",
      "Authors:  Toshinao Akuzawa\n",
      "abs page link: http://arxiv.org/abs/cs/0001004v1\n",
      "pdf link: http://arxiv.org/pdf/cs/0001004v1\n",
      "Journal reference: No journal ref found\n",
      "Comments: 11 pages, 2 figures\n",
      "Primary Category: cs.LG\n",
      "All Categories: cs.LG, G.1.6\n",
      "Abstract: The multiplicative Newton-like method developed by the author et al. is\n",
      "extended to the situation where the dynamics is restricted to the orthogonal\n",
      "group. A general framework is constructed without specifying the cost function.\n",
      "Though the restriction to the orthogonal groups makes the problem somewhat\n",
      "complicated, an explicit expression for the amount of individual jumps is\n",
      "obtained. This algorithm is exactly second-order-convergent. The global\n",
      "instability inherent in the Newton method is remedied by a\n",
      "Levenberg-Marquardt-type variation. The method thus constructed can readily be\n",
      "applied to the independent component analysis. Its remarkable performance is\n",
      "illustrated by a numerical simulation.\n",
      "e-print metadata\n",
      "arxiv-id: cs/0002006v1\n",
      "Published: 2000-02-09T06:44:28Z\n",
      "Title:  Multiplicative Nonholonomic/Newton -like Algorithm\n",
      "Last Author:  Noboru Murata (RIKEN BSI)\n",
      "Authors:  Toshinao Akuzawa, Noboru Murata\n",
      "abs page link: http://arxiv.org/abs/cs/0002006v1\n",
      "pdf link: http://arxiv.org/pdf/cs/0002006v1\n",
      "Journal reference: No journal ref found\n",
      "Comments: 12 pages\n",
      "Primary Category: cs.LG\n",
      "All Categories: cs.LG, G.1.6\n",
      "Abstract: We construct new algorithms from scratch, which use the fourth order cumulant\n",
      "of stochastic variables for the cost function. The multiplicative updating rule\n",
      "here constructed is natural from the homogeneous nature of the Lie group and\n",
      "has numerous merits for the rigorous treatment of the dynamics. As one\n",
      "consequence, the second order convergence is shown. For the cost function,\n",
      "functions invariant under the componentwise scaling are choosen. By identifying\n",
      "points which can be transformed to each other by the scaling, we assume that\n",
      "the dynamics is in a coset space. In our method, a point can move toward any\n",
      "direction in this coset. Thus, no prewhitening is required.\n",
      "e-print metadata\n",
      "arxiv-id: cs/0009001v3\n",
      "Published: 2000-09-05T18:54:58Z\n",
      "Title:  Complexity analysis for algorithmically simple strings\n",
      "Last Author:  Andrei N. Soklakov (Royal Holloway, University of London)\n",
      "Authors:  Andrei N. Soklakov\n",
      "abs page link: http://arxiv.org/abs/cs/0009001v3\n",
      "pdf link: http://arxiv.org/pdf/cs/0009001v3\n",
      "Journal reference: No journal ref found\n",
      "Comments: 10 pages\n",
      "Primary Category: cs.LG\n",
      "All Categories: cs.LG, E.4; F.2; I.2\n",
      "Abstract: Given a reference computer, Kolmogorov complexity is a well defined function\n",
      "on all binary strings. In the standard approach, however, only the asymptotic\n",
      "properties of such functions are considered because they do not depend on the\n",
      "reference computer. We argue that this approach can be more useful if it is\n",
      "refined to include an important practical case of simple binary strings.\n",
      "Kolmogorov complexity calculus may be developed for this case if we restrict\n",
      "the class of available reference computers. The interesting problem is to\n",
      "define a class of computers which is restricted in a {\\it natural} way modeling\n",
      "the real-life situation where only a limited class of computers is physically\n",
      "available to us. We give an example of what such a natural restriction might\n",
      "look like mathematically, and show that under such restrictions some error\n",
      "terms, even logarithmic in complexity, can disappear from the standard\n",
      "complexity calculus.\n",
      "  Keywords: Kolmogorov complexity; Algorithmic information theory.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "python_arXiv_parsing_example.py\n",
    "\n",
    "This sample script illustrates a basic arXiv api call\n",
    "followed by parsing of the results using the \n",
    "feedparser python module.\n",
    "\n",
    "Please see the documentation at \n",
    "http://export.arxiv.org/api_help/docs/user-manual.html\n",
    "for more information, or email the arXiv api \n",
    "mailing list at arxiv-api@googlegroups.com.\n",
    "\n",
    "urllib is included in the standard python library.\n",
    "feedparser can be downloaded from http://feedparser.org/ .\n",
    "\n",
    "Author: Julius B. Lucks\n",
    "\n",
    "This is free software.  Feel free to do what you want\n",
    "with it, but please play nice with the arXiv API!\n",
    "\"\"\"\n",
    "\n",
    "# Base api query url\n",
    "base_url = 'http://export.arxiv.org/api/query?';\n",
    "\n",
    "# Search parameters\n",
    "search_query = 'cat:cs.LG' # search for electron in all fields\n",
    "start = 0                     # retreive the first 5 results\n",
    "max_results = 5\n",
    "\n",
    "query = 'search_query=%s&start=%i&max_results=%i' % (search_query,\n",
    "                                                     start,\n",
    "                                                     max_results)\n",
    "corpusEntry=[]\n",
    "corpusPDF=[]\n",
    "corpusID = []\n",
    "# Opensearch metadata such as totalResults, startIndex, \n",
    "# and itemsPerPage live in the opensearch namespase.\n",
    "# Some entry metadata lives in the arXiv namespace.\n",
    "# This is a hack to expose both of these namespaces in\n",
    "# feedparser v4.1\n",
    "feedparser._FeedParserMixin.namespaces['http://a9.com/-/spec/opensearch/1.1/'] = 'opensearch'\n",
    "feedparser._FeedParserMixin.namespaces['http://arxiv.org/schemas/atom'] = 'arxiv'\n",
    "\n",
    "# perform a GET request using the base_url and query\n",
    "with libreq.urlopen(base_url+query) as url:\n",
    "    response = url.read()\n",
    "\n",
    "# parse the response using feedparser\n",
    "feed = feedparser.parse(response)\n",
    "\n",
    "# print out feed information\n",
    "print ('Feed title: %s' % feed.feed.title)\n",
    "print ('Feed last updated: %s' % feed.feed.updated)\n",
    "\n",
    "# print opensearch metadata\n",
    "print ('totalResults for this query: %s' % feed.feed.opensearch_totalresults)\n",
    "print ('itemsPerPage for this query: %s' % feed.feed.opensearch_itemsperpage)\n",
    "print ('startIndex for this query: %s'   % feed.feed.opensearch_startindex)\n",
    "\n",
    "# Run through each entry, and print out information\n",
    "for entry in feed.entries:\n",
    "    corpusEntry.append(entry)\n",
    "    print ('e-print metadata')\n",
    "    print ('arxiv-id: %s' % entry.id.split('/abs/')[-1])\n",
    "    corpusID.append(entry.id.split('/abs/')[-1])\n",
    "    print ('Published: %s' % entry.published)\n",
    "    print ('Title:  %s' % entry.title)\n",
    "    \n",
    "    # feedparser v4.1 only grabs the first author\n",
    "    author_string = entry.author\n",
    "    \n",
    "    # grab the affiliation in <arxiv:affiliation> if present\n",
    "    # - this will only grab the first affiliation encountered\n",
    "    #   (the first affiliation for the first author)\n",
    "    # Please email the list with a way to get all of this information!\n",
    "    try:\n",
    "        author_string += ' (%s)' % entry.arxiv_affiliation\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    print ('Last Author:  %s' % author_string)\n",
    "    \n",
    "    # feedparser v5.0.1 correctly handles multiple authors, print them all\n",
    "    try:\n",
    "        print ('Authors:  %s' % ', '.join(author.name for author in entry.authors))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    # get the links to the abs page and pdf for this e-print\n",
    "    for link in entry.links:\n",
    "        if link.rel == 'alternate':\n",
    "            print ('abs page link: %s' % link.href)\n",
    "        elif link.title == 'pdf':\n",
    "            \n",
    "            corpusPDF.append({\"pdf_url\": link.href})\n",
    "            print ('pdf link: %s' % link.href)\n",
    "    \n",
    "    # The journal reference, comments and primary_category sections live under \n",
    "    # the arxiv namespace\n",
    "    try:\n",
    "        journal_ref = entry.arxiv_journal_ref\n",
    "    except AttributeError:\n",
    "        journal_ref = 'No journal ref found'\n",
    "    print ('Journal reference: %s' % journal_ref)\n",
    "    \n",
    "    try:\n",
    "        comment = entry.arxiv_comment\n",
    "    except AttributeError:\n",
    "        comment = 'No comment found'\n",
    "    print ('Comments: %s' % comment)\n",
    "    \n",
    "    # Since the <arxiv:primary_category> element has no data, only\n",
    "    # attributes, feedparser does not store anything inside\n",
    "    # entry.arxiv_primary_category\n",
    "    # This is a dirty hack to get the primary_category, just take the\n",
    "    # first element in entry.tags.  If anyone knows a better way to do\n",
    "    # this, please email the list!\n",
    "    print ('Primary Category: %s' % entry.tags[0]['term'])\n",
    "    \n",
    "    # Lets get all the categories\n",
    "    all_categories = [t['term'] for t in entry.tags]\n",
    "    print ('All Categories: %s' % (', ').join(all_categories))\n",
    "    \n",
    "    # The abstract is in the <summary> element\n",
    "    print ('Abstract: %s' %  entry.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'arxiv_comment': '63 pages, 15 figures',\n",
       "  'arxiv_primary_category': {'scheme': 'http://arxiv.org/schemas/atom',\n",
       "   'term': 'cs.LG'},\n",
       "  'author': 'Thomas G. Dietterich',\n",
       "  'author_detail': {'name': 'Thomas G. Dietterich'},\n",
       "  'authors': [{'name': 'Thomas G. Dietterich'}],\n",
       "  'guidislink': True,\n",
       "  'id': 'http://arxiv.org/abs/cs/9905014v1',\n",
       "  'link': 'http://arxiv.org/abs/cs/9905014v1',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/cs/9905014v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'href': 'http://arxiv.org/pdf/cs/9905014v1',\n",
       "    'rel': 'related',\n",
       "    'title': 'pdf',\n",
       "    'type': 'application/pdf'}],\n",
       "  'published': '1999-05-21T14:26:07Z',\n",
       "  'published_parsed': time.struct_time(tm_year=1999, tm_mon=5, tm_mday=21, tm_hour=14, tm_min=26, tm_sec=7, tm_wday=4, tm_yday=141, tm_isdst=0),\n",
       "  'summary': 'This paper presents the MAXQ approach to hierarchical reinforcement learning\\nbased on decomposing the target Markov decision process (MDP) into a hierarchy\\nof smaller MDPs and decomposing the value function of the target MDP into an\\nadditive combination of the value functions of the smaller MDPs. The paper\\ndefines the MAXQ hierarchy, proves formal results on its representational\\npower, and establishes five conditions for the safe use of state abstractions.\\nThe paper presents an online model-free learning algorithm, MAXQ-Q, and proves\\nthat it converges wih probability 1 to a kind of locally-optimal policy known\\nas a recursively optimal policy, even in the presence of the five kinds of\\nstate abstraction. The paper evaluates the MAXQ representation and MAXQ-Q\\nthrough a series of experiments in three domains and shows experimentally that\\nMAXQ-Q (with state abstractions) converges to a recursively optimal policy much\\nfaster than flat Q learning. The fact that MAXQ learns a representation of the\\nvalue function has an important benefit: it makes it possible to compute and\\nexecute an improved, non-hierarchical policy via a procedure similar to the\\npolicy improvement step of policy iteration. The paper demonstrates the\\neffectiveness of this non-hierarchical execution experimentally. Finally, the\\npaper concludes with a comparison to related work and a discussion of the\\ndesign tradeoffs in hierarchical reinforcement learning.',\n",
       "  'summary_detail': {'base': '',\n",
       "   'language': None,\n",
       "   'type': 'text/plain',\n",
       "   'value': 'This paper presents the MAXQ approach to hierarchical reinforcement learning\\nbased on decomposing the target Markov decision process (MDP) into a hierarchy\\nof smaller MDPs and decomposing the value function of the target MDP into an\\nadditive combination of the value functions of the smaller MDPs. The paper\\ndefines the MAXQ hierarchy, proves formal results on its representational\\npower, and establishes five conditions for the safe use of state abstractions.\\nThe paper presents an online model-free learning algorithm, MAXQ-Q, and proves\\nthat it converges wih probability 1 to a kind of locally-optimal policy known\\nas a recursively optimal policy, even in the presence of the five kinds of\\nstate abstraction. The paper evaluates the MAXQ representation and MAXQ-Q\\nthrough a series of experiments in three domains and shows experimentally that\\nMAXQ-Q (with state abstractions) converges to a recursively optimal policy much\\nfaster than flat Q learning. The fact that MAXQ learns a representation of the\\nvalue function has an important benefit: it makes it possible to compute and\\nexecute an improved, non-hierarchical policy via a procedure similar to the\\npolicy improvement step of policy iteration. The paper demonstrates the\\neffectiveness of this non-hierarchical execution experimentally. Finally, the\\npaper concludes with a comparison to related work and a discussion of the\\ndesign tradeoffs in hierarchical reinforcement learning.'},\n",
       "  'tags': [{'label': None,\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'term': 'cs.LG'},\n",
       "   {'label': None,\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'term': 'I.2.6'}],\n",
       "  'title': 'Hierarchical Reinforcement Learning with the MAXQ Value Function\\n  Decomposition',\n",
       "  'title_detail': {'base': '',\n",
       "   'language': None,\n",
       "   'type': 'text/plain',\n",
       "   'value': 'Hierarchical Reinforcement Learning with the MAXQ Value Function\\n  Decomposition'},\n",
       "  'updated': '1999-05-21T14:26:07Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=1999, tm_mon=5, tm_mday=21, tm_hour=14, tm_min=26, tm_sec=7, tm_wday=4, tm_yday=141, tm_isdst=0)},\n",
       " {'arxiv_comment': '7 pages, 2 figures',\n",
       "  'arxiv_primary_category': {'scheme': 'http://arxiv.org/schemas/atom',\n",
       "   'term': 'cs.LG'},\n",
       "  'author': 'Thomas G. Dietterich',\n",
       "  'author_detail': {'name': 'Thomas G. Dietterich'},\n",
       "  'authors': [{'name': 'Thomas G. Dietterich'}],\n",
       "  'guidislink': True,\n",
       "  'id': 'http://arxiv.org/abs/cs/9905015v1',\n",
       "  'link': 'http://arxiv.org/abs/cs/9905015v1',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/cs/9905015v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'href': 'http://arxiv.org/pdf/cs/9905015v1',\n",
       "    'rel': 'related',\n",
       "    'title': 'pdf',\n",
       "    'type': 'application/pdf'}],\n",
       "  'published': '1999-05-21T14:49:39Z',\n",
       "  'published_parsed': time.struct_time(tm_year=1999, tm_mon=5, tm_mday=21, tm_hour=14, tm_min=49, tm_sec=39, tm_wday=4, tm_yday=141, tm_isdst=0),\n",
       "  'summary': 'Many researchers have explored methods for hierarchical reinforcement\\nlearning (RL) with temporal abstractions, in which abstract actions are defined\\nthat can perform many primitive actions before terminating. However, little is\\nknown about learning with state abstractions, in which aspects of the state\\nspace are ignored. In previous work, we developed the MAXQ method for\\nhierarchical RL. In this paper, we define five conditions under which state\\nabstraction can be combined with the MAXQ value function decomposition. We\\nprove that the MAXQ-Q learning algorithm converges under these conditions and\\nshow experimentally that state abstraction is important for the successful\\napplication of MAXQ-Q learning.',\n",
       "  'summary_detail': {'base': '',\n",
       "   'language': None,\n",
       "   'type': 'text/plain',\n",
       "   'value': 'Many researchers have explored methods for hierarchical reinforcement\\nlearning (RL) with temporal abstractions, in which abstract actions are defined\\nthat can perform many primitive actions before terminating. However, little is\\nknown about learning with state abstractions, in which aspects of the state\\nspace are ignored. In previous work, we developed the MAXQ method for\\nhierarchical RL. In this paper, we define five conditions under which state\\nabstraction can be combined with the MAXQ value function decomposition. We\\nprove that the MAXQ-Q learning algorithm converges under these conditions and\\nshow experimentally that state abstraction is important for the successful\\napplication of MAXQ-Q learning.'},\n",
       "  'tags': [{'label': None,\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'term': 'cs.LG'},\n",
       "   {'label': None,\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'term': 'I.2.6'}],\n",
       "  'title': 'State Abstraction in MAXQ Hierarchical Reinforcement Learning',\n",
       "  'title_detail': {'base': '',\n",
       "   'language': None,\n",
       "   'type': 'text/plain',\n",
       "   'value': 'State Abstraction in MAXQ Hierarchical Reinforcement Learning'},\n",
       "  'updated': '1999-05-21T14:49:39Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=1999, tm_mon=5, tm_mday=21, tm_hour=14, tm_min=49, tm_sec=39, tm_wday=4, tm_yday=141, tm_isdst=0)},\n",
       " {'arxiv_affiliation': 'RIKEN BSI',\n",
       "  'arxiv_comment': '11 pages, 2 figures',\n",
       "  'arxiv_primary_category': {'scheme': 'http://arxiv.org/schemas/atom',\n",
       "   'term': 'cs.LG'},\n",
       "  'author': 'Toshinao Akuzawa',\n",
       "  'author_detail': {'name': 'Toshinao Akuzawa'},\n",
       "  'authors': [{'name': 'Toshinao Akuzawa'}],\n",
       "  'guidislink': True,\n",
       "  'id': 'http://arxiv.org/abs/cs/0001004v1',\n",
       "  'link': 'http://arxiv.org/abs/cs/0001004v1',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/cs/0001004v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'href': 'http://arxiv.org/pdf/cs/0001004v1',\n",
       "    'rel': 'related',\n",
       "    'title': 'pdf',\n",
       "    'type': 'application/pdf'}],\n",
       "  'published': '2000-01-07T06:20:53Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2000, tm_mon=1, tm_mday=7, tm_hour=6, tm_min=20, tm_sec=53, tm_wday=4, tm_yday=7, tm_isdst=0),\n",
       "  'summary': 'The multiplicative Newton-like method developed by the author et al. is\\nextended to the situation where the dynamics is restricted to the orthogonal\\ngroup. A general framework is constructed without specifying the cost function.\\nThough the restriction to the orthogonal groups makes the problem somewhat\\ncomplicated, an explicit expression for the amount of individual jumps is\\nobtained. This algorithm is exactly second-order-convergent. The global\\ninstability inherent in the Newton method is remedied by a\\nLevenberg-Marquardt-type variation. The method thus constructed can readily be\\napplied to the independent component analysis. Its remarkable performance is\\nillustrated by a numerical simulation.',\n",
       "  'summary_detail': {'base': '',\n",
       "   'language': None,\n",
       "   'type': 'text/plain',\n",
       "   'value': 'The multiplicative Newton-like method developed by the author et al. is\\nextended to the situation where the dynamics is restricted to the orthogonal\\ngroup. A general framework is constructed without specifying the cost function.\\nThough the restriction to the orthogonal groups makes the problem somewhat\\ncomplicated, an explicit expression for the amount of individual jumps is\\nobtained. This algorithm is exactly second-order-convergent. The global\\ninstability inherent in the Newton method is remedied by a\\nLevenberg-Marquardt-type variation. The method thus constructed can readily be\\napplied to the independent component analysis. Its remarkable performance is\\nillustrated by a numerical simulation.'},\n",
       "  'tags': [{'label': None,\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'term': 'cs.LG'},\n",
       "   {'label': None,\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'term': 'G.1.6'}],\n",
       "  'title': 'Multiplicative Algorithm for Orthgonal Groups and Independent Component\\n  Analysis',\n",
       "  'title_detail': {'base': '',\n",
       "   'language': None,\n",
       "   'type': 'text/plain',\n",
       "   'value': 'Multiplicative Algorithm for Orthgonal Groups and Independent Component\\n  Analysis'},\n",
       "  'updated': '2000-01-07T06:20:53Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2000, tm_mon=1, tm_mday=7, tm_hour=6, tm_min=20, tm_sec=53, tm_wday=4, tm_yday=7, tm_isdst=0)},\n",
       " {'arxiv_affiliation': 'RIKEN BSI',\n",
       "  'arxiv_comment': '12 pages',\n",
       "  'arxiv_doi': '10.1016/S0960-0779(00)00077-1',\n",
       "  'arxiv_primary_category': {'scheme': 'http://arxiv.org/schemas/atom',\n",
       "   'term': 'cs.LG'},\n",
       "  'author': 'Noboru Murata',\n",
       "  'author_detail': {'name': 'Noboru Murata'},\n",
       "  'authors': [{'name': 'Toshinao Akuzawa'}, {'name': 'Noboru Murata'}],\n",
       "  'guidislink': True,\n",
       "  'id': 'http://arxiv.org/abs/cs/0002006v1',\n",
       "  'link': 'http://arxiv.org/abs/cs/0002006v1',\n",
       "  'links': [{'href': 'http://dx.doi.org/10.1016/S0960-0779(00)00077-1',\n",
       "    'rel': 'related',\n",
       "    'title': 'doi',\n",
       "    'type': 'text/html'},\n",
       "   {'href': 'http://arxiv.org/abs/cs/0002006v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'href': 'http://arxiv.org/pdf/cs/0002006v1',\n",
       "    'rel': 'related',\n",
       "    'title': 'pdf',\n",
       "    'type': 'application/pdf'}],\n",
       "  'published': '2000-02-09T06:44:28Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2000, tm_mon=2, tm_mday=9, tm_hour=6, tm_min=44, tm_sec=28, tm_wday=2, tm_yday=40, tm_isdst=0),\n",
       "  'summary': 'We construct new algorithms from scratch, which use the fourth order cumulant\\nof stochastic variables for the cost function. The multiplicative updating rule\\nhere constructed is natural from the homogeneous nature of the Lie group and\\nhas numerous merits for the rigorous treatment of the dynamics. As one\\nconsequence, the second order convergence is shown. For the cost function,\\nfunctions invariant under the componentwise scaling are choosen. By identifying\\npoints which can be transformed to each other by the scaling, we assume that\\nthe dynamics is in a coset space. In our method, a point can move toward any\\ndirection in this coset. Thus, no prewhitening is required.',\n",
       "  'summary_detail': {'base': '',\n",
       "   'language': None,\n",
       "   'type': 'text/plain',\n",
       "   'value': 'We construct new algorithms from scratch, which use the fourth order cumulant\\nof stochastic variables for the cost function. The multiplicative updating rule\\nhere constructed is natural from the homogeneous nature of the Lie group and\\nhas numerous merits for the rigorous treatment of the dynamics. As one\\nconsequence, the second order convergence is shown. For the cost function,\\nfunctions invariant under the componentwise scaling are choosen. By identifying\\npoints which can be transformed to each other by the scaling, we assume that\\nthe dynamics is in a coset space. In our method, a point can move toward any\\ndirection in this coset. Thus, no prewhitening is required.'},\n",
       "  'tags': [{'label': None,\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'term': 'cs.LG'},\n",
       "   {'label': None,\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'term': 'G.1.6'}],\n",
       "  'title': 'Multiplicative Nonholonomic/Newton -like Algorithm',\n",
       "  'title_detail': {'base': '',\n",
       "   'language': None,\n",
       "   'type': 'text/plain',\n",
       "   'value': 'Multiplicative Nonholonomic/Newton -like Algorithm'},\n",
       "  'updated': '2000-02-09T06:44:28Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2000, tm_mon=2, tm_mday=9, tm_hour=6, tm_min=44, tm_sec=28, tm_wday=2, tm_yday=40, tm_isdst=0)},\n",
       " {'arxiv_affiliation': 'Royal Holloway, University of London',\n",
       "  'arxiv_comment': '10 pages',\n",
       "  'arxiv_primary_category': {'scheme': 'http://arxiv.org/schemas/atom',\n",
       "   'term': 'cs.LG'},\n",
       "  'author': 'Andrei N. Soklakov',\n",
       "  'author_detail': {'name': 'Andrei N. Soklakov'},\n",
       "  'authors': [{'name': 'Andrei N. Soklakov'}],\n",
       "  'guidislink': True,\n",
       "  'id': 'http://arxiv.org/abs/cs/0009001v3',\n",
       "  'link': 'http://arxiv.org/abs/cs/0009001v3',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/cs/0009001v3',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'href': 'http://arxiv.org/pdf/cs/0009001v3',\n",
       "    'rel': 'related',\n",
       "    'title': 'pdf',\n",
       "    'type': 'application/pdf'}],\n",
       "  'published': '2000-09-05T18:54:58Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2000, tm_mon=9, tm_mday=5, tm_hour=18, tm_min=54, tm_sec=58, tm_wday=1, tm_yday=249, tm_isdst=0),\n",
       "  'summary': 'Given a reference computer, Kolmogorov complexity is a well defined function\\non all binary strings. In the standard approach, however, only the asymptotic\\nproperties of such functions are considered because they do not depend on the\\nreference computer. We argue that this approach can be more useful if it is\\nrefined to include an important practical case of simple binary strings.\\nKolmogorov complexity calculus may be developed for this case if we restrict\\nthe class of available reference computers. The interesting problem is to\\ndefine a class of computers which is restricted in a {\\\\it natural} way modeling\\nthe real-life situation where only a limited class of computers is physically\\navailable to us. We give an example of what such a natural restriction might\\nlook like mathematically, and show that under such restrictions some error\\nterms, even logarithmic in complexity, can disappear from the standard\\ncomplexity calculus.\\n  Keywords: Kolmogorov complexity; Algorithmic information theory.',\n",
       "  'summary_detail': {'base': '',\n",
       "   'language': None,\n",
       "   'type': 'text/plain',\n",
       "   'value': 'Given a reference computer, Kolmogorov complexity is a well defined function\\non all binary strings. In the standard approach, however, only the asymptotic\\nproperties of such functions are considered because they do not depend on the\\nreference computer. We argue that this approach can be more useful if it is\\nrefined to include an important practical case of simple binary strings.\\nKolmogorov complexity calculus may be developed for this case if we restrict\\nthe class of available reference computers. The interesting problem is to\\ndefine a class of computers which is restricted in a {\\\\it natural} way modeling\\nthe real-life situation where only a limited class of computers is physically\\navailable to us. We give an example of what such a natural restriction might\\nlook like mathematically, and show that under such restrictions some error\\nterms, even logarithmic in complexity, can disappear from the standard\\ncomplexity calculus.\\n  Keywords: Kolmogorov complexity; Algorithmic information theory.'},\n",
       "  'tags': [{'label': None,\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'term': 'cs.LG'},\n",
       "   {'label': None,\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'term': 'E.4; F.2; I.2'}],\n",
       "  'title': 'Complexity analysis for algorithmically simple strings',\n",
       "  'title_detail': {'base': '',\n",
       "   'language': None,\n",
       "   'type': 'text/plain',\n",
       "   'value': 'Complexity analysis for algorithmically simple strings'},\n",
       "  'updated': '2002-02-26T01:51:09Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2002, tm_mon=2, tm_mday=26, tm_hour=1, tm_min=51, tm_sec=9, tm_wday=1, tm_yday=57, tm_isdst=0)}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusEntry #This is the full entry, can access id, link, when updated etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pdf_url': 'http://arxiv.org/pdf/cs/9905014v1'},\n",
       " {'pdf_url': 'http://arxiv.org/pdf/cs/9905015v1'},\n",
       " {'pdf_url': 'http://arxiv.org/pdf/cs/0001004v1'},\n",
       " {'pdf_url': 'http://arxiv.org/pdf/cs/0002006v1'},\n",
       " {'pdf_url': 'http://arxiv.org/pdf/cs/0009001v3'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusPDF #This is the forced PDF link as a pdf_url item for the download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Al\\\\Documents\\\\ByteSizeArxiv\\\\library/0001004v1.pdf'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Override the default filename format by defining a slugify function. So can force pdf link for all even without listed\n",
    "arxiv.download(corpusPDF[2],r'C:\\Users\\Al\\Documents\\ByteSizeArxiv\\library', slugify=lambda x: corpusEntry[2].get('id').split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Read PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=pdfminer.high_level.extract_text('C:\\\\Users\\\\Al\\\\Documents\\\\ByteSizeArxiv\\\\library/0001004v1.pdf', codec='utf-8', laparams=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0\\n0\\n0\\n2\\n \\nn\\na\\nJ\\n \\n7\\n \\n \\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n \\n \\n1\\nv\\n4\\n0\\n0\\n1\\n0\\n0\\n0\\n/\\ns\\nc\\n:\\nv\\ni\\nX\\nr\\na\\n\\nMultiplicative Algorithm for Orthgonal Groups\\nand Independent Component Analysis\\n\\nToshinao Akuzawa∗\\n\\nBrain Science Institute\\nRIKEN\\n2-1 Hirosawa, Wako, Saitama 351-0198, Japan\\n\\nOctober 26, 2018\\n\\nAbstract\\n\\nThe multiplicative Newton-like method developed by the author et al. is extended to the situa-\\ntion where the dynamics is restricted to the orthogonal group. A general framework is constructed\\nwithout specifying the cost function. Though the restriction to the orthogonal groups makes the\\nproblem somewhat complicated, an explicit expression for the amount of individual jumps is ob-\\ntained. This algorithm is exactly second-order-convergent. The global instability inherent in the\\nNewton method is remedied by a Levenberg-Marquardt-type variation. The method thus con-\\nstructed can readily be applied to the independent component analysis. Its remarkable performance\\nis illustrated by a numerical simulation.\\n\\n1 Overview\\n\\nMany optimization problems take the form, “Find an optimal matrix under the constraints (1)..\\netc.” Some of these can be considered as optimizations on Lie groups. For groups, the\\n(2)..\\nIn considera-\\nfundamental manipulation is a multiplication whereas an addition is unnatural.\\ntion of this fact, we have constructed a multiplicative Newton-like algorithm for maximizing the\\n\\n∗akuzawa@brain.riken.go.jp\\n\\n1\\n\\n\\x0ckurtosis (a good barometer for the independence) in [T.Akuzawa & N.Murata,1999]. There the\\ndynamics takes place on the coset GL(1, R)N \\\\GL(N, R). We can apply the techniques devel-\\noped in [T.Akuzawa & N.Murata,1999] to many other optimization problems. The coset structure\\nGL(1, R)N \\\\GL(N, R) is, however, characteristic of the independent component analysis(ICA). It is\\nunderstood by the fact that the independence is nothing to do with the scaling. The redundancy\\nresulting from the invariance of the model under the componentwise scaling must be eliminated for\\na rigorous discussion and this redundancy corresponds to GL(1, R)N .\\n\\nAnother way to eliminate this redundancy is the prewhitening. The prewhitening is a linear\\ntransformation of the observed data which maps the covariance matrix to the unit matrix. If we\\ndeal with prewhitened data, we can legitimately narrow the sweeping range to the orthogonal group.\\nThe aim of this letter is the construction of a multiplicative algorithm for the orthogonal groups.\\nThe framework is as follows. N -dimensional prewhitened random variables {Xi|1 ≤ i ≤ N }\\nare available and it is anticipated that their origins are some unknown mutually independent com-\\nponents {Y ∗i |1 ≤ i ≤ N }. The goal of the ICA is the map {Xi} 7→ {Y ∗i }. We restrict ourselves\\nto the linear independent component analysis. There we want to ﬁnd a linear transformation\\n7→ Y ∗ = (Y ∗1 , · · · , Y ∗N )′ = C ∗X which minimizes some cost function\\nC ∗ : X = (X1, · · · , XN )′\\nthat measures the independence. Since we are assuming that the data is already prewhitened, the\\ncovariance matrix of X is the N × N unit matrix. If we do not take into account errors in the\\nprewhitening, the optimal point C ∗ must belong to O(N ).\\n\\nGiving up the analytical solution, we consider a sequence,\\n\\nC(0), C(1), C(2), C(3),\\n\\n· · · · · · ,\\n\\n(1.1)\\n\\nwhich converges to the optimal solution C ∗. The sequence {C(t)} is generated by the left-\\nmultiplication of another sequence of orthogonal matrices {D(t)}. Each D(t) is speciﬁed by the\\ncoordinate ∆(t) which satisﬁes D(t) = e∆(t). We assume that ∆(t) is an N × N skew-symmetric\\nmatrix, which implies that D(t) belongs to the identity component of O(N ). In practice the pro-\\ncedure is as follows. As an initial condition we set C(0). For t > 0 (t ∈ N+), we introduce ∆(t)\\nand denote C(t) as C(t + 1) = e∆(t)C(t). Under these settings, we determine ∆(t) by using the\\nNewton method with respect to the matrix elements of ∆(t). That is, we evaluate the cost function\\nat C(t + 1) by expanding it around C(t) in terms of the elements of ∆(t) up to the second order.\\nThen ∆(t) is choosen as the (unique) critical point of this second order expansion. We iteratively\\nfollow these procedures until we obtain a satisfactory solution.\\n\\nThis letter is organized as follows. In Section 2 we will give a complete description of a new\\nmultiplicative updating method for the orthogonal groups. This section is the main part of this\\nletter. Since our formulation does not depend on the details of the cost function the method can\\nbe useful for many problems other than the ICA. The performance of our method including the\\nsecond-order-convergence is discussed in Section 3. Section 4 is a survey of possible applications\\nof our method. The algorithm constructed in Section 2 is considered as a pure-Newton method\\non the orthogonal groups. To achive the global convergence, we must modify the method. This is\\n\\n2\\n\\n\\x0caccomplished in Section 5. Section 5 also includes a numerical examination of the performance of\\nour method. Section 6 is a summary.\\n\\n2 Multiplicative updating on O(N )\\n\\nWe assume that the cost function F takes the form,\\n\\nF (Y ) =\\n\\nE(fi(Yi)) ,\\n\\nN\\n\\ni=1\\nX\\n\\nwhere each fi : R → R is an unspeciﬁed function. Through this letter we denote by E(·) the\\nexpectation. We will determine the concrete procedures after the Newton manner. First, we\\nintroduce maps, R and {Ui(1 ≤ i ≤ N )}’s, from N -dimensional dataset to N × N matrices by\\n\\nand\\n\\n[R(Y )]ki = E\\n\\n∂fi(Yi)\\n∂Yi\\n\\nYk\\n\\n(cid:19)\\n\\n(cid:18)\\n\\n[Ui(Y )]kl = Uikl(Y ) = E\\n\\n∂2fi(Yi)\\n∂Y 2\\ni\\n\\n(cid:18)\\n\\nYkYl\\n\\n.\\n\\n(cid:19)\\n\\nThe goal is the construction of a sequence {Y (t)} of the estimates of the independent components,\\nwhich converges to the optimal point Y ∗. Within the framework of the linear analysis, we consider\\nthat this sequence is derived from another sequence {C(t)} of the linear transformation by the\\nrelation Y (t) = C(t)X, where X are the original data. Thus if we restate the problem, the task is\\nto determine a sequence {C(t)}. We assume that for each t ∈ N+ the estimates of the independent\\ncomponents at time t and and the estimates at time t + 1 are related by\\n\\nor equivalently\\n\\nY (t + 1) = D(t)Y (t)\\n\\nC(t + 1) = D(t)C(t) ,\\n\\nwhere D(t) is some orthogonal matrix to be ﬁxed. Our method is characterized by this left-\\nmultiplicative updating rule. As mentioned in the previous section, we assume that each D(t)\\nalways belongs to the identity component of the orthogonal group O(N ). This assumption is\\nreasonable, for example, if the original data X are already prewhitened in the case of the ICA.\\nAnyway, under this restriction D(t) is speciﬁed by an N × N anti-symmetric matrix ∆(t), which\\nsatisﬁes\\n\\nexp(∆(t)) = D(t) .\\n\\n(2.6)\\n\\n3\\n\\n(2.1)\\n\\n(2.2)\\n\\n(2.3)\\n\\n(2.4)\\n\\n(2.5)\\n\\n\\x0cFor brevity’s sake we will omit the argument (t) and denote Y (t + 1) by Z. F (Z) is expanded in\\nterms of {∆ij} as\\n\\nF (Z) = F (Y ) + tr(∆R(Y )) + tr\\n\\nR(Y )\\n\\n+\\n\\n∆ik∆ilUikl(Y ) + O(∆3) .\\n\\n(2.7)\\n\\n∆2\\n2\\n\\n(cid:18)\\n\\n1\\n2\\n\\n(cid:19)\\n\\nXi,k,l\\n\\nThrough the letter we denote by O(∆k) polynomials of matrix elements of ∆ which does not contain\\nterms with degrees less than k. Do not confuse this with the symbol for the orthogonal groups such\\nas O(N ). As in the usual Newton method, we truncate the expansion (2.7) at the second order\\nwith respect to {∆ij}. Then ∆ in this step is determined as the coordinate of the critical point\\nof this truncated expansion. The partial derivative of (2.7) is more convenient for the purpose. It\\nreads\\n\\n∂F (Z)\\n∂∆kl\\n\\n1\\n2\\n\\n= Rlk +\\n\\n[∆R + R∆]lk +\\n\\n∆kpUklp + O(∆2) ,\\n\\np\\nX\\n\\nwhere we have omitted the argument Y for R and U . Now let us introduce a map cs (the column\\nstring) as in the previous article [T.Akuzawa & N.Murata,1999]:\\n\\nMat(N, F) → FN 2\\n\\n· · · A1N\\nA11 A12\\nA21\\n. . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . AN N\\nAN 1\\n\\n\\uf8f6\\n\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n\\nA = \\uf8eb\\n\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n\\n7→ cs(A) = (A11 A21 · · · AN 1 A12 A22 · · · AN N )′ ,\\n\\nwhere Mat(N, F) is N × N matrices on some unspeciﬁed ﬁeld F. We denote by the upper subscript\\n′ the transposition and by † the complex conjugate. For the orthogonal groups it is rather simple\\nto move to the framework of the column string as compared to the case of GL(1, R)N \\\\GL(N, R):\\nBy neglecting O(∆2) terms, the right-hand-side of (2.8) is straightforwardly rewritten as\\n\\nRlk +\\n\\n[∆R + R∆]lk +\\n\\n∆kpUklp\\n\\n1\\n2\\n\\n\"\\n\\nL\\n\\np\\nX\\n\\n1\\n2\\n\\n(cid:0)\\n\\n=\\n\\ncs(R) +\\n\\nR′ ⊗ IN + IN ⊗ R\\n\\ncs(∆) +\\n\\nUk\\n\\nT cs(∆)\\n\\n,\\n\\n(2.10)\\n\\n(cid:1)\\n\\n(cid:0) Mk\\n\\n(cid:1)\\n\\n#l+(k\\n\\n1)N\\n\\n−\\n\\nwhere the symbol “\\n\\n” stands for the direct sum,\\n\\n(2.8)\\n\\n(2.9)\\n\\n(2.11)\\n\\n· · · · · ·\\n\\nU2 0\\n\\n0\\n· · · · · ·\\n\\nU1 0\\n0\\n. . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . .\\n0\\n0\\n\\n· · · · · · UN\\n−\\n· · · · · ·\\n0\\n\\n1 0\\n\\nUN\\n\\n,\\n\\n\\uf8f6\\n\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n\\nUk =\\n\\nN\\n\\nk=1\\nM\\n\\n\\uf8eb\\n\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n\\n4\\n\\n\\x0cT is an N 2 × N 2 matrix deﬁned by\\n\\ncs(A′) = T cs(A) for A ∈ Mat(N, F) ,\\n\\nand IN is the N ×N unit matrix. We denote the tensor product by ⊗ as usual. The “transposition”\\nT is also considered as an intertwiner between two equivalent representations:\\n\\nThe orthogonal group O(N ) has less degrees of freedom than the general linear group. The canonical\\nbasis of the Lie algebra, o(N ), of O(N ) is N (N − 1)/2 anti-symmetric matrices. We will introduce\\nsome operators which enable us to move to the coordinates based on the canonical basis on o(N ).\\nIn the ﬁrst place, we introduce an N 2 × N 2 matrix H by\\n\\nT (A ⊗ B)T = B ⊗ A .\\n\\nH =\\n\\nH (i,j) ,\\n\\ni>j\\nX\\n\\nwhere H (i,j) is a π/4 rotation between the j + N (i − 1)-th component and the i + N (j − 1)-th\\ncomponent:\\n\\nH (i,j)\\n\\nkl =\\n\\n1\\n√2\\n− 1\\n√2\\n1\\n√2\\n1\\n√2\\n0\\n\\n\\uf8f1\\n\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\n\\nThe projection operator PD,\\n\\nfor k = j + N (i − 1),\\nfor k = j + N (i − 1),\\nfor k = i + N (j − 1),\\nfor k = i + N (j − 1),\\n\\nl = j + M (i − 1)\\nl = i + M (j − 1)\\nl = j + M (i − 1)\\nl = i + M (j − 1)\\n\\notherwise.\\n\\nPD = diag(p1, · · · , pN 2) ,\\npk = 1\\npk = 0\\n\\n(cid:26)\\n\\nfor k = N (i − 1) + i, 1 ≤ i ≤ N\\notherwise ,\\n\\nis used to extract the diagonal elements of a matrix from its image by cs. Then the coordinate\\ntransformation is realized by a multiplication of\\n\\nto column string vectors. We need to introduce two more projection operators PS and PA deﬁned\\nby\\n\\nPS = diag(p1, p2, · · · , pN 2)\\nPA = diag(1 − p1, 1 − p2, · · · , 1 − pN 2) ,\\n\\nH + PD\\n\\n5\\n\\n(2.12)\\n\\n(2.13)\\n\\n(2.14)\\n\\n(2.15)\\n\\n(2.16)\\n\\n(2.17)\\n\\n(2.18)\\n\\n(2.19)\\n\\n\\x0cwhere\\n\\n1 if\\n0\\n\\n∃(i, j);\\notherwise.\\n\\npk =\\n\\n(cid:26)\\n\\nj ≤ i and k = i + N (j − 1)\\n\\n(2.20)\\n\\nBy the left-action of PS and PA to column string vectors rotated by H + PD we can extract,\\nrespectively, the symmetric components and the anti-symmetric components of the matrices. Then\\nthe conditions for the critical point of the second-order-expansion, which must be satisﬁed by ∆,\\nare translated into the following two conditions. First, symmetric components of ∆ must vanish.\\nThis condition is expressed as\\n\\n[(H + PD)cs(∆)]j+(i\\n\\n1)N = 0\\n\\n−\\n\\nfor\\n\\ni ≤ j\\n\\n⇐⇒ PS(H + PD)cs(∆) = 0\\n\\n.\\n\\n(2.21)\\n\\n(cid:19)\\n\\nSecondly, for the anti-symmetric components the condition for the critical point is transformed to\\n\\n[(H + PD)cs(R) + (H + PD)W cs(∆)]j+(i\\n\\n1)N = 0\\n\\nfor\\n\\ni > j ,\\n\\n(2.22)\\n\\n(cid:18)\\n\\n−\\n\\nwhere we have set\\n\\nW =\\n\\nR′ ⊗ IN + IN ⊗ R\\n\\n+\\n\\nUk\\n\\nT .\\n\\n(2.23)\\n\\n(cid:1)\\nThe conditions (2.21) and (2.22) are combined into an equation,\\n\\n(cid:0)\\n\\n(cid:1)\\n\\n(cid:0) Mk\\n\\nPA(H + PD)cs(R) +\\n\\nPA(H + PD)W (H + PD)′PA + PS\\n\\n(H + PD)cs(∆) = 0 .\\n\\n(2.24)\\n\\n1\\n2\\n\\n(cid:20)\\n\\nNote that\\n\\nPA(H + PD) = PAH .\\n\\n(2.25)\\n\\nThe optimal ∆ is immediately obtained from (2.24):\\n\\ncs(∆) = −(H + PD)′\\n\\nPA(H + PD)W (H + PD)′PA + PS\\n\\nPA(H + PD)cs(R)\\n\\n= −H ′\\n\\nPAHW H ′PA + PS\\n\\n1 PAHcs(R) .\\n\\n−\\n\\n(cid:20)\\n\\n(2.26)\\n\\nThus we have obtained the explicit updating rule. By iterating the procedure in this section from\\na starting point suﬃciently close to the optimal one, the sequences {C(t)} and {Y (t)} converge to\\nthe optimal solutions.\\n\\n(cid:0)\\n\\n(cid:1)\\n\\n(cid:21)\\n\\n1\\n\\n−\\n\\n(cid:21)\\n\\n6\\n\\n\\x0c3 Performance (theoretical aspects)\\n\\nThe second-order-convergence is one of the main advantages of this method.\\nIndeed, this algo-\\nrithm is rigorously second-order-convergent. The proof can be given almost in the same way as in\\n[T.Akuzawa & N.Murata,1999]. So we omit the proof in this letter.\\n\\nSometimes we have to deal with large matrices to apply the technique here constructed. Let us\\nexamine the situation. The N 2 × N 2 matrix PAHW H ′PA + PS is a direct sum of an N (N − 1)/2 ×\\nN (N − 1)/2 matrix and an N (N + 1)/2 × N (N + 1)/2 unit matrix. Within the N (N − 1)/2 ×\\nN (N − 1)/2 block the number of non-zero oﬀ-diagonal elements is no more than N (N − 1)(N − 2).\\nSo this is a very sparse matrix when N becomes large. Of course if N becomes extremely large,\\nour method requires quite large memories. But due to the sparseness, it remains to be a practical\\ntool for problems with considerably large N .\\n\\n0\\n\\n10\\n\\n20\\n\\n30\\n\\n40\\n\\n60\\n\\n70\\n\\n80\\n\\n90\\n\\n100\\n\\n50\\nnz = 820\\n\\nFigure 1: N = 10. The black dots denote non-zero elements of PAHW H ′PA + PS .\\n\\nAs is often the case with the Newton method, the global convergence is not assured by this\\nalgorithm. Fortunately it is possible to cure this fault. We will show the prescription to the global\\ninstability in Section 5.\\n\\n4 Applications to ICA\\n\\nSo far we have not speciﬁed the cost function beyond the assumption that the cost function is a\\nsum of the form (2.1). Many of the cost functions for the independent component analysis belong\\nto this class.\\n\\n0\\n\\n10\\n\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\n90\\n\\n100\\n\\n7\\n\\n\\x0c4.1 Kullback-Leibler information\\n\\nThe Kullback-Leibler information,\\n\\nN\\n\\ni=1\\nX\\n\\nN\\n\\ni=1\\nX\\n\\nN\\n\\nZ\\n\\ni=1\\nY\\n\\ndyiP (y)\\n\\nln P (y) −\\n\\nln Pi(yi)\\n\\n,\\n\\n(cid:26)\\n\\n(cid:27)\\n\\n(4.1)\\n\\nis a good measure for the independence. Here P is the joint probability density function of {Yi} and\\nPi is the probability density function of the i-th component. We have already restricted ourselves\\nto the case where the jacobian of the transformation equals one. Then the minimization of the\\nKullback-Leibler information is equivalent to the minimization of\\n\\n−\\n\\ndYiP (Y )\\n\\nln Pi(Yi) =\\n\\nE(− ln Pi(Yi)) .\\n\\n(4.2)\\n\\nN\\n\\ni=1\\nX\\n\\nZ Y\\ni\\n\\nThus we can legitimately transform the Kullback-Leibler information to a cost function of the form\\n(2.1), where we should set {fi}’s as\\n\\nfi(·) = − ln Pi(·) .\\n\\n(4.3)\\n\\nWe must evaluate {Pi}’s, their derivatives, and so on to determine the optimal solution. A robust\\nestimation of these quantities is possibly not an easy task[B.W.Sliverman,1986, D.Cox,1985].\\n\\n4.2 Cumulant of fourth order\\n\\nThe kurtosis of a random variable A is deﬁned by\\n\\nE(A4)\\n(E(A2))2 − 3 .\\nThe kurtosis is related to the cumulant of the fourth order,\\n\\nκ(A) =\\n\\nCum(4)(A) = E(A4) − 3(E(A2))2 ,\\n\\nby\\n\\nκ(A) =\\n\\nCum(4)(A)\\n(E(A2))2\\n\\n.\\n\\nFor prewhitened data the kurtosis equals the cumulant of the fourth order. As is well-\\nknown[A.Hyv¨arinen,1997, T.Akuzawa & N.Murata,1999], we can grab independent components in\\nmany cases by seeking the maximum of the absolute values of the kurtoses. Our method is appli-\\ncable by setting\\n\\nfi = −κ2\\n\\n8\\n\\n(4.4)\\n\\n(4.5)\\n\\n(4.6)\\n\\n(4.7)\\n\\n\\x0cfor all i. If it is known a priori that all the sources {Y ∗i } have positive kurtoses, we may use the\\nkurtosis itself and set\\n\\nfi = −κ .\\n\\n(4.8)\\n\\nFor these cost functions, R, {Ui}, and other quantities needed for determining each step are cal-\\nculated easily from the observed data. Thus applying our method for this cost function is highly\\npractical and reasonable choice.\\n\\n5 Levenberg-Marquardt-type variation and perfor-\\nmance in practice\\n\\nThe pure-Newton updating rule (2.26) has a poor global convergence property. This drawback\\nis remedied by the Levenberg-Marquardt-type variation[W.H.Press et al.,1988]. First, We modify\\n(2.26) as\\n\\ncs(∆) = −H ′\\n\\nPAHW H ′PA + PS + λIN 2\\n\\n1 PAHcs(R) .\\n\\n−\\n\\n(5.1)\\n\\nThe initial value λ0 for λ is ﬁxed at some positive value. We also ﬁx a real number α(> 1). (In the\\nfollowing example we set λ0 = 50 and α = 10.) Then the procedure at time t is as follows:\\n\\n(cid:0)\\n\\n(cid:1)\\n\\ni) Calculate ∆ by (5.1).\\nii) If F (e∆Y (t)) is larger than F (Y (t)), multiply λ by α and go back to i).\\n\\niii) Otherwise, multiply λ by 1/α and proceed to the next time step t + 1.\\n\\nOther parts of the algorithm is completely the same as in the pure-Newton version in Section 2.\\n\\nLet us examine the real performance of our method under this setting. For the cost function\\nwe choose the kurtosis as in Subsection 4.2. The source signals are three synthesizer-generated wav\\nﬁles(Fig.2). Pseudo-observed data are generated by mixing the source by a random matrix,\\n\\nA = I3 + S,\\n\\n(5.2)\\n\\nwhere each element of S is distributed uniformly on (−1/2, 1/2). The residual crosstalk of the\\nsignals demixed by our method is 1.29% on average. It takes about 122 seconds (CPU time) for\\none hundred iteration of the same problem on our workstation. For reference, we have also solved\\nthe same demixing problem by the FastICA[Hurri et al.,1998]. In this case the residual crosstalk is\\n1.36% on average and it takes about 156 seconds for one hundred iteration on the same workstation.\\nSince the author’s knowledge about the FastICA package is limited, one should not take this result\\nseriously. It can, however, be said that our method is quite good also in practice.\\n\\n9\\n\\n\\x0c−0.5\\n\\n−1\\n\\n0\\n\\n0.5\\n\\n1\\n\\n0\\n\\n0.5\\n\\n1\\n\\n0\\n\\n0.5\\n\\n1\\n\\n0\\n\\n−0.5\\n\\n−1\\n\\n0\\n\\n−0.5\\n\\n−1\\n\\n0\\n\\nsamisen\\n\\n2.5\\nbell\\n\\n2.5\\ndrum\\n\\n0.5\\n\\n1\\n\\n1.5\\n\\n2\\n\\n3\\n\\n3.5\\n\\n4\\n\\n4.5\\n\\n0.5\\n\\n1\\n\\n1.5\\n\\n2\\n\\n3\\n\\n3.5\\n\\n4\\n\\n4.5\\n\\n0.5\\n\\n1\\n\\n1.5\\n\\n2\\n\\n2.5\\n\\n3\\n\\n3.5\\n\\n4\\n\\n4.5\\n\\n5\\n4\\nx 10\\n\\n5\\n4\\nx 10\\n\\n5\\n4\\nx 10\\n\\nFigure 2: Sample data generated by a synthesizer (by courtesy of N.Murata).\\n\\n6 Summary\\n\\nWe have constructed a new algorithm for ﬁnding a critical point of broad classes of cost functions\\non the orthogonal groups. This method is second-order-convergent since it is in essence the Newton\\nmethod. The method here constructed is an extension (or a restriction) of the multiplicative\\nupdating method developed in our previous work[T.Akuzawa & N.Murata,1999]. The constraint\\nfor ∆ from the nature of the orthogonal groups makes the problem a little complicated. We have,\\nhowever, obtained a rigorous and explicit updating rule. We have also constructed a Levenberg-\\nMarquardt-type variation, which is suitable for practical purpose. The global instability inherent in\\nthe Newton method is remedied in this version. Since our discussion does not depend on the detail\\nof the cost function, this method is applicable to many concrete problems. The relatively mild\\nassumption (2.1) on the form of the cost function, however, implies that our algorithm is especially\\nsuitable for the ICA. Its practical utility for the ICA have been illustrated here by a numerical\\nsimulation.\\n\\nTo summarize, our algorithm has numerous theoretical virtues such as the rigorous second order\\nconvergence, the explicit and strict formulation, and so on. It provides, also in practice, fast and\\npowerful tools for the ICA and many other problems.\\n\\nAcknowledgments\\n\\nThe author would like to thank Noboru Murata and Shun-ichi Amari for valuable discussions and\\ncomments.\\n\\n10\\n\\n\\x0cReferences\\n\\n[A.Hyv¨arinen,1997] A.Hyv¨arinen (1997). A Fast Fixed-Point Algorithm for Independent Compo-\\n\\nnent Analysis. Neural Computation, 9 , 1483–1492.\\n\\n[B.W.Sliverman,1986] B.W.Sliverman (1986). Density Estimation for Statistics and Data Analysis.\\n\\nLondon: Chapman & Hall.\\n\\n[D.Cox,1985] D.Cox, D. (1985). A Penalty Method for Nonparametric Estimation of the Logarith-\\n\\nmic Derivative of a Density Function. Ann.Inst.Statist.Math., 37 , 271–288.\\n\\n[Hurri et al.,1998] Hurri, J., G¨avert, H., S¨alel¨a, J., & Hyv¨arinen, A. (1998). FastICA package for\\n\\nMATLAB. http://www.cis.hut.ﬁ/projects/ica/fastica/.\\n\\n[T.Akuzawa & N.Murata,1999] T.Akuzawa & N.Murata (1999).\\n\\nMultiplicative Nonholo-\\n\\nnomic/Newton -like Algorithm. preprint\\n(available from http://www.islab.brain.riken.go.jp/˜akuzawa/).\\n\\n[W.H.Press et al.,1988] W.H.Press, B.P.Flannery, S.A.Teukolsky, & W.T.Vetterling (1988). Nu-\\n\\nmerical Recipes in C . Cambridge: Cambridge U.P.\\n\\n11\\n\\n\\x0c'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
