{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import urllib.request as libreq\n",
    "import feedparser\n",
    "import pdfminer.layout\n",
    "import pdfminer.high_level\n",
    "from io import StringIO\n",
    "from pdfminer.layout import LAParams\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "import re\n",
    "import heapq\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=pdfminer.high_level.extract_text('C:\\\\Users\\\\Al\\\\Documents\\\\ByteSizeArxiv\\\\library/9905015v1.pdf', codec='utf-8', laparams=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9\\n9\\n9\\n1\\n \\ny\\na\\nM\\n1\\n2\\n \\n \\n]\\n\\n \\n\\nG\\nL\\n.\\ns\\nc\\n[\\n \\n \\n1\\nv\\n5\\n1\\n0\\n5\\n0\\n9\\n9\\n/\\ns\\nc\\n:\\nv\\ni\\nX\\nr\\na\\n\\nState Abstraction in MAXQ Hierarchical\\nReinforcement Learning\\n\\nThomas G. Dietterich\\nDepartment of Computer Science\\nOregon State University\\nCorvallis, Oregon 97331-3202\\ntgd@cs.orst.edu\\n\\nAbstract\\n\\nMany researchers have explored methods for hierarchical reinforce-\\nment learning (RL) with temporal abstractions, in which abstract\\nactions are deﬁned that can perform many primitive actions before\\nterminating. However, little is known about learning with state ab-\\nstractions, in which aspects of the state space are ignored. In previ-\\nous work, we developed the MAXQ method for hierarchical RL. In\\nthis paper, we deﬁne ﬁve conditions under which state abstraction\\ncan be combined with the MAXQ value function decomposition.\\nWe prove that the MAXQ-Q learning algorithm converges under\\nthese conditions and show experimentally that state abstraction is\\nimportant for the successful application of MAXQ-Q learning.\\nCategory: Reinforcement Learning and Control\\nPreference: Oral\\n\\n1\\n\\nIntroduction\\n\\nMost work on hierarchical reinforcement learning has focused on temporal abstrac-\\ntion. For example, in the Options framework [1, 2], the programmer deﬁnes a set of\\nmacro actions (“options”) and provides a policy for each. Learning algorithms (such\\nas semi-Markov Q learning) can then treat these temporally abstract actions as if\\nthey were primitives and learn a policy for selecting among them. Closely related\\nis the HAM framework, in which the programmer constructs a hierarchy of ﬁnite-\\nstate controllers [3]. Each controller can include non-deterministic states (where the\\nprogrammer was not sure what action to perform). The HAMQ learning algorithm\\ncan then be applied to learn a policy for making choices in the non-deterministic\\nstates. In both of these approaches—and in other studies of hierarchical RL (e.g.,\\n[4, 5, 6])—each option or ﬁnite state controller must have access to the entire state\\nspace. The one exception to this—the Feudal-Q method of Dayan and Hinton [7]—\\nintroduced state abstractions in an unsafe way, such that the resulting learning\\nproblem was only partially observable. Hence, they could not provide any formal\\nresults for the convergence or performance of their method.\\n\\nEven a brief consideration of human-level intelligence shows that such methods can-\\nnot scale. When deciding how to walk from the bedroom to the kitchen, we do not\\n\\n\\x0cneed to think about the location of our car. Without state abstractions, any RL\\nmethod that learns value functions must learn a separate value for each state of the\\nworld. Some argue that this can be solved by clever value function approximation\\nmethods—and there is some merit in this view. In this paper, however, we explore\\na diﬀerent approach in which we identify aspects of the MDP that permit state ab-\\nstractions to be safely incorporated in a hierarchical reinforcement learning method\\nwithout introducing function approximations. This permits us to obtain the ﬁrst\\nproof of the convergence of hierarchical RL to an optimal policy in the presence of\\nstate abstraction.\\n\\nWe introduce these state abstractions within the MAXQ framework [8], but the\\nbasic ideas are general. In our previous work with MAXQ, we brieﬂy discussed state\\nabstractions, and we employed them in our experiments. However, we could not\\nprove that our algorithm (MAXQ-Q) converged with state abstractions, and we did\\nnot have a usable characterization of the situations in which state abstraction could\\nbe safely employed. This paper solves these problems and in addition compares the\\neﬀectiveness of MAXQ-Q learning with and without state abstractions. The results\\nshow that state abstraction is very important, and in most cases essential, to the\\neﬀective application of MAXQ-Q learning.\\n\\n2 The MAXQ Framework\\n\\nLet M be a Markov decision problem with states S, actions A, reward function\\nR(s′|s, a) and probability transition function P (s′|s, a). Our results apply in both\\nthe ﬁnite-horizon undiscounted case and the inﬁnite-horizon discounted case. Let\\n{M0, . . . , Mn} be a set of subtasks of M , where each subtask Mi is deﬁned by a\\ntermination predicate Ti and a set of actions Ai (which may be other subtasks or\\nprimitive actions from A). The “goal” of subtask Mi is to move the environment into\\na state such that Ti is satisﬁed. (This can be reﬁned using a local reward function\\nto express preferences among the diﬀerent states satisfying Ti [8], but we omit this\\nreﬁnement in this paper.) The subtasks of M must form a DAG with a single “root”\\nnode—no subtask may invoke itself directly or indirectly. A hierarchical policy is\\na set of policies π = {π0, . . . , πn}, one for each subtask. A hierarchical policy\\nis executed using standard procedure-call-and-return semantics, starting with the\\nroot task M0 and unfolding recursively until primitive actions are executed. When\\nthe policy for Mi is invoked in state s, let P (s′, N |s, i) be the probability that it\\nterminates in state s′ after executing N primitive actions. A hierarchical policy is\\nrecursively optimal if each policy πi is optimal given the policies of its descendants\\nin the DAG.\\n\\nLet V (i, s) be the value function for subtask i in state s (i.e., the value of following\\nsome policy starting in s until we reach a state s′ satisfying Ti(s′)). Similarly, let\\nQ(i, s, j) be the Q value for subtask i of executing child action j in state s and\\nthen executing the current policy until termination. The MAXQ value function\\ndecomposition is based on the observation that each subtask Mi can be viewed as a\\nSemi-Markov Decision problem in which the reward for performing action j in state\\ns is equal to V (j, s), the value function for subtask j in state s. To see this, consider\\nthe sequence of rewards rt that will be received when we execute child action j and\\nthen continue with subsequent actions according to hierarchical policy π:\\n\\nQ(i, s, j) = E{rt + γrt+1 + γ2rt+2 + · · · |st = s, π}\\n\\nThe macro action j will execute for some number of steps N and then return. Hence,\\n\\n\\x0cwe can partition this sum into two terms:\\n\\nQ(i, s, j) = E\\n\\nγurt+u +\\n\\nγurt+u\\n\\nst = s, π\\n\\nN −1\\n\\n(\\n\\nu=0\\nX\\n\\n∞\\n\\nu=N\\nX\\n\\n)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nThe ﬁrst term is the discounted sum of rewards until subtask j terminates—V (j, s).\\nThe second term is the cost of ﬁnishing subtask i after j is executed (discounted\\nto the time when j is initiated). We call this second term the completion function,\\nand denote it C(i, s, j). We can then write the Bellman equation as\\n\\nQ(i, s, j) =\\n\\nP (s′, N |s, j) · [V (j, s) + γN max\\n\\nj′ Q(i, s′, j′\\n\\n)]\\n\\ns′,N\\nX\\n\\n= V (j, s) + C(i, s, j)\\n\\nTo terminate this recursion, deﬁne V (a, s) for a primitive action a to be the expected\\nreward of performing action a in state s.\\n\\nThe MAXQ-Q learning algorithm is a simple variation of Q learning in which at\\nsubtask Mi, state s, we choose a child action j and invoke its (current) policy. When\\nit returns, we observe the resulting state s′ and the number of elapsed time steps\\nN and update C(i, s, j) according to\\n\\nC(i, s, j) := (1 − αt)C(i, s, j) + αt · γN [max\\n\\na′ V (a′, s′\\n\\n) + C(i, s′, a′\\n\\n)].\\n\\nTo prove convergence, we require that the exploration policy executed during learn-\\ning be an ordered GLIE policy. An ordered policy is a policy that breaks Q-value\\nties among actions by preferring the action that comes ﬁrst in some ﬁxed ordering.\\nA GLIE policy [9] is a policy that (a) executes each action inﬁnitely often in every\\nstate that is visited inﬁnitely often and (b) converges with probability 1 to a greedy\\npolicy. The ordering condition is required to ensure that the recursively optimal\\npolicy is unique. Without this condition, there are potentially many diﬀerent re-\\ncursively optimal policies with diﬀerent values, depending on how ties are broken\\nwithin subtasks, subsubtasks, and so on.\\n\\nTheorem 1 Let M = hS, A, P, Ri be either an episodic MDP for which all de-\\nterministic policies are proper or a discounted inﬁnite horizon MDP with discount\\nfactor γ. Let H be a DAG deﬁned over subtasks {M0, . . . , Mk}. Let αt(i) > 0 be a\\nsequence of constants for each subtask Mi such that\\n\\nlim\\nT →∞\\n\\nT\\n\\nt=1\\nX\\n\\nlim\\nT →∞\\n\\nT\\n\\nt=1\\nX\\n\\nαt(i) = ∞ and\\n\\nα2\\n\\nt (i) < ∞\\n\\n(1)\\n\\nLet πx(i, s) be an ordered GLIE policy at each subtask Mi and state s and assume\\nthat |Vt(i, s)| and |Ct(i, s, a)| are bounded for all t, i, s, and a. Then with probability\\n1, algorithm MAXQ-Q converges to the unique recursively optimal policy for M\\nconsistent with H and πx.\\n\\nProof: (sketch) The proof is based on Proposition 4.5 from Bertsekas and Tsit-\\nsiklis [10] and follows the standard stochastic approximation argument due to [11]\\ngeneralized to the case of non-stationary noise. There are two key points in the\\nproof. Deﬁne Pt(s′, N |s, j) to be the probability transition function that describes\\nthe behavior of executing the current policy for subtask j at time t. By an inductive\\nargument, we show that this probability transition function converges (w.p. 1) to\\nthe probability transition function of the recursively optimal policy for j. Second,\\n\\n\\x0c4\\n\\n3\\n\\n2\\n\\n1\\n\\n0\\n\\nR\\n\\nY\\n\\n0\\n\\nG\\n\\nRoot\\n\\nGet\\n\\nPut\\n\\nt/source\\n\\nt/destination\\n\\nPickup\\n\\nNavigate(t)\\n\\nPutdown\\n\\nB\\n\\n3\\n\\n1\\n\\n2\\n\\n4\\n\\nNorth\\n\\nSouth\\n\\nEast\\n\\nWest\\n\\nFigure 1: Left: The Taxi Domain (taxi at row 3 column 0). Right: Task Graph.\\n\\nwe show how to convert the usual weighted max norm contraction for Q into a\\nweighted max norm contraction for C. This is straightforward, and completes the\\nproof.\\n\\nWhat is notable about MAXQ-Q is that it can learn the value functions of all\\nsubtasks simultaneously—it does not need to wait for the value function for subtask\\nj to converge before beginning to learn the value function for its parent task i. This\\ngives a completely online learning algorithm with wide applicability.\\n\\n3 Conditions for Safe State Abstraction\\n\\nTo motivate state abstraction, consider the simple Taxi Task shown in Figure 1.\\nThere are four special locations in this world, marked as R(ed), B(lue), G(reen),\\nand Y(ellow). In each episode, the taxi starts in a randomly-chosen square. There\\nis a passenger at one of the four locations (chosen randomly), and that passenger\\nwishes to be transported to one of the four locations (also chosen randomly). The\\ntaxi must go to the passenger’s location (the “source”), pick up the passenger, go\\nto the destination location (the “destination”), and put down the passenger there.\\nThe episode ends when the passenger is deposited at the destination location.\\n\\nThere are six primitive actions in this domain: (a) four navigation actions that\\nmove the taxi one square North, South, East, or West, (b) a Pickup action, and (c)\\na Putdown action. Each action is deterministic. There is a reward of −1 for each\\naction and an additional reward of +20 for successfully delivering the passenger.\\nThere is a reward of −10 if the taxi attempts to execute the Putdown or Pickup\\nactions illegally. If a navigation action would cause the taxi to hit a wall, the action\\nis a no-op, and there is only the usual reward of −1.\\n\\nThis task has a hierarchical structure (see Fig. 1) in which there are two main\\nsub-tasks: Get the passenger (Get) and Deliver the passenger (Put). Each of these\\nsubtasks in turn involves the subtask of navigating to one of the four locations\\n(Navigate(t); where t is bound to the desired target location) and then performing\\na Pickup or Putdown action. This task illustrates the need to support both tem-\\nporal abstraction and state abstraction. The temporal abstraction is obvious—for\\nexample, Get is a temporally extended action that can take diﬀerent numbers of\\nsteps to complete depending on the distance to the target. The top level policy (get\\npassenger; deliver passenger) can be expressed very simply with these abstractions.\\n\\nThe need for state abstraction is perhaps less obvious. Consider the Get subtask.\\nWhile this subtask is being solved, the destination of the passenger is completely\\nirrelevant—it cannot aﬀect any of the nagivation or pickup decisions. Perhaps more\\nimportantly, when navigating to a target location (either the source or destination\\n\\n\\x0clocation of the passenger), only the taxi’s location and identity of the target location\\nare important. The fact that in some cases the taxi is carrying the passenger and\\nin other cases it is not is irrelevant.\\n\\nWe now introduce the ﬁve conditions for state abstraction. We will assume that the\\nstate s of the MDP is represented as a vector of state variables. A state abstraction\\ncan be deﬁned for each combination of subtask Mi and child action j by identifying\\na subset X of the state variables that are relevant and deﬁning the value function\\nand the policy using only these relevant variables. Such value functions and policies\\nare said to be abstract.\\n\\nThe ﬁrst two conditions involve eliminating irrelevant variables within a subtask of\\nthe MAXQ decomposition.\\n\\nCondition 1: Subtask Irrelevance. Let Mi be a subtask of MDP M . A set\\nof state variables Y is irrelevant to subtask i if the state variables of M can be\\npartitioned into two sets X and Y such that for any stationary abstract hierarchical\\npolicy π executed by the descendants of Mi, the following two properties hold: (a)\\nthe state transition probability distribution P π(s′, N |s, j) for each child action j of\\nMi can be factored into the product of two distributions:\\n\\nP π(x′, y′, N |x, y, j) = P π(x′, N |x, j) · P π(y′\\n\\n(2)\\nwhere x and x′ give values for the variables in X, and y and y′ give values for the\\nvariables in Y ; and (b) for any pair of states s1 = (x, y1) and s2 = (x, y2) and any\\nchild action j, V π(j, s1) = V π(j, s2).\\n\\n|y, j),\\n\\nIn the Taxi problem, the source and destination of the passenger are irrelevant to\\nthe Navigate(t) subtask—only the target t and the current taxi position are relevant.\\n\\nCondition 2: Leaf Irrelevance. A set of state variables Y is irrelevant for a\\nprimitive action a if for any pair of states s1 and s2 that diﬀer only in their values\\nfor the variables in Y ,\\nP (s′\\n\\n1|s1, a)R(s′\\n\\n2|s2, a)R(s′\\n\\n1|s1, a) =\\n\\n2|s2, a).\\n\\nP (s′\\n\\nXs′\\n\\n1\\n\\nXs′\\n\\n2\\n\\nThis condition is satisﬁed by the primitive actions North, South, East, and West in\\nthe taxi task, where all state variables are irrelevant because R is constant.\\n\\nThe next two conditions involve “funnel” actions—macro actions that move the\\nenvironment from some large number of possible states to a small number of re-\\nsulting states. The completion function of such subtasks can be represented using\\na number of values proportional to the number of resulting states.\\n\\nCondition 3: Result Distribution Irrelevance (Undiscounted case.) A set\\nof state variables Yj is irrelevant for the result distribution of action j if, for all\\nabstract policies π executed by Mj and its descendants in the MAXQ hierarchy, the\\nfollowing holds: for all pairs of states s1 and s2 that diﬀer only in their values for\\nthe state variables in Yj,\\n\\n∀ s′ P π(s′\\n\\n|s1, j) = P π(s′\\n\\n|s2, j).\\n\\nConsider, for example, the Get subroutine under an optimal policy for the taxi\\ntask. Regardless of the taxi’s position in state s, the taxi will be at the passenger’s\\nstarting location when Get ﬁnishes executing (i.e., because the taxi will have just\\ncompleted picking up the passenger). Hence, the taxi’s initial position is irrelevant\\nto its resulting position. (Note that this is only true in the undiscounted setting—\\nwith discounting, the result distributions are not the same because the number of\\n\\n\\x0csteps N required for Get to ﬁnish depends very much on the starting location of the\\ntaxi. Hence this form of state abstraction is rarely useful for cumulative discounted\\nreward.)\\n\\nCondition 4: Termination. Let Mj be a child task of Mi with the property\\nthat whenever Mj terminates, it causes Mi to terminate too. Then the completion\\ncost C(i, s, j) = 0 and does not need to be represented. This is a particular kind of\\nfunnel action—it funnels all states into terminal states for Mi.\\n\\nFor example, in the Taxi task, in all states where the taxi is holding the passenger,\\nthe Put subroutine will succeed and result in a terminal state for Root. This is\\nbecause the termination predicate for Put (i.e., that the passenger is at his or her\\ndestination location) implies the termination condition for Root (which is the same).\\nThis means that C(Root, s, Put) is uniformly zero, for all states s where Put is not\\nterminated.\\n\\nCondition 5: Shielding. Consider subtask Mi and let s be a state such that\\nfor all paths from the root of the DAG down to Mi, there exists a subtask that is\\nterminated. Then no C values need to be represented for subtask Mi in state s,\\nbecause it can never be executed in s.\\n\\nIn the Taxi task, a simple example of this arises in the Put task, which is terminated\\nin all states where the passenger is not in the taxi. This means that we do not need\\nto represent C(Root, s, Put) in these states. The result is that, when combined\\nwith the Termination condition above, we do not need to explicitly represent the\\ncompletion function for Put at all!\\n\\nBy applying these abstraction conditions to the Taxi task, the value function can\\nbe represented using 632 values, which is much less than the 3,000 values required\\nby ﬂat Q learning. Without state abstractions, MAXQ requires 14,000 values!\\n\\nTheorem 2 (Convergence with State Abstraction) Let H be a MAXQ task\\ngraph that incorporates the ﬁve kinds of state abstractions deﬁned above. Let πx be\\nan ordered GLIE exploration policy that is abstract. Then under the same condi-\\ntions as Theorem 1, MAXQ-Q converges with probability 1 to the unique recursively\\noptimal policy π∗\\n\\nr deﬁned by πx and H.\\n\\nProof: (sketch) Consider a subtask Mi with relevant variables X and two ar-\\nbitrary states (x, y1) and (x, y2). We ﬁrst show that under the ﬁve abstraction\\nconditions, the value function of π∗\\nr can be represented using C(i, x, j) (i.e., ignor-\\nx′,N P (x′, N |x, j)V (i, x′), a\\ning the y values). To learn the values of C(i, x, j) =\\nQ-learning algorithm needs samples of x′ and N drawn according to P (x′, N |x, j).\\nThe second part of the proof involves showing that regardless of whether we execute\\nj in state (x, y1) or in (x, y2), the resulting x′ and N will have the same distribu-\\ntion, and hence, give the correct expectations. Analogous arguments apply for leaf\\nirrelevance and V (a, x). The termination and shielding cases are easy.\\n\\nP\\n\\n4 Experimental Results\\n\\nWe implemented MAXQ-Q for a noisy version of the Taxi domain and for Kael-\\nbling’s HDG navigation task [5] using Boltzmann exploration. Figure 2 shows the\\nperformance of ﬂat Q and MAXQ-Q with and without state abstractions on these\\ntasks. Learning rates and Boltzmann cooling rates were separately tuned to opti-\\nmize the performance of each method. The results show that without state abstrac-\\ntions, MAXQ-Q learning is slower to converge than ﬂat Q learning, but that with\\nstate abstraction, it is much faster.\\n\\n\\x0cMAXQ+Abstraction\\n\\nMAXQ+Abstraction\\n\\nFlat Q\\n\\nMAXQ No Abstraction\\n\\nd\\nr\\na\\nw\\ne\\nR\\n \\ne\\nv\\ni\\nt\\na\\nl\\nu\\nm\\nu\\nC\\nn\\na\\ne\\n\\n \\n\\nM\\n\\n0\\n\\n-20\\n\\n-40\\n\\n-60\\n\\n-80\\n\\n-100\\n\\n-120\\n\\n-140\\n\\nd\\nr\\na\\nw\\ne\\nR\\n \\ne\\nv\\ni\\nt\\na\\nl\\nu\\nm\\nu\\nC\\nn\\na\\ne\\n\\n \\n\\nM\\n\\n200\\n\\n0\\n\\n-200\\n\\n-400\\n\\n-600\\n\\n-800\\n\\n-1000\\n\\nMAXQ\\nNo Abstraction\\n\\nFlat Q\\n\\n5 Conclusion\\n\\n0\\n\\n20000\\n\\n40000\\n\\n60000\\n\\n80000 100000 120000 140000 160000\\n\\n0\\n\\n200000\\n\\n400000\\n\\n1e+06\\n\\n1.2e+06\\n\\n1.4e+06\\n\\n800000\\n600000\\nPrimitive Actions\\n\\nPrimitive Actions\\n\\nFigure 2: Comparison of MAXQ-Q with and without state abstraction to ﬂat Q learning\\non a noisy taxi domain (left) and Kaelbling’s HDG task (right). The horizontal axis gives\\nthe number of primitive actions executed by each method. The vertical axis plots the\\naverage of 100 separate runs.\\n\\nThis paper has shown that by understanding the reasons that state variables are\\nirrelevant, we can obtain a simple proof of the convergence of MAXQ-Q learning\\nunder state abstraction. This is much more fruitful than previous eﬀorts based\\nonly on weak notions of state aggregation [10], and it suggests that future research\\nshould focus on identifying other conditions that permit safe state abstraction.\\n\\nReferences\\n\\n[1] D. Precup and R. S. Sutton, “Multi-time models for temporally abstract planning,”\\n\\nin NIPS10, The MIT Press, 1998.\\n\\n[2] R. S. Sutton, D. Precup, and S. Singh, “Between MDPs and Semi-MDPs: Learn-\\ning, planning, and representing knowledge at multiple temporal scales,” tech. rep.,\\nUniv. Mass., Dept. Comp. Inf. Sci., Amherst, MA, 1998.\\n\\n[3] R. Parr and S. Russell, “Reinforcement learning with hierarchies of machines,” in\\n\\nNIPS-10, The MIT Press, 1998.\\n\\n[4] S. P. Singh, “Transfer of learning by composing solutions of elemental sequential\\n\\ntasks,” Machine Learning, vol. 8, p. 323, 1992.\\n\\n[5] L. P. Kaelbling, “Hierarchical reinforcement learning: Preliminary results,” in Pro-\\n\\nceedings ICML-10, pp. 167–173, Morgan Kaufmann, 1993.\\n\\n[6] M. Hauskrecht, N. Meuleau, C. Boutilier, L. Kaelbling, and T. Dean, “Hierarchical\\nsolution of Markov decision processes using macro-actions,” tech. rep., Brown Univ.,\\nDept. Comp. Sci., Providence, RI, 1998.\\n\\n[7] P. Dayan and G. Hinton, “Feudal reinforcement learning,” in NIPS-5, pp. 271–278,\\n\\nSan Francisco, CA: Morgan Kaufmann, 1993.\\n\\n[8] T. G. Dietterich, “The MAXQ method for hierarchical reinforcement learning,” in\\n\\nICML-15, Morgan Kaufmann, 1998.\\n\\n[9] S. Singh, T. Jaakkola, M. L. Littman, and C. Szpesvari, “Convergence results\\nfor single-step on-policy reinforcement-learning algorithms,” tech. rep., Univ. Col.,\\nDept. Comp. Sci., Boulder, CO, 1998.\\n\\n[10] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-Dynamic Programming. Belmont, MA:\\n\\nAthena Scientiﬁc, 1996.\\n\\n[11] T. Jaakkola, M. I. Jordan, and S. P. Singh, “On the convergence of stochastic iterative\\ndynamic programming algorithms,” Neur. Comp., vol. 6, no. 6, pp. 1185–1201, 1994.\\n\\n\\x0c'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedText = text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedText = re.sub(r'\\n',' ',cleanedText)#Get rid of new lines replace with spaces\n",
    "\n",
    "cleanedText = re.sub(r'- ','',cleanedText) #To get rid of hyphens next lines\n",
    "\n",
    "cleanedText = re.sub(r'\\x0c','', cleanedText) #Remove page breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedText = cleanedText.split(\"Abstract \",1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Many researchers have explored methods for hierarchical reinforcement learning (RL) with temporal abstractions, in which abstract actions are deﬁned that can perform many primitive actions before terminating. However, little is known about learning with state abstractions, in which aspects of the state space are ignored. In previous work, we developed the MAXQ method for hierarchical RL. In this paper, we deﬁne ﬁve conditions under which state abstraction can be combined with the MAXQ value function decomposition. We prove that the MAXQ-Q learning algorithm converges under these conditions and show experimentally that state abstraction is important for the successful application of MAXQ-Q learning. Category: Reinforcement Learning and Control Preference: Oral  1  Introduction  Most work on hierarchical reinforcement learning has focused on temporal abstraction. For example, in the Options framework [1, 2], the programmer deﬁnes a set of macro actions (“options”) and provides a policy for each. Learning algorithms (such as semi-Markov Q learning) can then treat these temporally abstract actions as if they were primitives and learn a policy for selecting among them. Closely related is the HAM framework, in which the programmer constructs a hierarchy of ﬁnitestate controllers [3]. Each controller can include non-deterministic states (where the programmer was not sure what action to perform). The HAMQ learning algorithm can then be applied to learn a policy for making choices in the non-deterministic states. In both of these approaches—and in other studies of hierarchical RL (e.g., [4, 5, 6])—each option or ﬁnite state controller must have access to the entire state space. The one exception to this—the Feudal-Q method of Dayan and Hinton [7]— introduced state abstractions in an unsafe way, such that the resulting learning problem was only partially observable. Hence, they could not provide any formal results for the convergence or performance of their method.  Even a brief consideration of human-level intelligence shows that such methods cannot scale. When deciding how to walk from the bedroom to the kitchen, we do not  need to think about the location of our car. Without state abstractions, any RL method that learns value functions must learn a separate value for each state of the world. Some argue that this can be solved by clever value function approximation methods—and there is some merit in this view. In this paper, however, we explore a diﬀerent approach in which we identify aspects of the MDP that permit state abstractions to be safely incorporated in a hierarchical reinforcement learning method without introducing function approximations. This permits us to obtain the ﬁrst proof of the convergence of hierarchical RL to an optimal policy in the presence of state abstraction.  We introduce these state abstractions within the MAXQ framework [8], but the basic ideas are general. In our previous work with MAXQ, we brieﬂy discussed state abstractions, and we employed them in our experiments. However, we could not prove that our algorithm (MAXQ-Q) converged with state abstractions, and we did not have a usable characterization of the situations in which state abstraction could be safely employed. This paper solves these problems and in addition compares the eﬀectiveness of MAXQ-Q learning with and without state abstractions. The results show that state abstraction is very important, and in most cases essential, to the eﬀective application of MAXQ-Q learning.  2 The MAXQ Framework  Let M be a Markov decision problem with states S, actions A, reward function R(s′|s, a) and probability transition function P (s′|s, a). Our results apply in both the ﬁnite-horizon undiscounted case and the inﬁnite-horizon discounted case. Let {M0, . . . , Mn} be a set of subtasks of M , where each subtask Mi is deﬁned by a termination predicate Ti and a set of actions Ai (which may be other subtasks or primitive actions from A). The “goal” of subtask Mi is to move the environment into a state such that Ti is satisﬁed. (This can be reﬁned using a local reward function to express preferences among the diﬀerent states satisfying Ti [8], but we omit this reﬁnement in this paper.) The subtasks of M must form a DAG with a single “root” node—no subtask may invoke itself directly or indirectly. A hierarchical policy is a set of policies π = {π0, . . . , πn}, one for each subtask. A hierarchical policy is executed using standard procedure-call-and-return semantics, starting with the root task M0 and unfolding recursively until primitive actions are executed. When the policy for Mi is invoked in state s, let P (s′, N |s, i) be the probability that it terminates in state s′ after executing N primitive actions. A hierarchical policy is recursively optimal if each policy πi is optimal given the policies of its descendants in the DAG.  Let V (i, s) be the value function for subtask i in state s (i.e., the value of following some policy starting in s until we reach a state s′ satisfying Ti(s′)). Similarly, let Q(i, s, j) be the Q value for subtask i of executing child action j in state s and then executing the current policy until termination. The MAXQ value function decomposition is based on the observation that each subtask Mi can be viewed as a Semi-Markov Decision problem in which the reward for performing action j in state s is equal to V (j, s), the value function for subtask j in state s. To see this, consider the sequence of rewards rt that will be received when we execute child action j and then continue with subsequent actions according to hierarchical policy π:  Q(i, s, j) = E{rt + γrt+1 + γ2rt+2 + · · · |st = s, π}  The macro action j will execute for some number of steps N and then return. Hence,  we can partition this sum into two terms:  Q(i, s, j) = E  γurt+u +  γurt+u  st = s, π  N −1  (  u=0 X  ∞  u=N X  )  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  The ﬁrst term is the discounted sum of rewards until subtask j terminates—V (j, s). The second term is the cost of ﬁnishing subtask i after j is executed (discounted to the time when j is initiated). We call this second term the completion function, and denote it C(i, s, j). We can then write the Bellman equation as  Q(i, s, j) =  P (s′, N |s, j) · [V (j, s) + γN max  j′ Q(i, s′, j′  )]  s′,N X  = V (j, s) + C(i, s, j)  To terminate this recursion, deﬁne V (a, s) for a primitive action a to be the expected reward of performing action a in state s.  The MAXQ-Q learning algorithm is a simple variation of Q learning in which at subtask Mi, state s, we choose a child action j and invoke its (current) policy. When it returns, we observe the resulting state s′ and the number of elapsed time steps N and update C(i, s, j) according to  C(i, s, j) := (1 − αt)C(i, s, j) + αt · γN [max  a′ V (a′, s′  ) + C(i, s′, a′  )].  To prove convergence, we require that the exploration policy executed during learning be an ordered GLIE policy. An ordered policy is a policy that breaks Q-value ties among actions by preferring the action that comes ﬁrst in some ﬁxed ordering. A GLIE policy [9] is a policy that (a) executes each action inﬁnitely often in every state that is visited inﬁnitely often and (b) converges with probability 1 to a greedy policy. The ordering condition is required to ensure that the recursively optimal policy is unique. Without this condition, there are potentially many diﬀerent recursively optimal policies with diﬀerent values, depending on how ties are broken within subtasks, subsubtasks, and so on.  Theorem 1 Let M = hS, A, P, Ri be either an episodic MDP for which all deterministic policies are proper or a discounted inﬁnite horizon MDP with discount factor γ. Let H be a DAG deﬁned over subtasks {M0, . . . , Mk}. Let αt(i) > 0 be a sequence of constants for each subtask Mi such that  lim T →∞  T  t=1 X  lim T →∞  T  t=1 X  αt(i) = ∞ and  α2  t (i) < ∞  (1)  Let πx(i, s) be an ordered GLIE policy at each subtask Mi and state s and assume that |Vt(i, s)| and |Ct(i, s, a)| are bounded for all t, i, s, and a. Then with probability 1, algorithm MAXQ-Q converges to the unique recursively optimal policy for M consistent with H and πx.  Proof: (sketch) The proof is based on Proposition 4.5 from Bertsekas and Tsitsiklis [10] and follows the standard stochastic approximation argument due to [11] generalized to the case of non-stationary noise. There are two key points in the proof. Deﬁne Pt(s′, N |s, j) to be the probability transition function that describes the behavior of executing the current policy for subtask j at time t. By an inductive argument, we show that this probability transition function converges (w.p. 1) to the probability transition function of the recursively optimal policy for j. Second,  4  3  2  1  0  R  Y  0  G  Root  Get  Put  t/source  t/destination  Pickup  Navigate(t)  Putdown  B  3  1  2  4  North  South  East  West  Figure 1: Left: The Taxi Domain (taxi at row 3 column 0). Right: Task Graph.  we show how to convert the usual weighted max norm contraction for Q into a weighted max norm contraction for C. This is straightforward, and completes the proof.  What is notable about MAXQ-Q is that it can learn the value functions of all subtasks simultaneously—it does not need to wait for the value function for subtask j to converge before beginning to learn the value function for its parent task i. This gives a completely online learning algorithm with wide applicability.  3 Conditions for Safe State Abstraction  To motivate state abstraction, consider the simple Taxi Task shown in Figure 1. There are four special locations in this world, marked as R(ed), B(lue), G(reen), and Y(ellow). In each episode, the taxi starts in a randomly-chosen square. There is a passenger at one of the four locations (chosen randomly), and that passenger wishes to be transported to one of the four locations (also chosen randomly). The taxi must go to the passenger’s location (the “source”), pick up the passenger, go to the destination location (the “destination”), and put down the passenger there. The episode ends when the passenger is deposited at the destination location.  There are six primitive actions in this domain: (a) four navigation actions that move the taxi one square North, South, East, or West, (b) a Pickup action, and (c) a Putdown action. Each action is deterministic. There is a reward of −1 for each action and an additional reward of +20 for successfully delivering the passenger. There is a reward of −10 if the taxi attempts to execute the Putdown or Pickup actions illegally. If a navigation action would cause the taxi to hit a wall, the action is a no-op, and there is only the usual reward of −1.  This task has a hierarchical structure (see Fig. 1) in which there are two main sub-tasks: Get the passenger (Get) and Deliver the passenger (Put). Each of these subtasks in turn involves the subtask of navigating to one of the four locations (Navigate(t); where t is bound to the desired target location) and then performing a Pickup or Putdown action. This task illustrates the need to support both temporal abstraction and state abstraction. The temporal abstraction is obvious—for example, Get is a temporally extended action that can take diﬀerent numbers of steps to complete depending on the distance to the target. The top level policy (get passenger; deliver passenger) can be expressed very simply with these abstractions.  The need for state abstraction is perhaps less obvious. Consider the Get subtask. While this subtask is being solved, the destination of the passenger is completely irrelevant—it cannot aﬀect any of the nagivation or pickup decisions. Perhaps more importantly, when navigating to a target location (either the source or destination  location of the passenger), only the taxi’s location and identity of the target location are important. The fact that in some cases the taxi is carrying the passenger and in other cases it is not is irrelevant.  We now introduce the ﬁve conditions for state abstraction. We will assume that the state s of the MDP is represented as a vector of state variables. A state abstraction can be deﬁned for each combination of subtask Mi and child action j by identifying a subset X of the state variables that are relevant and deﬁning the value function and the policy using only these relevant variables. Such value functions and policies are said to be abstract.  The ﬁrst two conditions involve eliminating irrelevant variables within a subtask of the MAXQ decomposition.  Condition 1: Subtask Irrelevance. Let Mi be a subtask of MDP M . A set of state variables Y is irrelevant to subtask i if the state variables of M can be partitioned into two sets X and Y such that for any stationary abstract hierarchical policy π executed by the descendants of Mi, the following two properties hold: (a) the state transition probability distribution P π(s′, N |s, j) for each child action j of Mi can be factored into the product of two distributions:  P π(x′, y′, N |x, y, j) = P π(x′, N |x, j) · P π(y′  (2) where x and x′ give values for the variables in X, and y and y′ give values for the variables in Y ; and (b) for any pair of states s1 = (x, y1) and s2 = (x, y2) and any child action j, V π(j, s1) = V π(j, s2).  |y, j),  In the Taxi problem, the source and destination of the passenger are irrelevant to the Navigate(t) subtask—only the target t and the current taxi position are relevant.  Condition 2: Leaf Irrelevance. A set of state variables Y is irrelevant for a primitive action a if for any pair of states s1 and s2 that diﬀer only in their values for the variables in Y , P (s′  1|s1, a)R(s′  2|s2, a)R(s′  1|s1, a) =  2|s2, a).  P (s′  Xs′  1  Xs′  2  This condition is satisﬁed by the primitive actions North, South, East, and West in the taxi task, where all state variables are irrelevant because R is constant.  The next two conditions involve “funnel” actions—macro actions that move the environment from some large number of possible states to a small number of resulting states. The completion function of such subtasks can be represented using a number of values proportional to the number of resulting states.  Condition 3: Result Distribution Irrelevance (Undiscounted case.) A set of state variables Yj is irrelevant for the result distribution of action j if, for all abstract policies π executed by Mj and its descendants in the MAXQ hierarchy, the following holds: for all pairs of states s1 and s2 that diﬀer only in their values for the state variables in Yj,  ∀ s′ P π(s′  |s1, j) = P π(s′  |s2, j).  Consider, for example, the Get subroutine under an optimal policy for the taxi task. Regardless of the taxi’s position in state s, the taxi will be at the passenger’s starting location when Get ﬁnishes executing (i.e., because the taxi will have just completed picking up the passenger). Hence, the taxi’s initial position is irrelevant to its resulting position. (Note that this is only true in the undiscounted setting— with discounting, the result distributions are not the same because the number of  steps N required for Get to ﬁnish depends very much on the starting location of the taxi. Hence this form of state abstraction is rarely useful for cumulative discounted reward.)  Condition 4: Termination. Let Mj be a child task of Mi with the property that whenever Mj terminates, it causes Mi to terminate too. Then the completion cost C(i, s, j) = 0 and does not need to be represented. This is a particular kind of funnel action—it funnels all states into terminal states for Mi.  For example, in the Taxi task, in all states where the taxi is holding the passenger, the Put subroutine will succeed and result in a terminal state for Root. This is because the termination predicate for Put (i.e., that the passenger is at his or her destination location) implies the termination condition for Root (which is the same). This means that C(Root, s, Put) is uniformly zero, for all states s where Put is not terminated.  Condition 5: Shielding. Consider subtask Mi and let s be a state such that for all paths from the root of the DAG down to Mi, there exists a subtask that is terminated. Then no C values need to be represented for subtask Mi in state s, because it can never be executed in s.  In the Taxi task, a simple example of this arises in the Put task, which is terminated in all states where the passenger is not in the taxi. This means that we do not need to represent C(Root, s, Put) in these states. The result is that, when combined with the Termination condition above, we do not need to explicitly represent the completion function for Put at all!  By applying these abstraction conditions to the Taxi task, the value function can be represented using 632 values, which is much less than the 3,000 values required by ﬂat Q learning. Without state abstractions, MAXQ requires 14,000 values!  Theorem 2 (Convergence with State Abstraction) Let H be a MAXQ task graph that incorporates the ﬁve kinds of state abstractions deﬁned above. Let πx be an ordered GLIE exploration policy that is abstract. Then under the same conditions as Theorem 1, MAXQ-Q converges with probability 1 to the unique recursively optimal policy π∗  r deﬁned by πx and H.  Proof: (sketch) Consider a subtask Mi with relevant variables X and two arbitrary states (x, y1) and (x, y2). We ﬁrst show that under the ﬁve abstraction conditions, the value function of π∗ r can be represented using C(i, x, j) (i.e., ignorx′,N P (x′, N |x, j)V (i, x′), a ing the y values). To learn the values of C(i, x, j) = Q-learning algorithm needs samples of x′ and N drawn according to P (x′, N |x, j). The second part of the proof involves showing that regardless of whether we execute j in state (x, y1) or in (x, y2), the resulting x′ and N will have the same distribution, and hence, give the correct expectations. Analogous arguments apply for leaf irrelevance and V (a, x). The termination and shielding cases are easy.  P  4 Experimental Results  We implemented MAXQ-Q for a noisy version of the Taxi domain and for Kaelbling’s HDG navigation task [5] using Boltzmann exploration. Figure 2 shows the performance of ﬂat Q and MAXQ-Q with and without state abstractions on these tasks. Learning rates and Boltzmann cooling rates were separately tuned to optimize the performance of each method. The results show that without state abstractions, MAXQ-Q learning is slower to converge than ﬂat Q learning, but that with state abstraction, it is much faster.  MAXQ+Abstraction  MAXQ+Abstraction  Flat Q  MAXQ No Abstraction  d r a w e R   e v i t a l u m u C n a e     M  0  -20  -40  -60  -80  -100  -120  -140  d r a w e R   e v i t a l u m u C n a e     M  200  0  -200  -400  -600  -800  -1000  MAXQ No Abstraction  Flat Q  5 Conclusion  0  20000  40000  60000  80000 100000 120000 140000 160000  0  200000  400000  1e+06  1.2e+06  1.4e+06  800000 600000 Primitive Actions  Primitive Actions  Figure 2: Comparison of MAXQ-Q with and without state abstraction to ﬂat Q learning on a noisy taxi domain (left) and Kaelbling’s HDG task (right). The horizontal axis gives the number of primitive actions executed by each method. The vertical axis plots the average of 100 separate runs.  This paper has shown that by understanding the reasons that state variables are irrelevant, we can obtain a simple proof of the convergence of MAXQ-Q learning under state abstraction. This is much more fruitful than previous eﬀorts based only on weak notions of state aggregation [10], and it suggests that future research should focus on identifying other conditions that permit safe state abstraction.  References  [1] D. Precup and R. S. Sutton, “Multi-time models for temporally abstract planning,”  in NIPS10, The MIT Press, 1998.  [2] R. S. Sutton, D. Precup, and S. Singh, “Between MDPs and Semi-MDPs: Learning, planning, and representing knowledge at multiple temporal scales,” tech. rep., Univ. Mass., Dept. Comp. Inf. Sci., Amherst, MA, 1998.  [3] R. Parr and S. Russell, “Reinforcement learning with hierarchies of machines,” in  NIPS-10, The MIT Press, 1998.  [4] S. P. Singh, “Transfer of learning by composing solutions of elemental sequential  tasks,” Machine Learning, vol. 8, p. 323, 1992.  [5] L. P. Kaelbling, “Hierarchical reinforcement learning: Preliminary results,” in Pro ceedings ICML-10, pp. 167–173, Morgan Kaufmann, 1993.  [6] M. Hauskrecht, N. Meuleau, C. Boutilier, L. Kaelbling, and T. Dean, “Hierarchical solution of Markov decision processes using macro-actions,” tech. rep., Brown Univ., Dept. Comp. Sci., Providence, RI, 1998.  [7] P. Dayan and G. Hinton, “Feudal reinforcement learning,” in NIPS-5, pp. 271–278,  San Francisco, CA: Morgan Kaufmann, 1993.  [8] T. G. Dietterich, “The MAXQ method for hierarchical reinforcement learning,” in  ICML-15, Morgan Kaufmann, 1998.  [9] S. Singh, T. Jaakkola, M. L. Littman, and C. Szpesvari, “Convergence results for single-step on-policy reinforcement-learning algorithms,” tech. rep., Univ. Col., Dept. Comp. Sci., Boulder, CO, 1998.  [10] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-Dynamic Programming. Belmont, MA:  Athena Scientiﬁc, 1996.  [11] T. Jaakkola, M. I. Jordan, and S. P. Singh, “On the convergence of stochastic iterative dynamic programming algorithms,” Neur. Comp., vol. 6, no. 6, pp. 1185–1201, 1994.  '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Calculate Freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens= nltk.sent_tokenize(cleanedText)\n",
    "word_tokens = nltk.word_tokenize(cleanedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate frequency\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wordFreqs = {}\n",
    "\n",
    "for word in word_tokens:\n",
    "    if word not in stopwords:\n",
    "        if word not in wordFreqs.keys():\n",
    "            wordFreqs[word] = 1\n",
    "        else:\n",
    "            wordFreqs[word] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 2,\n",
       " '(': 114,\n",
       " ')': 114,\n",
       " '+': 8,\n",
       " '+20': 1,\n",
       " ',': 286,\n",
       " '-100': 1,\n",
       " '-1000': 1,\n",
       " '-120': 1,\n",
       " '-140': 1,\n",
       " '-20': 1,\n",
       " '-200': 1,\n",
       " '-40': 1,\n",
       " '-400': 1,\n",
       " '-60': 1,\n",
       " '-600': 1,\n",
       " '-80': 1,\n",
       " '-800': 1,\n",
       " '.': 174,\n",
       " '/': 1,\n",
       " '0': 11,\n",
       " '1': 22,\n",
       " '1.2e+06': 1,\n",
       " '1.4e+06': 1,\n",
       " '10': 3,\n",
       " '100': 1,\n",
       " '100000': 1,\n",
       " '11': 2,\n",
       " '1185–1201': 1,\n",
       " '120000': 1,\n",
       " '14,000': 1,\n",
       " '140000': 1,\n",
       " '160000': 1,\n",
       " '167–173': 1,\n",
       " '1992': 1,\n",
       " '1993': 2,\n",
       " '1994': 1,\n",
       " '1996': 1,\n",
       " '1998': 6,\n",
       " '1e+06': 1,\n",
       " '1|s1': 2,\n",
       " '2': 12,\n",
       " '200': 1,\n",
       " '20000': 1,\n",
       " '200000': 1,\n",
       " '271–278': 1,\n",
       " '2|s2': 2,\n",
       " '3': 7,\n",
       " '3,000': 1,\n",
       " '323': 1,\n",
       " '4': 6,\n",
       " '4.5': 1,\n",
       " '40000': 1,\n",
       " '400000': 1,\n",
       " '5': 7,\n",
       " '6': 4,\n",
       " '60000': 1,\n",
       " '600000': 1,\n",
       " '632': 1,\n",
       " '7': 2,\n",
       " '8': 4,\n",
       " '80000': 1,\n",
       " '800000': 1,\n",
       " '9': 7,\n",
       " '97331-3202': 1,\n",
       " ':': 26,\n",
       " ';': 3,\n",
       " '<': 1,\n",
       " '=': 18,\n",
       " '>': 1,\n",
       " '@': 1,\n",
       " 'A': 11,\n",
       " 'Abstract': 1,\n",
       " 'Abstraction': 5,\n",
       " 'Actions': 2,\n",
       " 'Ai': 1,\n",
       " 'Amherst': 1,\n",
       " 'An': 1,\n",
       " 'Analogous': 1,\n",
       " 'Athena': 1,\n",
       " 'B': 2,\n",
       " 'Bellman': 1,\n",
       " 'Belmont': 1,\n",
       " 'Bertsekas': 2,\n",
       " 'Between': 1,\n",
       " 'Boltzmann': 2,\n",
       " 'Boulder': 1,\n",
       " 'Boutilier': 1,\n",
       " 'Brown': 1,\n",
       " 'By': 2,\n",
       " 'C': 14,\n",
       " 'C.': 3,\n",
       " 'CA': 1,\n",
       " 'CO': 1,\n",
       " 'Category': 1,\n",
       " 'Closely': 1,\n",
       " 'Col.': 1,\n",
       " 'Comp': 3,\n",
       " 'Comp.': 1,\n",
       " 'Comparison': 1,\n",
       " 'Computer': 1,\n",
       " 'Conclusion': 1,\n",
       " 'Condition': 5,\n",
       " 'Conditions': 1,\n",
       " 'Consider': 4,\n",
       " 'Control': 1,\n",
       " 'Convergence': 2,\n",
       " 'Corvallis': 1,\n",
       " 'D.': 3,\n",
       " 'DAG': 4,\n",
       " 'Dayan': 2,\n",
       " 'Dean': 1,\n",
       " 'Decision': 1,\n",
       " 'Deliver': 1,\n",
       " 'Department': 1,\n",
       " 'Dept': 3,\n",
       " 'Deﬁne': 1,\n",
       " 'Dietterich': 2,\n",
       " 'Distribution': 1,\n",
       " 'Domain': 1,\n",
       " 'E': 2,\n",
       " 'Each': 3,\n",
       " 'East': 3,\n",
       " 'Even': 1,\n",
       " 'Experimental': 1,\n",
       " 'Feudal': 1,\n",
       " 'Feudal-Q': 1,\n",
       " 'Fig': 1,\n",
       " 'Figure': 4,\n",
       " 'Flat': 2,\n",
       " 'For': 2,\n",
       " 'Framework': 1,\n",
       " 'Francisco': 1,\n",
       " 'G': 3,\n",
       " 'G.': 3,\n",
       " 'GLIE': 4,\n",
       " 'Get': 8,\n",
       " 'Graph': 1,\n",
       " 'H': 3,\n",
       " 'H.': 1,\n",
       " 'HAM': 1,\n",
       " 'HAMQ': 1,\n",
       " 'HDG': 2,\n",
       " 'Hauskrecht': 1,\n",
       " 'Hence': 4,\n",
       " 'Hierarchical': 3,\n",
       " 'Hinton': 2,\n",
       " 'However': 2,\n",
       " 'I.': 1,\n",
       " 'ICML-10': 1,\n",
       " 'ICML-15': 1,\n",
       " 'If': 1,\n",
       " 'In': 8,\n",
       " 'Inf': 1,\n",
       " 'Introduction': 1,\n",
       " 'Irrelevance': 3,\n",
       " 'J.': 1,\n",
       " 'Jaakkola': 2,\n",
       " 'Jordan': 1,\n",
       " 'Kaelbling': 4,\n",
       " 'Kaufmann': 3,\n",
       " 'L': 1,\n",
       " 'L.': 3,\n",
       " 'Leaf': 1,\n",
       " 'Learning': 6,\n",
       " 'Left': 1,\n",
       " 'Let': 11,\n",
       " 'Littman': 1,\n",
       " 'M': 10,\n",
       " 'M.': 3,\n",
       " 'M0': 3,\n",
       " 'MA': 2,\n",
       " 'MAXQ': 14,\n",
       " 'MAXQ+Abstraction': 2,\n",
       " 'MAXQ-Q': 14,\n",
       " 'MDP': 5,\n",
       " 'MDPs': 1,\n",
       " 'MIT': 2,\n",
       " 'Machine': 1,\n",
       " 'Many': 1,\n",
       " 'Markov': 2,\n",
       " 'Mass.': 1,\n",
       " 'Meuleau': 1,\n",
       " 'Mi': 18,\n",
       " 'Mj': 3,\n",
       " 'Mk': 1,\n",
       " 'Mn': 1,\n",
       " 'Morgan': 3,\n",
       " 'Most': 1,\n",
       " 'Multi-time': 1,\n",
       " 'N': 17,\n",
       " 'N.': 2,\n",
       " 'NIPS-10': 1,\n",
       " 'NIPS-5': 1,\n",
       " 'NIPS10': 1,\n",
       " 'Navigate': 3,\n",
       " 'Neur': 1,\n",
       " 'Neuro-Dynamic': 1,\n",
       " 'No': 2,\n",
       " 'North': 3,\n",
       " 'Note': 1,\n",
       " 'On': 1,\n",
       " 'Options': 1,\n",
       " 'Oral': 1,\n",
       " 'Oregon': 2,\n",
       " 'Our': 1,\n",
       " 'P': 15,\n",
       " 'P.': 5,\n",
       " 'Parr': 1,\n",
       " 'Perhaps': 1,\n",
       " 'Pickup': 4,\n",
       " 'Precup': 2,\n",
       " 'Preference': 1,\n",
       " 'Preliminary': 1,\n",
       " 'Press': 2,\n",
       " 'Primitive': 2,\n",
       " 'Pro': 1,\n",
       " 'Programming': 1,\n",
       " 'Proof': 2,\n",
       " 'Proposition': 1,\n",
       " 'Providence': 1,\n",
       " 'Pt': 1,\n",
       " 'Put': 9,\n",
       " 'Putdown': 4,\n",
       " 'Q': 15,\n",
       " 'Q-learning': 1,\n",
       " 'Q-value': 1,\n",
       " 'R': 8,\n",
       " 'R.': 3,\n",
       " 'RI': 1,\n",
       " 'RL': 5,\n",
       " 'References': 1,\n",
       " 'Regardless': 1,\n",
       " 'Reinforcement': 3,\n",
       " 'Result': 1,\n",
       " 'Results': 1,\n",
       " 'Ri': 1,\n",
       " 'Right': 1,\n",
       " 'Root': 5,\n",
       " 'Russell': 1,\n",
       " 'S': 1,\n",
       " 'S.': 7,\n",
       " 'Safe': 1,\n",
       " 'San': 1,\n",
       " 'Sci.': 3,\n",
       " 'Science': 1,\n",
       " 'Scientiﬁc': 1,\n",
       " 'Second': 1,\n",
       " 'Semi-MDPs': 1,\n",
       " 'Semi-Markov': 1,\n",
       " 'Shielding': 1,\n",
       " 'Similarly': 1,\n",
       " 'Singh': 4,\n",
       " 'Some': 1,\n",
       " 'South': 3,\n",
       " 'State': 4,\n",
       " 'Subtask': 1,\n",
       " 'Such': 1,\n",
       " 'Sutton': 2,\n",
       " 'Szpesvari': 1,\n",
       " 'T': 4,\n",
       " 'T.': 4,\n",
       " 'Task': 2,\n",
       " 'Taxi': 7,\n",
       " 'Termination': 2,\n",
       " 'The': 32,\n",
       " 'Then': 4,\n",
       " 'Theorem': 3,\n",
       " 'There': 6,\n",
       " 'This': 14,\n",
       " 'Thomas': 1,\n",
       " 'Ti': 4,\n",
       " 'To': 5,\n",
       " 'Transfer': 1,\n",
       " 'Tsitsiklis': 2,\n",
       " 'Undiscounted': 1,\n",
       " 'Univ': 2,\n",
       " 'Univ.': 1,\n",
       " 'University': 1,\n",
       " 'V': 10,\n",
       " 'We': 8,\n",
       " 'West': 3,\n",
       " 'What': 1,\n",
       " 'When': 3,\n",
       " 'While': 1,\n",
       " 'Without': 3,\n",
       " 'X': 10,\n",
       " 'Xs′': 2,\n",
       " 'Y': 7,\n",
       " 'Yj': 2,\n",
       " '[': 25,\n",
       " ']': 25,\n",
       " 'abstract': 7,\n",
       " 'abstraction': 20,\n",
       " 'abstractions': 14,\n",
       " 'access': 1,\n",
       " 'according': 3,\n",
       " 'action': 23,\n",
       " 'actions': 17,\n",
       " 'actions—macro': 1,\n",
       " 'action—it': 1,\n",
       " 'addition': 1,\n",
       " 'additional': 1,\n",
       " 'aggregation': 1,\n",
       " 'algorithm': 7,\n",
       " 'algorithms': 3,\n",
       " 'also': 1,\n",
       " 'among': 3,\n",
       " 'applicability': 1,\n",
       " 'application': 2,\n",
       " 'applied': 1,\n",
       " 'apply': 2,\n",
       " 'applying': 1,\n",
       " 'approach': 1,\n",
       " 'approaches—and': 1,\n",
       " 'approximation': 2,\n",
       " 'approximations': 1,\n",
       " 'arbitrary': 1,\n",
       " 'argue': 1,\n",
       " 'argument': 2,\n",
       " 'arguments': 1,\n",
       " 'arises': 1,\n",
       " 'aspects': 2,\n",
       " 'assume': 2,\n",
       " 'attempts': 1,\n",
       " 'average': 1,\n",
       " 'axis': 2,\n",
       " 'a′': 3,\n",
       " 'aﬀect': 1,\n",
       " 'b': 3,\n",
       " 'based': 3,\n",
       " 'basic': 1,\n",
       " 'bedroom': 1,\n",
       " 'beginning': 1,\n",
       " 'behavior': 1,\n",
       " 'bound': 1,\n",
       " 'bounded': 1,\n",
       " 'breaks': 1,\n",
       " 'brief': 1,\n",
       " 'brieﬂy': 1,\n",
       " 'broken': 1,\n",
       " 'c': 3,\n",
       " 'call': 1,\n",
       " 'car': 1,\n",
       " 'carrying': 1,\n",
       " 'case': 4,\n",
       " 'cases': 4,\n",
       " 'cause': 1,\n",
       " 'causes': 1,\n",
       " 'ceedings': 1,\n",
       " 'characterization': 1,\n",
       " 'child': 7,\n",
       " 'choices': 1,\n",
       " 'choose': 1,\n",
       " 'chosen': 2,\n",
       " 'cid:12': 5,\n",
       " 'clever': 1,\n",
       " 'column': 1,\n",
       " 'combination': 1,\n",
       " 'combined': 2,\n",
       " 'comes': 1,\n",
       " 'compares': 1,\n",
       " 'complete': 1,\n",
       " 'completed': 1,\n",
       " 'completely': 2,\n",
       " 'completes': 1,\n",
       " 'completion': 4,\n",
       " 'composing': 1,\n",
       " 'condition': 5,\n",
       " 'conditions': 9,\n",
       " 'consider': 2,\n",
       " 'consideration': 1,\n",
       " 'consistent': 1,\n",
       " 'constant': 1,\n",
       " 'constants': 1,\n",
       " 'constructs': 1,\n",
       " 'continue': 1,\n",
       " 'contraction': 2,\n",
       " 'controller': 2,\n",
       " 'controllers': 1,\n",
       " 'converge': 2,\n",
       " 'converged': 1,\n",
       " 'convergence': 5,\n",
       " 'converges': 5,\n",
       " 'convert': 1,\n",
       " 'cooling': 1,\n",
       " 'correct': 1,\n",
       " 'cost': 2,\n",
       " 'could': 3,\n",
       " 'cs.orst.edu': 1,\n",
       " 'cumulative': 1,\n",
       " 'current': 4,\n",
       " 'deciding': 1,\n",
       " 'decision': 2,\n",
       " 'decisions': 1,\n",
       " 'decomposition': 3,\n",
       " 'deliver': 1,\n",
       " 'delivering': 1,\n",
       " 'denote': 1,\n",
       " 'depending': 2,\n",
       " 'depends': 1,\n",
       " 'deposited': 1,\n",
       " 'descendants': 3,\n",
       " 'describes': 1,\n",
       " 'desired': 1,\n",
       " 'destination': 7,\n",
       " 'deterministic': 2,\n",
       " 'developed': 1,\n",
       " 'deﬁne': 2,\n",
       " 'deﬁned': 6,\n",
       " 'deﬁnes': 1,\n",
       " 'deﬁning': 1,\n",
       " 'directly': 1,\n",
       " 'discount': 1,\n",
       " 'discounted': 5,\n",
       " 'discounting': 1,\n",
       " 'discussed': 1,\n",
       " 'distance': 1,\n",
       " 'distribution': 3,\n",
       " 'distributions': 2,\n",
       " 'diﬀer': 2,\n",
       " 'diﬀerent': 5,\n",
       " 'domain': 3,\n",
       " 'drawn': 1,\n",
       " 'due': 1,\n",
       " 'dynamic': 1,\n",
       " 'e': 6,\n",
       " 'e.g.': 1,\n",
       " 'easy': 1,\n",
       " 'ed': 1,\n",
       " 'either': 2,\n",
       " 'elapsed': 1,\n",
       " 'elemental': 1,\n",
       " 'eliminating': 1,\n",
       " 'ellow': 1,\n",
       " 'employed': 2,\n",
       " 'ends': 1,\n",
       " 'ensure': 1,\n",
       " 'entire': 1,\n",
       " 'environment': 2,\n",
       " 'episode': 2,\n",
       " 'episodic': 1,\n",
       " 'equal': 1,\n",
       " 'equation': 1,\n",
       " 'essential': 1,\n",
       " 'every': 1,\n",
       " 'example': 5,\n",
       " 'exception': 1,\n",
       " 'execute': 4,\n",
       " 'executed': 8,\n",
       " 'executes': 1,\n",
       " 'executing': 5,\n",
       " 'exists': 1,\n",
       " 'expectations': 1,\n",
       " 'expected': 1,\n",
       " 'experimentally': 1,\n",
       " 'experiments': 1,\n",
       " 'explicitly': 1,\n",
       " 'exploration': 3,\n",
       " 'explore': 1,\n",
       " 'explored': 1,\n",
       " 'express': 1,\n",
       " 'expressed': 1,\n",
       " 'extended': 1,\n",
       " 'eﬀective': 1,\n",
       " 'eﬀectiveness': 1,\n",
       " 'eﬀorts': 1,\n",
       " 'fact': 1,\n",
       " 'factor': 1,\n",
       " 'factored': 1,\n",
       " 'faster': 1,\n",
       " 'focus': 1,\n",
       " 'focused': 1,\n",
       " 'following': 3,\n",
       " 'follows': 1,\n",
       " 'form': 2,\n",
       " 'formal': 1,\n",
       " 'four': 5,\n",
       " 'framework': 3,\n",
       " 'fruitful': 1,\n",
       " 'function': 20,\n",
       " 'functions': 3,\n",
       " 'funnel': 2,\n",
       " 'funnels': 1,\n",
       " 'future': 1,\n",
       " 'general': 1,\n",
       " 'generalized': 1,\n",
       " 'get': 1,\n",
       " 'give': 3,\n",
       " 'given': 1,\n",
       " 'gives': 2,\n",
       " 'go': 2,\n",
       " 'goal': 1,\n",
       " 'graph': 1,\n",
       " 'greedy': 1,\n",
       " 'hS': 1,\n",
       " 'hence': 1,\n",
       " 'hierarchical': 13,\n",
       " 'hierarchies': 1,\n",
       " 'hierarchy': 2,\n",
       " 'hit': 1,\n",
       " 'hold': 1,\n",
       " 'holding': 1,\n",
       " 'holds': 1,\n",
       " 'horizon': 1,\n",
       " 'horizontal': 1,\n",
       " 'however': 1,\n",
       " 'human-level': 1,\n",
       " 'i.e.': 4,\n",
       " 'ideas': 1,\n",
       " 'identify': 1,\n",
       " 'identifying': 2,\n",
       " 'identity': 1,\n",
       " 'ignored': 1,\n",
       " 'ignorx′': 1,\n",
       " 'illegally': 1,\n",
       " 'illustrates': 1,\n",
       " 'implemented': 1,\n",
       " 'implies': 1,\n",
       " 'important': 3,\n",
       " 'importantly': 1,\n",
       " 'include': 1,\n",
       " 'incorporated': 1,\n",
       " 'incorporates': 1,\n",
       " 'indirectly': 1,\n",
       " 'inductive': 1,\n",
       " 'ing': 1,\n",
       " 'initial': 1,\n",
       " 'initiated': 1,\n",
       " 'intelligence': 1,\n",
       " 'introduce': 2,\n",
       " 'introduced': 1,\n",
       " 'introducing': 1,\n",
       " 'invoke': 2,\n",
       " 'invoked': 1,\n",
       " 'involve': 2,\n",
       " 'involves': 2,\n",
       " 'inﬁnite': 1,\n",
       " 'inﬁnite-horizon': 1,\n",
       " 'inﬁnitely': 2,\n",
       " 'irrelevance': 1,\n",
       " 'irrelevant': 9,\n",
       " 'irrelevant—it': 1,\n",
       " 'iterative': 1,\n",
       " 'j': 45,\n",
       " 'j′': 2,\n",
       " 'key': 1,\n",
       " 'kind': 1,\n",
       " 'kinds': 1,\n",
       " 'kitchen': 1,\n",
       " 'knowledge': 1,\n",
       " 'known': 1,\n",
       " 'l': 2,\n",
       " 'large': 1,\n",
       " 'leaf': 1,\n",
       " 'learn': 6,\n",
       " 'learning': 25,\n",
       " 'learns': 1,\n",
       " 'left': 1,\n",
       " 'less': 2,\n",
       " 'let': 3,\n",
       " 'level': 1,\n",
       " 'lim': 2,\n",
       " 'little': 1,\n",
       " 'local': 1,\n",
       " 'location': 12,\n",
       " 'locations': 4,\n",
       " 'lue': 1,\n",
       " 'machines': 1,\n",
       " 'macro': 2,\n",
       " 'macro-actions': 1,\n",
       " 'main': 1,\n",
       " 'making': 1,\n",
       " 'many': 2,\n",
       " 'marked': 1,\n",
       " 'max': 4,\n",
       " 'may': 2,\n",
       " 'means': 2,\n",
       " 'merit': 1,\n",
       " 'method': 8,\n",
       " 'methods': 2,\n",
       " 'methods—and': 1,\n",
       " 'models': 1,\n",
       " 'motivate': 1,\n",
       " 'move': 3,\n",
       " 'much': 4,\n",
       " 'multiple': 1,\n",
       " 'must': 4,\n",
       " 'n': 2,\n",
       " 'nagivation': 1,\n",
       " 'navigating': 2,\n",
       " 'navigation': 3,\n",
       " 'need': 8,\n",
       " 'needs': 1,\n",
       " 'never': 1,\n",
       " 'next': 1,\n",
       " 'no-op': 1,\n",
       " 'node—no': 1,\n",
       " 'noise': 1,\n",
       " 'noisy': 2,\n",
       " 'non-deterministic': 2,\n",
       " 'non-stationary': 1,\n",
       " 'norm': 2,\n",
       " 'notable': 1,\n",
       " 'notions': 1,\n",
       " 'number': 8,\n",
       " 'numbers': 1,\n",
       " 'observable': 1,\n",
       " 'observation': 1,\n",
       " 'observe': 1,\n",
       " 'obtain': 2,\n",
       " 'obvious': 1,\n",
       " 'obvious—for': 1,\n",
       " 'often': 2,\n",
       " 'omit': 1,\n",
       " 'on-policy': 1,\n",
       " 'one': 6,\n",
       " 'online': 1,\n",
       " 'optimal': 9,\n",
       " 'optimize': 1,\n",
       " 'option': 1,\n",
       " 'options': 1,\n",
       " 'ordered': 4,\n",
       " 'ordering': 2,\n",
       " 'p.': 1,\n",
       " 'pair': 2,\n",
       " 'pairs': 1,\n",
       " 'paper': 5,\n",
       " 'parent': 1,\n",
       " 'part': 1,\n",
       " 'partially': 1,\n",
       " 'particular': 1,\n",
       " 'partition': 1,\n",
       " 'partitioned': 1,\n",
       " 'passenger': 20,\n",
       " 'paths': 1,\n",
       " 'perform': 2,\n",
       " 'performance': 3,\n",
       " 'performing': 3,\n",
       " 'perhaps': 1,\n",
       " 'permit': 2,\n",
       " 'permits': 1,\n",
       " 'pick': 1,\n",
       " 'picking': 1,\n",
       " 'pickup': 1,\n",
       " 'planning': 2,\n",
       " 'plots': 1,\n",
       " 'points': 1,\n",
       " 'policies': 6,\n",
       " 'policy': 31,\n",
       " 'position': 4,\n",
       " 'possible': 1,\n",
       " 'potentially': 1,\n",
       " 'pp': 3,\n",
       " 'predicate': 2,\n",
       " 'preferences': 1,\n",
       " 'preferring': 1,\n",
       " 'presence': 1,\n",
       " 'previous': 3,\n",
       " 'primitive': 9,\n",
       " 'primitives': 1,\n",
       " 'probability': 9,\n",
       " 'problem': 4,\n",
       " 'problems': 1,\n",
       " 'procedure-call-and-return': 1,\n",
       " 'processes': 1,\n",
       " 'product': 1,\n",
       " 'programmer': 3,\n",
       " 'programming': 1,\n",
       " 'proof': 6,\n",
       " 'proper': 1,\n",
       " 'properties': 1,\n",
       " 'property': 1,\n",
       " 'proportional': 1,\n",
       " 'prove': 3,\n",
       " 'provide': 1,\n",
       " 'provides': 1,\n",
       " 'put': 1,\n",
       " 'r': 5,\n",
       " 'randomly': 2,\n",
       " 'randomly-chosen': 1,\n",
       " 'rarely': 1,\n",
       " 'rates': 2,\n",
       " 'reach': 1,\n",
       " 'reasons': 1,\n",
       " 'received': 1,\n",
       " 'recursion': 1,\n",
       " 'recursively': 7,\n",
       " 'reen': 1,\n",
       " 'regardless': 1,\n",
       " 'reinforcement': 6,\n",
       " 'reinforcement-learning': 1,\n",
       " 'related': 1,\n",
       " 'relevant': 4,\n",
       " 'rep.': 3,\n",
       " 'represent': 2,\n",
       " 'represented': 6,\n",
       " 'representing': 1,\n",
       " 'require': 1,\n",
       " 'required': 3,\n",
       " 'requires': 1,\n",
       " 'research': 1,\n",
       " 'researchers': 1,\n",
       " 'result': 4,\n",
       " 'resulting': 6,\n",
       " 'results': 6,\n",
       " 'return': 1,\n",
       " 'returns': 1,\n",
       " 'reward': 9,\n",
       " 'rewards': 2,\n",
       " 'reﬁned': 1,\n",
       " 'reﬁnement': 1,\n",
       " 'right': 1,\n",
       " 'root': 3,\n",
       " 'row': 1,\n",
       " 'rt': 2,\n",
       " 'runs': 1,\n",
       " 's.': 3,\n",
       " 's1': 4,\n",
       " 's2': 4,\n",
       " 'safe': 1,\n",
       " 'safely': 2,\n",
       " 'said': 1,\n",
       " 'samples': 1,\n",
       " 'satisfying': 2,\n",
       " 'satisﬁed': 2,\n",
       " 'scale': 1,\n",
       " 'scales': 1,\n",
       " 'second': 3,\n",
       " 'see': 2,\n",
       " 'selecting': 1,\n",
       " 'semantics': 1,\n",
       " 'semi-Markov': 1,\n",
       " 'separate': 2,\n",
       " 'separately': 1,\n",
       " 'sequence': 2,\n",
       " 'sequential': 1,\n",
       " 'set': 7,\n",
       " 'sets': 1,\n",
       " 'setting—': 1,\n",
       " 'shielding': 1,\n",
       " 'show': 6,\n",
       " 'showing': 1,\n",
       " 'shown': 2,\n",
       " 'shows': 2,\n",
       " 'simple': 4,\n",
       " 'simply': 1,\n",
       " 'simultaneously—it': 1,\n",
       " 'single': 1,\n",
       " 'single-step': 1,\n",
       " 'situations': 1,\n",
       " 'six': 1,\n",
       " 'sketch': 2,\n",
       " 'slower': 1,\n",
       " 'small': 1,\n",
       " 'solution': 1,\n",
       " 'solutions': 1,\n",
       " 'solved': 2,\n",
       " 'solves': 1,\n",
       " 'source': 3,\n",
       " 'space': 2,\n",
       " 'special': 1,\n",
       " 'square': 2,\n",
       " 'st': 1,\n",
       " 'standard': 2,\n",
       " 'starting': 4,\n",
       " 'starts': 1,\n",
       " 'state': 61,\n",
       " 'states': 17,\n",
       " 'stationary': 1,\n",
       " 'steps': 4,\n",
       " 'stochastic': 2,\n",
       " 'straightforward': 1,\n",
       " 'structure': 1,\n",
       " 'studies': 1,\n",
       " 'sub-tasks': 1,\n",
       " 'subroutine': 2,\n",
       " 'subsequent': 1,\n",
       " 'subset': 1,\n",
       " 'subsubtasks': 1,\n",
       " 'subtask': 26,\n",
       " 'subtasks': 8,\n",
       " 'subtask—only': 1,\n",
       " 'succeed': 1,\n",
       " 'successful': 1,\n",
       " 'successfully': 1,\n",
       " 'suggests': 1,\n",
       " 'sum': 2,\n",
       " 'support': 1,\n",
       " 'sure': 1,\n",
       " 's′': 19,\n",
       " 's′|s': 2,\n",
       " 't.': 1,\n",
       " 't/destination': 1,\n",
       " 't/source': 1,\n",
       " 't=1': 2,\n",
       " 'take': 1,\n",
       " 'target': 5,\n",
       " 'task': 14,\n",
       " 'tasks': 2,\n",
       " 'taxi': 19,\n",
       " 'tech': 3,\n",
       " 'temporal': 5,\n",
       " 'temporally': 3,\n",
       " 'term': 3,\n",
       " 'terminal': 2,\n",
       " 'terminate': 2,\n",
       " 'terminated': 3,\n",
       " 'terminates': 2,\n",
       " 'terminates—V': 1,\n",
       " 'terminating': 1,\n",
       " 'termination': 5,\n",
       " 'terms': 1,\n",
       " 'tgd': 1,\n",
       " 'think': 1,\n",
       " 'this—the': 1,\n",
       " 'ties': 2,\n",
       " 'time': 3,\n",
       " 'top': 1,\n",
       " 'transition': 5,\n",
       " 'transported': 1,\n",
       " 'treat': 1,\n",
       " 'true': 1,\n",
       " 'tuned': 1,\n",
       " 'turn': 1,\n",
       " 'two': 9,\n",
       " 'u': 4,\n",
       " 'u=0': 1,\n",
       " 'u=N': 1,\n",
       " 'understanding': 1,\n",
       " 'undiscounted': 2,\n",
       " 'unfolding': 1,\n",
       " 'uniformly': 1,\n",
       " 'unique': 3,\n",
       " 'unsafe': 1,\n",
       " 'update': 1,\n",
       " 'us': 1,\n",
       " 'usable': 1,\n",
       " 'useful': 1,\n",
       " 'using': 8,\n",
       " 'usual': 2,\n",
       " 'v': 4,\n",
       " 'value': 16,\n",
       " 'values': 12,\n",
       " 'variables': 15,\n",
       " 'variation': 1,\n",
       " 'vector': 1,\n",
       " 'version': 1,\n",
       " 'vertical': 1,\n",
       " 'view': 1,\n",
       " 'viewed': 1,\n",
       " 'visited': 1,\n",
       " 'vol': 2,\n",
       " 'w': 2,\n",
       " 'w.p': 1,\n",
       " 'wait': 1,\n",
       " 'walk': 1,\n",
       " 'wall': 1,\n",
       " 'way': 1,\n",
       " 'weak': 1,\n",
       " 'weighted': 2,\n",
       " 'whenever': 1,\n",
       " 'whether': 1,\n",
       " 'wide': 1,\n",
       " 'wishes': 1,\n",
       " 'within': 3,\n",
       " 'without': 5,\n",
       " 'work': 3,\n",
       " 'world': 2,\n",
       " 'would': 1,\n",
       " 'write': 1,\n",
       " 'x': 10,\n",
       " 'x′': 8,\n",
       " 'y1': 3,\n",
       " 'y2': 3,\n",
       " 'y′': 3,\n",
       " 'zero': 1,\n",
       " '{': 4,\n",
       " '|': 2,\n",
       " '|Ct': 1,\n",
       " '|Vt': 1,\n",
       " '|s': 4,\n",
       " '|s1': 1,\n",
       " '|s2': 1,\n",
       " '|st': 1,\n",
       " '|x': 4,\n",
       " '|y': 1,\n",
       " '}': 4,\n",
       " '·': 6,\n",
       " 'α2': 1,\n",
       " 'αt': 4,\n",
       " 'γ': 1,\n",
       " 'γ2rt+2': 1,\n",
       " 'γN': 2,\n",
       " 'γrt+1': 1,\n",
       " 'γurt+u': 2,\n",
       " 'π': 14,\n",
       " 'π0': 1,\n",
       " 'πi': 1,\n",
       " 'πn': 1,\n",
       " 'πx': 4,\n",
       " 'π∗': 2,\n",
       " '—': 1,\n",
       " '—each': 1,\n",
       " '’': 7,\n",
       " '“': 16,\n",
       " '”': 16,\n",
       " '→∞': 2,\n",
       " '∀': 1,\n",
       " '−': 1,\n",
       " '−1': 3,\n",
       " '−10': 1,\n",
       " '∞': 3,\n",
       " 'ﬁnish': 1,\n",
       " 'ﬁnishes': 1,\n",
       " 'ﬁnishing': 1,\n",
       " 'ﬁnite': 1,\n",
       " 'ﬁnite-horizon': 1,\n",
       " 'ﬁnitestate': 1,\n",
       " 'ﬁrst': 5,\n",
       " 'ﬁve': 4,\n",
       " 'ﬁxed': 1,\n",
       " 'ﬂat': 4}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordFreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'9': 0.024475524475524476, '1': 0.07692307692307693, 'M': 0.03496503496503497, '2': 0.04195804195804196, ']': 0.08741258741258741, 'G': 0.01048951048951049, 'L': 0.0034965034965034965, '.': 0.6083916083916084, 'c': 0.01048951048951049, '[': 0.08741258741258741, 'v': 0.013986013986013986, '5': 0.024475524475524476, '0': 0.038461538461538464, '/': 0.0034965034965034965, ':': 0.09090909090909091, 'X': 0.03496503496503497, 'r': 0.017482517482517484, 'State': 0.013986013986013986, 'Abstraction': 0.017482517482517484, 'MAXQ': 0.04895104895104895, 'Hierarchical': 0.01048951048951049, 'Reinforcement': 0.01048951048951049, 'Learning': 0.02097902097902098, 'Thomas': 0.0034965034965034965, 'G.': 0.01048951048951049, 'Dietterich': 0.006993006993006993, 'Department': 0.0034965034965034965, 'Computer': 0.0034965034965034965, 'Science': 0.0034965034965034965, 'Oregon': 0.006993006993006993, 'University': 0.0034965034965034965, 'Corvallis': 0.0034965034965034965, ',': 1.0, '97331-3202': 0.0034965034965034965, 'tgd': 0.0034965034965034965, '@': 0.0034965034965034965, 'cs.orst.edu': 0.0034965034965034965, 'Abstract': 0.0034965034965034965, 'Many': 0.0034965034965034965, 'researchers': 0.0034965034965034965, 'explored': 0.0034965034965034965, 'methods': 0.006993006993006993, 'hierarchical': 0.045454545454545456, 'reinforcement': 0.02097902097902098, 'learning': 0.08741258741258741, '(': 0.3986013986013986, 'RL': 0.017482517482517484, ')': 0.3986013986013986, 'temporal': 0.017482517482517484, 'abstractions': 0.04895104895104895, 'abstract': 0.024475524475524476, 'actions': 0.05944055944055944, 'deﬁned': 0.02097902097902098, 'perform': 0.006993006993006993, 'many': 0.006993006993006993, 'primitive': 0.03146853146853147, 'terminating': 0.0034965034965034965, 'However': 0.006993006993006993, 'little': 0.0034965034965034965, 'known': 0.0034965034965034965, 'state': 0.21328671328671328, 'aspects': 0.006993006993006993, 'space': 0.006993006993006993, 'ignored': 0.0034965034965034965, 'In': 0.027972027972027972, 'previous': 0.01048951048951049, 'work': 0.01048951048951049, 'developed': 0.0034965034965034965, 'method': 0.027972027972027972, 'paper': 0.017482517482517484, 'deﬁne': 0.006993006993006993, 'ﬁve': 0.013986013986013986, 'conditions': 0.03146853146853147, 'abstraction': 0.06993006993006994, 'combined': 0.006993006993006993, 'value': 0.055944055944055944, 'function': 0.06993006993006994, 'decomposition': 0.01048951048951049, 'We': 0.027972027972027972, 'prove': 0.01048951048951049, 'MAXQ-Q': 0.04895104895104895, 'algorithm': 0.024475524475524476, 'converges': 0.017482517482517484, 'show': 0.02097902097902098, 'experimentally': 0.0034965034965034965, 'important': 0.01048951048951049, 'successful': 0.0034965034965034965, 'application': 0.006993006993006993, 'Category': 0.0034965034965034965, 'Control': 0.0034965034965034965, 'Preference': 0.0034965034965034965, 'Oral': 0.0034965034965034965, 'Introduction': 0.0034965034965034965, 'Most': 0.0034965034965034965, 'focused': 0.0034965034965034965, 'For': 0.006993006993006993, 'example': 0.017482517482517484, 'Options': 0.0034965034965034965, 'framework': 0.01048951048951049, 'programmer': 0.01048951048951049, 'deﬁnes': 0.0034965034965034965, 'set': 0.024475524475524476, 'macro': 0.006993006993006993, '“': 0.055944055944055944, 'options': 0.0034965034965034965, '”': 0.055944055944055944, 'provides': 0.0034965034965034965, 'policy': 0.10839160839160839, 'algorithms': 0.01048951048951049, 'semi-Markov': 0.0034965034965034965, 'Q': 0.05244755244755245, 'treat': 0.0034965034965034965, 'temporally': 0.01048951048951049, 'primitives': 0.0034965034965034965, 'learn': 0.02097902097902098, 'selecting': 0.0034965034965034965, 'among': 0.01048951048951049, 'Closely': 0.0034965034965034965, 'related': 0.0034965034965034965, 'HAM': 0.0034965034965034965, 'constructs': 0.0034965034965034965, 'hierarchy': 0.006993006993006993, 'ﬁnitestate': 0.0034965034965034965, 'controllers': 0.0034965034965034965, '3': 0.024475524475524476, 'Each': 0.01048951048951049, 'controller': 0.006993006993006993, 'include': 0.0034965034965034965, 'non-deterministic': 0.006993006993006993, 'states': 0.05944055944055944, 'sure': 0.0034965034965034965, 'action': 0.08041958041958042, 'The': 0.11188811188811189, 'HAMQ': 0.0034965034965034965, 'applied': 0.0034965034965034965, 'making': 0.0034965034965034965, 'choices': 0.0034965034965034965, 'approaches—and': 0.0034965034965034965, 'studies': 0.0034965034965034965, 'e.g.': 0.0034965034965034965, '4': 0.02097902097902098, '6': 0.013986013986013986, '—each': 0.0034965034965034965, 'option': 0.0034965034965034965, 'ﬁnite': 0.0034965034965034965, 'must': 0.013986013986013986, 'access': 0.0034965034965034965, 'entire': 0.0034965034965034965, 'one': 0.02097902097902098, 'exception': 0.0034965034965034965, 'this—the': 0.0034965034965034965, 'Feudal-Q': 0.0034965034965034965, 'Dayan': 0.006993006993006993, 'Hinton': 0.006993006993006993, '7': 0.006993006993006993, '—': 0.0034965034965034965, 'introduced': 0.0034965034965034965, 'unsafe': 0.0034965034965034965, 'way': 0.0034965034965034965, 'resulting': 0.02097902097902098, 'problem': 0.013986013986013986, 'partially': 0.0034965034965034965, 'observable': 0.0034965034965034965, 'Hence': 0.013986013986013986, 'could': 0.01048951048951049, 'provide': 0.0034965034965034965, 'formal': 0.0034965034965034965, 'results': 0.02097902097902098, 'convergence': 0.017482517482517484, 'performance': 0.01048951048951049, 'Even': 0.0034965034965034965, 'brief': 0.0034965034965034965, 'consideration': 0.0034965034965034965, 'human-level': 0.0034965034965034965, 'intelligence': 0.0034965034965034965, 'shows': 0.006993006993006993, 'scale': 0.0034965034965034965, 'When': 0.01048951048951049, 'deciding': 0.0034965034965034965, 'walk': 0.0034965034965034965, 'bedroom': 0.0034965034965034965, 'kitchen': 0.0034965034965034965, 'need': 0.027972027972027972, 'think': 0.0034965034965034965, 'location': 0.04195804195804196, 'car': 0.0034965034965034965, 'Without': 0.01048951048951049, 'learns': 0.0034965034965034965, 'functions': 0.01048951048951049, 'separate': 0.006993006993006993, 'world': 0.006993006993006993, 'Some': 0.0034965034965034965, 'argue': 0.0034965034965034965, 'solved': 0.006993006993006993, 'clever': 0.0034965034965034965, 'approximation': 0.006993006993006993, 'methods—and': 0.0034965034965034965, 'merit': 0.0034965034965034965, 'view': 0.0034965034965034965, 'however': 0.0034965034965034965, 'explore': 0.0034965034965034965, 'diﬀerent': 0.017482517482517484, 'approach': 0.0034965034965034965, 'identify': 0.0034965034965034965, 'MDP': 0.017482517482517484, 'permit': 0.006993006993006993, 'safely': 0.006993006993006993, 'incorporated': 0.0034965034965034965, 'without': 0.017482517482517484, 'introducing': 0.0034965034965034965, 'approximations': 0.0034965034965034965, 'This': 0.04895104895104895, 'permits': 0.0034965034965034965, 'us': 0.0034965034965034965, 'obtain': 0.006993006993006993, 'ﬁrst': 0.017482517482517484, 'proof': 0.02097902097902098, 'optimal': 0.03146853146853147, 'presence': 0.0034965034965034965, 'introduce': 0.006993006993006993, 'within': 0.01048951048951049, '8': 0.013986013986013986, 'basic': 0.0034965034965034965, 'ideas': 0.0034965034965034965, 'general': 0.0034965034965034965, 'brieﬂy': 0.0034965034965034965, 'discussed': 0.0034965034965034965, 'employed': 0.006993006993006993, 'experiments': 0.0034965034965034965, 'converged': 0.0034965034965034965, 'usable': 0.0034965034965034965, 'characterization': 0.0034965034965034965, 'situations': 0.0034965034965034965, 'solves': 0.0034965034965034965, 'problems': 0.0034965034965034965, 'addition': 0.0034965034965034965, 'compares': 0.0034965034965034965, 'eﬀectiveness': 0.0034965034965034965, 'cases': 0.013986013986013986, 'essential': 0.0034965034965034965, 'eﬀective': 0.0034965034965034965, 'Framework': 0.0034965034965034965, 'Let': 0.038461538461538464, 'Markov': 0.006993006993006993, 'decision': 0.006993006993006993, 'S': 0.0034965034965034965, 'A': 0.038461538461538464, 'reward': 0.03146853146853147, 'R': 0.027972027972027972, 's′|s': 0.006993006993006993, 'probability': 0.03146853146853147, 'transition': 0.017482517482517484, 'P': 0.05244755244755245, 'Our': 0.0034965034965034965, 'apply': 0.006993006993006993, 'ﬁnite-horizon': 0.0034965034965034965, 'undiscounted': 0.006993006993006993, 'case': 0.013986013986013986, 'inﬁnite-horizon': 0.0034965034965034965, 'discounted': 0.017482517482517484, '{': 0.013986013986013986, 'M0': 0.01048951048951049, 'Mn': 0.0034965034965034965, '}': 0.013986013986013986, 'subtasks': 0.027972027972027972, 'subtask': 0.09090909090909091, 'Mi': 0.06293706293706294, 'termination': 0.017482517482517484, 'predicate': 0.006993006993006993, 'Ti': 0.013986013986013986, 'Ai': 0.0034965034965034965, 'may': 0.006993006993006993, 'goal': 0.0034965034965034965, 'move': 0.01048951048951049, 'environment': 0.006993006993006993, 'satisﬁed': 0.006993006993006993, 'reﬁned': 0.0034965034965034965, 'using': 0.027972027972027972, 'local': 0.0034965034965034965, 'express': 0.0034965034965034965, 'preferences': 0.0034965034965034965, 'satisfying': 0.006993006993006993, 'omit': 0.0034965034965034965, 'reﬁnement': 0.0034965034965034965, 'form': 0.006993006993006993, 'DAG': 0.013986013986013986, 'single': 0.0034965034965034965, 'root': 0.01048951048951049, 'node—no': 0.0034965034965034965, 'invoke': 0.006993006993006993, 'directly': 0.0034965034965034965, 'indirectly': 0.0034965034965034965, 'policies': 0.02097902097902098, 'π': 0.04895104895104895, '=': 0.06293706293706294, 'π0': 0.0034965034965034965, 'πn': 0.0034965034965034965, 'executed': 0.027972027972027972, 'standard': 0.006993006993006993, 'procedure-call-and-return': 0.0034965034965034965, 'semantics': 0.0034965034965034965, 'starting': 0.013986013986013986, 'task': 0.04895104895104895, 'unfolding': 0.0034965034965034965, 'recursively': 0.024475524475524476, 'invoked': 0.0034965034965034965, 'let': 0.01048951048951049, 's′': 0.06643356643356643, 'N': 0.05944055944055944, '|s': 0.013986013986013986, 'terminates': 0.006993006993006993, 'executing': 0.017482517482517484, 'πi': 0.0034965034965034965, 'given': 0.0034965034965034965, 'descendants': 0.01048951048951049, 'V': 0.03496503496503497, 'i.e.': 0.013986013986013986, 'following': 0.01048951048951049, 'reach': 0.0034965034965034965, 'Similarly': 0.0034965034965034965, 'j': 0.15734265734265734, 'child': 0.024475524475524476, 'current': 0.013986013986013986, 'based': 0.01048951048951049, 'observation': 0.0034965034965034965, 'viewed': 0.0034965034965034965, 'Semi-Markov': 0.0034965034965034965, 'Decision': 0.0034965034965034965, 'performing': 0.01048951048951049, 'equal': 0.0034965034965034965, 's.': 0.01048951048951049, 'To': 0.017482517482517484, 'see': 0.006993006993006993, 'consider': 0.006993006993006993, 'sequence': 0.006993006993006993, 'rewards': 0.006993006993006993, 'rt': 0.006993006993006993, 'received': 0.0034965034965034965, 'execute': 0.013986013986013986, 'continue': 0.0034965034965034965, 'subsequent': 0.0034965034965034965, 'according': 0.01048951048951049, 'E': 0.006993006993006993, '+': 0.027972027972027972, 'γrt+1': 0.0034965034965034965, 'γ2rt+2': 0.0034965034965034965, '·': 0.02097902097902098, '|st': 0.0034965034965034965, 'number': 0.027972027972027972, 'steps': 0.013986013986013986, 'return': 0.0034965034965034965, 'partition': 0.0034965034965034965, 'sum': 0.006993006993006993, 'two': 0.03146853146853147, 'terms': 0.0034965034965034965, 'γurt+u': 0.006993006993006993, 'st': 0.0034965034965034965, '−1': 0.01048951048951049, 'u=0': 0.0034965034965034965, '∞': 0.01048951048951049, 'u=N': 0.0034965034965034965, 'cid:12': 0.017482517482517484, 'term': 0.01048951048951049, 'terminates—V': 0.0034965034965034965, 'second': 0.01048951048951049, 'cost': 0.006993006993006993, 'ﬁnishing': 0.0034965034965034965, 'time': 0.01048951048951049, 'initiated': 0.0034965034965034965, 'call': 0.0034965034965034965, 'completion': 0.013986013986013986, 'denote': 0.0034965034965034965, 'C': 0.04895104895104895, 'write': 0.0034965034965034965, 'Bellman': 0.0034965034965034965, 'equation': 0.0034965034965034965, 'γN': 0.006993006993006993, 'max': 0.013986013986013986, 'j′': 0.006993006993006993, 'terminate': 0.006993006993006993, 'recursion': 0.0034965034965034965, 'expected': 0.0034965034965034965, 'simple': 0.013986013986013986, 'variation': 0.0034965034965034965, 'choose': 0.0034965034965034965, 'returns': 0.0034965034965034965, 'observe': 0.0034965034965034965, 'elapsed': 0.0034965034965034965, 'update': 0.0034965034965034965, '−': 0.0034965034965034965, 'αt': 0.013986013986013986, 'a′': 0.01048951048951049, 'require': 0.0034965034965034965, 'exploration': 0.01048951048951049, 'ordered': 0.013986013986013986, 'GLIE': 0.013986013986013986, 'An': 0.0034965034965034965, 'breaks': 0.0034965034965034965, 'Q-value': 0.0034965034965034965, 'ties': 0.006993006993006993, 'preferring': 0.0034965034965034965, 'comes': 0.0034965034965034965, 'ﬁxed': 0.0034965034965034965, 'ordering': 0.006993006993006993, 'executes': 0.0034965034965034965, 'inﬁnitely': 0.006993006993006993, 'often': 0.006993006993006993, 'every': 0.0034965034965034965, 'visited': 0.0034965034965034965, 'b': 0.01048951048951049, 'greedy': 0.0034965034965034965, 'condition': 0.017482517482517484, 'required': 0.01048951048951049, 'ensure': 0.0034965034965034965, 'unique': 0.01048951048951049, 'potentially': 0.0034965034965034965, 'values': 0.04195804195804196, 'depending': 0.006993006993006993, 'broken': 0.0034965034965034965, 'subsubtasks': 0.0034965034965034965, 'Theorem': 0.01048951048951049, 'hS': 0.0034965034965034965, 'Ri': 0.0034965034965034965, 'either': 0.006993006993006993, 'episodic': 0.0034965034965034965, 'deterministic': 0.006993006993006993, 'proper': 0.0034965034965034965, 'inﬁnite': 0.0034965034965034965, 'horizon': 0.0034965034965034965, 'discount': 0.0034965034965034965, 'factor': 0.0034965034965034965, 'γ': 0.0034965034965034965, 'H': 0.01048951048951049, 'Mk': 0.0034965034965034965, '>': 0.0034965034965034965, 'constants': 0.0034965034965034965, 'lim': 0.006993006993006993, 'T': 0.013986013986013986, '→∞': 0.006993006993006993, 't=1': 0.006993006993006993, 'α2': 0.0034965034965034965, '<': 0.0034965034965034965, 'πx': 0.013986013986013986, 'assume': 0.006993006993006993, '|Vt': 0.0034965034965034965, '|': 0.006993006993006993, '|Ct': 0.0034965034965034965, 'bounded': 0.0034965034965034965, 'Then': 0.013986013986013986, 'consistent': 0.0034965034965034965, 'Proof': 0.006993006993006993, 'sketch': 0.006993006993006993, 'Proposition': 0.0034965034965034965, '4.5': 0.0034965034965034965, 'Bertsekas': 0.006993006993006993, 'Tsitsiklis': 0.006993006993006993, '10': 0.01048951048951049, 'follows': 0.0034965034965034965, 'stochastic': 0.006993006993006993, 'argument': 0.006993006993006993, 'due': 0.0034965034965034965, '11': 0.006993006993006993, 'generalized': 0.0034965034965034965, 'non-stationary': 0.0034965034965034965, 'noise': 0.0034965034965034965, 'There': 0.02097902097902098, 'key': 0.0034965034965034965, 'points': 0.0034965034965034965, 'Deﬁne': 0.0034965034965034965, 'Pt': 0.0034965034965034965, 'describes': 0.0034965034965034965, 'behavior': 0.0034965034965034965, 't.': 0.0034965034965034965, 'By': 0.006993006993006993, 'inductive': 0.0034965034965034965, 'w.p': 0.0034965034965034965, 'Second': 0.0034965034965034965, 'Y': 0.024475524475524476, 'Root': 0.017482517482517484, 'Get': 0.027972027972027972, 'Put': 0.03146853146853147, 't/source': 0.0034965034965034965, 't/destination': 0.0034965034965034965, 'Pickup': 0.013986013986013986, 'Navigate': 0.01048951048951049, 'Putdown': 0.013986013986013986, 'B': 0.006993006993006993, 'North': 0.01048951048951049, 'South': 0.01048951048951049, 'East': 0.01048951048951049, 'West': 0.01048951048951049, 'Figure': 0.013986013986013986, 'Left': 0.0034965034965034965, 'Taxi': 0.024475524475524476, 'Domain': 0.0034965034965034965, 'taxi': 0.06643356643356643, 'row': 0.0034965034965034965, 'column': 0.0034965034965034965, 'Right': 0.0034965034965034965, 'Task': 0.006993006993006993, 'Graph': 0.0034965034965034965, 'convert': 0.0034965034965034965, 'usual': 0.006993006993006993, 'weighted': 0.006993006993006993, 'norm': 0.006993006993006993, 'contraction': 0.006993006993006993, 'C.': 0.01048951048951049, 'straightforward': 0.0034965034965034965, 'completes': 0.0034965034965034965, 'What': 0.0034965034965034965, 'notable': 0.0034965034965034965, 'simultaneously—it': 0.0034965034965034965, 'wait': 0.0034965034965034965, 'converge': 0.006993006993006993, 'beginning': 0.0034965034965034965, 'parent': 0.0034965034965034965, 'gives': 0.006993006993006993, 'completely': 0.006993006993006993, 'online': 0.0034965034965034965, 'wide': 0.0034965034965034965, 'applicability': 0.0034965034965034965, 'Conditions': 0.0034965034965034965, 'Safe': 0.0034965034965034965, 'motivate': 0.0034965034965034965, 'shown': 0.006993006993006993, 'four': 0.017482517482517484, 'special': 0.0034965034965034965, 'locations': 0.013986013986013986, 'marked': 0.0034965034965034965, 'ed': 0.0034965034965034965, 'lue': 0.0034965034965034965, 'reen': 0.0034965034965034965, 'ellow': 0.0034965034965034965, 'episode': 0.006993006993006993, 'starts': 0.0034965034965034965, 'randomly-chosen': 0.0034965034965034965, 'square': 0.006993006993006993, 'passenger': 0.06993006993006994, 'chosen': 0.006993006993006993, 'randomly': 0.006993006993006993, 'wishes': 0.0034965034965034965, 'transported': 0.0034965034965034965, 'also': 0.0034965034965034965, 'go': 0.006993006993006993, '’': 0.024475524475524476, 'source': 0.01048951048951049, 'pick': 0.0034965034965034965, 'destination': 0.024475524475524476, 'put': 0.0034965034965034965, 'ends': 0.0034965034965034965, 'deposited': 0.0034965034965034965, 'six': 0.0034965034965034965, 'domain': 0.01048951048951049, 'navigation': 0.01048951048951049, 'additional': 0.0034965034965034965, '+20': 0.0034965034965034965, 'successfully': 0.0034965034965034965, 'delivering': 0.0034965034965034965, '−10': 0.0034965034965034965, 'attempts': 0.0034965034965034965, 'illegally': 0.0034965034965034965, 'If': 0.0034965034965034965, 'would': 0.0034965034965034965, 'cause': 0.0034965034965034965, 'hit': 0.0034965034965034965, 'wall': 0.0034965034965034965, 'no-op': 0.0034965034965034965, 'structure': 0.0034965034965034965, 'Fig': 0.0034965034965034965, 'main': 0.0034965034965034965, 'sub-tasks': 0.0034965034965034965, 'Deliver': 0.0034965034965034965, 'turn': 0.0034965034965034965, 'involves': 0.006993006993006993, 'navigating': 0.006993006993006993, ';': 0.01048951048951049, 'bound': 0.0034965034965034965, 'desired': 0.0034965034965034965, 'target': 0.017482517482517484, 'illustrates': 0.0034965034965034965, 'support': 0.0034965034965034965, 'obvious—for': 0.0034965034965034965, 'extended': 0.0034965034965034965, 'take': 0.0034965034965034965, 'numbers': 0.0034965034965034965, 'complete': 0.0034965034965034965, 'distance': 0.0034965034965034965, 'top': 0.0034965034965034965, 'level': 0.0034965034965034965, 'get': 0.0034965034965034965, 'deliver': 0.0034965034965034965, 'expressed': 0.0034965034965034965, 'simply': 0.0034965034965034965, 'perhaps': 0.0034965034965034965, 'less': 0.006993006993006993, 'obvious': 0.0034965034965034965, 'Consider': 0.013986013986013986, 'While': 0.0034965034965034965, 'irrelevant—it': 0.0034965034965034965, 'aﬀect': 0.0034965034965034965, 'nagivation': 0.0034965034965034965, 'pickup': 0.0034965034965034965, 'decisions': 0.0034965034965034965, 'Perhaps': 0.0034965034965034965, 'importantly': 0.0034965034965034965, 'identity': 0.0034965034965034965, 'fact': 0.0034965034965034965, 'carrying': 0.0034965034965034965, 'irrelevant': 0.03146853146853147, 'represented': 0.02097902097902098, 'vector': 0.0034965034965034965, 'variables': 0.05244755244755245, 'combination': 0.0034965034965034965, 'identifying': 0.006993006993006993, 'subset': 0.0034965034965034965, 'relevant': 0.013986013986013986, 'deﬁning': 0.0034965034965034965, 'Such': 0.0034965034965034965, 'said': 0.0034965034965034965, 'involve': 0.006993006993006993, 'eliminating': 0.0034965034965034965, 'Condition': 0.017482517482517484, 'Subtask': 0.0034965034965034965, 'Irrelevance': 0.01048951048951049, 'partitioned': 0.0034965034965034965, 'sets': 0.0034965034965034965, 'stationary': 0.0034965034965034965, 'properties': 0.0034965034965034965, 'hold': 0.0034965034965034965, 'distribution': 0.01048951048951049, 'factored': 0.0034965034965034965, 'product': 0.0034965034965034965, 'distributions': 0.006993006993006993, 'x′': 0.027972027972027972, 'y′': 0.01048951048951049, '|x': 0.013986013986013986, 'x': 0.03496503496503497, 'give': 0.01048951048951049, 'pair': 0.006993006993006993, 's1': 0.013986013986013986, 'y1': 0.01048951048951049, 's2': 0.013986013986013986, 'y2': 0.01048951048951049, '|y': 0.0034965034965034965, 'subtask—only': 0.0034965034965034965, 'position': 0.013986013986013986, 'Leaf': 0.0034965034965034965, 'diﬀer': 0.006993006993006993, '1|s1': 0.006993006993006993, '2|s2': 0.006993006993006993, 'Xs′': 0.006993006993006993, 'constant': 0.0034965034965034965, 'next': 0.0034965034965034965, 'funnel': 0.006993006993006993, 'actions—macro': 0.0034965034965034965, 'large': 0.0034965034965034965, 'possible': 0.0034965034965034965, 'small': 0.0034965034965034965, 'proportional': 0.0034965034965034965, 'Result': 0.0034965034965034965, 'Distribution': 0.0034965034965034965, 'Undiscounted': 0.0034965034965034965, 'Yj': 0.006993006993006993, 'result': 0.013986013986013986, 'Mj': 0.01048951048951049, 'holds': 0.0034965034965034965, 'pairs': 0.0034965034965034965, '∀': 0.0034965034965034965, '|s1': 0.0034965034965034965, '|s2': 0.0034965034965034965, 'subroutine': 0.006993006993006993, 'Regardless': 0.0034965034965034965, 'ﬁnishes': 0.0034965034965034965, 'completed': 0.0034965034965034965, 'picking': 0.0034965034965034965, 'initial': 0.0034965034965034965, 'Note': 0.0034965034965034965, 'true': 0.0034965034965034965, 'setting—': 0.0034965034965034965, 'discounting': 0.0034965034965034965, 'ﬁnish': 0.0034965034965034965, 'depends': 0.0034965034965034965, 'much': 0.013986013986013986, 'rarely': 0.0034965034965034965, 'useful': 0.0034965034965034965, 'cumulative': 0.0034965034965034965, 'Termination': 0.006993006993006993, 'property': 0.0034965034965034965, 'whenever': 0.0034965034965034965, 'causes': 0.0034965034965034965, 'particular': 0.0034965034965034965, 'kind': 0.0034965034965034965, 'action—it': 0.0034965034965034965, 'funnels': 0.0034965034965034965, 'terminal': 0.006993006993006993, 'holding': 0.0034965034965034965, 'succeed': 0.0034965034965034965, 'implies': 0.0034965034965034965, 'means': 0.006993006993006993, 'uniformly': 0.0034965034965034965, 'zero': 0.0034965034965034965, 'terminated': 0.01048951048951049, 'Shielding': 0.0034965034965034965, 'paths': 0.0034965034965034965, 'exists': 0.0034965034965034965, 'never': 0.0034965034965034965, 'arises': 0.0034965034965034965, 'represent': 0.006993006993006993, 'explicitly': 0.0034965034965034965, '!': 0.006993006993006993, 'applying': 0.0034965034965034965, '632': 0.0034965034965034965, '3,000': 0.0034965034965034965, 'ﬂat': 0.013986013986013986, 'requires': 0.0034965034965034965, '14,000': 0.0034965034965034965, 'Convergence': 0.006993006993006993, 'graph': 0.0034965034965034965, 'incorporates': 0.0034965034965034965, 'kinds': 0.0034965034965034965, 'π∗': 0.006993006993006993, 'H.': 0.0034965034965034965, 'arbitrary': 0.0034965034965034965, 'ignorx′': 0.0034965034965034965, 'ing': 0.0034965034965034965, 'Q-learning': 0.0034965034965034965, 'needs': 0.0034965034965034965, 'samples': 0.0034965034965034965, 'drawn': 0.0034965034965034965, 'part': 0.0034965034965034965, 'showing': 0.0034965034965034965, 'regardless': 0.0034965034965034965, 'whether': 0.0034965034965034965, 'hence': 0.0034965034965034965, 'correct': 0.0034965034965034965, 'expectations': 0.0034965034965034965, 'Analogous': 0.0034965034965034965, 'arguments': 0.0034965034965034965, 'leaf': 0.0034965034965034965, 'irrelevance': 0.0034965034965034965, 'shielding': 0.0034965034965034965, 'easy': 0.0034965034965034965, 'Experimental': 0.0034965034965034965, 'Results': 0.0034965034965034965, 'implemented': 0.0034965034965034965, 'noisy': 0.006993006993006993, 'version': 0.0034965034965034965, 'Kaelbling': 0.013986013986013986, 'HDG': 0.006993006993006993, 'Boltzmann': 0.006993006993006993, 'tasks': 0.006993006993006993, 'rates': 0.006993006993006993, 'cooling': 0.0034965034965034965, 'separately': 0.0034965034965034965, 'tuned': 0.0034965034965034965, 'optimize': 0.0034965034965034965, 'slower': 0.0034965034965034965, 'faster': 0.0034965034965034965, 'MAXQ+Abstraction': 0.006993006993006993, 'Flat': 0.006993006993006993, 'No': 0.006993006993006993, 'w': 0.006993006993006993, 'e': 0.02097902097902098, 'l': 0.006993006993006993, 'u': 0.013986013986013986, 'n': 0.006993006993006993, '-20': 0.0034965034965034965, '-40': 0.0034965034965034965, '-60': 0.0034965034965034965, '-80': 0.0034965034965034965, '-100': 0.0034965034965034965, '-120': 0.0034965034965034965, '-140': 0.0034965034965034965, '200': 0.0034965034965034965, '-200': 0.0034965034965034965, '-400': 0.0034965034965034965, '-600': 0.0034965034965034965, '-800': 0.0034965034965034965, '-1000': 0.0034965034965034965, 'Conclusion': 0.0034965034965034965, '20000': 0.0034965034965034965, '40000': 0.0034965034965034965, '60000': 0.0034965034965034965, '80000': 0.0034965034965034965, '100000': 0.0034965034965034965, '120000': 0.0034965034965034965, '140000': 0.0034965034965034965, '160000': 0.0034965034965034965, '200000': 0.0034965034965034965, '400000': 0.0034965034965034965, '1e+06': 0.0034965034965034965, '1.2e+06': 0.0034965034965034965, '1.4e+06': 0.0034965034965034965, '800000': 0.0034965034965034965, '600000': 0.0034965034965034965, 'Primitive': 0.006993006993006993, 'Actions': 0.006993006993006993, 'Comparison': 0.0034965034965034965, 'left': 0.0034965034965034965, 'right': 0.0034965034965034965, 'horizontal': 0.0034965034965034965, 'axis': 0.006993006993006993, 'vertical': 0.0034965034965034965, 'plots': 0.0034965034965034965, 'average': 0.0034965034965034965, '100': 0.0034965034965034965, 'runs': 0.0034965034965034965, 'understanding': 0.0034965034965034965, 'reasons': 0.0034965034965034965, 'fruitful': 0.0034965034965034965, 'eﬀorts': 0.0034965034965034965, 'weak': 0.0034965034965034965, 'notions': 0.0034965034965034965, 'aggregation': 0.0034965034965034965, 'suggests': 0.0034965034965034965, 'future': 0.0034965034965034965, 'research': 0.0034965034965034965, 'focus': 0.0034965034965034965, 'safe': 0.0034965034965034965, 'References': 0.0034965034965034965, 'D.': 0.01048951048951049, 'Precup': 0.006993006993006993, 'R.': 0.01048951048951049, 'S.': 0.024475524475524476, 'Sutton': 0.006993006993006993, 'Multi-time': 0.0034965034965034965, 'models': 0.0034965034965034965, 'planning': 0.006993006993006993, 'NIPS10': 0.0034965034965034965, 'MIT': 0.006993006993006993, 'Press': 0.006993006993006993, '1998': 0.02097902097902098, 'Singh': 0.013986013986013986, 'Between': 0.0034965034965034965, 'MDPs': 0.0034965034965034965, 'Semi-MDPs': 0.0034965034965034965, 'representing': 0.0034965034965034965, 'knowledge': 0.0034965034965034965, 'multiple': 0.0034965034965034965, 'scales': 0.0034965034965034965, 'tech': 0.01048951048951049, 'rep.': 0.01048951048951049, 'Univ': 0.006993006993006993, 'Mass.': 0.0034965034965034965, 'Dept': 0.01048951048951049, 'Comp': 0.01048951048951049, 'Inf': 0.0034965034965034965, 'Sci.': 0.01048951048951049, 'Amherst': 0.0034965034965034965, 'MA': 0.006993006993006993, 'Parr': 0.0034965034965034965, 'Russell': 0.0034965034965034965, 'hierarchies': 0.0034965034965034965, 'machines': 0.0034965034965034965, 'NIPS-10': 0.0034965034965034965, 'P.': 0.017482517482517484, 'Transfer': 0.0034965034965034965, 'composing': 0.0034965034965034965, 'solutions': 0.0034965034965034965, 'elemental': 0.0034965034965034965, 'sequential': 0.0034965034965034965, 'Machine': 0.0034965034965034965, 'vol': 0.006993006993006993, 'p.': 0.0034965034965034965, '323': 0.0034965034965034965, '1992': 0.0034965034965034965, 'L.': 0.01048951048951049, 'Preliminary': 0.0034965034965034965, 'Pro': 0.0034965034965034965, 'ceedings': 0.0034965034965034965, 'ICML-10': 0.0034965034965034965, 'pp': 0.01048951048951049, '167–173': 0.0034965034965034965, 'Morgan': 0.01048951048951049, 'Kaufmann': 0.01048951048951049, '1993': 0.006993006993006993, 'M.': 0.01048951048951049, 'Hauskrecht': 0.0034965034965034965, 'N.': 0.006993006993006993, 'Meuleau': 0.0034965034965034965, 'Boutilier': 0.0034965034965034965, 'T.': 0.013986013986013986, 'Dean': 0.0034965034965034965, 'solution': 0.0034965034965034965, 'processes': 0.0034965034965034965, 'macro-actions': 0.0034965034965034965, 'Brown': 0.0034965034965034965, 'Univ.': 0.0034965034965034965, 'Providence': 0.0034965034965034965, 'RI': 0.0034965034965034965, 'Feudal': 0.0034965034965034965, 'NIPS-5': 0.0034965034965034965, '271–278': 0.0034965034965034965, 'San': 0.0034965034965034965, 'Francisco': 0.0034965034965034965, 'CA': 0.0034965034965034965, 'ICML-15': 0.0034965034965034965, 'Jaakkola': 0.006993006993006993, 'Littman': 0.0034965034965034965, 'Szpesvari': 0.0034965034965034965, 'single-step': 0.0034965034965034965, 'on-policy': 0.0034965034965034965, 'reinforcement-learning': 0.0034965034965034965, 'Col.': 0.0034965034965034965, 'Boulder': 0.0034965034965034965, 'CO': 0.0034965034965034965, 'J.': 0.0034965034965034965, 'Neuro-Dynamic': 0.0034965034965034965, 'Programming': 0.0034965034965034965, 'Belmont': 0.0034965034965034965, 'Athena': 0.0034965034965034965, 'Scientiﬁc': 0.0034965034965034965, '1996': 0.0034965034965034965, 'I.': 0.0034965034965034965, 'Jordan': 0.0034965034965034965, 'On': 0.0034965034965034965, 'iterative': 0.0034965034965034965, 'dynamic': 0.0034965034965034965, 'programming': 0.0034965034965034965, 'Neur': 0.0034965034965034965, 'Comp.': 0.0034965034965034965, '1185–1201': 0.0034965034965034965, '1994': 0.0034965034965034965}\n"
     ]
    }
   ],
   "source": [
    "mostFreqy = max(wordFreqs.values())\n",
    "\n",
    "for word in wordFreqs.keys():\n",
    "    wordFreqs[word] = (wordFreqs[word]/mostFreqy)\n",
    "    \n",
    "print(wordFreqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentance Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "\n",
    "for sentence in sentence_tokens:\n",
    "    for word in nltk.word_tokenize(sentence.lower()):\n",
    "        if word in wordFreqs.keys():\n",
    "            if len(sentence.split(' ')) <30:\n",
    "                if sentence not in sentence_scores.keys():\n",
    "                    sentence_scores[sentence] = wordFreqs[word]\n",
    "                else:\n",
    "                    sentence_scores[sentence] += wordFreqs[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(This can be reﬁned using a local reward function to express preferences among the diﬀerent states satisfying Ti [8], but we omit this reﬁnement in this paper.)': 2.856643356643357,\n",
       " ', Mk}.': 1.6223776223776225,\n",
       " ', πn}, one for each subtask.': 2.7377622377622375,\n",
       " '.': 3.6503496503496504,\n",
       " '1) in which there are two main sub-tasks: Get the passenger (Get) and Deliver the passenger (Put).': 2.9615384615384617,\n",
       " '1) to the probability transition function of the recursively optimal policy for j.': 1.5244755244755246,\n",
       " '1185–1201, 1994.': 1.6153846153846154,\n",
       " '167–173, Morgan Kaufmann, 1993.': 2.6188811188811187,\n",
       " '2 The MAXQ Framework  Let M be a Markov decision problem with states S, actions A, reward function R(s′|s, a) and probability transition function P (s′|s, a).': 6.657342657342657,\n",
       " '271–278,  San Francisco, CA: Morgan Kaufmann, 1993.': 3.7097902097902096,\n",
       " '3 Conditions for Safe State Abstraction  To motivate state abstraction, consider the simple Taxi Task shown in Figure 1.': 2.458041958041958,\n",
       " '6, no.': 1.6223776223776225,\n",
       " '6, pp.': 1.632867132867133,\n",
       " '8, p. 323, 1992.': 2.6328671328671325,\n",
       " '9 9 9 1   y a M 1 2     ]     G L .': 0.9720279720279721,\n",
       " 'A hierarchical policy is a set of policies π = {π0, .': 1.937062937062937,\n",
       " 'A hierarchical policy is executed using standard procedure-call-and-return semantics, starting with the root task M0 and unfolding recursively until primitive actions are executed.': 2.052447552447552,\n",
       " 'A hierarchical policy is recursively optimal if each policy πi is optimal given the policies of its descendants in the DAG.': 0.9965034965034965,\n",
       " 'An ordered policy is a policy that breaks Q-value ties among actions by preferring the action that comes ﬁrst in some ﬁxed ordering.': 1.034965034965035,\n",
       " 'Analogous arguments apply for leaf irrelevance and V (a, x).': 2.472027972027972,\n",
       " 'Belmont, MA:  Athena Scientiﬁc, 1996.': 2.7027972027972025,\n",
       " 'Category: Reinforcement Learning and Control Preference: Oral  1  Introduction  Most work on hierarchical reinforcement learning has focused on temporal abstraction.': 1.2307692307692308,\n",
       " 'Closely related is the HAM framework, in which the programmer constructs a hierarchy of ﬁnitestate controllers [3].': 1.84965034965035,\n",
       " 'Col., Dept.': 1.6083916083916083,\n",
       " 'Comp.': 1.8251748251748254,\n",
       " 'Comp., vol.': 1.6153846153846154,\n",
       " 'Condition 1: Subtask Irrelevance.': 0.8881118881118881,\n",
       " 'Condition 2: Leaf Irrelevance.': 0.7657342657342658,\n",
       " 'Condition 3: Result Distribution Irrelevance (Undiscounted case.)': 1.5874125874125873,\n",
       " 'Condition 4: Termination.': 0.7552447552447553,\n",
       " 'Condition 5: Shielding.': 0.7447552447552448,\n",
       " 'Consider the Get subtask.': 0.7097902097902098,\n",
       " 'Consider, for example, the Get subroutine under an optimal policy for the taxi task.': 2.8986013986013983,\n",
       " 'Each action is deterministic.': 0.6958041958041958,\n",
       " 'Each controller can include non-deterministic states (where the programmer was not sure what action to perform).': 1.5839160839160837,\n",
       " 'Even a brief consideration of human-level intelligence shows that such methods cannot scale.': 0.6398601398601399,\n",
       " 'Figure 2 shows the performance of ﬂat Q and MAXQ-Q with and without state abstractions on these tasks.': 0.9685314685314685,\n",
       " 'For example, in the Options framework [1, 2], the programmer deﬁnes a set of macro actions (“options”) and provides a policy for each.': 5.0629370629370625,\n",
       " 'For example, in the Taxi task, in all states where the taxi is holding the passenger, the Put subroutine will succeed and result in a terminal state for Root.': 4.199300699300699,\n",
       " 'Hence this form of state abstraction is rarely useful for cumulative discounted reward.)': 1.3601398601398602,\n",
       " 'Hence, the taxi’s initial position is irrelevant to its resulting position.': 1.7867132867132867,\n",
       " 'Hence, they could not provide any formal results for the convergence or performance of their method.': 1.7062937062937062,\n",
       " 'However, little is known about learning with state abstractions, in which aspects of the state space are ignored.': 3.1993006993006996,\n",
       " 'If a navigation action would cause the taxi to hit a wall, the action is a no-op, and there is only the usual reward of −1.': 2.9125874125874125,\n",
       " 'In both of these approaches—and in other studies of hierarchical RL (e.g., [4, 5, 6])—each option or ﬁnite state controller must have access to the entire state space.': 5.1678321678321675,\n",
       " 'In each episode, the taxi starts in a randomly-chosen square.': 1.6958041958041963,\n",
       " 'In our previous work with MAXQ, we brieﬂy discussed state abstractions, and we employed them in our experiments.': 2.909090909090909,\n",
       " 'In previous work, we developed the MAXQ method for hierarchical RL.': 1.7062937062937062,\n",
       " 'In this paper, we deﬁne ﬁve conditions under which state abstraction can be combined with the MAXQ value function decomposition.': 2.1048951048951055,\n",
       " 'Inf.': 0.6083916083916084,\n",
       " 'Learning algorithms (such as semi-Markov Q learning) can then treat these temporally abstract actions as if they were primitives and learn a policy for selecting among them.': 1.8356643356643358,\n",
       " 'Learning rates and Boltzmann cooling rates were separately tuned to optimize the performance of each method.': 0.7622377622377623,\n",
       " 'Let H be a DAG deﬁned over subtasks {M0, .': 1.6818181818181817,\n",
       " 'Let Mi be a subtask of MDP M .': 0.7097902097902098,\n",
       " 'Let Mj be a child task of Mi with the property that whenever Mj terminates, it causes Mi to terminate too.': 1.716783216783217,\n",
       " 'Let {M0, .': 1.632867132867133,\n",
       " 'Let πx be an ordered GLIE exploration policy that is abstract.': 0.7902097902097902,\n",
       " 'Mass., Dept.': 1.6083916083916083,\n",
       " 'Our results apply in both the ﬁnite-horizon undiscounted case and the inﬁnite-horizon discounted case.': 0.6958041958041958,\n",
       " 'P  4 Experimental Results  We implemented MAXQ-Q for a noisy version of the Taxi domain and for Kaelbling’s HDG navigation task [5] using Boltzmann exploration.': 1.062937062937063,\n",
       " 'References  [1] D. Precup and R. S. Sutton, “Multi-time models for temporally abstract planning,”  in NIPS10, The MIT Press, 1998.': 5.048951048951048,\n",
       " 'Right: Task Graph.': 0.7552447552447553,\n",
       " 'Sci., Amherst, MA, 1998.': 3.629370629370629,\n",
       " 'Sci., Boulder, CO, 1998.': 3.629370629370629,\n",
       " 'Sci., Providence, RI, 1998.': 3.629370629370629,\n",
       " 'Similarly, let Q(i, s, j) be the Q value for subtask i of executing child action j in state s and then executing the current policy until termination.': 5.370629370629372,\n",
       " 'Some argue that this can be solved by clever value function approximation methods—and there is some merit in this view.': 0.7657342657342658,\n",
       " 'Such value functions and policies are said to be abstract.': 0.7237762237762239,\n",
       " 'The HAMQ learning algorithm can then be applied to learn a policy for making choices in the non-deterministic states.': 0.9265734265734267,\n",
       " 'The completion function of such subtasks can be represented using a number of values proportional to the number of resulting states.': 0.951048951048951,\n",
       " 'The episode ends when the passenger is deposited at the destination location.': 0.7587412587412588,\n",
       " 'The fact that in some cases the taxi is carrying the passenger and in other cases it is not is irrelevant.': 0.8111888111888113,\n",
       " 'The horizontal axis gives the number of primitive actions executed by each method.': 0.8006993006993007,\n",
       " 'The need for state abstraction is perhaps less obvious.': 0.9335664335664335,\n",
       " 'The next two conditions involve “funnel” actions—macro actions that move the environment from some large number of possible states to a small number of resulting states.': 1.0874125874125875,\n",
       " 'The one exception to this—the Feudal-Q method of Dayan and Hinton [7]— introduced state abstractions in an unsafe way, such that the resulting learning problem was only partially observable.': 2.251748251748252,\n",
       " 'The ordering condition is required to ensure that the recursively optimal policy is unique.': 0.8216783216783218,\n",
       " 'The result is that, when combined with the Termination condition above, we do not need to explicitly represent the completion function for Put at all!': 2.1888111888111887,\n",
       " 'The results show that state abstraction is very important, and in most cases essential, to the eﬀective application of MAXQ-Q learning.': 3.0594405594405596,\n",
       " 'The results show that without state abstractions, MAXQ-Q learning is slower to converge than ﬂat Q learning, but that with state abstraction, it is much faster.': 4.430069930069931,\n",
       " 'The second term is the cost of ﬁnishing subtask i after j is executed (discounted to the time when j is initiated).': 1.902097902097902,\n",
       " 'The subtasks of M must form a DAG with a single “root” node—no subtask may invoke itself directly or indirectly.': 0.8986013986013985,\n",
       " 'The taxi must go to the passenger’s location (the “source”), pick up the passenger, go to the destination location (the “destination”), and put down the passenger there.': 5.905594405594406,\n",
       " 'The temporal abstraction is obvious—for example, Get is a temporally extended action that can take diﬀerent numbers of steps to complete depending on the distance to the target.': 1.884615384615385,\n",
       " 'The termination and shielding cases are easy.': 0.6468531468531469,\n",
       " 'The top level policy (get passenger; deliver passenger) can be expressed very simply with these abstractions.': 1.7342657342657346,\n",
       " 'The vertical axis plots the average of 100 separate runs.': 0.6398601398601399,\n",
       " 'The “goal” of subtask Mi is to move the environment into a state such that Ti is satisﬁed.': 1.0524475524475525,\n",
       " 'The ﬁrst two conditions involve eliminating irrelevant variables within a subtask of the MAXQ decomposition.': 0.8951048951048952,\n",
       " 'Then the completion cost C(i, s, j) = 0 and does not need to be represented.': 3.7447552447552446,\n",
       " 'Then with probability 1, algorithm MAXQ-Q converges to the unique recursively optimal policy for M consistent with H and πx.': 1.9510489510489508,\n",
       " 'Theorem 2 (Convergence with State Abstraction) Let H be a MAXQ task graph that incorporates the ﬁve kinds of state abstractions deﬁned above.': 2.1153846153846154,\n",
       " 'There are four special locations in this world, marked as R(ed), B(lue), G(reen), and Y(ellow).': 7.884615384615383,\n",
       " 'There are two key points in the proof.': 0.6678321678321679,\n",
       " 'There is a passenger at one of the four locations (chosen randomly), and that passenger wishes to be transported to one of the four locations (also chosen randomly).': 3.486013986013986,\n",
       " 'There is a reward of −1 for each action and an additional reward of +20 for successfully delivering the passenger.': 0.8461538461538463,\n",
       " 'There is a reward of −10 if the taxi attempts to execute the Putdown or Pickup actions illegally.': 0.7937062937062938,\n",
       " 'This gives a completely online learning algorithm with wide applicability.': 0.7447552447552448,\n",
       " 'This is a particular kind of funnel action—it funnels all states into terminal states for Mi.': 0.7552447552447553,\n",
       " 'This is because the termination predicate for Put (i.e., that the passenger is at his or her destination location) implies the termination condition for Root (which is the same).': 3.43006993006993,\n",
       " 'This means that C(Root, s, Put) is uniformly zero, for all states s where Put is not terminated.': 4.5174825174825175,\n",
       " 'This means that we do not need to represent C(Root, s, Put) in these states.': 3.5314685314685317,\n",
       " 'This paper has shown that by understanding the reasons that state variables are irrelevant, we can obtain a simple proof of the convergence of MAXQ-Q learning under state abstraction.': 2.3671328671328675,\n",
       " 'This paper solves these problems and in addition compares the eﬀectiveness of MAXQ-Q learning with and without state abstractions.': 1.0104895104895104,\n",
       " 'This permits us to obtain the ﬁrst proof of the convergence of hierarchical RL to an optimal policy in the presence of state abstraction.': 1.1503496503496504,\n",
       " 'This task has a hierarchical structure (see Fig.': 1.111888111888112,\n",
       " 'This task illustrates the need to support both temporal abstraction and state abstraction.': 1.062937062937063,\n",
       " 'To learn the values of C(i, x, j) = Q-learning algorithm needs samples of x′ and N drawn according to P (x′, N |x, j).': 6.818181818181818,\n",
       " 'To prove convergence, we require that the exploration policy executed during learning be an ordered GLIE policy.': 1.9965034965034962,\n",
       " 'We call this second term the completion function, and denote it C(i, s, j).': 4.685314685314685,\n",
       " 'We introduce these state abstractions within the MAXQ framework [8], but the basic ideas are general.': 2.097902097902098,\n",
       " 'We now introduce the ﬁve conditions for state abstraction.': 0.9440559440559442,\n",
       " 'We prove that the MAXQ-Q learning algorithm converges under these conditions and show experimentally that state abstraction is important for the successful application of MAXQ-Q learning.': 1.1958041958041958,\n",
       " 'We will assume that the state s of the MDP is represented as a vector of state variables.': 1.1188811188811187,\n",
       " 'When deciding how to walk from the bedroom to the kitchen, we do not  \\x0cneed to think about the location of our car.': 1.6993006993006992,\n",
       " 'While this subtask is being solved, the destination of the passenger is completely irrelevant—it cannot aﬀect any of the nagivation or pickup decisions.': 1.8251748251748254,\n",
       " 'Without state abstractions, MAXQ requires 14,000 values!': 1.3356643356643356,\n",
       " 'Without state abstractions, any RL method that learns value functions must learn a separate value for each state of the world.': 2.3041958041958046,\n",
       " 'Without this condition, there are potentially many diﬀerent recursively optimal policies with diﬀerent values, depending on how ties are broken within subtasks, subsubtasks, and so on.': 4.867132867132867,\n",
       " '[10] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-Dynamic Programming.': 1.8006993006993008,\n",
       " '[11] T. Jaakkola, M. I. Jordan, and S. P. Singh, “On the convergence of stochastic iterative dynamic programming algorithms,” Neur.': 4.965034965034964,\n",
       " '[2] R. S. Sutton, D. Precup, and S. Singh, “Between MDPs and Semi-MDPs: Learning, planning, and representing knowledge at multiple temporal scales,” tech.': 7.185314685314684,\n",
       " '[3] R. Parr and S. Russell, “Reinforcement learning with hierarchies of machines,” in  NIPS-10, The MIT Press, 1998.': 5.066433566433567,\n",
       " '[4] S. P. Singh, “Transfer of learning by composing solutions of elemental sequential  tasks,” Machine Learning, vol.': 4.132867132867133,\n",
       " '[5] L. P. Kaelbling, “Hierarchical reinforcement learning: Preliminary results,” in Pro ceedings ICML-10, pp.': 4.2027972027972025,\n",
       " '[6] M. Hauskrecht, N. Meuleau, C. Boutilier, L. Kaelbling, and T. Dean, “Hierarchical solution of Markov decision processes using macro-actions,” tech.': 7.013986013986013,\n",
       " '[7] P. Dayan and G. Hinton, “Feudal reinforcement learning,” in NIPS-5, pp.': 4.024475524475525,\n",
       " '[8] T. G. Dietterich, “The MAXQ method for hierarchical reinforcement learning,” in  ICML-15, Morgan Kaufmann, 1998.': 5.115384615384615,\n",
       " '[9] S. Singh, T. Jaakkola, M. L. Littman, and C. Szpesvari, “Convergence results for single-step on-policy reinforcement-learning algorithms,” tech.': 6.003496503496503,\n",
       " 'rep., Brown Univ., Dept.': 2.6188811188811187,\n",
       " 'rep., Univ.': 3.2377622377622375,\n",
       " 'we show how to convert the usual weighted max norm contraction for Q into a weighted max norm contraction for C. This is straightforward, and completes the proof.': 1.737762237762238}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryML = heapq.nlargest(10, sentence_scores, key = sentence_scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There are four special locations in this world, marked as R(ed), B(lue), G(reen), and Y(ellow).',\n",
       " '[2] R. S. Sutton, D. Precup, and S. Singh, “Between MDPs and Semi-MDPs: Learning, planning, and representing knowledge at multiple temporal scales,” tech.',\n",
       " '[6] M. Hauskrecht, N. Meuleau, C. Boutilier, L. Kaelbling, and T. Dean, “Hierarchical solution of Markov decision processes using macro-actions,” tech.',\n",
       " 'To learn the values of C(i, x, j) = Q-learning algorithm needs samples of x′ and N drawn according to P (x′, N |x, j).',\n",
       " '2 The MAXQ Framework  Let M be a Markov decision problem with states S, actions A, reward function R(s′|s, a) and probability transition function P (s′|s, a).',\n",
       " '[9] S. Singh, T. Jaakkola, M. L. Littman, and C. Szpesvari, “Convergence results for single-step on-policy reinforcement-learning algorithms,” tech.',\n",
       " 'The taxi must go to the passenger’s location (the “source”), pick up the passenger, go to the destination location (the “destination”), and put down the passenger there.',\n",
       " 'Similarly, let Q(i, s, j) be the Q value for subtask i of executing child action j in state s and then executing the current policy until termination.',\n",
       " 'In both of these approaches—and in other studies of hierarchical RL (e.g., [4, 5, 6])—each option or ﬁnite state controller must have access to the entire state space.',\n",
       " '[8] T. G. Dietterich, “The MAXQ method for hierarchical reinforcement learning,” in  ICML-15, Morgan Kaufmann, 1998.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
